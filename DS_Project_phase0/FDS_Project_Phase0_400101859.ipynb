{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=10>\n",
        "    Data Science <br>\n",
        "<font color=0F5298 size=5>\n",
        "    Electrical Engineering Department <br>\n",
        "    Fall 2024 <br>\n",
        "    Parham Gilani - 400101859 <br>\n",
        "    Project Phase 0<br>\n",
        "    \n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crawling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD7RQ3gd8Cjf",
        "outputId": "b0a6320c-3fbb-4a0e-ab07-209f1163514c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping data for topic: Foundation Models\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to Foundation_Models_data.json\n",
            "Completed scraping for topic: Foundation Models\n",
            "Scraping data for topic: Generative Models\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 900/1000\n",
            "Data saved to Generative_Models_data.json\n",
            "Completed scraping for topic: Generative Models\n",
            "Scraping data for topic: LLM\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to LLM_data.json\n",
            "Completed scraping for topic: LLM\n",
            "Scraping data for topic: VLM\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Error: Received 400. Message: {'error': 'Requested data for this limit and/or offset is not available'}.\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to VLM_data.json\n",
            "Completed scraping for topic: VLM\n",
            "Scraping data for topic: Diffusion Models\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to Diffusion_Models_data.json\n",
            "Completed scraping for topic: Diffusion Models\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Fetch papers from the Semantic Scholar API\n",
        "def scrape_semantic_scholar_api(topic, year_range, limit=1000):\n",
        "    api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "    collected_papers = []\n",
        "    current_offset = 0\n",
        "\n",
        "    while len(collected_papers) < limit:\n",
        "        if current_offset % 300 == 0:\n",
        "            print(f\"Scraped: {current_offset}/{limit}\")\n",
        "\n",
        "        # Request parameters\n",
        "        query_params = {\n",
        "            \"query\": topic,\n",
        "            \"fields\": \"title,abstract,authors,citationCount\",\n",
        "            \"offset\": current_offset,\n",
        "            \"limit\": 100,\n",
        "            \"year\": year_range,\n",
        "            \"sort\": \"relevance\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(api_url, params=query_params)\n",
        "\n",
        "            if response.status_code == 429:\n",
        "                print(\"Hit rate limit. Pausing for 10 seconds...\")\n",
        "                time.sleep(10)\n",
        "                continue\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error: Received {response.status_code}. Message: {response.json()}.\")\n",
        "                break\n",
        "\n",
        "            response_data = response.json()\n",
        "            paper_list = response_data.get(\"data\", [])\n",
        "\n",
        "            if not paper_list:\n",
        "                print(\"No additional papers found.\")\n",
        "                break\n",
        "\n",
        "            for paper in paper_list:\n",
        "                try:\n",
        "                    collected_papers.append({\n",
        "                        'title': paper.get(\"title\", \"No Title\"),\n",
        "                        'abstract': paper.get(\"abstract\", \"No Abstract\"),\n",
        "                        'authors': \", \".join([author.get(\"name\", \"Unknown\") for author in paper.get(\"authors\", [])]),\n",
        "                        'citations': paper.get(\"citationCount\", 0)\n",
        "                    })\n",
        "\n",
        "                    if len(collected_papers) >= limit:\n",
        "                        break\n",
        "\n",
        "                except Exception as inner_error:\n",
        "                    print(f\"Error processing paper data: {inner_error}\")\n",
        "\n",
        "            current_offset += 100\n",
        "            time.sleep(5)\n",
        "\n",
        "        except Exception as outer_error:\n",
        "            print(f\"Network or request error: {outer_error}. Retrying in 10 seconds...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "    return collected_papers\n",
        "\n",
        "\n",
        "# Save the scraped data to a JSON file for each topic\n",
        "def save_to_json_file(topic, all_year_data):\n",
        "    if not all_year_data:\n",
        "        print(f\"No data to save for topic: {topic}\")\n",
        "        return\n",
        "\n",
        "    # Create a unique filename for the topic\n",
        "    file_name = f\"{topic.replace(' ', '_')}_data.json\"\n",
        "\n",
        "    # Save the data to the JSON file\n",
        "    with open(file_name, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_year_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Data saved to {file_name}\")\n",
        "\n",
        "\n",
        "topics = [\"Foundation Models\", \"Generative Models\", \"LLM\", \"VLM\", \"Diffusion Models\"]\n",
        "year_ranges = [\"2017-2023\", \"2024-2025\"]\n",
        "\n",
        "for topic in topics:\n",
        "    print(f\"Scraping data for topic: {topic}\")\n",
        "\n",
        "    all_year_data = []\n",
        "\n",
        "    # Scrape and collect data for each year range\n",
        "    for year_range in year_ranges:\n",
        "        data = scrape_semantic_scholar_api(topic, year_range, limit=1000)\n",
        "        all_year_data.append({\n",
        "            'papers': data\n",
        "        })\n",
        "\n",
        "    # Save all data for the topic in one file\n",
        "    save_to_json_file(topic, all_year_data)\n",
        "\n",
        "    print(f\"Completed scraping for topic: {topic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script fetches academic papers from the Semantic Scholar API based on specified topics and year ranges. It defines functions to send API requests with parameters like title, abstract, authors, and citation count while managing pagination. For each topic, papers are collected in a list, and if the rate limit is reached, the script pauses before retrying. The script ensures that the data collection stops once the specified limit is reached. Error handling is implemented to retry requests in case of failures, such as network issues or invalid responses. It checks for missing or empty data and prints appropriate messages. Once the papers are gathered, they are saved to a JSON file for each topic, named after the topic with `_data.json`. This includes data for all year ranges provided in the script. The process repeats for multiple topics such as \"Foundation Models,\" \"Generative Models,\" etc. The script handles multiple topics and year ranges sequentially. Finally, the data is stored, ensuring that each topic's papers are grouped and saved correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cleaning the dataset and merge:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmQFqzqjrqn2",
        "outputId": "7d901e65-5ec1-4bbf-9b9c-b71598e210c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to Foundation_Models_data.json\n",
            "Data saved to Generative_Models_data.json\n",
            "Data saved to LLM_data.json\n",
            "Data saved to VLM_data.json\n",
            "Data saved to Diffusion_Models_data.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Load the existing file and get the data\n",
        "def load_data_from_file(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            try:\n",
        "                return json.load(file)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error loading data from {file_path}. File may be empty or corrupted.\")\n",
        "                return []\n",
        "    else:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "        return []\n",
        "\n",
        "# Save the merged data back to the file\n",
        "def save_data_to_file(file_path, merged_data):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(merged_data, file, ensure_ascii=False, indent=4)\n",
        "    print(f\"Data saved to {file_path}\")\n",
        "\n",
        "# Merge the \"papers\" parts and save them to the same file\n",
        "def merge_papers_in_file(file_path):\n",
        "    # Load the data from the file\n",
        "    data = load_data_from_file(file_path)\n",
        "\n",
        "    # Check if the file contains more than one \"papers\" part (assuming there are separate parts)\n",
        "    if len(data) == 2:\n",
        "        # Merge the papers from both parts into one list\n",
        "        merged_papers = data[0].get('papers', []) + data[1].get('papers', [])\n",
        "\n",
        "        # Save the merged data back into the same file\n",
        "        merged_data = [{\n",
        "            'papers': merged_papers\n",
        "        }]\n",
        "\n",
        "        save_data_to_file(file_path, merged_data)\n",
        "    else:\n",
        "        print(f\"Data in {file_path} does not contain two parts to merge.\")\n",
        "\n",
        "# List of file names\n",
        "file_names = [\n",
        "    \"Foundation_Models_data.json\",\n",
        "    \"Generative_Models_data.json\",\n",
        "    \"LLM_data.json\",\n",
        "    \"VLM_data.json\",\n",
        "    \"Diffusion_Models_data.json\"\n",
        "]\n",
        "\n",
        "# Merge the papers parts in each file\n",
        "for file_name in file_names:\n",
        "    merge_papers_in_file(file_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upMpa2hXsfRY",
        "outputId": "fbf764d3-d2f9-4488-dd75-2a1354ef1ebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First paper from Foundation_Models_data.json:\n",
            "Title: On the Opportunities and Risks of Foundation Models\n",
            "Abstract: AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.\n",
            "Authors: Rishi Bommasani, Drew A. Hudson, E. Adeli, R. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, E. Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, M. Doumbouya, Esin Durmus, Stefano Ermon, J. Etchemendy, Kawin Ethayarajh, L. Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, S. Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, G. Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, M. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, J. Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, E. Mitchell, Zanele Munyikwa, Suraj Nair, A. Narayan, D. Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, H. Nilforoshan, J. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Papadimitriou, J. Park, C. Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, K. Srinivasan, Alex Tamkin, Rohan Taori, A. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, M. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang\n",
            "Citations: 3601\n",
            "----------------------------------------\n",
            "\n",
            "First paper from Generative_Models_data.json:\n",
            "Title: Elucidating the Design Space of Diffusion-Based Generative Models\n",
            "Abstract: We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.\n",
            "Authors: Tero Karras, M. Aittala, Timo Aila, S. Laine\n",
            "Citations: 1410\n",
            "----------------------------------------\n",
            "\n",
            "First paper from LLM_data.json:\n",
            "Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\n",
            "Abstract: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\n",
            "Authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, E. Xing, Haotong Zhang, Joseph E. Gonzalez, Ion Stoica\n",
            "Citations: 2755\n",
            "----------------------------------------\n",
            "\n",
            "First paper from VLM_data.json:\n",
            "Title: F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models\n",
            "Abstract: We present F-VLM, a simple open-vocabulary object detection method built upon Frozen Vision and Language Models. F-VLM simplifies the current multi-stage training pipeline by eliminating the need for knowledge distillation or detection-tailored pretraining. Surprisingly, we observe that a frozen VLM: 1) retains the locality-sensitive features necessary for detection, and 2) is a strong region classifier. We finetune only the detector head and combine the detector and VLM outputs for each region at inference time. F-VLM shows compelling scaling behavior and achieves +6.5 mask AP improvement over the previous state of the art on novel categories of LVIS open-vocabulary detection benchmark. In addition, we demonstrate very competitive results on COCO open-vocabulary detection benchmark and cross-dataset transfer detection, in addition to significant training speed-up and compute savings. Code will be released at the https://sites.google.com/view/f-vlm/home\n",
            "Authors: Weicheng Kuo, Yin Cui, Xiuye Gu, A. Piergiovanni, A. Angelova\n",
            "Citations: 109\n",
            "----------------------------------------\n",
            "\n",
            "First paper from Diffusion_Models_data.json:\n",
            "Title: High-Resolution Image Synthesis with Latent Diffusion Models\n",
            "Abstract: By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.\n",
            "Authors: Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, B. Ommer\n",
            "Citations: 11883\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Load the existing file and get the data\n",
        "def load_data_from_file(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            try:\n",
        "                return json.load(file)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error loading data from {file_path}. File may be empty or corrupted.\")\n",
        "                return []\n",
        "    else:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "        return []\n",
        "\n",
        "# Show the first item (paper) from the \"papers\" list in the file\n",
        "def show_first_paper_from_file(file_path):\n",
        "    data = load_data_from_file(file_path)\n",
        "\n",
        "    if data and 'papers' in data[0]:\n",
        "        papers = data[0]['papers']\n",
        "\n",
        "        if papers:\n",
        "            first_paper = papers[0]\n",
        "            print(f\"\\nFirst paper from {file_path}:\")\n",
        "            print(f\"Title: {first_paper.get('title', 'No Title')}\")\n",
        "            print(f\"Abstract: {first_paper.get('abstract', 'No Abstract')}\")\n",
        "            print(f\"Authors: {first_paper.get('authors', 'No Authors')}\")\n",
        "            print(f\"Citations: {first_paper.get('citations', 0)}\")\n",
        "            print(\"-\" * 40)\n",
        "        else:\n",
        "            print(f\"No papers found in {file_path}.\")\n",
        "    else:\n",
        "        print(f\"No 'papers' key found in {file_path} or the file is empty.\")\n",
        "\n",
        "# List of file names\n",
        "file_names = [\n",
        "    \"Foundation_Models_data.json\",\n",
        "    \"Generative_Models_data.json\",\n",
        "    \"LLM_data.json\",\n",
        "    \"VLM_data.json\",\n",
        "    \"Diffusion_Models_data.json\"\n",
        "]\n",
        "\n",
        "# Show the first paper from each file\n",
        "for file_name in file_names:\n",
        "    show_first_paper_from_file(file_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
