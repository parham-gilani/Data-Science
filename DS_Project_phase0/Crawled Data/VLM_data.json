[
    {
        "papers": [
            {
                "title": "F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models",
                "abstract": "We present F-VLM, a simple open-vocabulary object detection method built upon Frozen Vision and Language Models. F-VLM simplifies the current multi-stage training pipeline by eliminating the need for knowledge distillation or detection-tailored pretraining. Surprisingly, we observe that a frozen VLM: 1) retains the locality-sensitive features necessary for detection, and 2) is a strong region classifier. We finetune only the detector head and combine the detector and VLM outputs for each region at inference time. F-VLM shows compelling scaling behavior and achieves +6.5 mask AP improvement over the previous state of the art on novel categories of LVIS open-vocabulary detection benchmark. In addition, we demonstrate very competitive results on COCO open-vocabulary detection benchmark and cross-dataset transfer detection, in addition to significant training speed-up and compute savings. Code will be released at the https://sites.google.com/view/f-vlm/home",
                "authors": "Weicheng Kuo, Yin Cui, Xiuye Gu, A. Piergiovanni, A. Angelova",
                "citations": 109
            },
            {
                "title": "HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks",
                "abstract": "Human robot interaction is an exciting task, which aimed to guide robots following instructions from human. Since huge gap lies between human natural language and machine codes, end to end human robot interaction models is fair challenging. Further, visual information receiving from sensors of robot is also a hard language for robot to perceive. In this work, HuBo-VLM is proposed to tackle perception tasks associated with human robot interaction including object detection and visual grounding by a unified transformer based vision language model. Extensive experiments on the Talk2Car benchmark demonstrate the effectiveness of our approach. Code would be publicly available in https://github.com/dzcgaara/HuBo-VLM.",
                "authors": "Zichao Dong, Weikun Zhang, Xufeng Huang, Hang Ji, Xin Zhan, Junbo Chen",
                "citations": 3
            },
            {
                "title": "Prediction of Mean Sea Level with GNSS-VLM Correction Using a Hybrid Deep Learning Model in Australia",
                "abstract": "The prediction of sea level rise is extremely important for improved future climate change mitigation and adaptation strategies. This study uses a hybrid convolutional neural Network (CNN) and a bidirectional long short-term (BiLSTM) model with successive variational mode decomposition (SVMD) to predict the absolute sea level for two study sites in Australia (Port Kembla and Milner Bay). More importantly, the sea level measurements using a tide gauge were corrected using Global Navigation Satellite System (GNSS) measurements of the vertical land movement (VLM). The SVMD-CNN-BiLSTM model was benchmarked by a multi-layer perceptron (MLP), support vector regression (SVR) and gradient boosting (GB). The SVMD-CNN-BiLSTM model outperformed all the comparative models with high correlation values of more than 0.95 for Port Kembla and Milner Bay. Similarly, the SVMD-CNN-BiLSTM model achieved the highest values for the Willmott index, the Nash–Sutcliffe index and the Legates and McCabe index for both study sites. The projected linear trend showed the expected annual mean sea rise for 2030. Using the current trend, Port Kembla was projected to have an MSL value of 1.03 m with a rate rise of approx. 4.5 mm/year. The rate of the MSL for Milner Bay was comparatively lower with a value of approx. 2.75 mm/year and an expected MSL value of 1.27 m for the year 2030.",
                "authors": "N. Raj, Jason Brown",
                "citations": 4
            },
            {
                "title": "X 2 -VLM: All-In-One Pre-trained Model For Vision-Language Tasks",
                "abstract": "Vision language pre-training aims to learn alignments between vision and language from a large amount of data. We proposed multi-grained vision language pre-training, a uniﬁed approach which can learn vision language alignments in multiple granularity. This paper advances the proposed method by unifying image and video encoding in one model and scaling up the model with large-scale data. We present X 2 -VLM , a pre-trained VLM with a modular architecture for both image-text tasks and video-text tasks. Experiment results show that X 2 -VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of X 2 -VLM results in high transferability for X 2 -VLM to be utilized in any language or domain. For example, by simply replacing the text encoder with XLM-R, X 2 -VLM outperforms state-of-the-art multilingual multi-modal pre-trained models without any multilingual pre-training. The code and pre-trained models will be available at github.com/zengyan-97/X2-VLM .",
                "authors": "Yan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang, Jipeng Zhang, Hkust Wangchunshu Zhou, E. Zurich",
                "citations": 40
            },
            {
                "title": "Leveraging VLM-Based Pipelines to Annotate 3D Objects",
                "abstract": "Pretrained vision language models (VLMs) present an opportunity to caption unlabeled 3D objects at scale. The leading approach to summarize VLM descriptions from different views of an object (Luo et al., 2023) relies on a language model (GPT4) to produce the final output. This text-based aggregation is susceptible to hallucinations as it merges potentially contradictory descriptions. We propose an alternative algorithm to marginalize over factors such as the viewpoint that affect the VLM's response. Instead of merging text-only responses, we utilize the VLM's joint image-text likelihoods. We show our probabilistic aggregation is not only more reliable and efficient, but sets the SoTA on inferring object types with respect to human-verified labels. The aggregated annotations are also useful for conditional inference; they improve downstream predictions (e.g., of object material) when the object's type is specified as an auxiliary text-based input. Such auxiliary inputs allow ablating the contribution of visual reasoning over visionless reasoning in an unsupervised setting. With these supervised and unsupervised evaluations, we show how a VLM-based pipeline can be leveraged to produce reliable annotations for 764K objects from the Objaverse dataset.",
                "authors": "Rishabh Kabra, L. Matthey, Alexander Lerchner, N. Mitra",
                "citations": 1
            },
            {
                "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
                "abstract": "We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks. Existing pre-training are task-specific by adopting either a single cross-modal encoder that requires both modalities, limiting their use for retrieval-style end tasks or more complex multitask learning with two unimodal encoders, limiting early cross-modal fusion. We instead introduce new pretraining masking schemes that better mix across modalities (e.g. by forcing masks for text to predict the closest video embeddings) while also maintaining separability (e.g. unimodal predictions are sometimes required, without using all the input). Experimental results show strong performance across a wider range of tasks than any previous methods, often outperforming task-specific pre-training. Code is made available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.",
                "authors": "Hu Xu, Gargi Ghosh, Po-Yao (Bernie) Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, Luke Zettlemoyer",
                "citations": 118
            },
            {
                "title": "VLM-BCD: Unsupervised Building Change Detection",
                "abstract": "Building Change Detection (BCD) is one of the most important parts of remote sensing analysis. However, most of the existing BCD approaches require a large amount of pixel-level annotation, which limits their applicability due to intensive labour costs. To alleviate this issue, we propose a vision-language model-based framework, VLM-BCD, which performs BCD tasks without requiring any labels. Specifically, the proposed framework consists of two stages: 1) Bi-temporal building localisation by leveraging open-vocabulary DETR. 2) Unchanged mask suppressing by the Change Resolver module to detect the building change in bi-temporal satellite images. An application with an interactive dashboard is implemented to maximise the usability of the developed framework.",
                "authors": "Yiyun Zhang, Zijian Wang",
                "citations": 0
            },
            {
                "title": "ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints",
                "abstract": "Recent text-to-image generative models have enabled us to transform our words into vibrant, captivating imagery. The surge of personalization techniques that has followed has also allowed us to imagine unique concepts in new scenes. However, an intriguing question remains: How can we generate a new, imaginary concept that has never been seen before? In this article, we present the task of creative text-to-image generation, where we seek to generate new members of a broad category (e.g., generating a pet that differs from all existing pets). We leverage the under-studied Diffusion Prior models and show that the creative generation problem can be formulated as an optimization process over the output space of the diffusion prior, resulting in a set of “prior constraints.” To keep our generated concept from converging into existing members, we incorporate a question-answering Vision-Language Model that adaptively adds new constraints to the optimization problem, encouraging the model to discover increasingly more unique creations. Finally, we show that our prior constraints can also serve as a strong mixing mechanism allowing us to create hybrids between generated concepts, introducing even more flexibility into the creative process.",
                "authors": "Elad Richardson, Kfir Goldberg, Yuval Alaluf, D. Cohen-Or",
                "citations": 2
            },
            {
                "title": "VLM-Eval: A General Evaluation on Video Large Language Models",
                "abstract": "Despite the rapid development of video Large Language Models (LLMs), a comprehensive evaluation is still absent. In this paper, we introduce a unified evaluation that encompasses multiple video tasks, including captioning, question and answering, retrieval, and action recognition. In addition to conventional metrics, we showcase how GPT-based evaluation can match human-like performance in assessing response quality across multiple aspects. We propose a simple baseline: Video-LLaVA, which uses a single linear projection and outperforms existing video LLMs. Finally, we evaluate video LLMs beyond academic datasets, which show encouraging recognition and reasoning capabilities in driving scenarios with only hundreds of video-instruction pairs for fine-tuning. We hope our work can serve as a unified evaluation for video LLMs, and help expand more practical scenarios. The evaluation code will be available soon.",
                "authors": "Shuailin Li, Yuang Zhang, Yucheng Zhao, Qiuyue Wang, Fan Jia, Yingfei Liu, Tiancai Wang",
                "citations": 1
            },
            {
                "title": "Trajectory optimization of the Brazilian multistage launch vehicle VLM-1",
                "abstract": null,
                "authors": "Guilherme da Silveira, Sandro da Silva Fernandes",
                "citations": 1
            },
            {
                "title": "M$^{2}$Chat: Empowering VLM for Multimodal LLM Interleaved Text-Image Generation",
                "abstract": "While current LLM chatbots like GPT-4V bridge the gap between human instructions and visual representations to enable text-image generations, they still lack efficient alignment methods for high-fidelity performance on multiple downstream tasks. In this paper, we propose \\textbf{$M^{2}Chat$}, a novel unified multimodal LLM framework for generating interleaved text-image conversation across various scenarios. Specifically, we propose an $M^{3}Adapter$ that efficiently integrates granular low-level visual information and high-level semantic features from multi-modality prompts. Upon the well-aligned fused feature, $M^{3}Adapter$ tailors a learnable gating strategy to balance the model creativity and consistency across various tasks adaptively. Moreover, to further enhance the effectiveness of $M^{3}Adapter$ while preserving the coherence of semantic context comprehension, we introduce a two-stage $M^{3}FT$ fine-tuning strategy. This strategy optimizes disjoint groups of parameters for image-text alignment and visual-instruction respectively. Extensive experiments demonstrate our $M^{2}Chat$ surpasses state-of-the-art counterparts across diverse benchmarks, showcasing its prowess in interleaving generation, storytelling, and multimodal dialogue systems. The demo and code are available at \\red{https://mattie-e.github.io/M2Chat.github.io}.",
                "authors": "Xiaowei Chi, Yijiang Liu, Zhengkai Jiang, Rongyu Zhang, Ziyi Lin, Renrui Zhang, Peng Gao, Chaoyou Fu, Shanghang Zhang, Qi-fei Liu, Yi-Ting Guo",
                "citations": 1
            },
            {
                "title": "X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq1-3339661.gif\"/></alternatives></inline-formula>-VLM: All-in-One Pre-Trained Model for Vision-Language Tasks",
                "abstract": "Vision language pre-training aims to learn alignments between vision and language from a large amount of data. Most existing methods only learn image-text alignments. Some others utilize pre-trained object detectors to leverage vision language alignments at the object level. In this paper, we propose to learn multi-grained vision language alignments by a unified pre-training framework that learns multi-grained aligning and multi-grained localization simultaneously. Based on it, we present X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq3-3339661.gif\"/></alternatives></inline-formula>-VLM, an all-in-one model with a flexible modular architecture, in which we further unify image-text pre-training and video-text pre-training in one model. X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq4-3339661.gif\"/></alternatives></inline-formula>-VLM is able to learn unlimited visual concepts associated with diverse text descriptions. Experiment results show that X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq5-3339661.gif\"/></alternatives></inline-formula>-VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq6-3339661.gif\"/></alternatives></inline-formula>-VLM results in high transferability for it to be utilized in any language or domain. For example, by simply replacing the text encoder with XLM-R, X<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zeng-ieq7-3339661.gif\"/></alternatives></inline-formula>-VLM outperforms state-of-the-art multilingual multi-modal pre-trained models without any multilingual pre-training.",
                "authors": "Yan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang, Jipeng Zhang, Hkust Wangchunshu Zhou, E. Zurich",
                "citations": 11
            },
            {
                "title": "Besdata Video Laryngeal Mask (BD-VLM)- a New Vision-Incorporated 3rd Generation Video Laryngeal Mask Airway",
                "abstract": null,
                "authors": "L. Z. Tan, E. Seet, C. M. Kumar",
                "citations": 0
            },
            {
                "title": "VLM catecholaminergic neurons control tumor growth by regulating CD8+ T cells",
                "abstract": "Significance We have discovered that the ventrolateral medulla (VLM) catecholaminergic (CA) neurons, a group of neurons that control mouse stress response, are activated in tumor-bearing mice, and the neuronal activity promotes tumor growth in multiple syngeneic and spontaneous mouse tumor models. The tumor-promoting effect of these VLM CA neurons is mediated by cytotoxic T cells. These findings establish an interaction between a tumor and a group of neurons in the mouse brain that influences tumorigenesis and tumor growth by modulating adaptive immunity. It is known that tumor growth can be influenced by the nervous system. It is not known, however, if tumors communicate directly with the central nervous system (CNS) or if such interactions may impact tumor growth. Here, we report that ventrolateral medulla (VLM) catecholaminergic (CA) neurons in the mouse brain are activated in tumor-bearing mice and the activity of these neurons significantly alter tumor growth in multiple syngeneic and spontaneous mouse tumor models. Specific ablation of VLM CA neurons by a dopamine-β-hydroxylase (DBH) promotor-activated apoptosis-promoting caspase-3 in Dbh-Cre mice as well as inhibition of these neurons by a chemogenetic method slowed tumor progression. Consistently, chemogenetic activation of VLM CA neurons promoted tumor growth. The tumor inhibition effect of VLM CA neuron ablation is mitigated in Dbh-Cre;Rag1−/− mice, indicating that this regulatory effect is mediated by the adaptive immune system. Specific depletion of CD8+ T cells using an anti-CD8+ antibody also mitigated the tumor suppression resulting from the VLM CA neuron ablation. Finally, we showed that the VLM CA neuronal ablation had an additive antitumor effect with paclitaxel treatment. Collectively, our study uncovered the role of VLM CA neurons in the mouse brain in controlling tumor growth in the mouse body.",
                "authors": "Ze Zhang, Yehua Li, Xueyuan Lv, Linlin Zhao, Xiaodong Wang",
                "citations": 18
            },
            {
                "title": "An optimisation Model for minimising Totes exchange in VLM and SBS/RS integrated System",
                "abstract": null,
                "authors": "Jakob Marolt, Goran Đukić, F. Sgarbossa, T. Lerher",
                "citations": 2
            },
            {
                "title": "SST-VLM: Sparse Sampling-Twice Inspired Video-Language Model",
                "abstract": null,
                "authors": "Yizhao Gao, Zhiwu Lu",
                "citations": 1
            },
            {
                "title": "VLM PRINTING PROMISES ADDITIVE BREAKTHROUGH",
                "abstract": null,
                "authors": "",
                "citations": 0
            },
            {
                "title": "Single-Tray VLM vs Dual-Tray VLM",
                "abstract": "In this paper quantitative comparison of resulting throughputs for single-tray and dual-tray VLM devices is presented. Comparison is based on mathematical models for throughput approximating dual command times of VLM’s crane, for selected parameters of VLM device (height and crane’s velocity) and selected picking times per delivered tray. Analysis showed that throughput increase achieved by using dual-tray VLM’s depends mostly on the average picking time relative to the expected dual command time of the VLM’s crane. Highest improvements are possible for picking time equal to expected dual command time and amounts over 80%, however for extremely low or high picking times improvements are significantly reduced.",
                "authors": "G. Dukic, Tihomir Opetuk, B. Gajšek, T. Lerher",
                "citations": 0
            },
            {
                "title": "PAPR Reduction in OFDM System using Clipping and Filtering based new Hybrid VLM Pre-Coded SLM",
                "abstract": ": One of the latest communications technologies is wireless networking. If it uses a given frequency range for transmitting huge amounts of data, the communication system is called effective. This provides efficient data transfer and an increasing 4G to 5G communications system. The OFDM system theory notes that it is a type of multi-carrier modulation of Orthogonal Frequency Division Multiplexing (OFDM) that has a large demand for digital wideband communication. It encodes the data at different frequencies of the carrier. Digital audio and high definition television, broadband networks and 4G mobile communications are included in these areas of use. It is widely used for communication purposes to reduce inter-symbol interference, decreased nonlinear distortion due to its great advantages as it provides high spectral efficiency. The OFDM system suffers from the inconvenience of high PAPR, i.e. Peak to average power ratio. In this paper, clipping and filtering based new hybrid VLM pre-coded SLM technique was proposed to minimize PAPR in OFDM. And it was found that the proposed scheme achieved a significant gain in reducing PAPR without increasing the process complexity and impacting the system’s error output.",
                "authors": "Muneeb Manzoor Bhat, Preeti Sondhi",
                "citations": 0
            },
            {
                "title": "Single-Tray VLM vs Dual-Tray VLM: Quantitative Throughput Comparison",
                "abstract": ": In this paper quantitative comparison of resulting throughputs for single - tray and dual -tray VLM devices is presented. Comparison is based on mathematical models for throughput approximating dual command times of VLM’s crane, for selected parameters of VLM device (height and crane’s velocity) and selected picking times per delivered tray . Analysi s showed that throughput increase achieved by using dual - tray VLM’s depends mostly on the average picking time relative to the expected dual command time of the VLM’s crane. Highest improvements are possible for picking time equal to expected dual command time and amounts over 80%, however for extremely low or high picking times improvements are significantly reduced.",
                "authors": "Goran Đukić, Tihomir Opetuk, B. Gajšek, T. Lerher",
                "citations": 0
            },
            {
                "title": "GABAergic modulation of NTS‐VLM neurons from Wistar Hannover and Sprague Dawley Rats is reduced by sustained hypoxia",
                "abstract": "Short‐term sustained Hypoxia (SH) induces a different pattern of cardiovascular and respiratory responses in rats from Wistar Ribeirão Preto (WRP), Wistar Hannover (WH) and Sprague Dawley (SD) strains. SH induces an increase in the respiratory parameters of rats from these three strains and mean arterial pressure in WRP and SD but not in WH rats. In WRP rats submitted to SH, we documented an increase in the excitability of NTS neurons sending projections to the ventral medulla (NTS‐VLM), which is part of the chemoreflex pathways. However, the role of GABAergic modulation on the neuronal excitability of NTS neurons from different strains of rats submitted to SH was not yet explored. In the present study, we evaluated the inhibitory modulation (GABAergic synaptic activity) onto NTS‐VLM neurons from WH and SD rats previously submitted to SH. The experimental protocols were approved by the Institutional Ethics Committee (#136/2018). NTS‐VLM neurons were labeled with retrograde tracer (Greenbeads) previously microinjected into VLM of WH and SD rats (25‐30 days old). Ten days later, these rats were submitted to SH protocol (24hs, FiO2 0.1). Then NTS slices were obtained, and the evoked and spontaneous inhibitory postsynaptic currents (eIPSCs and sIPSCs, respectively) were recorded in labeled NTS‐VLM neurons using whole‐cell patch‐clamp. SH decreased the amplitude of eIPSCs in NTS‐VLM neurons from WH [‐172 ± 36 (n=7) vs ‐58.1 ± 9.3 pA (n=7), p=0.0100] and also from SD rats [‐231.8 ± 33.9 (n=9) vs ‐48.9 ± 10.5 pA (n=6), p=0.0009]. SH decreased the decay‐time in NTS‐VLM neurons from SD rats (25.4 ± 5.1 vs 9.2 ± 1.2 ms, p= 0.0247) but produced no change in the rise‐time of the events in both rat strains. SH decreased the frequency of sIPSCs in NTS‐VLM neurons from WH [1.7 ± 0.4 (n=8) vs 0.8 ± 0.2 Hz, (n=9), p=0.0290] and from SD rats [2.3 ± 0.17 (n=5) vs 0.51 ± 0.13 ms (n=6), p<0.0001]. No significant changes were observed in amplitude and half‐width of sIPSCs of NTS‐VLM neurons from both rat strains. The data show that SH decreases the evoked and spontaneous GABAergic neurotransmission on NTS‐VLM neurons from WH and SD rats, probably by affecting the presynaptic terminal. The observed change in GABAergic modulation onto the NTS neuronal excitability may explain the observed alterations in the respiratory pattern in WH and SD rats in response to SH.",
                "authors": "Júlio C. Pascoaloti-Lima, B. Machado, D. Accorsi-Mendonça",
                "citations": 0
            },
            {
                "title": "Pedicled chimeric sensitive fasciocutaneous anterolateral thigh (ALT) and vastus lateralis muscle (VLM) flap for groin defect reconstruction: A case report",
                "abstract": "The anterolateral thigh (ALT) flap is one of the most commonly used flap worldwide, as both free flap and pedicled local flap. Here, we report the use of a pedicled chimeric sensitive ALT and vastus lateralis muscle (VLM) flap in a patient with a 12 cm × 8 cm contaminated soft tissue defect of the right inguinal region with exposed femoral vessels. The flap was harvested based on two perforators, one musculocutaneous and one pure muscular, each nourished separately a sensitive fasciocutaneous component and a vastus lateralis muscle component, respectively. The muscle part was tailored to wrap around the exposed vascular structures, while the innervated skin and fascia component of the flap provided a tension‐free closure of the wound. The post‐operative course was uneventful and the patient was discharged at 1 week post‐operative. Even though it requires technical skills and experience in perforator dissection, we believe that the pedicled chimeric sensitive ALT and VLM flap may be one of the best solutions in case of exposed femoral vessels in contaminated wounds.",
                "authors": "M. Scaglioni, A. Franchi, P. Giovanoli",
                "citations": 17
            },
            {
                "title": "Effect of electro-acupuncture on regulating the swallowing by activating the interneuron in ventrolateral medulla (VLM)",
                "abstract": null,
                "authors": "Qiuping Ye, Chunyan Liu, Junheng Shi, Hue You, Jiaying Zhao, Jianhua Liu, Nenggui Xu, Zhenhua Xu",
                "citations": 15
            },
            {
                "title": "Throughput models for a dual-bay VLM order picking system under different configurations",
                "abstract": "\nPurpose\nVertical lift module (VLM) is a parts-to-picker system for order picking of small products, which are stored into two columns of trays served by a lifting crane. A dual-bay VLM order picking (dual-bay VLM-OP) system is a particular solution where the operator works in parallel with the crane, allowing higher throughput performance. The purpose of this paper is to define models for different operating configurations able to improve the total throughput of the dual-bay VLM-OP system.\n\n\nDesign/methodology/approach\nAnalytical models are developed to estimate the throughput of a dual-bay VLM-OP. A deep evaluation has been carried out, considering different storage assignment policies and the sequencing retrieval of trays.\n\n\nFindings\nA more accurate estimation of the throughput is demonstrated, compared to the application of previous models. Some use guidelines for practitioners and academics are derived from the analysis based on real data.\n\n\nOriginality/value\nDiffering from previous contributions, these models include the acceleration/deceleration of the crane and the probability of storage and retrieve of each single tray. This permits to apply these models to different storage assignment policies and to suggest when these policies can be profitably applied. They can also model the sequencing retrieval of trays.\n",
                "authors": "F. Sgarbossa, Martina Calzavara, A. Persona",
                "citations": 9
            },
            {
                "title": "A robust technique based on VLM and Frangi filter for retinal vessel extraction and denoising",
                "abstract": "The exploration of retinal vessel structure is colossally important on account of numerous diseases including stroke, Diabetic Retinopathy (DR) and coronary heart diseases, which can damage the retinal vessel structure. The retinal vascular network is very hard to be extracted due to its spreading and diminishing geometry and contrast variation in an image. The proposed technique consists of unique parallel processes for denoising and extraction of blood vessels in retinal images. In the preprocessing section, an adaptive histogram equalization enhances dissimilarity between the vessels and the background and morphological top-hat filters are employed to eliminate macula and optic disc, etc. To remove local noise, the difference of images is computed from the top-hat filtered image and the high-boost filtered image. Frangi filter is applied at multi scale for the enhancement of vessels possessing diverse widths. Segmentation is performed by using improved Otsu thresholding on the high-boost filtered image and Frangi’s enhanced image, separately. In the postprocessing steps, a Vessel Location Map (VLM) is extracted by using raster to vector transformation. Postprocessing steps are employed in a novel way to reject misclassified vessel pixels. The final segmented image is obtained by using pixel-by-pixel AND operation between VLM and Frangi output image. The method has been rigorously analyzed on the STARE, DRIVE and HRF datasets.",
                "authors": "Khan Bahadar Khan, Amir A. Khaliq, A. Jalil, Muhammad Shahid",
                "citations": 39
            },
            {
                "title": "VLM Coupled with 2.5D RANS Sectional Data for High-Lift Design",
                "abstract": null,
                "authors": "M. Parenteau, K. Sermeus, E. Laurendeau",
                "citations": 25
            },
            {
                "title": "COMPARING VLM AND CFD MANEUVER LOADS CALCULATIONS FOR A FLYING WING CONFIGURATION",
                "abstract": "This work presents the results of computational fluid dynamics (CFD) based maneuver loads calculations for a flying wing configuration. Euler solutions of the DLR Tau code are compared to vortex lattice method (VLM) results for maneuver loads in the preliminary design stage. The trim parameters of the quasi-steady maneuver load case, the structural deformation and the flow solution are determined in an iterative process. The focus is on a comprehensive loads analysis including a broad selection of load cases to cover the whole flight envelope. This is necessary to ensure a thorough preliminary design. Integration of this approach in an automated, preliminary design process and application of parametric, aeroelastic modeling allows to perform structural optimization loops to evaluate the difference between VLM and CFD on the structural design in terms of structural net mass. 1 MOTIVATION AND INTRODUCTION The design process for new aircraft configurations is complex, costly, and involves various disciplines like aerodynamics, structure, loads analysis, aeroelasticity, flight mechanics, and weights. The task is to substantiate the selected design, based on physically meaningful simulations and analyses. Modifications are much more costly at a later stage of the design process. Thus, the preliminary design should be as good as possible to avoid “surprises” at a 1 Figure 1: The MULDICON structural layout and mass discretization of the basic flight design mass (BFDM).",
                "authors": "A. Voss",
                "citations": 4
            },
            {
                "title": "CFD and VLM Simulation of the Novel Twin-body Asymmetric Flying-wing Aircraft",
                "abstract": "Twin-body aircraft has the advantages of heavy load and long voyage, which make it suitable to execute the task. However, it also has some problems such as high-strength mid-wing requirement and no usable airport in the practical application. In order to solve these problems and promote twin-body aircraft’s adaptability, this paper conducts a research of a new type of twin-body asymmetric flying-wing aircraft (TAFA). Two kinds of simulations (CFD and VLM) are conducted to prove the effectiveness of its flight performance. The results show that the flight performance of TAFA is the best among the four different kinds of plane that can perform the same tasks.",
                "authors": "Xin Guo, Bo Fan, Junsen Huang, Jingfeng Xie",
                "citations": 0
            },
            {
                "title": "Constraint of GIA in Northern Europe with Geological RSL and VLM Data",
                "abstract": "\n <p>In this study, we focus on better constraint of the long term glacial isostatic adjustment (GIA) signal at present-day, and its role as a contributor to total present-day rates of change. The main study area extends from the coastal regions of northern Europe to Scandinavia. Both Holocene relative sea level (RSL) data as well as vertical land motion (VLM) data are incorporated as constraints in a semi-empirical GIA model. Specifically, 70 geological rates of GIA-driven RSL change are inferred from Holocene data; peak RSL fall is indicated in central Scandinavia and the northern British Isles where past ice sheets were thickest, RSL rise is indicated in the southern British Isles and along the northern European coastline. Rates of vertical land motion from GPS at 108 sites provide an additional measure of regional GIA deformation. Within the study area, the geological RSL data complement the spatial gaps of the VLM data and vice versa; both datasets are inverted in a semi-empirical GIA model to yield updated estimates of regional present-day GIA deformations. A regional validation is presented for the North Sea, where the GIA signal may be complicated by lateral variations in Earth structure and existing predictions of regional and global GIA models are discrepant. The model validation in the North Sea region indicates that geological data are needed to fit independent estimates of GIA-related RSL change inferred from tide gauge rates, suggesting that the geological rates provide an important additional constraint of present-day GIA.</p>\n",
                "authors": "K. Simon, R. Riva",
                "citations": 0
            },
            {
                "title": "Use of the Totaltrack VLM as a rescue device following failed tracheal intubation.",
                "abstract": "References 1 Moriarty A. Pediatric epidural analgesia (PEA). Paediatr Anaesth 2012; 22:51–55. 2 Lam DK, Corry GN, Tsui BC. Evidence for the use of ultrasound imaging in pediatric regional anesthesia: a systematic review. Reg Anesth Pain Med 2016; 41:229–241. 3 Carnie J, Boden J, Gao Smith F. Prediction by computerised tomography of distance from skin to epidural space during thoracic epidural insertion. Anaesthesia 2002; 57:701–704. 4 Kil HK, Cho JE, Kim WO, et al. Prepuncture ultrasound-measured distance: an accurate reflection of epidural depth in infants and small children. Reg Anesth Pain Med 2007; 32:102–106.",
                "authors": "M. Gómez-Ríos, E. Freire-Vila, J. M. Calvo-Vecino",
                "citations": 8
            },
            {
                "title": "Detection and characterization of two VLM binaries: LP 1033-31 and LP 877-72",
                "abstract": "\n Using the high-resolution near-infrared adaptive optics imaging from the NaCo instrument at the Very Large Telescope, we report the discovery of a new binary companion to the M-dwarf LP 1033-31 and also confirm the binarity of LP 877-72. We have characterized both the stellar systems and estimated the properties of their individual components. We have found that LP 1033-31 AB with the spectral type of M4.5+M4.5 has a projected separation of 6.7 ± 1.3 AU. Whereas with the spectral type of M1+M4, the projected separation of LP 877-72 AB is estimated to be 45.8 ± 0.3 AU. The binary companions of LP 1033-31 AB are found to have similar masses, radii, effective temperatures, and log g with the estimated values of 0.20 ± 0.04 $\\rm {M}_{\\odot }$, 0.22 ± 0.03 $\\rm {R}_{\\odot }$, and 3200 K, 5.06 ± 0.04. However, the primary of LP 877-72 AB is found to be twice as massive as the secondary with the derived mass of 0.520 ± 0.006 $\\rm {M}_{\\odot }$. The radius and log g for the primary of LP 877-72 AB are found to be 1.8 and 0.95 times that of the secondary component with the estimated values of 0.492 ± 0.011 $\\rm {R}_{\\odot }$ and 4.768 ± 0.005, respectively. With an effective temperature of 3750 ± 15 K, the primary of LP 877-72 AB is also estimated to be ∼400 K hotter than the secondary component. We have also estimated the orbital period of LP 1033-31 and LP 877-72 to be ∼28 and ∼349 yr, respectively. The binding energies for both systems are found to be >1043 erg, which signifies that both systems are stable.",
                "authors": "S. Karmakar, A. Rajpurohit, F. Allard, D. Homeier",
                "citations": 2
            },
            {
                "title": "Use of Totaltrack VLM as a rescue device after failed ventilation and tracheal intubation with LMA Fastrach in emergent difficult airways.",
                "abstract": null,
                "authors": "M. Gómez-Ríos, E. Freire-Vila, A. Abad-Gurumeta, P. Barreto-Calvo, J. M. Calvo-Vecino",
                "citations": 7
            },
            {
                "title": "Combined high-speed and high-lift wing aerodynamic optimization using a coupled VLM-2.5D RANS approach",
                "abstract": null,
                "authors": "M. Parenteau, E. Laurendeau, G. Carrier",
                "citations": 14
            },
            {
                "title": "EFEKTIVITAS PENGGUNAAN MEDIA VIDEO LEARNING MULTIMEDIA (VLM) TERHADAP PENGETAHUAN INFEKSI MENULAR SEKSUAL (IMS) (STUDI PADA WARIA DI KOTA MAKASSAR)",
                "abstract": "One cause of transmission of STIs is the knowledge and attitudes of transvestites about risk behavior. Minimal knowledge about sexually transmitted diseases does not need to use a condom when transacting at the request of a sexy partner. In Makassar City, there are many transgender groups and actively engaging in sexual relations that cause STIs which are the entry point for HIV / AIDS. The disease is increasing every year, sure in 2015 there were 665 new HIV positive, in 2016 it increased to 773, then in 2017 there were 1038 new HIV and until June 2018 there were 354 people who bought new HIV. The purpose of this study is to determine the effectiveness of the use of video media and leaflets on the knowledge and attitudes of transgender people about sexually transmitted infections (STI) in Makassar City. The population was transvestites in the Mamajang Region as many as 84 people, the sample was determined by purposive sampling, so that 30 transvestites were obtained. Based on the research results obtained by respondents the lowest was 17 years (3.3%) and those aged 24 years (13.3%)) which produces age. Based on the pretest and posttest, there was no significant effect between the provision of VLM on the knowledge of Transvestites with a value of 0.105, as well as the attitude of Transgender with a value of p 0.125. This study concludes that there is no difference between VLM and knowledge and attitudes about STIs. ABSTRAK Salah satu penyebab terjadinya penularan IMS adalah pengetahuan dan sikap waria mengenai perilaku berisiko. Pengetahuan yang minim mengenai penularan penyakit menular seksual sehingga tidak menggunakan kondom saat bertransaksi seksual kecuali atas permintaan mitra seksualnya. Di Kota Makassar, terdapat banyak kelompok-kelompok waria dan aktif melakukan hubungan seksual berisiko yang berpotensi menyebabkan IMS yang merupakan pintu masuk penyakit HIV/AIDS. Penyakit ini mengalami peningkatan setiap tahunnya, yakin tahun 2015 tercatat sebanyak 665 HIV positif baru, tahun 2016 meningkat menjadi 773, lalu 2017 sebanyak 1038 HIV baru dan hingga Juni 2018 terdapat 354 orang yang terinfeksi HIV baru.  Tujuan penelitian ini adalah penelitian ini adalah untuk Mengetahui Efektivitas Penggunaan Media Video dan  Leaflet Terhadap Pengetahuan dan Sikap Waria Mengenai Infeksi Menular Seksual (IMS) di Kota Makassar.Penelitian adalah quasi experiment (eksperimen semu) dengan menggunakan pretest-posttest control group design. Populasi adalah  waria di Wilayah Kecamatan Mamajang sebanyak 84 orang, sampel ditentukan dengan purposive sampling, sehingga didapatkan 30 waria.Berdasarkan hasil penelitian didapatkan bahwa usia terendah responden adalah 17 tahun (3,3%)dan yang tertinggi usia 24 tahun (13,3%) yang mengindikasikan bahwa usia waria berada pada usia yang produktif. Berdasarkan pretest dan posttest, tidak ada pengaruh yang signifikan antara pemberian VLM terhadap pengetahuan Waria mengenai denganp value 0,105, begitupun dengan sikap Waria dengan p value 0,125. Penelitian ini menyimpulkan tidak ada pengaruh antara VLM dengan pengetahuan dan sikap waria mengenai IMS.",
                "authors": "A. Asrina, R. Sudirman",
                "citations": 1
            },
            {
                "title": "Comparison between VLM and CFD Maneuver Loads Calculation at the Example of a Flying Wing Configuration",
                "abstract": "This work presents the results of computational fluid dynamics (CFD) based maneuver loads calculations for a flying wing configuration. Euler solutions of the DLR Tau code are compared to vortex lattice method (VLM) results for maneuver loads in the preliminary design stage. The trim parameters of the quasi-steady maneuver load case, the structural deformation and the flow solution are determined in an iterative process. The focus is on a comprehensive loads analysis including a broad selection of load cases to cover the whole flight envelope. This is necessary to ensure a thorough preliminary design. Integration of this approach in an automated, preliminary design process and application of parametric, aeroelastic modeling allows to perform structural optimization loops to evaluate the difference between VLM and CFD on the structural design in terms of structural net mass.",
                "authors": "A. Voss",
                "citations": 1
            },
            {
                "title": "PENGARUH PENDIDIKAN KESEHATAN MANAJEMEN LAKTASI MELALUI MEDIA VLM TERHADAP TINGKAT PENGETAHUAN IBU SC DI RSUD UNGARAN",
                "abstract": "Latar Belakang: Proses persalinan adalah proses yang sangat kompleks yang bertujuan menyelamatkan ibu dan bayinya. SC merupakan proses melahirkan janin melalui irisan pada dinding perut dan dinding uterus. Salah satu faktor penghambat ibu SC menyusui yaitu kurangnya pengetahuan mengenai manajemen laktasi. Perlunya pengetahuan manajemen laktasi dapat meningkatkan kemampuan ibu untuk menyusui. Penelitian ini bertujuan untuk mengetahui pengaruh pendidikan kesehatan manajemen laktasi dengan VLM terhadap tingkat pengetahuan ibu SC di RSUD Ungaran. \nMetode: Penelitian ini menggunakan desain penelitian quasi eksperimental design dan rancangan nonequivalent control group design. Teknik pengambilan sampel menggunakan teknik purposive sampling sebanyak 46 ibu SC di RSUD Ungaran. Instrumen yang digunakan yaitu kuesioner yang sudah diuji validitas dan reliabilitas. Pemilihan sampel menggunakan slovin. Analisis data menggunakan uji Mann Whitney. \nHasil: Ada pengaruh pendidikan kesehatan manajemen laktasi melalui media VLM terhadap tingkat pengetahuan ibu SC di RSUD Ungaran. Hasil Mann Whitney test p-value 0,000 (p",
                "authors": "N. Destiyanti",
                "citations": 1
            },
            {
                "title": "The totaltrack VLM: a novel video-assisted intubating laryngeal mask.",
                "abstract": null,
                "authors": "M. Gómez-Ríos, C. Bonome",
                "citations": 8
            },
            {
                "title": "Histopathological Study of Different VLM Stages of Toxocara canis Infection in Liver of Rabbits",
                "abstract": "Toxocariasis is a zoonotic parasitic disease caused by Toxocara canis nematode emberyonated egg which is usually transmitted to humans mainly in children via the faecal–oral route, or accidently ingested larvae from uncooked liver or meat of infected ruminants and poultry.  T. canis larvae remains a problem throughout the world because it remains on arrested stage without development to adult stage and causes multisystem disease in the paratenic hosts such as humans, ruminants, poultry and rodents, most infections are asymptomatic and manifests in humans causing the well-characterized syndrome; Visceral Larva Migrans (VLM).  The result of this study indicated detection of T. canis larval stage in liver tissues of infected rabbits at third week post infection. In conclusion the result indicated the possibility of using histopathological examination to diagnosis if there was an infection with T. canis larval stage in paratenic host tissues. It was also recommend that this test could be used to ensure that meat and its products of any local or imported are free from this infection.",
                "authors": "B. Hade, Amer M. Abd Al-Amer Zainab I. Ibrahim², S. Saadedin",
                "citations": 3
            },
            {
                "title": "Methodology of Estimation of Aerodynamic Coefficients of the UAS-E4 Ehécatl using Datcom and VLM Procedure",
                "abstract": null,
                "authors": "M. Kuitche, R. Botez",
                "citations": 12
            },
            {
                "title": "VizieR Online Data Catalog: Predicted Microlensing Events by nearby VLM objects (Nielsen+ 2018)",
                "abstract": null,
                "authors": "M. Nielsen, D. Bramich",
                "citations": 0
            },
            {
                "title": "Generazione dei load models per modelli aeroelastici dedicati al calcolo dei carichi basato su database CFD e Data Set in sostituzione/correzione del database VLM",
                "abstract": null,
                "authors": "Martina Battaglia",
                "citations": 0
            },
            {
                "title": "OMIA RAPPORT 1: Inspiratietrajecten voor het operationaliseren van omgevingskwaliteit en ecosysteemdiensten voor gebiedsgerichte VLM projecten",
                "abstract": null,
                "authors": "Francis Turkelboom, Wim Verheyden, Lies Messely, M. Koopmans, Bert Barla, D. Mortelmans",
                "citations": 0
            },
            {
                "title": "VizieR Online Data Catalog: Masses & radii of 4 VLM stars in EB systems (Chaturvedi+, 2018)",
                "abstract": null,
                "authors": "P. Chaturvedi, Rishikesh Sharma, A. Chakraborty, B. Anandarao, N. J. Prasad",
                "citations": 0
            },
            {
                "title": "Aerodynamic Optimization of Aircraft Wings Using a Coupled VLM-2.5D RANS Approach",
                "abstract": "RESUME \nLe processus de conception d’avion de transport civil transsonique est complexe et requiert une forte gouvernance afin de gerer toutes les phases de developpements de programme. Il y a un besoin dans la communaute de developper des modeles numeriques pour toutes les disciplines qui permettent de relier les phases de design conceptuel, preliminaire et detaillee \nde facon continue, de telle sorte que les choix faits soient consistants entre eux. \nL’objectif de ce travail est de developper un modele aerodynamique adapte pour l’optimisation conceptuelle multidisciplinaire avec un faible cout de calcul et une fidelite suffisante pour explorer un vaste espace de conception dans les regimes transsoniques et basses vitesses avec systemes hypersustentateurs. L’approche est basee sur la Methode non-visqueuse Vortex Lattice Method (VLM), selectionnee pour son faible temps de calcul. Les effets visqueux sont modelises avec des calculs RANS bidimensionnels haute fidelite effectues a differentes \nsections le long de l’envergure de l’aile. Les donnees de sections visqueuses sont calculees avec les conditions d’une aile en fleche infinie pour inclure les effets de l’ecoulement transverse qui sont important dans la prediction du coefficient de portance maximal. Ces effets visqueux sont incorpores iterativement avec le VLM a l’aide d’un algorithme de couplage de type alpha \nmodifie specialement pour prendre en compte des donnees avec aile en fleche. De plus, une dissipation artificielle est ajoutee afin de stabiliser la solution dans la region post-decrochage. \nLa precision de la methode est comparee a celle des solutions 3D RANS sur le Bombardier Research Wing (BRW) avec et sans systemes hypersustentateurs. Les resultats demontrent une precision impressionnante de l’approche RANS VLM/2.5D par rapport aux solutions 3D RANS. De plus, les solutions de l’approche RANS VLM/2.5D s’effectuent en quelques secondes seulement sur un ordinateur classique. \nFinalement, le solveur aerodynamique est implemente dans un cadre d’optimisation avec une methode de type Covariant Matrix Adaptation Evolution Strategy (CMA-ES). Des optimisations a basse vitesse et haute vitesse avec fonction mono-objective sont realisees, ainsi que \ndes optimisations avec fonction objective-composee en combinant des objectifs basses vitesses et hautes vitesses. D’autre part, l’approche VLM/2.5D est capable de capter les cellules de decrochage. Par consequent, cette caracteristique est utilisee pour definir un nouveau critere de decrochage selon l’envergure de l’aile afin d’etre utilise comme contrainte d’optimisation. \nLe travail conclue sur les limites de la methode et sur les prochains developpements possibles.----------ABSTRACT \nThe design process of transonic civil aircraft is complex and requires strong governance to manage the various program development phases. There is a need in the community to have numerical models in all disciplines that span the conceptual, preliminary and detail design \nphases in a seamless fashion so that choices made in each phase remain consistent with each other. \nThe objective of this work is to develop an aerodynamic model suitable for conceptual multidisciplinary design optimization with low computational cost and sufficient fidelity to explore a large design space in the transonic and high-lift regimes. The physics-based reduce order \nmodel is based on the inviscid Vortex Lattice Method (VLM), selected for its low computation time. Viscous effects are modeled with two-dimensional high-fidelity RANS calculations at various sections along the span and incorporated as an angle of attack correction inside \nthe VLM. The viscous sectional data are calculated with infinite swept wing conditions to allow viscous crossflow effects to be included for a more accurate maximum lift coefficient and spanload evaluations. These viscous corrections are coupled through a modified alpha coupling \nmethod for 2.5D RANS sectional data, stabilized in the post-stall region with artificial dissipation. \nThe fidelity of the method is verified against 3D RANS flow solver solutions on the Bombardier Research Wing (BRW). Clean and high-lift configurations are investigated. The overall results show impressive precision of the VLM/2.5D RANS approach compared to \n3D RANS solutions and in compute times in the order of seconds on a standard desktop computer. \nFinally, the aerodynamic solver is implemented in an optimization framework with a Covariant Matrix Adaptation Evolution Strategy (CMA-ES) optimizer to explore the design space of aerodynamic wing planform. Single-objective low-speed and high-speed optimizations are \nperformed along with composite-objective functions for combined low-speed and high-speed optimizations with high-lift configurations as well. Moreover, the VLM/2.5D approach is capable of capturing stall cells phenomena and this characteristic is used to define a new spanwise stall criteria to be introduced as an optimization constraint. The work concludes on the limitations of the method and possible avenues for further research.",
                "authors": "M. Parenteau",
                "citations": 4
            },
            {
                "title": "Visual Experimental and Numerical Investigations Around the VLM-1 Microsatellite Launch Vehicle at Transonic Regime",
                "abstract": "It is performed and presented an experimental and numerical investigation over the flow patterns around the forebody section of a microsatellite launch vehicle in development at Instituto de Aeronautica e Espaco. The experimental investigation with a VLM-1 model in 1:50 scale is carried out at the Brazilian Pilot Transonic Wind Tunnel, located in the Aerodynamics Division of the mentioned Institute, using the classical schlieren flow visualization technique. Schlieren images are obtained for nominal Mach number varying from 0.9 to 1.01. Numerical simulation using Stanford’s SU2 code is conducted together with the experimental investigation in order to improve the understanding of the complex physical phenomena associated with the experimental results of this particular regime. The combination of the 2 techniques allowed the assessment of some important aspects on the flow field around the vehicle in the conditions considered in this study, such as shock wave/boundary-layer interaction. The numerical simulation is also very important, allowing the quantification of some important parameters and confirming the shock wave formation patterns observed in the simulation when compared with the schlieren images. A good agreement regarding the position of the shock wave, when compared with the schlieren images, with a maximum error of about 6%, is observed over the VLM model.",
                "authors": "Henrique Oliveira da Mata, J. B. F. Filho, A. C. Avelar, Leonardo de Oliveira Carvalho, J. Azevedo",
                "citations": 3
            },
            {
                "title": "On Boeing 737 – 300 Wing Aerodynamics Calculations Based on VLM Theory",
                "abstract": "In this paper, aerodynamics coefficients of Boeing 737-300 were calculated using VLM (vortex lattice method) theory. The wing was assumed to be planar and was divided into 6×6 panels, which were in the trapezoid shape. Aerodynamics lifting and moment coefficients were calculated. Also, center of pressure location was found using data from VLM and wing geometry. Comparisons between literature, finite wing theory and VLM theory were done. It was found that maximum lifting coefficient error between literature and VLM was about 4.0%. Moreover, that between finite wing theory and VLM was about 2.2%. Center of pressure location error between finite wing theory and VLM was about 0.5%.",
                "authors": "J. Nagler",
                "citations": 1
            },
            {
                "title": "Use of the Totaltrack VLM in unexpected difficult airway in urgent caesarean section.",
                "abstract": null,
                "authors": "M. Gómez-Ríos, E. Freire-Vila, David Gómez-Ríos, M. Diéguez-Fernández",
                "citations": 3
            },
            {
                "title": "Design of a Wing with Bell-shaped Span-load using VLM Method",
                "abstract": null,
                "authors": "Vinayak Bembrekar, Akshay Rasane, A. Jadhav, Om Vaishnav, Sandip Mirdude",
                "citations": 3
            },
            {
                "title": "Wide low and VLM binary systems using VO tools",
                "abstract": "The frequency of multiple systems and their properties are key constraints of stellar formation and evolution. Formation mechanisms of very low-mass (VLM) objects are still under considerable debate and an accurate assessment of their multiplicity and orbital properties are essential for constraining current theoretical models. Taking advantage of the Virtual Observatory capabilities, we looked for comoving low and VLM binary (or multiple) systems using the Large Area Survey of the UKIDSS LAS DR10, SDSS DR9, and the 2MASS Catalogues. Other catalogues (WISE, GLIMPSE, SuperCosmos ...) were used to derive the physical parameters of the systems. We report the identification of 36 low and VLM (∼M0-L0 spectral types) candidates to binary/multiple system (separations between 200 and 92000 AU), whose physical association is confirmed through common proper motion, distance and low probability of chance alignment. This new system list notably increases the previous sampling in their mass-separation parameter space (∼100). We have also found 50 low-mass objects that we can classify as ∼L0-T2 according to their photometric information. Only one of these objects presents a common proper motion high-mass companion. Although we could not constrain the age of the majority of the candidates, probably most of them are still bound except four that may be under disruption processes. We suggest that our sample could be divided in two populations: one tightly bound wide VLM systems that are expected to last more than 10 Gyr, and other formed by weak bound wide VLM systems that will dissipate within a few Gyrs.",
                "authors": "M. Gálvez-Ortiz, E. Solano, N. Lodieu, M. Aberasturi",
                "citations": 0
            },
            {
                "title": "Use of the TotalTrack VLM for endotracheal intubation in a patient with a giant thyroglossal duct cyst and airway compression.",
                "abstract": null,
                "authors": "M. Gómez-Ríos, Iria Silva-Carballal, E. Freire-Vila",
                "citations": 5
            },
            {
                "title": "Use of the TotalTrack VLM for emergent endotracheal intubation in predicted difficult airway with obstruction by expanding space-occupying lesions and reduced interincisor opening.",
                "abstract": null,
                "authors": "B. Izquierdo-González, M. Gómez-Ríos, E. Freire-Vila",
                "citations": 5
            },
            {
                "title": "Uso del Totaltrack VLM en una vía aérea difícil imprevista en una cesárea urgente",
                "abstract": null,
                "authors": "M. Gómez-Ríos, E. Freire-Vila, David Gómez-Ríos, M. Diéguez-Fernández",
                "citations": 1
            },
            {
                "title": "Retraction: A robust technique based on VLM and Frangi filter for retinal vessel extraction and denoising",
                "abstract": null,
                "authors": "",
                "citations": 0
            },
            {
                "title": "Verification of Optimum Spanwise Lift Distribution Design Method Applying VLM",
                "abstract": null,
                "authors": "Y. Oda, Tatsunori Yuhara, K. Rinoie",
                "citations": 0
            },
            {
                "title": "Spring 5-1-2002 VLM 2 : A Very Lightweight Mobile Multicast System for Wireless Sensor Networks ; CU-CS-938-02",
                "abstract": null,
                "authors": "Anmol Sheth, B. Shucker, Richard Han",
                "citations": 0
            },
            {
                "title": "Totaltrack VLM®, our case series: New solutions for old problems",
                "abstract": null,
                "authors": "I. H. Blanco, F. J. Carrasco, I. Vila, R. Hoffmann, M. Moral",
                "citations": 0
            },
            {
                "title": "Elucidating the True Binary Fraction of VLM Stars and Brown Dwarfs with Spectral Binaries",
                "abstract": null,
                "authors": "D. B. Gagliuffi, A. Burgasser, C. Gelino, J. Sahlmann, S. Schmidt, J. Gagné, N. Skrzypek",
                "citations": 0
            },
            {
                "title": "Use of the TotalTrack VLM for emergent endotracheal intubation in predicted difficult airway with obstruction by expanding space-occupying lesions and reduced interincisor opening.",
                "abstract": null,
                "authors": "B. Izquierdo-González, M. Gómez-Ríos, E. Freire-Vila",
                "citations": 0
            },
            {
                "title": "TotalTrack VLM - nová pomůcka pro obtížné zajištění dýchacích cest",
                "abstract": null,
                "authors": "T. Brozek, P. Michálek, J. Votruba",
                "citations": 0
            },
            {
                "title": "Use of the Totaltrack VLM in unexpected difficult airway in urgent caesarean section",
                "abstract": null,
                "authors": "M. Gómez-Ríos, E. Freire-Vila, David Gómez-Ríos, M. Diéguez-Fernández",
                "citations": 1
            },
            {
                "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
                "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
                "authors": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, A. Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, O. Vinyals, Andrew Zisserman, K. Simonyan",
                "citations": 2703
            },
            {
                "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
                "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
                "authors": "Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei",
                "citations": 373
            },
            {
                "title": "CogAgent: A Visual Language Model for GUI Agents",
                "abstract": "People are spending an enormous amount of time on dig-ital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogA-gent supports input at a resolution of1120 × 1120, enabling it to recognize tiny page elements and text. As a general-ist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK- VQA, Text- Vqa, St- Vqa, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks-Mind2Web and AITW, ad-vancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM.",
                "authors": "Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, Jie Tang",
                "citations": 215
            },
            {
                "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training",
                "abstract": "We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in a pre-training manner. Borrow ideas from cross-lingual pre-trained models, such as XLM (Lample and Conneau 2019) and Unicoder (Huang et al. 2019), both visual and linguistic contents are fed into a multi-layer Transformer (Vaswani et al. 2017) for the cross-modal pre-training, where three pre-trained tasks are employed, including Masked Language Modeling(MLM), Masked Object Classification(MOC) and Visual-linguistic Matching(VLM). The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large-scale image-caption pairs, we transfer Unicoder-VL to caption-based image-text retrieval and visual commonsense reasoning, with just one additional output layer. We achieve state-of-the-art or comparable results on both two tasks and show the powerful ability of the cross-modal pre-training.",
                "authors": "Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, Ming Zhou",
                "citations": 857
            },
            {
                "title": "VILA: On Pre-training for Visual Language Models",
                "abstract": "Visual language models (VLMs) rapidly progressed with the recent success of large language models. There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities. In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles. Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge. VILA is also deployable on Jetson Orin for on-device VLM.",
                "authors": "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han",
                "citations": 203
            },
            {
                "title": "Vision-Language Models for Vision Tasks: A Survey",
                "abstract": "Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm. To address the two challenges, Vision-Language Models (VLMs) have been intensively investigated recently, which learns rich vision-language correlation from web-scale image-text pairs that are almost infinitely available on the Internet and enables zero-shot predictions on various visual recognition tasks with a single VLM. This paper provides a systematic review of visual language models for various visual recognition tasks, including: (1) the background that introduces the development of visual recognition paradigms; (2) the foundations of VLM that summarize the widely-adopted network architectures, pre-training objectives, and downstream tasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4) the review and categorization of existing VLM pre-training methods, VLM transfer learning methods, and VLM knowledge distillation methods; (5) the benchmarking, analysis and discussion of the reviewed methods; (6) several research challenges and potential research directions that could be pursued in the future VLM studies for visual recognition.",
                "authors": "Jingyi Zhang, Jiaxing Huang, Sheng Jin, Shijian Lu",
                "citations": 274
            },
            {
                "title": "Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization",
                "abstract": "Prompt tuning is an effective way to adapt the pretrained visual-language model (VLM) to the downstream task using task-related textual tokens. Representative CoOp-based work combines the learnable textual tokens with the class tokens to obtain specific textual knowledge. However, the specific textual knowledge is worse generalization to the unseen classes because it forgets the essential general textual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guided Context Optimization (KgCoOp) to enhance the generalization ability of the learnable prompt for unseen classes. The key insight of KgCoOp is that the forgetting about essential knowledge can be alleviated by reducing the discrepancy between the learnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the discrepancy between the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can make a discriminative prompt for both seen and unseen tasks. Extensive evaluation of several benchmarks demonstrates that the proposed Knowledge-guided Context Optimization is an efficient method for prompt tuning, i.e., achieves better performance with less training time. code.",
                "authors": "Hantao Yao, Rui Zhang, Changsheng Xu",
                "citations": 131
            },
            {
                "title": "DriveLM: Driving with Graph Visual Question Answering",
                "abstract": "We study how vision-language models (VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users. While recent approaches adapt VLMs to driving via single-round visual question answering (VQA), human drivers reason about decisions in multiple steps. Starting from the localization of key objects, humans estimate object interactions before taking actions. The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process. We instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving. The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures. Notably, its benefits are pronounced when it is evaluated zero-shot on unseen objects or sensor configurations. We hope this work can be the starting point to shed new light on how to apply VLMs for autonomous driving. To facilitate future research, all code, data, and models are available to the public.",
                "authors": "Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, Hongyang Li",
                "citations": 106
            },
            {
                "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning",
                "abstract": "Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks. In this paper, we address the limitation above by 1) introducing vision-language Model with Multi-Modal In-Context Learning(MMICL), a new approach to allow the VLM to deal with multi-modal inputs efficiently; 2) proposing a novel context scheme to augment the in-context learning ability of the VLM; 3) constructing the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to understand complex multi-modal prompts. Our experiments confirm that MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks, especially for complex benchmarks, including MME and MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge of complex multi-modal prompt understanding and emerges the impressive ICL ability. Furthermore, we observe that MMICL successfully alleviates language bias in VLMs, a common issue for VLMs that often leads to hallucination when faced with extensive textual context. Our code, dataset, dataset tool, and model are available at https://github.com/PKUnlp-icler/MIC",
                "authors": "Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang",
                "citations": 114
            },
            {
                "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
                "abstract": "Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese. We have open-sourced the dataset to encourage further research.",
                "authors": "Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, Qi Liu",
                "citations": 102
            },
            {
                "title": "HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models",
                "abstract": "Large language models (LLMs), after being aligned with vision models and integrated into vision-language models (VLMs), can bring impressive improvement in image reasoning tasks. This was shown by the recently released GPT-4V(ison), LLaVA-1.5, etc. However, the strong language prior in these SOTA LVLMs can be a double-edged sword: they may ignore the image context and solely rely on the (even contradictory) language prior for reasoning. In contrast, the vision modules in VLMs are weaker than LLMs and may result in misleading visual representations, which are then translated to confident mistakes by LLMs. To study these two types of VLM mistakes, i.e., language hallucination and visual illusion , we curated “H ALLUSION B ENCH 1 ,” an image-context reasoning benchmark that is still challenging to even GPT-4V and LLaVA-1.5. We provide a detailed analysis of examples in H ALLUSION B ENCH , which sheds novel insights on the illusion or hallucination of VLMs and how to improve them in the future. The benchmark and codebase will be released at https://github.com/tianyi-lab/HallusionBench.",
                "authors": "Fuxiao Liu, Tianrui Guan, Xiyang Wu, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou",
                "citations": 93
            },
            {
                "title": "Aligning Bag of Regions for Open-Vocabulary Object Detection",
                "abstract": "Pre-trained vision-language models (VLMs) learn to align vision and language representations on large-scale datasets, where each image-text pair usually contains a bag of semantic concepts. However, existing open-vocabulary object detectors only align region embeddings individually with the corresponding features extracted from the VLMs. Such a design leaves the compositional structure of semantic concepts in a scene under-exploited, although the structure may be implicitly learned by the VLMs. In this work, we propose to align the embedding of bag of regions beyond individual regions. The proposed method groups contextually interrelated regions as a bag. The embeddings of regions in a bag are treated as embeddings of words in a sentence, and they are sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which is learned to be aligned to the corresponding features extracted by a frozen VLM. Applied to the commonly used Faster R-CNN, our approach surpasses the previous best results by 4.6 box AP50 and 2.8 mask AP on novel categories of open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are available at https://github.com/wusize/ovdet.",
                "authors": "Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, Chen Change Loy",
                "citations": 85
            },
            {
                "title": "GeoChat:Grounded Large Vision-Language Model for Remote Sensing",
                "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have shown great promise in natural image domains, allowing users to hold a dialogue about given visual content. However, such general-domain VLMs perform poorly for Remote Sensing (RS) scenarios, leading to inaccurate or fabricated information when presented with RS domain-specific queries. Such a behavior emerges due to the unique challenges introduced by RS imagery. For example, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-level reasoning is necessary alongside holistic scene inter-pretation. Furthermore, the lack of domain-specific multimodal instruction following data as well as strong back-bone models for RS make it hard for the models to align their behavior with user queries. To address these limitations, we propose GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabilities with high-resolution RS images. Specifically, GeoChat can not only answer image-level queries but also accepts region inputs to hold region-specific dialogue. Further-more, it can visually ground objects in its responses by referring to their spatial coordinates. To address the lack of domain-specific datasets, we generate a novel RS multimodal instruction-following dataset by extending image-text pairs from existing diverse RS datasets. We establish a comprehensive benchmarkfor RS multitask conversations and compare with a number of baseline methods. GeoChat demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conversations and referring detection. Our code is available here.",
                "authors": "Kartik Kuckreja, M. S. Danish, Muzammal Naseer, Abhijit Das, Salman H. Khan, F. Khan",
                "citations": 74
            },
            {
                "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
                "abstract": "This paper presents PaLI-3, a smaller, faster, and stronger vision language model (VLM) that compares favorably to similar models that are 10x larger. As part of arriving at this strong performance, we compare Vision Transformer (ViT) models pretrained using classification objectives to contrastively (SigLIP) pretrained ones. We find that, while slightly underperforming on standard image classification benchmarks, SigLIP-based PaLI shows superior performance across various multimodal benchmarks, especially on localization and visually-situated text understanding. We scale the SigLIP image encoder up to 2 billion parameters, and achieves a new state-of-the-art on multilingual cross-modal retrieval. We hope that PaLI-3, at only 5B parameters, rekindles research on fundamental pieces of complex VLMs, and could fuel a new generation of scaled-up models.",
                "authors": "Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, P. Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim M. Alabdulmohsin, Piotr Padlewski, Daniel M. Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiao-Qi Zhai, Radu Soricut",
                "citations": 77
            },
            {
                "title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
                "abstract": "Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PHYSOBJECTS, an object-centric dataset of 39.6K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically grounded VLMs. We additionally illustrate the benefits of our physically grounded VLM on a real robot, where it improves task success rates. We release our dataset and provide further details and visualizations of our results at https://iliad.stanford.edu/pg-vlm/.",
                "authors": "Jensen Gao, Bidipta Sarkar, F. Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh",
                "citations": 81
            },
            {
                "title": "Prompt-aligned Gradient for Prompt Tuning",
                "abstract": "Thanks to the large pre-trained vision-language models (VLMs) like CLIP [37], we can craft a zero-shot classifier by discrete prompt design, e.g., the confidence score of an image being \"[CLASS]\" can be obtained by using the VLM provided similarity between the image and the prompt sentence \"a photo of a [CLASS]\". Furthermore, prompting shows great potential for fast adaptation of VLMs to downstream tasks if we fine-tune the soft prompts with few samples. However, we find a common failure that improper fine-tuning or learning with extremely few-shot samples may even under-perform the zero-shot prediction. Existing methods still address this problem by using traditional anti-overfitting techniques such as early stopping and data augmentation, which lack a principled solution specific to prompting. In this paper, we present Prompt-aligned Gradient, dubbed ProGrad to prevent prompt tuning from forgetting the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradient is aligned (or non-conflicting) to the general knowledge, which is represented as the optimization direction offered by the pre-defined prompt predictions. Extensive experiments under the few-shot learning, domain generalization, base-to-new generalization and cross-dataset transfer settings demonstrate the stronger few-shot generalization ability of ProGrad over state-of-the-art prompt tuning methods.",
                "authors": "Beier Zhu, Yulei Niu, Yucheng Han, Yuehua Wu, Hanwang Zhang",
                "citations": 210
            },
            {
                "title": "On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving",
                "abstract": "The pursuit of autonomous driving technology hinges on the sophisticated integration of perception, decision-making, and control systems. Traditional approaches, both data-driven and rule-based, have been hindered by their inability to grasp the nuance of complex driving environments and the intentions of other road users. This has been a significant bottleneck, particularly in the development of common sense reasoning and nuanced scene understanding necessary for safe and reliable autonomous driving. The advent of Visual Language Models (VLM) represents a novel frontier in realizing fully autonomous vehicle driving. This report provides an exhaustive evaluation of the latest state-of-the-art VLM, GPT-4V(ision), and its application in autonomous driving scenarios. We explore the model's abilities to understand and reason about driving scenes, make decisions, and ultimately act in the capacity of a driver. Our comprehensive tests span from basic scene recognition to complex causal reasoning and real-time decision-making under varying conditions. Our findings reveal that GPT-4V demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems. It showcases the potential to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts. However, challenges remain, particularly in direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks. These limitations underscore the need for further research and development. Project is now available on GitHub for interested parties to access and utilize: \\url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}",
                "authors": "Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, Zheng Zhu, Shaoyan Sun, Yeqi Bai, Xinyu Cai, Min Dou, Shuanglu Hu, Botian Shi, Yu Qiao",
                "citations": 66
            },
            {
                "title": "Teaching CLIP to Count to Ten",
                "abstract": "Large vision-language models (VLMs), such as CLIP, learn rich joint image-text representations, facilitating advances in numerous downstream tasks, including zero-shot classification and text-to-image generation. Nevertheless, existing VLMs exhibit a prominent well-documented limitation – they fail to encapsulate compositional concepts such as counting. We introduce a simple yet effective method to improve the quantitative understanding of VLMs, while maintaining their overall performance on common benchmarks. Specifically, we propose a new counting-contrastive loss used to finetune a pre-trained VLM in tandem with its original objective. Our counting loss is deployed over automatically-created counterfactual examples, each consisting of an image and a caption containing an incorrect object count. For example, an image depicting three dogs is paired with the caption \"Six dogs playing in the yard\" as a negative example. Our loss encourages discrimination between the correct caption and its counterfactual variant which serves as a hard negative example. To the best of our knowledge, this work is the first to extend CLIP’s capabilities to object counting. Furthermore, we introduce \"CountBench\" – a new image-text counting benchmark for evaluating object counting capabilities. We demonstrate a significant improvement over state-of-the-art baseline models on this task. Finally, we leverage our counting-aware CLIP model for image retrieval and text-conditioned image generation, demonstrating that our model can produce specific counts of objects more reliably than existing ones.",
                "authors": "Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, M. Irani, Tali Dekel",
                "citations": 67
            },
            {
                "title": "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts",
                "abstract": "Most existing methods in vision language pre-training rely on object-centric features extracted through object detection and make fine-grained alignments between the extracted features and texts. It is challenging for these methods to learn relations among multiple objects. To this end, we propose a new method called X-VLM to perform `multi-grained vision language pre-training.' The key to learning multi-grained alignments is to locate visual concepts in the image given the associated texts, and in the meantime align the texts with the visual concepts, where the alignments are in multi-granularity. Experimental results show that X-VLM effectively leverages the learned multi-grained alignments to many downstream vision language tasks and consistently outperforms state-of-the-art methods.",
                "authors": "Yan Zeng, Xinsong Zhang, Hang Li",
                "citations": 264
            },
            {
                "title": "CLIP2: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data",
                "abstract": "Contrastive Language-Image Pre-training, benefiting from large-scale unlabeled text-image pairs, has demonstrated great performance in open-world vision understanding tasks. However, due to the limited Text-3D data pairs, adapting the success of 2D Vision-Language Models (VLM) to the 3D space remains an open problem. Existing works that leverage VLM for 3D understanding generally resort to constructing intermediate 2D representations for the 3D data, but at the cost of losing 3D geometry information. To take a step toward open-world 3D vision understanding, we propose Contrastive Language-Image-Point Cloud Pretraining (CLIP2) to directly learn the transferable 3D point cloud representation in realistic scenarios with a novel proxy alignment mechanism. Specifically, we exploit naturally-existed correspondences in 2D and 3D scenarios, and build well-aligned and instance-based text-image-point proxies from those complex scenarios. On top of that, we propose a cross-modal contrastive objective to learn semantic and instance-level aligned point cloud representation. Experimental results on both indoor and outdoor scenarios show that our learned 3D representation has great transfer ability in downstream tasks, including zero-shot and few-shot 3D recognition, which boosts the state-of-the-art methods by large margins. Furthermore, we provide analyses of the capability of different representations in real scenarios and present the optional ensemble scheme.",
                "authors": "Yi Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han, Chao Ye, Qingqiu Huang, Dit-Yan Yeung, Zhen Yang, Xiaodan Liang, Hang Xu",
                "citations": 57
            },
            {
                "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
                "abstract": "Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour Matching to craft hijacks for four types of attack, forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.",
                "authors": "Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons",
                "citations": 54
            },
            {
                "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
                "abstract": "Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second\"baseline\"prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.",
                "authors": "Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David Lindner",
                "citations": 43
            },
            {
                "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models",
                "abstract": "The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT",
                "authors": "Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, Shih-Fu Chang",
                "citations": 39
            },
            {
                "title": "SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing",
                "abstract": "Remote sensing imagery, despite its broad applications in helping achieve Sustainable Development Goals and tackle climate change, has not yet benefited from the recent advancements of versatile, task-agnostic vision language models (VLMs). A key reason is that the large-scale, semantically diverse image-text dataset required for developing VLMs is still absent for remote sensing images. Unlike natural images, remote sensing images and their associated text descriptions cannot be efficiently collected from the public Internet at scale. In this work, we bridge this gap by using geo-coordinates to automatically connect open, unlabeled remote sensing images with rich semantics covered in OpenStreetMap, and thus construct SkyScript, a comprehensive vision-language dataset for remote sensing images, comprising 2.6 million image-text pairs covering 29K distinct semantic tags. \nWith continual pre-training on this dataset, we obtain a VLM that surpasses baseline models with a 6.2% average accuracy gain in zero-shot scene classification across seven benchmark datasets. It also demonstrates the ability of zero-shot transfer for fine-grained object attribute classification and cross-modal retrieval. We hope this dataset can support the advancement of VLMs for various multi-modal tasks in remote sensing, such as open-vocabulary classification, retrieval, captioning, and text-to-image synthesis.",
                "authors": "Zhecheng Wang, R. Prabha, Tianyuan Huang, Jiajun Wu, Ram Rajagopal",
                "citations": 31
            },
            {
                "title": "RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model",
                "abstract": "Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we tried several Parameter-Efficient Fine-Tuning methods on RS5M to implement the DFM. Experimental results show that our proposed dataset is highly effective for various tasks, improving upon the baseline by 8% „ 16% in zero-shot classification tasks, and obtaining good results in both Vision-Language Retrieval and Semantic Localization tasks. https",
                "authors": "Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin",
                "citations": 30
            },
            {
                "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
                "abstract": "Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. When integrated into an embodied agent, existing embodied VLM works either output detailed action sequences at the manipulation level or only provide plans at an abstract level, leaving a gap between high-level planning and real-world manipulation. To bridge this gap, we introduce Octopus, an embodied vision-language programmer that uses executable code generation as a medium to connect planning and manipulation. Octopus is designed to 1) proficiently comprehend an agent's visual and textual task objectives, 2) formulate intricate action sequences, and 3) generate executable code. To facilitate Octopus model development, we introduce OctoVerse: a suite of environments tailored for benchmarking vision-based code generators on a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games such as Grand Theft Auto (GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an explorative agent that generates training data, i.e., action blueprints and corresponding executable code. We also collect feedback that enables an enhanced training scheme called Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's functionality and present compelling results, showing that the proposed RLEF refines the agent's decision-making. By open-sourcing our simulation environments, dataset, and model architecture, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.",
                "authors": "Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu",
                "citations": 31
            },
            {
                "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
                "abstract": "Given an image and a target modification (e.g an image of the Eiffel tower and the text\"without people and at night-time\"), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we propose to tackle CIR in a training-free manner via our Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods. Moreover, the modularity of CIReVL offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results. Finally, we show that CIReVL makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases. Code will be released upon acceptance.",
                "authors": "Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, Zeynep Akata",
                "citations": 30
            },
            {
                "title": "Learning without Forgetting for Vision-Language Models",
                "abstract": "Class-Incremental Learning (CIL) or continual learning is a desired capability in the real world, which requires a learning system to adapt to new tasks without forgetting former ones. While traditional CIL methods focus on visual information to grasp core features, recent advances in Vision-Language Models (VLM) have shown promising capabilities in learning generalizable representations with the aid of textual information. However, when continually trained with new classes, VLMs often suffer from catastrophic forgetting of former knowledge. Applying VLMs to CIL poses two major challenges: 1) how to adapt the model without forgetting; and 2) how to make full use of the multi-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that enables VLMs to learn without forgetting. To handle the first challenge, we propose training task-specific projections based on the frozen image/text encoders. When facing new tasks, new projections are expanded and former projections are fixed, alleviating the forgetting of old concepts. For the second challenge, we propose the fusion module to better utilize the cross-modality information. By jointly adjusting visual and textual features, the model can capture semantic information with stronger representation ability. Extensive experiments on nine benchmark datasets validate PROOF achieves state-of-the-art performance.",
                "authors": "Da-Wei Zhou, Yuanhan Zhang, Jingyi Ning, Han-Jia Ye, De-chuan Zhan, Ziwei Liu",
                "citations": 29
            },
            {
                "title": "Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment",
                "abstract": "We introduce a method to train vision-language models for remote-sensing images without using any textual annotations. Our key insight is to use co-located internet imagery taken on the ground as an intermediary for connecting remote-sensing images and language. Specifically, we train an image encoder for remote sensing images to align with the image encoder of CLIP using a large amount of paired internet and satellite images. Our unsupervised approach enables the training of a first-of-its-kind large-scale vision language model (VLM) for remote sensing images at two different resolutions. We show that these VLMs enable zero-shot, open-vocabulary image classification, retrieval, segmentation and visual question answering for satellite images. On each of these tasks, our VLM trained without textual annotations outperforms existing VLMs trained with supervision, with gains of up to 20% for classification and 80% for segmentation.",
                "authors": "Utkarsh Mall, Cheng Perng Phoo, Meilin Kelsey Liu, Carl Vondrick, B. Hariharan, Kavita Bala",
                "citations": 24
            },
            {
                "title": "WonderJourney: Going from Anywhere to Everywhere",
                "abstract": "We introduce WonderJourney, a modular framework for perpetual 3D scene generation. Unlike prior work on view generation that focuses on a single type of scenes, we start at any user-provided location (by a text description or an image), and generate a journey through a long sequence of diverse yet coherently connected 3D scenes. We leverage an LLM to generate textual descriptions of the scenes in this journey, a text-driven point cloud generation pipeline to make a compelling and coherent sequence of 3D scenes, and a large VLM to verify the generated scenes. We show compelling, diverse visual results across various scene types and styles, forming imaginary “wonder journeys ”. Project website: https://kovenyu.com/WonderJourney/. “No, no! The adventures first, explanations take such a dreadful time.“ - Alice's Adventures in Wonderland",
                "authors": "Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T. Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, Charles Herrmann",
                "citations": 26
            },
            {
                "title": "Language Reward Modulation for Pretraining Reinforcement Learning",
                "abstract": "Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\\textbf{LA}$nguage Reward $\\textbf{M}$odulated $\\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with reinforcement learning to acquire a language-conditioned, pretrained policy. Our VLM pretraining approach, which is a departure from previous attempts to use LRFs, can warmstart sample-efficient learning on robot manipulation tasks in RLBench.",
                "authors": "Ademi Adeniji, Amber Xie, Carmelo Sferrazza, Younggyo Seo, Stephen James, P. Abbeel",
                "citations": 22
            },
            {
                "title": "RS5M and GeoRSCLIP: A Large-Scale Vision- Language Dataset and a Large Vision-Language Model for Remote Sensing",
                "abstract": "Pretrained vision-language models (VLMs) utilizing extensive image–text paired data have demonstrated unprecedented image–text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pretrained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this article, we present an image–text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image–text paired datasets and captioning label-only RS datasets with pretrained VLM. These constitute the first large-scale RS image–text paired dataset. Additionally, we present GeoRSCLIP by fine-tuning (FT) or applying parameter-efficient FT (PEFT) methods to the CLIP model using RS5M. Experimental results show that our proposed dataset is highly effective for various tasks, and our model GeoRSCLIP improves upon the baseline or previous state-of-the-art model by 3%–20% in zero-shot classification (ZSC) tasks, 3%–6% in RS cross-modal text–image retrieval (RSCTIR) and 4%–5% in semantic localization (SeLo) tasks. Dataset and models have been released in: https://github.com/om-ai-lab/RS5M.",
                "authors": "Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin",
                "citations": 20
            },
            {
                "title": "CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor",
                "abstract": "Existing open-vocabulary image segmentation methods re-quire a fine-tuning step on mask labels and/or image-text datasets. Mask labels are labor-intensive, which limits the number of categories in segmentation datasets. Con-sequently, the vocabulary capacity of pre-trained VLMs is severely reduced after fine-tuning. However, without fine-tuning, VLMs trained under weak image-text supervision tend to make suboptimal mask predictions. To alleviate these issues, we introduce a novel recurrent framework that progressively filters out irrelevant texts and enhances mask quality without training efforts. The recurrent unit is a two-stage segmenter built upon a frozen VLM. Thus, our model retains the VLM's broad vocabulary space and equips it with segmentation ability. Experiments show that our method outperforms not only the training-free counter-parts, but also those fine-tuned with millions of data sam-ples, and sets the new state-of-the-art records for both zero-shot semantic and referring segmentation. Concretely, we improve the current record by 28.8, 16.0, and 6.9 mloU on Pascal VOC, COCO Object, and Pascal Context.",
                "authors": "Shuyang Sun, Runjia Li, Philip Torr, Xiuye Gu, Siyang Li",
                "citations": 20
            },
            {
                "title": "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models",
                "abstract": "We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a preexisting vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as \"ideal words\" for generating concepts directly within embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP’s embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic operations on embedding vectors can be used as compositional and interpretable methods for regulating the behavior of VLMs.",
                "authors": "Matthew Trager, Pramuditha Perera, L. Zancato, A. Achille, Parminder Bhatia, S. Soatto",
                "citations": 21
            },
            {
                "title": "Measuring Progress in Fine-grained Vision-and-Language Understanding",
                "abstract": "While pretraining on large-scale image–text data from the Web has facilitated rapid progress on many vision-and-language (V&L) tasks, recent work has demonstrated that pretrained models lack “fine-grained” understanding, such as the ability to recognise relationships, verbs, and numbers in images. This has resulted in an increased interest in the community to either develop new benchmarks or models for such capabilities. To better understand and quantify progress in this direction, we investigate four competitive V&L models on four fine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al., 2022) consistently outperforms other baselines, and that modelling innovations can impact performance more than scaling Web data, which even degrades performance sometimes. Through a deeper investigation of X-VLM, we highlight the importance of both novel losses and rich data sources for learning fine-grained skills. Finally, we inspect training dynamics, and discover that for some tasks, performance peaks early in training or significantly fluctuates, never converging.",
                "authors": "Emanuele Bugliarello, Laurent Sartran, Aishwarya Agrawal, Lisa Anne Hendricks, Aida Nematzadeh",
                "citations": 21
            },
            {
                "title": "Distilling Internet-Scale Vision-Language Models into Embodied Agents",
                "abstract": "Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.",
                "authors": "T. Sumers, Kenneth Marino, Arun Ahuja, R. Fergus, Ishita Dasgupta",
                "citations": 20
            },
            {
                "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
                "abstract": "While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals. Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics. On the other hand, training an embodied agent in a noisy visual world without expert guidance is often chal-lenging and inefficient. In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world. Specifically, we distill LLM's reflection outcomes (improved actions by analyzing mistakes) in a text world's tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dy-namics. Such cross-modality imitation learning between the two parallel worlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert. Extensive evaluations on the ALFWorld benchmark's diverse tasks highlight EMMA's superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement in the success rate.",
                "authors": "Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, Yuhui Shi",
                "citations": 20
            },
            {
                "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
                "abstract": "Solving complex visual tasks such as “Who invented the musical instrument on the right?” involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge. Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models. However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incor-rect outputs. Moreover, they require loading multiple models, incurring high latency and computation costs. We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) ca-pable of solving complex visual tasks with a single forward pass. VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM. Exten-sive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally. Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes. An evaluation with human annotators also confirms that VPD improves model response factuality and consistency. Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data.",
                "authors": "Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, K. Hata, Enming Luo, Ranjay Krishna, Ariel Fuxman",
                "citations": 17
            },
            {
                "title": "ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation",
                "abstract": "Large-scale pre-trained vision-language models (VLM) such as CLIP [32] have demonstrated noteworthy zero-shot classification capability, achieving 76.3% top-1 accuracy on ImageNet without seeing any examples. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, a novel source-free domain adaptation method for VLMs, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels. Then, it deploys cross-modality self-training with the pseudo labels to update visual and text encoders, refine labels and reduce domain gaps and misalignment iteratively. With extensive experiments, we show that ReCLIP outperforms all the baselines significantly and improves the average accuracy of CLIP from 69.83% to 74.94% on 22 image classification benchmarks.",
                "authors": "Xuefeng Hu, Ke Zhang, Lu Xia, Albert Y. C. Chen, Jiajia Luo, Yuyin Sun, Ke Min Wang, Nan Qiao, Xiao Zeng, Min Sun, Cheng-Hao Kuo, R. Nevatia",
                "citations": 15
            },
            {
                "title": "Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory",
                "abstract": "Human Object Interaction (HOI) detection aims to localize and infer the relationships between a human and an object. Arguably, training supervised models for this task from scratch presents challenges due to the performance drop over rare classes and the high computational cost and time required to handle long-tailed distributions of HOIs in complex HOI scenes in realistic settings. This observation motivates us to design an HOI detector that can be trained even with long-tailed labeled data and can leverage existing knowledge from pre-trained models. Inspired by the powerful generalization ability of the large Vision-Language Models (VLM) on classification and retrieval tasks, we propose an efficient Adaptive HOI Detector with Concept-guided Memory (ADA-CM). ADA-CM has two operating modes. The first mode makes it tunable without learning new parameters in a training-free paradigm. Its second mode incorporates an instance-aware adapter mechanism that can further efficiently boost performance if updating a lightweight set of parameters can be afforded. Our proposed method achieves competitive results with state-of-the-art on the HICO-DET and V-COCO datasets with much less training time. Code can be found at https://github.com/ltttpku/ADA-CM.",
                "authors": "Ting Lei, Fabian Caba, Qingchao Chen, Hailin Jin, Yuxin Peng, Yang Liu",
                "citations": 14
            },
            {
                "title": "Hierarchical Prompt Learning for Multi-Task Learning",
                "abstract": "Vision-language models (VLMs) can effectively transfer to various vision tasks via prompt learning. Real-world scenarios often require adapting a model to multiple similar yet distinct tasks. Existing methods focus on learning a specific prompt for each task, limiting the ability to exploit potentially shared information from other tasks. Naively training a task-shared prompt using a combination of all tasks ignores fine-grained task correlations. Significant discrepancies across tasks could cause negative transferring. Considering this, we present Hierarchical Prompt (HiPro) learning, a simple and effective method for jointly adapting a pre-trained VLM to multiple downstream tasks. Our method quantifies inter-task affinity and subsequently constructs a hierarchical task tree. Task-shared prompts learned by internal nodes explore the information within the corresponding task group, while task-individual prompts learned by leaf nodes obtain fine-grained information targeted at each task. The combination of hierarchical prompts provides high-quality content of different granularity. We evaluate HiPro on four multi-task learning datasets. The results demonstrate the effectiveness of our method.",
                "authors": "Yajing Liu, Yuning Lu, Hao Liu, Yaozu An, Zhuoran Xu, Zhuokun Yao, B. Zhang, Zhiwei Xiong, Chenguang Gui",
                "citations": 16
            },
            {
                "title": "Rethinking Benchmarks for Cross-modal Image-text Retrieval",
                "abstract": "Image-text retrieval, as a fundamental and important branch of information retrieval, has attracted extensive research attentions. The main challenge of this task is cross-modal semantic understanding and matching. Some recent works focus more on fine-grained cross-modal semantic matching. With the prevalence of large scale multimodal pretraining models, several state-of-the-art models (e.g. X-VLM) have achieved near-perfect performance on widely-used image-text retrieval benchmarks, i.e. MSCOCO-Test-5K and Flickr30K-Test-1K. In this paper, we review the two common benchmarks and observe that they are insufficient to assess the true capability of models on fine-grained cross-modal semantic matching. The reason is that a large amount of images and texts in the benchmarks are coarse-grained. Based on the observation, we renovate the coarse-grained images and texts in the old benchmarks and establish the improved benchmarks called MSCOCO-FG and Flickr30K-FG. Specifically, on the image side, we enlarge the original image pool by adopting more similar images. On the text side, we propose a novel semi-automatic renovation approach to refine coarse-grained sentences into finer-grained ones with little human effort. Furthermore, we evaluate representative image-text retrieval models on our new benchmarks to demonstrate the effectiveness of our method. We also analyze the capability of models on fine-grained semantic comprehension through extensive experiments. The results show that even the state-of-the-art models have much room for improvement in fine-grained semantic understanding, especially in distinguishing attributes of close objects in images. Our code and improved benchmark datasets are publicly available1 which we hope will inspire further in-depth research on cross-modal retrieval.",
                "authors": "Wei Chen, Linli Yao, Qin Jin",
                "citations": 16
            },
            {
                "title": "MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices",
                "abstract": "We present MobileVLM, a competent multimodal vision language model (MMVLM) targeted to run on mobile devices. It is an amalgamation of a myriad of architectural designs and techniques that are mobile-oriented, which comprises a set of language models at the scale of 1.4B and 2.7B parameters, trained from scratch, a multimodal vision model that is pre-trained in the CLIP fashion, cross-modality interaction via an efficient projector. We evaluate MobileVLM on several typical VLM benchmarks. Our models demonstrate on par performance compared with a few much larger models. More importantly, we measure the inference speed on both a Qualcomm Snapdragon 888 CPU and an NVIDIA Jeston Orin GPU, and we obtain state-of-the-art performance of 21.5 tokens and 65.3 tokens per second, respectively. Our code will be made available at: https://github.com/Meituan-AutoML/MobileVLM.",
                "authors": "Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, Chunhua Shen",
                "citations": 16
            },
            {
                "title": "Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models",
                "abstract": "We introduce Dream2Real, a robotics framework which integrates vision-language models (VLMs) trained on 2D data into a 3D object rearrangement pipeline. This is achieved by the robot autonomously constructing a 3D representation of the scene, where objects can be rearranged virtually and an image of the resulting arrangement rendered. These renders are evaluated by a VLM, so that the arrangement which best satisfies the user instruction is selected and recreated in the real world with pick-and-place. This enables language-conditioned rearrangement to be performed zero-shot, without needing to collect a training dataset of example arrangements. Results on a series of real-world tasks show that this framework is robust to distractors, controllable by language, capable of understanding complex multi-object relations, and readily applicable to both tabletop and 6-DoF rearrangement tasks. Videos are available on our webpage at: https://www.robot-learning.uk/dream2real.",
                "authors": "Ivan Kapelyukh, Yifei Ren, Ignacio Alzugaray, Edward Johns",
                "citations": 14
            },
            {
                "title": "Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!",
                "abstract": "Finetuning a large vision language model (VLM) on a target dataset after large scale pretraining is a dominant paradigm in visual question answering (VQA). Datasets for specialized tasks such as knowledge-based VQA or VQA in non natural-image domains are orders of magnitude smaller than those for general-purpose VQA. While collecting additional labels for specialized tasks or domains can be challenging, unlabeled images are often available. We introduce SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets. SelTDA uses the VLM and target dataset to build a teacher model that can generate question-answer pseudolabels directly conditioned on an image alone, allowing us to pseudolabel unlabeled images. SelTDA then finetunes the initial VLM on the original dataset augmented with freshly pseudolabeled images. We describe a series of experiments showing that our self-taught data augmentation increases robustness to adversarially searched questions, counterfactual examples and rephrasings, improves domain generalization, and results in greater retention of numerical reasoning skills. The proposed strategy requires no additional annotations or architectural modifications, and is compatible with any modern encoder-decoder multimodal transformer. Code available at https://github.com/codezakh/SelTDA.",
                "authors": "Zaid Khan, B. Vijaykumar, S. Schulter, Xiang Yu, Y. Fu, Manmohan Chandraker",
                "citations": 14
            },
            {
                "title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
                "abstract": "Pre-trained and frozen large language models (LLMs) can effectively map simple scene rearrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for TfD. Our models, code, and video results can be found in our project's website: https://helper-agent-llm.github.io.",
                "authors": "Gabriel Sarch, Yue Wu, Michael J. Tarr, Katerina Fragkiadaki",
                "citations": 14
            },
            {
                "title": "Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study",
                "abstract": "The large-scale pre-trained vision language models (VLM) have shown remarkable domain transfer capability on natural images. However, it remains unknown whether this capability can also apply to the medical image domain. This paper thoroughly studies the knowledge transferability of pre-trained VLMs to the medical domain, where we show that well-designed medical prompts are the key to elicit knowledge from pre-trained VLMs. We demonstrate that by prompting with expressive attributes that are shared between domains, the VLM can carry the knowledge across domains and improve its generalization. This mechanism empowers VLMs to recognize novel objects with fewer or without image samples. Furthermore, to avoid the laborious manual designing process, we develop three approaches for automatic generation of medical prompts, which can inject expert-level medical knowledge and image-specific information into the prompts for fine-grained grounding. We conduct extensive experiments on thirteen different medical datasets across various modalities, showing that our well-designed prompts greatly improve the zero-shot performance compared to the default prompts, and our fine-tuned models surpass the supervised models by a significant margin.",
                "authors": "Ziyuan Qin, Huahui Yi, Qicheng Lao, Kang Li",
                "citations": 51
            },
            {
                "title": "Towards Evaluating Generalist Agents: An Automated Benchmark in Open World",
                "abstract": "Evaluating generalist agents presents significant challenges due to their wide-ranging abilities and the limitations of current benchmarks in assessing true generalization. We introduce the Minecraft Universe (MCU), a fully automated benchmarking framework set within the open-world game Minecraft. MCU dynamically generates and evaluates a broad spectrum of tasks, offering three core components: 1) a task generation mechanism that provides high degrees of freedom and variability, 2) an ever-expanding set of over 3K composable atomic tasks, and 3) a general evaluation framework that supports open-ended task assessment. By integrating large language models (LLMs), MCU dynamically creates diverse environments for each evaluation, fostering agent generalization. The framework uses a vision-language model (VLM) to automatically generate evaluation criteria, achieving over 90% agreement with human ratings across multi-dimensional assessments, which demonstrates that MCU is a scalable and explainable solution for evaluating generalist agents. Additionally, we show that while state-of-the-art foundational models perform well on specific tasks, they often struggle with increased task diversity and difficulty.",
                "authors": "Haowei Lin, Zihao Wang, Jianzhu Ma, Yitao Liang",
                "citations": 13
            },
            {
                "title": "DST-Det: Simple Dynamic Self-Training for Open-Vocabulary Object Detection",
                "abstract": "Open-vocabulary object detection (OVOD) aims to detect the objects beyond the set of classes observed during training. This work introduces a straightforward and efficient strategy that utilizes pre-trained vision-language models (VLM), like CLIP, to identify potential novel classes through zero-shot classification. Previous methods use a class-agnostic region proposal network to detect object proposals and consider the proposals that do not match the ground truth as background. Unlike these methods, our method will select a subset of proposals that will be considered as background during the training. Then, we treat them as novel classes during training. We refer to this approach as the self-training strategy, which enhances recall and accuracy for novel classes without requiring extra annotations, datasets, and re-training. Compared to previous pseudo methods, our approach does not require re-training and offline labeling processing, which is more efficient and effective in one-shot training. Empirical evaluations on three datasets, including LVIS, V3Det, and COCO, demonstrate significant improvements over the baseline performance without incurring additional parameters or computational costs during inference. In addition, we also apply our method to various baselines. In particular, compared with the previous method, F-VLM, our method achieves a 1.7% improvement on the LVIS dataset. Combined with the recent method CLIPSelf, our method also achieves 46.7 novel class AP on COCO without introducing extra data for pertaining. We also achieve over 6.5% improvement over the F-VLM baseline in the recent challenging V3Det dataset. We release our code and models at https://github.com/xushilin1/dst-det.",
                "authors": "Shilin Xu, Xiangtai Li, Size Wu, Wenwei Zhang, Yining Li, Guangliang Cheng, Yunhai Tong, Kai Chen, Chen Change Loy",
                "citations": 12
            },
            {
                "title": "GIA Model Statistics for GRACE Hydrology, Cryosphere, and Ocean Science",
                "abstract": "We provide a new analysis of glacial isostatic adjustment (GIA) with the goal of assembling the model uncertainty statistics required for rigorously extracting trends in surface mass from the Gravity Recovery and Climate Experiment (GRACE) mission. Such statistics are essential for deciphering sea level, ocean mass, and hydrological changes because the latter signals can be relatively small (≤2 mm/yr water height equivalent) over very large regions, such as major ocean basins and watersheds. With abundant new >7 year continuous measurements of vertical land motion (VLM) reported by Global Positioning System stations on bedrock and new relative sea level records, our new statistical evaluation of GIA uncertainties incorporates Bayesian methodologies. A unique aspect of the method is that both the ice history and 1‐D Earth structure vary through a total of 128,000 forward models. We find that best fit models poorly capture the statistical inferences needed to correctly invert for lower mantle viscosity and that GIA uncertainty exceeds the uncertainty ascribed to trends from 14 years of GRACE data in polar regions.",
                "authors": "L. Caron, E. Ivins, E. Larour, S. Adhikari, J. Nilsson, G. Blewitt",
                "citations": 168
            },
            {
                "title": "LOVM: Language-Only Vision Model Selection",
                "abstract": "Pre-trained multi-modal vision-language models (VLMs) are becoming increasingly popular due to their exceptional performance on downstream vision applications, particularly in the few- and zero-shot settings. However, selecting the best-performing VLM for some downstream applications is non-trivial, as it is dataset and task-dependent. Meanwhile, the exhaustive evaluation of all available VLMs on a novel application is not only time and computationally demanding but also necessitates the collection of a labeled dataset for evaluation. As the number of open-source VLM variants increases, there is a need for an efficient model selection strategy that does not require access to a curated evaluation dataset. This paper proposes a novel task and benchmark for efficiently evaluating VLMs' zero-shot performance on downstream applications without access to the downstream task dataset. Specifically, we introduce a new task LOVM: Language-Only Vision Model Selection, where methods are expected to perform both model selection and performance prediction based solely on a text description of the desired downstream application. We then introduced an extensive LOVM benchmark consisting of ground-truth evaluations of 35 pre-trained VLMs and 23 datasets, where methods are expected to rank the pre-trained VLMs and predict their zero-shot performance.",
                "authors": "O. Zohar, Shih-Cheng Huang, Kuan Wang, Serena Yeung",
                "citations": 10
            },
            {
                "title": "SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language Guidance",
                "abstract": "In semi-supervised semantic segmentation, a model is trained with a limited number of labeled images along with a large corpus of unlabeled images to reduce the high annotation effort. While previous methods are able to learn good segmentation boundaries, they are prone to confuse classes with similar visual appearance due to the limited supervision. On the other hand, vision-language models (VLMs) are able to learn diverse semantic knowledge from image-caption datasets but produce noisy segmentation due to the image-level training. In SemiVL, we propose to integrate rich priors from VLM pre-training into semi-supervised semantic segmentation to learn better semantic decision boundaries. To adapt the VLM from global to local reasoning, we introduce a spatial fine-tuning strategy for label-efficient learning. Further, we design a language-guided decoder to jointly reason over vision and language. Finally, we propose to handle inherent ambiguities in class labels by providing the model with language guidance in the form of class definitions. We evaluate SemiVL on 4 semantic segmentation datasets, where it significantly outperforms previous semi-supervised methods. For instance, SemiVL improves the state-of-the-art by +13.5 mIoU on COCO with 232 annotated images and by +6.1 mIoU on Pascal VOC with 92 labels. Project page: https://github.com/google-research/semivl",
                "authors": "Lukas Hoyer, D. Tan, Muhammad Ferjad Naeem, L. V. Gool, F. Tombari",
                "citations": 11
            },
            {
                "title": "Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models",
                "abstract": "One fascinating aspect of pre-trained vision-language models~(VLMs) learning under language supervision is their impressive zero-shot generalization capability. However, this ability is hindered by distribution shifts between the training and testing data. Previous test time adaptation~(TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions. In this work, we propose TTA with feedback to rectify the model output and prevent the model from becoming blindly confident. Specifically, a CLIP model is adopted as the reward model during TTA and provides feedback for the VLM. Given a single test sample, the VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution. The proposed \\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly flexible and universal. Beyond the classification task, with task-specific sampling strategies and a proper reward baseline choice, RLCF can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning, improving the zero-shot generalization capacity of VLMs. According to the characteristics of these VL tasks, we build different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs. Extensive experiments along with promising empirical results demonstrate the effectiveness of RLCF. The code is available at https://github.com/mzhaoshuai/RLCF.",
                "authors": "Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yezhou Yang",
                "citations": 11
            },
            {
                "title": "ASCO-OFDM based VLC system throughput improvement using PAPR precoding reduction techniques",
                "abstract": null,
                "authors": "Sara M. Farid, Mona Z. Saleh, Hesham M. Elbadawy, Salwa H. Elramly",
                "citations": 10
            },
            {
                "title": "ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations",
                "abstract": "Multimodal Vision-Language Models (VLMs) enable powerful applications from their fused understanding of images and language, but many perform poorly on UI tasks due to the lack of UI training data. In this paper, we adapt a recipe for generating paired text-image training data for VLMs to the UI domain by combining existing pixel-based methods with a Large Language Model (LLM). Unlike prior art, our method requires no human-provided annotations, and it can be applied to any dataset of UI screenshots. We generate a dataset of 335K conversational examples paired with UIs that cover Q&A, UI descriptions, and planning, and use it to fine-tune a conversational VLM for UI tasks. To assess the performance of our model, we benchmark it on UI element detection tasks, evaluate response quality, and showcase its applicability to multi-step UI navigation and planning.",
                "authors": "Yue Jiang, E. Schoop, Amanda Swearngin, Jeffrey Nichols",
                "citations": 10
            },
            {
                "title": "Distilling DETR with Visual-Linguistic Knowledge for Open-Vocabulary Object Detection",
                "abstract": "Current methods for open-vocabulary object detection (OVOD) rely on a pre-trained vision-language model (VLM) to acquire the recognition ability. In this paper, we propose a simple yet effective framework to Distill the Knowledge from the VLM to a DETR-like detector, termed DK-DETR. Specifically, we present two ingenious distillation schemes named semantic knowledge distillation (SKD) and relational knowledge distillation (RKD). To utilize the rich knowledge from the VLM systematically, SKD transfers the semantic knowledge explicitly, while RKD exploits implicit relationship information between objects. Furthermore, a distillation branch including a group of auxiliary queries is added to the detector to mitigate the negative effect on base categories. Equipped with SKD and RKD on the distillation branch, DK-DETR improves the detection performance of novel categories significantly and avoids disturbing the detection of base categories. Extensive experiments on LVIS and COCO datasets show that DK-DETR surpasses existing OVOD methods under the setting that the base-category supervision is solely available. The code and models are available at https://github.com/hikvision-research/opera.",
                "authors": "Liangqi Li, Jiaxu Miao, Dahu Shi, Wenming Tan, Ye Ren, Yi Yang, Shiliang Pu",
                "citations": 10
            },
            {
                "title": "Vision-Language Models for Zero-Shot Classification of Remote Sensing Images",
                "abstract": "Zero-shot classification presents a challenge since it necessitates a model to categorize images belonging to classes it has not encountered during its training phase. Previous research in the field of remote sensing (RS) has explored this task by training image-based models on known RS classes and then attempting to predict the outcomes for unfamiliar classes. Despite these endeavors, the outcomes have proven to be less than satisfactory. In this paper, we propose an alternative approach that leverages vision-language models (VLMs), which have undergone pre-training to grasp the associations between general computer vision image-text pairs in diverse datasets. Specifically, our investigation focuses on thirteen VLMs derived from Contrastive Language-Image Pre-Training (CLIP/Open-CLIP) with varying levels of parameter complexity. In our experiments, we ascertain the most suitable prompt for RS images to query the language capabilities of the VLM. Furthermore, we demonstrate that the accuracy of zero-shot classification, particularly when using large CLIP models, on three widely recognized RS scene datasets yields superior results compared to existing RS solutions.",
                "authors": "Mohamad Mahmoud Al Rahhal, Y. Bazi, Hebah Elgibreen, M. Zuair",
                "citations": 10
            },
            {
                "title": "TCP: Textual-based Class-aware Prompt tuning for Visual-Language Model",
                "abstract": "Prompt tuning represents a valuable technique for adapting pre-trained visual-language models (VLM) to various downstream tasks. Recent advancements in CoOp-based methods propose a set of learnable domain-shared or image-conditional textual tokens to facilitate the generation of task-specific textual classifiers. However, those textual tokens have a limited generalization ability regarding unseen domains, as they cannot dynamically adjust to the distribution of testing classes. To tackle this issue, we present a novel Textual-based Class-aware Prompt tuning(TCP) that explicitly incorporates prior knowledge about classes to enhance their discriminability. The critical concept of TCP involves leveraging Textual Knowledge Embedding (TKE) to map the high generalizability of class-level textual knowledge into class-aware textual tokens. By seamlessly integrating these class-aware prompts into the Text Encoder, a dynamic class-aware classifier is generated to enhance discriminability for unseen domains. During inference, TKE dynamically generates class-aware prompts related to the unseen classes. Comprehensive evaluations demonstrate that TKE serves as a plug-and-play module effortlessly combinable with existing methods. Furthermore, TCP consistently achieves superior performance while demanding less training time. Code:https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/",
                "authors": "Hantao Yao, Rui Zhang, Changsheng Xu",
                "citations": 9
            },
            {
                "title": "Vision-Language Modelling For Radiological Imaging and Reports In The Low Data Regime",
                "abstract": "This paper explores training medical vision-language models (VLMs) -- where the visual and language inputs are embedded into a common space -- with a particular focus on scenarios where training data is limited, as is often the case in clinical datasets. We explore several candidate methods to improve low-data performance, including: (i) adapting generic pre-trained models to novel image and text domains (i.e. medical imaging and reports) via unimodal self-supervision; (ii) using local (e.g. GLoRIA)&global (e.g. InfoNCE) contrastive loss functions as well as a combination of the two; (iii) extra supervision during VLM training, via: (a) image- and text-only self-supervision, and (b) creating additional positive image-text pairs for training through augmentation and nearest-neighbour search. Using text-to-image retrieval as a benchmark, we evaluate the performance of these methods with variable sized training datasets of paired chest X-rays and radiological reports. Combined, they significantly improve retrieval compared to fine-tuning CLIP, roughly equivalent to training with the data. A similar pattern is found in the downstream task classification of CXR-related conditions with our method outperforming CLIP and also BioVIL, a strong CXR VLM benchmark, in the zero-shot and linear probing settings. We conclude with a set of recommendations for researchers aiming to train vision-language models on other medical imaging modalities when training data is scarce. To facilitate further research, we will make our code and models publicly available.",
                "authors": "Rhydian Windsor, A. Jamaludin, T. Kadir, Andrew Zisserman",
                "citations": 9
            },
            {
                "title": "InstructDET: Diversifying Referring Object Detection with Generalized Instructions",
                "abstract": "We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are from foundation models. Our InDET is developed from existing REC datasets and object detection datasets, with the expanding potential that any image with object bbxs can be incorporated through using our InstructDET method. By using our InDET dataset, we show that a conventional ROD model surpasses existing methods on standard REC datasets and our InDET test set. Our data-centric method InstructDET, with automatic data expansion by leveraging foundation models, directs a promising field that ROD can be greatly diversified to execute common object detection instructions.",
                "authors": "Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Qi Chen, Feng Zhu, Rui Zhao, Yibing Song",
                "citations": 9
            },
            {
                "title": "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning",
                "abstract": "Pre-trained vision-language models (VLMs) have achieved impressive results in a range of vision-language tasks. However, popular VLMs usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and deployment in real-world applications due to space, memory, and latency constraints. In this work, we introduce a distilling then pruning framework to compress large vision-language models into smaller, faster, and more accurate ones. We first shrink the size of a pre-trained large VLM and apply knowledge distillation in the vision-language pre-training stage to obtain a task-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm to automatically infer the importance of vision and language modalities for different downstream tasks and adaptively remove redundant structures and neurons in different encoders with controllable target sparsity. We apply our framework to train EfficientVLM, a fast and accurate vision-language model consisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers, accounting for only 93 million parameters in total, which is 44.3% of the teacher model. EfficientVLM retains 98.4% performance of the teacher model and accelerates its inference speed by 2.2x. EfficientVLM achieves a large absolute improvement over previous SoTA efficient VLMs of similar sizes by a large margin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2 (+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation (CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.",
                "authors": "Tiannan Wang, Wangchunshu Zhou, Yan Zeng, Xinsong Zhang",
                "citations": 28
            },
            {
                "title": "Zero-Shot Video Moment Retrieval from Frozen Vision-Language Models",
                "abstract": "Accurate video moment retrieval (VMR) requires universal visual-textual correlations that can handle unknown vocabulary and unseen scenes. However, the learned correlations are likely either biased when derived from a limited amount of moment-text data which is hard to scale up because of the prohibitive annotation cost (fully-supervised), or unreliable when only the video-text pairwise relationships are available without fine-grained temporal annotations (weakly-supervised). Recently, the vision-language models (VLM) demonstrate a new transfer learning paradigm to benefit different vision tasks through the universal visual-textual correlations derived from large-scale vision-language pairwise web data, which has also shown benefits to VMR by fine-tuning in the target domains.In this work, we propose a zero-shot method for adapting generalisable visual-textual priors from arbitrary VLM to facilitate moment-text alignment, without the need for accessing the VMR data. To this end, we devise a conditional feature refinement module to generate boundary-aware visual features conditioned on text queries to enable better moment boundary understanding. Additionally, we design a bottom-up proposal generation strategy that mitigates the impact of domain discrepancies and breaks down complex-query retrieval tasks into individual action retrievals, thereby maximizing the benefits of VLM. Extensive experiments conducted on three VMR benchmark datasets demonstrate the notable performance advantages of our zero-shot algorithm, especially in the novel-word and novel-location out-of-distribution setups.",
                "authors": "Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, Yang Liu",
                "citations": 8
            },
            {
                "title": "LONG DISTANCE LAB AFFAIRS: PHYSICS ACHIEVEMENT AND METACOGNITION EFFECTS OF DISTANCE LABORATORIES IN A SENIOR HIGH SCHOOL IN THE PHILIPPINES",
                "abstract": "Due to the necessity to continue learning even during the pandemic, schools opened utilizing distance learning modalities. However, there is a dearth of evidence on the effectivity of this modalities in physics. In this study, we investigated the effects of three physics distance learning modes; the module-only (MO), virtual lab plus module (VLM), and the physical lab plus module (PLM) classes in physics achievement and metacognition employing the pretest-posttest and repeated measures research designs. All learning modules used were in digital formats sent through free messaging platforms. Analysis of data includes paired samples t-test, one-way ANOVA, repeated measures ANOVA, and independent samples t-test. Results revealed that all three distance learning modes have significantly higher post-test than pre-test scores. Further analysis showed, however, that only VLM had significantly higher gain scores than MO. Initially, at pre-MO and post-MO administrations, male students had significantly higher metacognition but this diminished after they perform both virtual and physical labs. It was in post-PLM where students have significantly better metacognition than pre-MO and post-MO. This study showed that not only do physical and virtual labs supplement distance modular learning, they are also complementary that both must be used in distance learning.",
                "authors": "",
                "citations": 2
            },
            {
                "title": "Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding",
                "abstract": "Vision language models (VLM) have demonstrated re-markable performance across various downstream tasks. However, understanding fine-grained visual-linguistic con-cepts, such as attributes and inter-object relationships, re-mains a significant challenge. While several benchmarks aim to evaluate VLMs in finer granularity, their primary fo-cus remains on the linguistic aspect, neglecting the visual dimension. Here, we highlight the importance of evaluating VLMs from both a textual and visual perspective. We intro-duce a progressive pipeline to synthesize images that vary in a specific attribute while ensuring consistency in all other aspects. Utilizing this data engine, we carefully design a benchmark, SPEC, to diagnose the comprehension of object size, position, existence, and count. Subsequently, we con-duct a thorough evaluation offour leading VLMs on SPEC. Surprisingly, their performance is close to random guess, revealing significant limitations. With this in mind, we pro-pose a simple yet effective approach to optimize VLMs in fine- grained understanding, achieving significant improve-ments on SPEC without compromising the zero-shot performance. Results on two additional fine-grained benchmarks also show consistent improvements, further validating the transferability of our approach. Code and data are available at https://github.com/wjpoom/SPEC.",
                "authors": "Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, Zuxuan Wu",
                "citations": 8
            },
            {
                "title": "Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Fine-grained Understanding",
                "abstract": "Current Vision and Language Models (VLMs) demonstrate strong performance across various vision-language tasks, yet they struggle with ﬁne-grained understanding. This issue stems from weak image-caption alignment in pretraining datasets and a simpliﬁed contrastive objective that fails to distinguish nuanced grounding elements such as relations, actions, and attributes. As a result, the models tend to learn bag-of-words representations. To mitigate these challenges, we introduce an intra-modal contrastive loss and a unique cross-modal rank loss with an adaptive threshold that serves as curriculum learning, utilizing our automatically generated hard negatives to augment the model’s capacity. Our strategy, which does not necessitate additional annotations or parameters, can be incorporated into any VLM trained with an image-text contrastive loss. Upon application to CLIP, our method leads to signiﬁcant improvements on three ﬁne-grained benchmarks, and it also enhances the performance of X-VLM, which is the state-of-art moodel on ﬁne-grained reasoning. 1",
                "authors": "Le Zhang, Rabiul Awal, Aishwarya Agrawal",
                "citations": 6
            },
            {
                "title": "Retrieval-based Video Language Model for Efficient Long Video Question Answering",
                "abstract": "The remarkable natural language understanding, reasoning, and generation capabilities of large language models (LLMs) have made them attractive for application to video question answering (Video QA) tasks, utilizing video tokens as contextual input. However, employing LLMs for long video understanding presents significant challenges and remains under-explored. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video QA process. To address these issues, we introduce a simple yet effective retrieval-based video language model (R-VLM) for efficient and interpretable long video QA. Specifically, given a question (query) and a long video, our model identifies and selects the most relevant $K$ video chunks and uses their associated visual tokens to serve as context for the LLM inference. This effectively reduces the number of video tokens, eliminates noise interference, and enhances system performance. Our experimental results validate the effectiveness of our framework for comprehending long videos. Furthermore, based on the retrieved chunks, our model is interpretable that provides the justifications on where we get the answers.",
                "authors": "Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu",
                "citations": 6
            },
            {
                "title": "PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation",
                "abstract": "This paper proposes a cross-modal distillation frame-work, PartDistill, which transfers 2D knowledge from vision-language models (VLMs) to facilitate 3D shape part segmentation. PartDistill addresses three major challenges in this task: the lack of 3D segmentation in invisible or undetected regions in the 2D projections, inconsistent 2D predictions by VLMs, and the lack of knowledge accumu-lation across different 3D shapes. PartDistill consists of a teacher network that uses a VLM to make 2D predictions and a student network that learns from the 2D pre-dictions while extracting geometrical features from multi-ple 3D shapes to carry out 3D part segmentation. A bi-directional distillation, including forward and backward distillations, is carried out within the framework, where the former forward distills the 2D predictions to the student net-work, and the latter improves the quality of the 2D predictions, which subsequently enhances the final 3D segmen-tation. Moreover, PartDistill can exploit generative mod-els that facilitate effortless 3D shape creation for generating knowledge sources to be distilled. Through extensive experiments, PartDistill boosts the existing methods with substantial margins on widely used ShapeNetPart and Part-NetE datasets, by more than 15% and 12% higher mIoU scores, respectively. The code for this work is available at https://github.com/ardianumam/PartDistill.",
                "authors": "Ardian Umam, Cheng-Kun Yang, Min-Hung Chen, Jen-Hui Chuang, Yen-Yu Lin",
                "citations": 6
            },
            {
                "title": "Sea level rise projections up to 2150 in the northern Mediterranean coasts",
                "abstract": "Vertical land movements (VLM) play a crucial role in affecting the sea level rise along the coasts. They need to be estimated and included in the analysis for more accurate Sea Level (SL) projections. Here we focus on the Mediterranean basin characterized by spatially variable rates of VLM that affect the future SL along the coasts. To estimate the VLM rates we used geodetic data from continuous global navigation satellite system stations with time series longer than 4.5 years in the 1996–2023 interval, belonging to Euro-Mediterranean networks and located within 5 km from the coast. Revised SL projections up to the year 2150 are provided at 265 points on a geographical grid and at the locations of 51 tide gauges of the Permanent Service for Mean Sea Level, by including the estimated VLM in the SL projections released by the Intergovernmental Panel on Climate Change (IPCC) in the AR6 Report. Results show that the IPCC projections underestimate future SL along the coasts of the Mediterranean Sea since the effects of tectonics and other local factors were not properly considered. Here we show that revised SL projections at 2100, when compared to the IPCC, show a maximum and minimum differences of 1094 ± 103 mm and −773 ± 106 mm, respectively, with an average value that exceeds by about 80 mm that of the IPCC in the reference Shared Socio-economic Pathways and different global warming levels. Finally, the projections indicate that about 19.000 km2 of the considered Mediterranean coasts will be more exposed to risk of inundation for the next decades, leading to enhanced impacts on the environment, human activities and infrastructures, thus suggesting the need for concrete actions to support vulnerable populations to adapt to the expected SL rise and coastal hazards by the end of this century.",
                "authors": "A. Vecchio, M. Anzidei, E. Serpelloni",
                "citations": 8
            },
            {
                "title": "Learning Domain-Aware Detection Head with Prompt Tuning",
                "abstract": "Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. However, existing methods focus on reducing the domain bias of the detection backbone by inferring a discriminative visual encoder, while ignoring the domain bias in the detection head. Inspired by the high generalization of vision-language models (VLMs), applying a VLM as the robust detection backbone following a domain-aware detection head is a reasonable way to learn the discriminative detector for each domain, rather than reducing the domain bias in traditional methods. To achieve the above issue, we thus propose a novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies the learnable domain-adaptive prompt to generate the dynamic detection head for each domain. Formally, the domain-adaptive prompt consists of the domain-invariant tokens, domain-specific tokens, and the domain-related textual description along with the class label. Furthermore, two constraints between the source and target domains are applied to ensure that the domain-adaptive prompt can capture the domains-shared and domain-specific knowledge. A prompt ensemble strategy is also proposed to reduce the effect of prompt disturbance. Comprehensive experiments over multiple cross-domain adaptation tasks demonstrate that using the domain-adaptive prompt can produce an effectively domain-related detection head for boosting domain-adaptive object detection. Our code is available at https://github.com/Therock90421/DA-Pro.",
                "authors": "Haochen Li, Rui Zhang, Hantao Yao, Xinkai Song, Yifan Hao, Yongwei Zhao, Ling Li, Yunji Chen",
                "citations": 8
            },
            {
                "title": "Video Action Recognition with Attentive Semantic Units",
                "abstract": "Visual-Language Models (VLMs) have significantly advanced video action recognition. Supervised by the semantics of action labels, recent works adapt the visual branch of VLMs to learn video representations. Despite the effectiveness proved by these works, we believe that the potential of VLMs has yet to be fully harnessed. In light of this, we exploit the semantic units (SU) hiding behind the action labels and leverage their correlations with fine-grained items in frames for more accurate action recognition. SUs are entities extracted from the language descriptions of the entire action set, including body parts, objects, scenes, and motions. To further enhance the alignments between visual contents and the SUs, we introduce a multi-region attention module (MRA) to the visual branch of the VLM. The MRA allows the perception of region-aware visual features beyond the original global feature. Our method adaptively attends to and selects relevant SUs with visual features of frames. With a cross-modal decoder, the selected SUs serve to decode spatiotemporal video representations. In summary, the SUs as the medium can boost discriminative ability and transferability. Specifically, in fully-supervised learning, our method achieved 87.8% top-1 accuracy on Kinetics-400. In K=2 few-shot experiments, our method surpassed the previous state-of-the-art by +7.1% and +15.0% on HMDB-51 and UCF-101, respectively.",
                "authors": "Yifei Chen, Dapeng Chen, Ruijin Liu, Hao Li, Wei Peng",
                "citations": 8
            },
            {
                "title": "Reinforcement Learning Friendly Vision-Language Model for Minecraft",
                "abstract": null,
                "authors": "Ziluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang, Zongqing Lu",
                "citations": 8
            },
            {
                "title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation",
                "abstract": "Fine-grained vision-language models (VLM) have been widely used for inter-modality local alignment between the predefined fixed patches and textual words. However, in the medical analysis, lesions exhibit varying sizes and positions, and the fixed patches may cause incomplete representations of lesions. Moreover, these methods provide explain-ability by using heatmaps to show the general potential image areas associated with texts rather than specific regions, making their explanations not explicit and specific enough. To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainabil-ity for the generation process. AdaMatch exploits the fine-grained relation between adaptive patches and words to provide explanations of specific image regions with corresponding words. To capture the abnormal regions of varying sizes and positions, we introduce the Adaptive Patch extraction (AdaPatch) module to acquire the adaptive patches for these regions adaptively. In order to provide explicit explainability for CXR-report generation task, we propose an AdaMatch-based bidirectional large language model for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords for CXR images and ‘keypatches’ for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets prove the effectiveness of our method and its superior performance to existing methods.",
                "authors": "Wenting Chen, Linlin Shen, Xiang Li, Yixuan Yuan",
                "citations": 6
            },
            {
                "title": "Conceptual Design and Optimization of Distributed Electric Propulsion General Aviation Aircraft",
                "abstract": "The interaction between the slipstream of the propellers and the wing of an aircraft with distributed electric propulsion (DEP) could benefit aerodynamics. A conceptual design and optimization are carried out in order to increase the range of an electric general aviation aircraft without affecting its takeoff and landing velocity in the same fuselage condition. Propellers are modelled using the actuator disk (AD) theory, and the aircraft is modelled using the vortex lattice method (VLM) to obtain DEP aircraft’s aerodynamics in conceptual design. The DIRECT method is used for global optimization. To concentrate on the layout of the propellers and wing, a propeller with the same chord distribution, twist distribution, and number of blades is selected. The design and optimization of DEP aircraft’s range is carried out with the objective of achieving the maximum product of the lift–drag ratio with propeller efficiency under force balance constrains. Additionally, to decrease the takeoff and landing distance, the DEP aircraft’s takeoff and landing performance are optimized with the objective of the smallest velocity at an angle near the tail down angle under the constrains of acceleration bigger than 0 and a Mach number at the tip of blades smaller than 0.7. The CFD simulation was used to confirm the DEP aircraft’s pretty accurate aerodynamics. Compared to the reference aircraft, the improved DEP aircraft with 10 high-lift propellers on the leading edge of the wing and 2 wing-tip propellers may boost cruise performance by 6% while maintaining takeoff and landing velocity. Furthermore, it has been shown that the stall speed of DEP aircraft with smaller wings would rise proportionally when compared to conventional design aircraft, and the power need of DEP aircraft will be increased as a result of the operation of high-lift propellers. The conceptual design and optimal approach suggested in this work has some reference value for the design and research of the fixed-wing DEP general aviation aircraft.",
                "authors": "Jiang Wu, F. Gao, Shengwen Li, Fengtian Yang",
                "citations": 6
            },
            {
                "title": "Meta-Personalizing Vision-Language Models to Find Named Instances in Video",
                "abstract": "Large-scale vision-language models (VLM) have shown impressive results for language-guided search applications. While these models allow category-level queries, they currently struggle with personalized searches for moments in a video where a specific object instance such as “My dog Biscuit” appears. We present the following three contributions to address this problem. First, we describe a method to meta-personalize a pre-trained VLM, i.e., learning how to learn to personalize a VLM at test time to search in video. Our method extends the VLM's token vocabulary by learning novel word embeddings specific to each instance. To capture only instance-specific features, we represent each instance embedding as a combination of shared and learned global category features. Second, we propose to learn such personalization without explicit human supervision. Our approach automatically identifies moments of named visual instances in video using transcripts and vision-language similarity in the VLM's embedding space. Finally, we introduce This-Is-My, a personal video instance retrieval benchmark. We evaluate our approach on This-Is-My and Deep-Fashion2 and show that we obtain a 15% relative improvement over the state of the art on the latter dataset.",
                "authors": "Chun-Hsiao Yeh, Bryan C. Russell, Josef Sivic, Fabian Caba Heilbron, S. Jenni",
                "citations": 6
            },
            {
                "title": "Disruptive Role of Vertical Land Motion in Future Assessments of Climate Change‐Driven Sea‐Level Rise and Coastal Flooding Hazards in the Chesapeake Bay",
                "abstract": "Future projections of sea‐level rise (SLR) used to assess coastal flooding hazards and exposure throughout the 21st century and devise risk mitigation efforts often lack an accurate estimate of coastal vertical land motion (VLM) rate, driven by anthropogenic or non‐climate factors in addition to climatic factors. The Chesapeake Bay (CB) region of the United States is experiencing one of the fastest rates of relative sea‐level rise on the Atlantic coast of the United States. This study uses a combination of space‐borne Interferometric Synthetic Aperture Radar (InSAR), Global Navigation Satellite System (GNSS), Light Detecting and Ranging (LiDAR) data sets, available National Oceanic and Atmospheric Administration (NOAA) long‐term tide gauge data, and SLR projections from the Intergovernmental Panel on Climate Change (IPCC), AR6 WG1 to quantify the regional rate of relative SLR and future flooding hazards for the years 2030, 2050, and 2100. By the year 2100, the total inundated areas from SLR and subsidence are projected to be 454(316–549)–600(535–690) km2 ${\\mathrm{k}\\mathrm{m}}^{2}$ for Shared Socioeconomic Pathways (SSPs) 1–1.9 to 5–8.5, respectively, and 342(132–552)–627(526–735) km2 ${\\mathrm{k}\\mathrm{m}}^{2}$ only from SLR. The effect of storm surges based on Hurricane Isabel can increase the inundated area to 849(832–867)–1,117(1,054–1,205) km2 under different VLM and SLR scenarios. We suggest that accurate estimates of VLM rate, such as those obtained here, are essential to revise IPCC projections and obtain accurate maps of coastal flooding and inundation hazards. The results provided here inform policymakers when assessing hazards associated with global climate changes and local factors in CB, required for developing risk management and disaster resilience plans.",
                "authors": "S. F. Sherpa, M. Shirzaei, Chandrakanta Ojha",
                "citations": 7
            },
            {
                "title": "TAP: Targeted Prompting for Task Adaptive Generation of Textual Training Instances for Visual Classification",
                "abstract": "Vision and Language Models (VLMs), such as CLIP, have enabled visual recognition of a potentially unlimited set of categories described by text prompts. However, for the best visual recognition performance, these models still require tuning to better fit the data distributions of the downstream tasks, in order to overcome the domain shift from the web-based pre-training data. Recently, it has been shown that it is possible to effectively tune VLMs without any paired data, and in particular to effectively improve VLMs visual recognition performance using text-only training data generated by Large Language Models (LLMs). In this paper, we dive deeper into this exciting text-only VLM training approach and explore ways it can be significantly further improved taking the specifics of the downstream task into account when sampling text data from LLMs. In particular, compared to the SOTA text-only VLM training approach, we demonstrate up to 8.4% performance improvement in (cross) domain-specific adaptation, up to 8.7% improvement in fine-grained recognition, and 3.1% overall average improvement in zero-shot classification compared to strong baselines.",
                "authors": "M. J. Mirza, Leonid Karlinsky, Wei Lin, Horst Possegger, Rogério Feris, Horst Bischof",
                "citations": 6
            },
            {
                "title": "Localized uplift, widespread subsidence, and implications for sea level rise in the New York City metropolitan area",
                "abstract": "Regional relative sea level rise is exacerbating flooding hazards in the coastal zone. In addition to changes in the ocean, vertical land motion (VLM) is a driver of spatial variation in sea level change that can either diminish or enhance flood risk. Here, we apply state-of-the-art interferometric synthetic aperture radar and global navigation satellite system time series analysis to estimate velocities and corresponding uncertainties at 30-m resolution in the New York City metropolitan area, revealing VLM with unprecedented detail. We find broad subsidence of 1.6 mm/year, consistent with glacial isostatic adjustment to the melting of the former ice sheets, and previously undocumented hot spots of both subsidence and uplift that can be physically explained in some locations. Our results inform ongoing efforts to adapt to sea level rise and reveal points of VLM that motivate both future scientific investigations into surface geology and assessments of engineering projects.",
                "authors": "B. Buzzanga, D. Bekaert, B. Hamlington, Robert E. Kopp, Marin Govorcin, Kenneth G. Miller",
                "citations": 6
            },
            {
                "title": "Vision Language Models in Autonomous Driving: A Survey and Outlook",
                "abstract": "The applications of Vision-Language Models (VLMs) in the field of Autonomous Driving (AD) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By incorporating language data, driving systems can gain a better understanding of real-world environments, thereby enhancing driving safety and efficiency. In this work, we present a comprehensive and systematic survey of the advances in vision language models in this domain, encompassing perception and understanding, navigation and planning, decision-making and control, end-to-end autonomous driving, and data generation. We introduce the mainstream VLM tasks in AD and the commonly utilized metrics. Additionally, we review current studies and applications in various areas and summarize the existing language-enhanced autonomous driving datasets thoroughly. Lastly, we discuss the benefits and challenges of VLMs in AD and provide researchers with the current research gaps and future trends.",
                "authors": "Xingcheng Zhou, Mingyu Liu, Ekim Yurtsever, B. L. Žagar, Walter Zimmer, Hu Cao, Alois C. Knoll",
                "citations": 7
            },
            {
                "title": "Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision and Language Models",
                "abstract": "Utilizing vision and language models (VLMs) pre-trained on large-scale image-text pairs is becoming a promising paradigm for open-vocabulary visual recognition. In this work, we extend this paradigm by leveraging motion and audio that naturally exist in video. We present \\textbf{MOV}, a simple yet effective method for \\textbf{M}ultimodal \\textbf{O}pen-\\textbf{V}ocabulary video classification. In MOV, we directly use the vision encoder from pre-trained VLMs with minimal modifications to encode video, optical flow and audio spectrogram. We design a cross-modal fusion mechanism to aggregate complimentary multimodal information. Experiments on Kinetics-700 and VGGSound show that introducing flow or audio modality brings large performance gains over the pre-trained VLM and existing methods. Specifically, MOV greatly improves the accuracy on base classes, while generalizes better on novel classes. MOV achieves state-of-the-art results on UCF and HMDB zero-shot video classification benchmarks, significantly outperforming both traditional zero-shot methods and recent methods based on VLMs. Code and models will be released.",
                "authors": "Rui Qian, Yeqing Li, Zheng Xu, Ming Yang, Serge J. Belongie, Yin Cui",
                "citations": 22
            },
            {
                "title": "Review of vortex lattice method for supersonic aircraft design",
                "abstract": "\n There has been a renewed interest in developing environmentally friendly, economically viable, and technologically feasible supersonic transport aircraft and reduced order modeling methods can play an important contribution in accelerating the design process of these future aircraft. This paper reviews the use of the vortex lattice method (VLM) in modeling the general aerodynamics of subsonic and supersonic aircraft. The historical overview of the vortex lattice method is reviewed which indicates the use of this method for over a century for development and advancements in the aerodynamic analysis of subsonic and supersonic aircraft. The preference of VLM over other potential flow-solvers is because of its low order highly efficient computational analysis which is quick and efficient. Developments in VLM covering steady, unsteady state, linear and non-linear aerodynamic characteristics for different wing planform for the purpose of several different types of design optimisation is reviewed. For over a decade classical vortex lattice method has been used for multi-objective optimisation studies for commercial aircraft and unmanned aerial vehicle’s aerodynamic performance optimisation. VLM was one of the major potential flow solvers for studying the aerodynamic and aeroelastic characteristics of many wings and aircraft for NASA’s supersonic transport mission (SST). VLM is a preferred means for solving large numbers of computational design parameters in less time, more efficiently, and cheaper when compared to conventional CFD analysis which lends itself more to detailed study and solving the more challenging configuration and aerodynamic features of civil supersonic transport.",
                "authors": "H. Joshi, P. Thomas",
                "citations": 4
            },
            {
                "title": "ANSEL Photobot: A Robot Event Photographer with Semantic Intelligence",
                "abstract": "Our work examines the way in which large language models can be used for robotic planning and sampling in the context of automated photographic documentation. Specifically, we illustrate how to produce a photo-taking robot with an exceptional level of semantic awareness by leveraging recent advances in general purpose language (LM) and vision-language (VLM) models. Given a high-level description of an event we use an LM to generate a natural-language list of photo descriptions that one would expect a photographer to capture at the event. We then use a VLM to identify the best matches to these descriptions in the robot's video stream. The photo portfolios generated by our method are consistently rated as more appropriate to the event by human evaluators than those generated by existing methods.",
                "authors": "D. Rivkin, Gregory Dudek, Nikhil Kakodkar, D. Meger, Oliver Limoyo, Xue Liu, F. Hogan",
                "citations": 5
            },
            {
                "title": "A 3-D Printed Ultra-Wideband Achromatic Metalens Antenna",
                "abstract": "Lens antennas, which can transform the incident spherical wavefronts to planar above the radiating aperture, have attracted increasing attention due to their simple structure and high gain performance. Nevertheless, conventional lens antennas face with the problem of dispersion effects, which hinders their applications in large bandwidth and multi-channel communications. In this article, we propose a millimeter-wave achromatic metalens antenna using three-dimensional (3D) printing technology to reduce the dispersion effect and enlarge its bandwidth. The proposed ultra-wideband achromatic metalens antenna consists of a convex-liked metalens (VLM) and a concave-liked metalens (CLM) integrated as a metalens group. The VLM is designed on the basis of dielectric posts with different heights. The calculated transmission phase (from 0 to $2\\pi$ ) of VLM can be realized by changing the height of dielectric posts. The CLM consists of discrete variable-width dielectric posts with two different heights to achieve desired transmission phase. Measured results demonstrate that the maximum realized gain is 23.27 dBi, and the return loss is smaller than −15 dB within the whole operating bandwidth. More importantly, a broad 3-dB gain bandwidth of more than 68.4% has been achieved to cover nearly the entire V and W bands, ranging from 50 to 102 GHz.",
                "authors": "Yu-Xuan Xie, Gengbo Wu, Wenhui Deng, Shuyan Zhu, C. Chan",
                "citations": 5
            },
            {
                "title": "Performances of Dried Blood Spots and Point-of-Care Devices to Identify Virological Failure in HIV-Infected Patients: A Systematic Review and Meta-Analysis.",
                "abstract": "To broaden access to HIV viral load monitoring (VLM), the use of blood samples from dried blood spots (DBS) or point-of-care (POC) devices, could be of great help in settings where plasma is not easily accessible. The variety of assays available makes the choice complex. This systematic review and meta-analysis aims to estimate the sensitivity and specificity of DBS and POC devices to identify patients in virological failure using World Health Organization (WHO) recommendations (viral load ≥1000 copies/mL), compared with plasma, for the assays currently available. Four databases were searched for articles, and two reviewers independently identified articles reporting sensitivity and specificity of DBS and/or POC to identify patients in virological failure. We excluded articles that used other thresholds as well as articles with a total number of participants below 50 to avoid reporting bias. Heterogeneity and factors associated with assays' performances were assessed by I2 statistics and metaregression. The protocol of this review follows the PRISMA guidelines. Out of 941 articles, 47 were included: 32 DBS evaluations and 16 POC evaluations. Overall, when using DBS, the Abbott RT HIV-1, Roche CAP-CTM, NucliSENS BioMerieux and Aptima assays presented sensitivity and specificity exceeding 85%, but reported results were highly heterogeneous. Factors associated with better performances were high volume of blood and the use of the same assay for DBS and plasma VLM. Regarding the POC devices, SAMBA I, SAMBA II, and GeneXpert devices presented high sensitivity and specificity exceeding 90%, with less heterogeneity. DBS is suitable VLM, but performances can vary greatly depending on the protocols, and should be performed in trained centers. POC is suitable for VLM with less risk of heterogeneity but is more intensive in costs and logistics.",
                "authors": "Liem Binh Luong Nguyen, Abou Aissata Soumah, V. Hoang, Anh Tuan Nguyen, T. H. Pham, Sandrine Royer-Devaux, Y. Madec",
                "citations": 5
            },
            {
                "title": "Geometrically-Driven Aggregation for Zero-Shot 3D Point Cloud Understanding",
                "abstract": "Zero-shot 3D point cloud understanding can be achieved via 2D Vision-Language Models (VLMs). Existing strategies directly map VLM representations from 2D pixels of rendered or captured views to 3D points, overlooking the inherent and expressible point cloud geometric structure. Geometrically similar or close regions can be exploited for bolstering point cloud understanding as they are likely to share semantic information. To this end, we introduce the first training-free aggregation technique that leverages the point cloud's 3D geometric structure to improve the quality of the transferred VLM representations. Our approach operates iteratively, performing local-to-global aggregation based on geometric and semantic point-level reasoning. We benchmark our approach on three downstream tasks, including classification, part segmentation, and semantic segmentation, with a variety of datasets representing both synthetic/real-world, and indoor/outdoor scenarios. Our approach achieves new state-of-the-art results in all benchmarks. Code and dataset are available at https://luigiriz.github.io/geoze-website/",
                "authors": "Guofeng Mei, Luigi Riz, Yiming Wang, Fabio Poiesi",
                "citations": 5
            },
            {
                "title": "Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study",
                "abstract": "Most approaches to cross-modal retrieval (CMR) focus either on object-centric datasets, meaning that each document depicts or describes a single object, or on scene-centric datasets, meaning that each image depicts or describes a complex scene that involves multiple objects and relations between them. We posit that a robust CMR model should generalize well across both dataset types. Despite recent advances in CMR, the reproducibility of the results and their generalizability across different dataset types has not been studied before. We address this gap and focus on the reproducibility of the state-of-the-art CMR results when evaluated on object-centric and scene-centric datasets. We select two state-of-the-art CMR models with different architectures: (i) CLIP; and (ii) X-VLM. Additionally, we select two scene-centric datasets, and three object-centric datasets, and determine the relative performance of the selected models on these datasets. We focus on reproducibility, replicability, and generalizability of the outcomes of previously published CMR experiments. We discover that the experiments are not fully reproducible and replicable. Besides, the relative performance results partially generalize across object-centric and scene-centric datasets. On top of that, the scores obtained on object-centric datasets are much lower than the scores obtained on scene-centric datasets. For reproducibility and transparency we make our source code and the trained models publicly available.",
                "authors": "Mariya Hendriksen, Svitlana Vakulenko, E. Kuiper, M. de Rijke",
                "citations": 5
            },
            {
                "title": "Suprapontine Structures Modulate Brainstem and Spinal Networks",
                "abstract": null,
                "authors": "Atiyeh Mohammadshirazi, Rosamaria Apicella, Benjamín A. Zylberberg, G. Mazzone, G. Taccola",
                "citations": 4
            },
            {
                "title": "Relative Sea Level Trends for the Coastal Areas of Peninsular and East Malaysia Based on Remote and In Situ Observations",
                "abstract": "Absolute sea-level rise has become an important topic globally due to climate change. In addition, relative sea-level rise due to the vertical land motion in coastal areas can have a big societal impact. Vertical land motion (VLM) in Southeast Asia includes a tectonically induced component: uplift and subsidence in plate boundary zones where both Peninsular and East Malaysia are located. In this paper, the relative sea-level trends and (seismic cycle-induced) temporal changes across Malaysia were investigated. To do so, the data (1984–2019) from 21 tide gauges were analyzed, along with a subset (1994–2021) of nearby Malaysian GNSS stations. Changes in absolute sea level (ASL) at these locations (1992–2021) were also estimated from satellite altimetry data. As a first for Peninsular and East Malaysia, the combination ASL minus VLM was robustly used to validate relative sea-level rise from tide-gauge data and provide relative sea-level trend estimates based on a common data period of 25+ years. A good match between both the remote and in situ sea-level rise estimations was observed, especially for Peninsular Malaysia (differences < 1 mm/year), when split trends were estimated from the tide gauges and GNSS time series to distinguish between the different VLM regimes that exist due to the 2004 Sumatra–Andaman megathrust earthquake. As in the south of Thailand, post-seismic-induced negative VLM has increased relative sea-level rise by 2–3 mm/year along the Andaman Sea and Malacca Strait coastlines since 2005. For East Malaysia, the validation shows higher differences (bias of 2–3 mm/year), but this poorer match is significantly improved by either not including data after 1 January 2014 or applying a generic jump to all East Malay tide gauges from that date onwards. Overall, the present relative sea-level trends range from 4 to 6 mm/year for Malaysia with a few regions showing up to 9 mm/year due to human-induced land subsidence.",
                "authors": "W. Simons, M. Naeije, Zaki Ghazali, Wan Darani Rahman, Sanusi Cob, M. Kadir, Asrul Mustafar, Ami Hassan Md Din, J. Efendi, P. Noppradit",
                "citations": 4
            },
            {
                "title": "The SAVEMEDCOASTS-2 webGIS: The Online Platform for Relative Sea Level Rise and Storm Surge Scenarios up to 2100 for the Mediterranean Coasts",
                "abstract": "Here we show the SAVEMEDCOASTS-2 web-based geographic information system (webGIS) that supports land planners and decision makers in considering the ongoing impacts of Relative Sea Level Rise (RSLR) when formulating and prioritizing climate-resilient adaptive pathways for the Mediterranean coasts. The webGIS was developed within the framework of the SAVEMEDCOASTS and SAVEMEDCOASTS-2 projects, funded by the European Union, which respond to the need to protect people and assets from natural disasters along the Mediterranean coasts that are vulnerable to the combined effects of Sea Level Rise (SLR) and Vertical Land Movements (VLM). The geospatial data include available or new high-resolution Digital Terrain Models (DTM), bathymetric data, rates of VLM, and multi-temporal coastal flooding scenarios for 2030, 2050, and 2100 with respect to 2021, as a consequence of RSLR. The scenarios are derived from the 5th Assessment Report (AR5) provided by the Intergovernmental Panel on Climate Change (IPCC) and encompass different Representative Concentration Pathways (RCP2.6 and RCP8.5) for climate projections. The webGIS reports RSLR scenarios that incorporate the temporary contribution of both the highest astronomical tides (HAT) and storm surges (SS), which intensify risks to the coastal infrastructure, local community, and environment.",
                "authors": "Antonio Falciano, M. Anzidei, Michele Greco, M. Trivigno, A. Vecchio, C. Georgiadis, P. Patias, M. Crosetto, J. Navarro, E. Serpelloni, C. Tolomei, G. Martino, Giuseppe Mancino, F. Arbia, C. Bignami, F. Doumaz",
                "citations": 5
            },
            {
                "title": "High-Fat Diet Modulates the Excitability of Neurons within the Brain–Liver Pathway",
                "abstract": "Stimulation of hepatic sympathetic nerves increases glucose production and glycogenolysis. Activity of pre-sympathetic neurons in the paraventricular nucleus (PVN) of the hypothalamus and in the ventrolateral and ventromedial medulla (VLM/VMM) largely influence the sympathetic output. Increased activity of the sympathetic nervous system (SNS) plays a role in the development and progression of metabolic diseases; however, despite the importance of the central circuits, the excitability of pre-sympathetic liver-related neurons remains to be determined. Here, we tested the hypothesis that the activity of liver-related neurons in the PVN and VLM/VMM is altered in diet-induced obese mice, as well as their response to insulin. Patch-clamp recordings were conducted from liver-related PVN neurons, VLM-projecting PVN neurons, and pre-sympathetic liver-related neurons in the ventral brainstem. Our data demonstrate that the excitability of liver-related PVN neurons increased in high-fat diet (HFD)-fed mice compared to mice fed with control diet. Insulin receptor expression was detected in a population of liver-related neurons, and insulin suppressed the firing activity of liver-related PVN and pre-sympathetic VLM/VMM neurons in HFD mice; however, it did not affect VLM-projecting liver-related PVN neurons. These findings further suggest that HFD alters the excitability of pre-autonomic neurons as well as their response to insulin.",
                "authors": "Adrien J R Molinas, Lucie D. Desmoulins, Roslyn Davis, Hong Gao, Ryousuke Satou, A. Derbenev, A. Zsombok",
                "citations": 4
            },
            {
                "title": "Toward Grounded Commonsense Reasoning",
                "abstract": "Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the \"tidying.\" How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded commonsense reasoning. To evaluate our framework at scale, we release the MessySurfaces dataset which contains images of 70 real-world surfaces that need to be cleaned. We additionally illustrate our approach with a robot on 2 carefully designed surfaces. We find an average 12.9% improvement on the MessySurfaces benchmark and an average 15% improvement on the robot experiments over baselines that do not use active perception. The dataset, code, and videos of our approach can be found at https://minaek.github.io/grounded_commonsense_reasoning/.",
                "authors": "Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, A. Dragan, Dorsa Sadigh",
                "citations": 5
            },
            {
                "title": "Text Descriptions are Compressive and Invariant Representations for Visual Learning",
                "abstract": "Modern image classification is based upon directly predicting classes via large discriminative networks, which do not directly contain information about the intuitive visual features that may constitute a classification decision. Recently, work in vision-language models (VLM) such as CLIP has provided ways to specify natural language descriptions of image classes, but typically focuses on providing single descriptions for each class. In this work, we demonstrate that an alternative approach, in line with humans' understanding of multiple visual features per class, can also provide compelling performance in the robust few-shot learning setting. In particular, we introduce a novel method, \\textit{SLR-AVD (Sparse Logistic Regression using Augmented Visual Descriptors)}. This method first automatically generates multiple visual descriptions of each class via a large language model (LLM), then uses a VLM to translate these descriptions to a set of visual feature embeddings of each image, and finally uses sparse logistic regression to select a relevant subset of these features to classify each image. Core to our approach is the fact that, information-theoretically, these descriptive features are more invariant to domain shift than traditional image embeddings, even though the VLM training process is not explicitly designed for invariant representation learning. These invariant descriptive features also compose a better input compression scheme. When combined with finetuning, we show that SLR-AVD is able to outperform existing state-of-the-art finetuning approaches on both in-distribution and out-of-distribution performance.",
                "authors": "Zhili Feng, Anna Bair, J. Z. Kolter",
                "citations": 5
            },
            {
                "title": "Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors",
                "abstract": "Human-object interaction (HOI) detection aims to extract interacting human-object pairs and their interaction categories from a given natural image. Even though the labeling effort required for building HOI detection datasets is inherently more extensive than for many other computer vision tasks, weakly-supervised directions in this area have not been sufficiently explored due to the difficulty of learning human-object interactions with weak supervision, rooted in the combinatorial nature of interactions over the object and predicate space. In this paper, we tackle HOI detection with the weakest supervision setting in the literature, using only image-level interaction labels, with the help of a pretrained vision-language model (VLM) and a large language model (LLM). We first propose an approach to prune non-interacting human and object proposals to increase the quality of positive pairs within the bag, exploiting the grounding capability of the vision-language model. Second, we use a large language model to query which interactions are possible between a human and a given object category, in order to force the model not to put emphasis on unlikely interactions. Lastly, we use an auxiliary weakly-supervised preposition prediction task to make our model explicitly reason about space. Extensive experiments and ablations show that all of our contributions increase HOI detection performance.",
                "authors": "Mesut Erhan Unal, Adriana Kovashka",
                "citations": 4
            },
            {
                "title": "Interannual variability of vertical land motion over High Mountain Central Asia from GPS and GRACE/GRACE-FO observations",
                "abstract": null,
                "authors": "Yuanjin Pan, Weiping Jiang, H. Ding, C. Shum, J. Jiao, Yixin Xiao, Qiwen Wu",
                "citations": 4
            },
            {
                "title": "20thto 21stCentury Relative Sea and Land LevelChanges in Northern California:Tectonic Land Level Changes and theirContribution to Sea-Level Rise, Humboldt BayRegion, Northern California",
                "abstract": "Sea-level changes are modulated in coastal northern California by land-level changes due to the earthquake cycle along the Cascadia subduction zone, the San Andreas plate boundary fault system, and crustal faults. Sea-level rise (SLR) subjects ecological and anthropogenic infrastructure to increased vulnerability to changes in habitat and increased risk for physical damage. The degree to which each of these forcing factors drives this modulation is poorly resolved. We use NOAA tide gage data and ‘campaign’ tide gage deployments, Global Navigation Satellite System (GNSS) data, and National Geodetic Survey (NGS) first-order levelling data to calculate vertical land motion (VLM) rates in coastal northern California. Sea-level observations, highway level surveys, and GNSS data all confirm that land is subsiding in Humboldt Bay, in contrast to Crescent City where the land is rising. Subtracting absolute sea-level rate (~1.99 mm/year) from Crescent City (CC) and North Spit (NS) gage relative sea-level rates reveals that CC is uplifting at ~2.83 mm/year and NS is subsiding at ~3.21mm/year. GNSS vertical deformation reveals similar rates of ~2.60 mm/year of uplift at Crescent City. In coastal northern California, there is an E-W trending variation in vertical land motion that is primarily due to Cascadia megathrust fault seismogenic coupling. This interseismic subsidence also dominates the N-S variation in vertical land motion in most of the study region. There exists a second-order heterogeneous N-S trend in vertical land motion that we associate to crustal fault-related strain. There may be non-tectonic contributions to the observed VLM rates.",
                "authors": "J. Patton, T. Williams, Jeffrey L. Anderson, M. Hemphill-Haley, R. Burgette, Ray Weldon II, R. McPherson, T. Leroy",
                "citations": 4
            },
            {
                "title": "Zero-Shot Image Harmonization with Generative Model Prior",
                "abstract": "We propose a zero-shot approach to image harmonization, aiming to overcome the reliance on large amounts of synthetic composite images in existing methods. These methods, while showing promising results, involve significant training expenses and often struggle with generalization to unseen images. To this end, we introduce a fully modularized framework inspired by human behavior. Leveraging the reasoning capabilities of recent foundation models in language and vision, our approach comprises three main stages. Initially, we employ a pretrained vision-language model (VLM) to generate descriptions for the composite image. Subsequently, these descriptions guide the foreground harmonization direction of a text-to-image generative model (T2I). We refine text embeddings for enhanced representation of imaging conditions and employ self-attention and edge maps for structure preservation. Following each harmonization iteration, an evaluator determines whether to conclude or modify the harmonization direction. The resulting framework, mirroring human behavior, achieves harmonious results without the need for extensive training. We present compelling visual results across diverse scenes and objects, along with a user study validating the effectiveness of our approach.",
                "authors": "Jianqi Chen, Zhengxia Zou, Yilan Zhang, Keyan Chen, Z. Shi",
                "citations": 4
            },
            {
                "title": "Prompt Algebra for Task Composition",
                "abstract": "We investigate whether prompts learned independently for different tasks can be later combined through prompt algebra to obtain a model that supports composition of tasks. We consider Visual Language Models (VLM) with prompt tuning as our base classifier and formally define the notion of prompt algebra. We propose constrained prompt tuning to improve performance of the composite classifier. In the proposed scheme, prompts are constrained to appear in the lower dimensional subspace spanned by the basis vectors of the pre-trained vocabulary. Further regularization is added to ensure that the learned prompt is grounded correctly to the existing pre-trained vocabulary. We demonstrate the effectiveness of our method on object classification and object-attribute classification datasets. On average, our composite model obtains classification accuracy within 2.5% of the best base model. On UTZappos it improves classification accuracy over the best base model by 8.45% on average.",
                "authors": "Pramuditha Perera, Matthew Trager, L. Zancato, A. Achille, S. Soatto",
                "citations": 5
            },
            {
                "title": "Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification",
                "abstract": "Vision-Language Models (VLMs) such as CLIP are trained on large amounts of image-text pairs, resulting in remarkable generalization across several data distributions. However, in several cases, their expensive training and data collection/curation costs do not justify the end application. This motivates a vendor-client paradigm, where a vendor trains a large-scale VLM and grants only input-output access to clients on a pay-per-query basis in a black-box setting. The client aims to minimize inference cost by distilling the VLM to a student model using the limited available task-specific data, and further deploying this student model in the downstream application. While naive distillation largely improves the In-Domain (ID) accuracy of the student, it fails to transfer the superior out-of-distribution (OOD) generalization of the VLM teacher using the limited available labeled images. To mitigate this, we propose Vision-Language to Vision - Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and language modalities of the teacher model with the vision modality of a pre-trained student model, and further distills the aligned VLM representations to the student. This maximally retains the pre-trained features of the student, while also incorporating the rich representations of the VLM image encoder and the superior generalization of the text embeddings. The proposed approach achieves state-of-the-art results on the standard Domain Generalization benchmarks in a black-box teacher setting as well as a white-box setting where the weights of the VLM are accessible. Project page: http://val.cds.iisc.ac.in/VL2V-ADiP/",
                "authors": "Sravanti Addepalli, Ashish Ramayee Asokan, Lakshay Sharma, R. Babu",
                "citations": 3
            },
            {
                "title": "ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition",
                "abstract": "Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even without any video data pre-training, and an accuracy of 96.11% and 75.75% after kinetics pre-training.",
                "authors": "S. Chaudhuri, Saumik Bhattacharya",
                "citations": 3
            },
            {
                "title": "C-SAW: Self-Supervised Prompt Learning for Image Generalization in Remote Sensing",
                "abstract": "We focus on domain and class generalization problems in analyzing optical remote sensing images, using the large-scale pre-trained vision-language model (VLM), CLIP. While contrastively trained VLMs show impressive zero-shot generalization performance, their effectiveness is limited when dealing with diverse domains during training and testing. Existing prompt learning techniques overlook the importance of incorporating domain and content information into the prompts, which results in a drop in performance while dealing with such multi-domain data. To address these challenges, we propose a solution that ensures domain-invariant prompt learning while enhancing the expressiveness of visual features. We observe that CLIP’s vision encoder struggles to identify contextual image information, particularly when image patches are jumbled up. This issue is especially severe in optical remote sensing images, where land-cover classes exhibit well-defined contextual appearances. To this end, we introduce C-SAW, a method that complements CLIP with a self-supervised loss in the visual space and a novel prompt learning technique that emphasizes both visual domain and content-specific features. We keep the CLIP backbone frozen and introduce a small set of projectors for both the CLIP encoders to train C-SAW contrastively. Experimental results demonstrate the superiority of C-SAW across multiple remote sensing benchmarks and different generalization tasks.",
                "authors": "Avigyan Bhattacharya, M. Singha, Ankit Jha, Biplab Banerjee",
                "citations": 3
            },
            {
                "title": "Exploiting CLIP for Zero-shot HOI Detection Requires Knowledge Distillation at Multiple Levels",
                "abstract": "In this paper, we investigate the task of zero-shot human-object interaction (HOI) detection, a novel paradigm for identifying HOIs without the need for task-specific annotations. To address this challenging task, we employ CLIP, a large-scale pre-trained vision-language model (VLM), for knowledge distillation on multiple levels. Specifically, we design a multi-branch neural network that leverages CLIP for learning HOI representations at various levels, including global images, local union regions encompassing human-object pairs, and individual instances of humans or objects. To train our model, CLIP is utilized to generate HOI scores for both global images and local union regions that serve as supervision signals. The extensive experiments demonstrate the effectiveness of our novel multi-level CLIP knowledge integration strategy. Notably, the model achieves strong performance, which is even comparable with some fully-supervised and weakly-supervised methods on the public HICO-DET benchmark. Code is available at https://github.com/bobwan1995/Zeroshot-HOI-with-CLIP.",
                "authors": "Bo Wan, T. Tuytelaars",
                "citations": 3
            },
            {
                "title": "Virtual Network Embedding Over Multi-Band Elastic Optical Network Based on Cross-Matching Mechanism and Hypergraph Theory",
                "abstract": "The commercialization of 5G and the explosive emergence of new applications stimulate the exponential growth of network traffic and diversification of services. It is promising to integrate multi-band elastic optical network (MBEON) and network virtualization for large volume traffic transmission and highly diverse services. However, performing virtual network embedding (VNE) for network virtualization over MBEON faces the challenge of severe inter-channel stimulated Raman scattering (ISRS) effect, which complicates the underlying physical layer effect in the substrate network. In this paper, we investigate the ISRS-aware VNE over MBEON, where a cross-matching mechanism is proposed for virtual node mapping (VNM) and a hypergraph is introduced for parallel virtual link mapping (VLM). A lightpath-level integer linear programming model is first formulated. To integrate the cost and availability of VLM, which significantly affect the performance of the VNE under the ISRS effect, into the VNM process, the “virtual node-substrate node” mapping pairs are specifically evaluated through the cross-matching mechanism. Moreover, to tackle the couplings among multiple lightpaths induced by the wide spectrum ISRS effect, hypergraphs are used to model the ISRS effect-aware quality of transmission (QoT) constraints among multiple lightpaths. A hypergraph maximal weight independent set heuristic is presented for lightpath selection, which guarantees the obedience of basic constraints and generates near-optimal solutions. Experimental results show that the proposed methods decrease blocking ratio by more than 30% compared with the benchmarks with similar computational complexity.",
                "authors": "Zeyuan Yang, Rentao Gu, Yuefeng Ji",
                "citations": 3
            },
            {
                "title": "New vision-incorporated third-generation video laryngeal mask airways for intubation of patients in prone position",
                "abstract": null,
                "authors": "T. Gaszyński",
                "citations": 3
            },
            {
                "title": "Hepatic Innervations and Nonalcoholic Fatty Liver Disease",
                "abstract": "Abbreviations graphical abstract: VMN/PVN, hypothalamic ventromedial nucleus/paraventricular nucleus; VLM/VMM, ventrolateral medulla/ventromedial medulla; SMG/CG, superior mesenteric ganglion/caeliac ganglia; NTS, nucleus of the solitary tract; NG, nodose ganglion. Nonalcoholic fatty liver disease (NAFLD) is the most common chronic liver disorder. Increased sympathetic (noradrenergic) nerve tone has a complex role in the etiopathomechanism of NAFLD, affecting the development/progression of steatosis, inflammation, fibrosis, and liver hemodynamical alterations. Also, lipid sensing by vagal afferent fibers is an important player in the development of hepatic steatosis. Moreover, disorganization and progressive degeneration of liver sympathetic nerves were recently described in human and experimental NAFLD. These structural alterations likely come along with impaired liver sympathetic nerve functionality and lack of adequate hepatic noradrenergic signaling. Here, we first overview the anatomy and physiology of liver nerves. Then, we discuss the nerve impairments in NAFLD and their pathophysiological consequences in hepatic metabolism, inflammation, fibrosis, and hemodynamics. We conclude that further studies considering the spatial-temporal dynamics of structural and functional changes in the hepatic nervous system may lead to more targeted pharmacotherapeutic advances in NAFLD.",
                "authors": "M. Ádori, S. Bhat, R. Gramignoli, Ismael Valladolid-Acebes, T. Bengtsson, Mathias Uhlén, C. Adori",
                "citations": 3
            },
            {
                "title": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering",
                "abstract": "Recently, to comprehensively improve Vision Language Models (VLMs) for Visual Question Answering (VQA), several methods have been proposed to further reinforce the inference capabilities of VLMs to independently tackle VQA tasks rather than some methods that only utilize VLMs as aids to Large Language Models (LLMs). However, these methods ignore the rich common-sense knowledge inside the given VQA image sampled from the real world. Thus, they cannot fully use the powerful VLM for the given VQA question to achieve optimal performance. Attempt to overcome this limitation and inspired by the human top-down reasoning process, i.e., systematically exploring relevant issues to derive a comprehensive answer, this work introduces a novel, explainable multi-agent collaboration framework by leveraging the expansive knowledge of Large Language Models (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our framework comprises three agents, i.e., Responder, Seeker, and Integrator, to collaboratively answer the given VQA question by seeking its relevant issues and generating the final answer in such a top-down reasoning process. The VLM-based Responder agent generates the answer candidates for the question and responds to other relevant issues. The Seeker agent, primarily based on LLM, identifies relevant issues related to the question to inform the Responder agent and constructs a Multi-View Knowledge Base (MVKB) for the given visual scene by leveraging the build-in world knowledge of LLM. The Integrator agent combines knowledge from the Seeker agent and the Responder agent to produce the final VQA answer. Extensive and comprehensive evaluations on diverse VQA datasets with a variety of VLMs demonstrate the superior performance and interpretability of our framework over the baseline method in the zero-shot setting without extra training cost.",
                "authors": "Zeqing Wang, Wentao Wan, Runmeng Chen, Qiqing Lao, Minjie Lang, Keze Wang",
                "citations": 3
            },
            {
                "title": "Open-Vocabulary Camouflaged Object Segmentation",
                "abstract": "Recently, the emergence of the large-scale vision-language model (VLM), such as CLIP, has opened the way towards open-world object perception. Many works have explored the utilization of pre-trained VLM for the challenging open-vocabulary dense prediction task that requires perceiving diverse objects with novel classes at inference time. Existing methods construct experiments based on the public datasets of related tasks, which are not tailored for open vocabulary and rarely involve imperceptible objects camouflaged in complex scenes due to data collection bias and annotation costs. To fill in the gaps, we introduce a new task, open-vocabulary camouflaged object segmentation (OVCOS), and construct a large-scale complex scene dataset (\\textbf{OVCamo}) containing 11,483 hand-selected images with fine annotations and corresponding object classes. Further, we build a strong single-stage open-vocabulary \\underline{c}amouflaged \\underline{o}bject \\underline{s}egmentation transform\\underline{er} baseline \\textbf{OVCoser} attached to the parameter-fixed CLIP with iterative semantic guidance and structure enhancement. By integrating the guidance of class semantic knowledge and the supplement of visual structure cues from the edge and depth information, the proposed method can efficiently capture camouflaged objects. Moreover, this effective framework also surpasses previous state-of-the-arts of open-vocabulary semantic image segmentation by a large margin on our OVCamo dataset. With the proposed dataset and baseline, we hope that this new task with more practical value can further expand the research on open-vocabulary dense prediction tasks. Our code and data can be found in the \\href{https://github.com/lartpang/OVCamo}{link}.",
                "authors": "Youwei Pang, Xiaoqi Zhao, Jiaming Zuo, Lihe Zhang, Huchuan Lu",
                "citations": 2
            },
            {
                "title": "Emergent Open-Vocabulary Semantic Segmentation from Off-the-Shelf Vision-Language Models",
                "abstract": "From image-text pairs, large-scale vision-language models (VLMs) learn to implicitly associate image regions with words, which prove effective for tasks like visual question answering. However, leveraging the learned association for open-vocabulary semantic segmentation remains a challenge. In this paper, we propose a simple, yet extremely effective, training-free technique, Plug-and-Play Open- Vocabulary Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with direct text-to-image cross-attention and an image-text matching loss. To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask. PnP-OVSS does not require any neural net-work training and performs hyperparameter tuning without the need for any segmentation annotations, even for a validation set. PnP-OVSS demonstrates substantial improvements over comparable baselines (+29.4% mIoU on Pascal VOC, +13.2% mIoU on Pascal Context, +14.0% mIoU on MS COCO, +2.4% mIoU on COCO Stuff) and even outper-forms most baselines that conduct additional network training on top of pretrained VLMs. Our codebase is at https://github.com/letitiabanana/PnP-OVSS.",
                "authors": "Jiayun Luo, Siddhesh Khandelwal, Leonid Sigal, Boyang Albert Li",
                "citations": 2
            },
            {
                "title": "Resolving References in Visually-Grounded Dialogue via Text Generation",
                "abstract": "Vision-language models (VLMs) have shown to be effective at image retrieval based on simple text queries, but text-image retrieval based on conversational input remains a challenge. Consequently, if we want to use VLMs for reference resolution in visually-grounded dialogue, the discourse processing capabilities of these models need to be augmented. To address this issue, we propose fine-tuning a causal large language model (LLM) to generate definite descriptions that summarize coreferential information found in the linguistic context of references. We then use a pretrained VLM to identify referents based on the generated descriptions, zero-shot. We evaluate our approach on a manually annotated dataset of visually-grounded dialogues and achieve results that, on average, exceed the performance of the baselines we compare against. Furthermore, we find that using referent descriptions based on larger context windows has the potential to yield higher returns.",
                "authors": "Bram Willemsen, Livia Qian, Gabriel Skantze",
                "citations": 2
            },
            {
                "title": "Bi-VLGM : Bi-Level Class-Severity-Aware Vision-Language Graph Matching for Text Guided Medical Image Segmentation",
                "abstract": "Medical reports containing specific diagnostic results and additional information not present in medical images can be effectively employed to assist image understanding tasks, and the modality gap between vision and language can be bridged by vision-language matching (VLM). However, current vision-language models distort the intra-model relation and only include class information in reports that is insufficient for segmentation task. In this paper, we introduce a novel Bi-level class-severity-aware Vision-Language Graph Matching (Bi-VLGM) for text guided medical image segmentation, composed of a word-level VLGM module and a sentence-level VLGM module, to exploit the class-severity-aware relation among visual-textual features. In word-level VLGM, to mitigate the distorted intra-modal relation during VLM, we reformulate VLM as graph matching problem and introduce a vision-language graph matching (VLGM) to exploit the high-order relation among visual-textual features. Then, we perform VLGM between the local features for each class region and class-aware prompts to bridge their gap. In sentence-level VLGM, to provide disease severity information for segmentation task, we introduce a severity-aware prompting to quantify the severity level of disease lesion, and perform VLGM between the global features and the severity-aware prompts. By exploiting the relation between the local (global) and class (severity) features, the segmentation model can include the class-aware and severity-aware information to promote segmentation performance. Extensive experiments proved the effectiveness of our method and its superiority to existing methods. The source code will be released.",
                "authors": "Wenting Chen, Jie Liu, Yixuan Yuan",
                "citations": 2
            },
            {
                "title": "IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models",
                "abstract": "Large language models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in various tasks and attracted an increasing interest as a natural language interface across many domains. Recently, large vision-language models (VLMs) like BLIP-2 and GPT-4 have been intensively investigated, which learn rich vision-language correlation from image-text pairs. However, despite these developments, the application of LLMs and VLMs in image quality assessment (IQA), particularly in medical imaging, remains to be explored, which is valuable for objective performance evaluation and potential supplement or even replacement of radiologists' opinions. To this end, this paper introduces IQAGPT, an innovative image quality assessment system integrating an image quality captioning VLM with ChatGPT for generating quality scores and textual reports. First, we build a CT-IQA dataset for training and evaluation, comprising 1,000 CT slices with diverse quality levels professionally annotated. To better leverage the capabilities of LLMs, we convert annotated quality scores into semantically rich text descriptions using a prompt template. Second, we fine-tune the image quality captioning VLM on the CT-IQA dataset to generate quality descriptions. The captioning model fuses the image and text features through cross-modal attention. Third, based on the quality descriptions, users can talk with ChatGPT to rate image quality scores or produce a radiological quality report. Our preliminary results demonstrate the feasibility of assessing image quality with large models. Remarkably, our IQAGPT outperforms GPT-4 and CLIP-IQA, as well as the multi-task classification and regression models that solely rely on images.",
                "authors": "Zhihao Chen, Bin Hu, Chuang Niu, Tao Chen, Yuxin Li, Hongming Shan, Ge Wang",
                "citations": 2
            },
            {
                "title": "Manipulating the Label Space for In-Context Classification",
                "abstract": "After pre-training by generating the next word conditional on previous words, the Language Model (LM) acquires the ability of In-Context Learning (ICL) that can learn a new task conditional on the context of the given in-context examples (ICEs). Similarly, visually-conditioned Language Modelling is also used to train Vision-Language Models (VLMs) with ICL ability. However, such VLMs typically exhibit weaker classification abilities compared to contrastive learning-based models like CLIP, since the Language Modelling objective does not directly contrast whether an object is paired with a text. To improve the ICL of classification, using more ICEs to provide more knowledge is a straightforward way. However, this may largely increase the selection time, and more importantly, the inclusion of additional in-context images tends to extend the length of the in-context sequence beyond the processing capacity of a VLM. To alleviate these limitations, we propose to manipulate the label space of each ICE to increase its knowledge density, allowing for fewer ICEs to convey as much information as a larger set would. Specifically, we propose two strategies which are Label Distribution Enhancement and Visual Descriptions Enhancement to improve In-context classification performance on diverse datasets, including the classic ImageNet and more fine-grained datasets like CUB-200. Specifically, using our approach on ImageNet, we increase accuracy from 74.70\\% in a 4-shot setting to 76.21\\% with just 2 shots. surpassing CLIP by 0.67\\%. On CUB-200, our method raises 1-shot accuracy from 48.86\\% to 69.05\\%, 12.15\\% higher than CLIP. The code is given in https://anonymous.4open.science/r/MLS_ICC.",
                "authors": "Haokun Chen, Xu Yang, Yuhang Huang, Zihan Wu, Jing Wang, Xin Geng",
                "citations": 2
            },
            {
                "title": "LiFT: Transfer Learning in Vision-Language Models for Downstream Adaptation and Generalization",
                "abstract": "Pre-trained Vision-Language Models (VLMs) on large-scale image-text pairs, e.g., CLIP, have shown promising performance on zero-shot knowledge transfer. Recently, fine-tuning pre-trained VLMs to downstream few-shot classification with limited image annotation data yields significant gains. However, there are two limitations. First, most of the methods for fine-tuning VLMs only update newly added parameters while keeping the whole VLM frozen. Thus, it remains unclear how to directly update the VLM itself. Second, fine-tuning VLMs to a specific set of base classes would deteriorate the well-learned representation space such that the VLMs generalize poorly on novel classes. To address these issues, we first propose Layer-wise Fine-Tuning (LiFT) which achieves average gains of 3.9%, 4.3%, 4.2% and 4.5% on base classes under 2-, 4-, 8- and 16-shot respectively compared to the baseline CoOp over 11 datasets. Alternatively, we provide a parameter-efficient LiFT-Adapter exhibiting favorable performance while updating only 1.66% of total parameters. Further, we design scalable LiFT-NCD to identify both base classes and novel classes, which boosts the accuracy by an average of 5.01% over zero-shot generalization of CLIP, exploring the potential of VLMs in discovering novel classes.",
                "authors": "Jingzheng Li, Hailong Sun",
                "citations": 2
            },
            {
                "title": "What Makes Good Open-Vocabulary Detector: A Disassembling Perspective",
                "abstract": "Open-vocabulary detection (OVD) is a new object detection paradigm, aiming to localize and recognize unseen objects defined by an unbounded vocabulary. This is challenging since traditional detectors can only learn from pre-defined categories and thus fail to detect and localize objects out of pre-defined vocabulary. To handle the challenge, OVD leverages pre-trained cross-modal VLM, such as CLIP, ALIGN, etc. Previous works mainly focus on the open vocabulary classification part, with less attention on the localization part. We argue that for a good OVD detector, both classification and localization should be parallelly studied for the novel object categories. We show in this work that improving localization as well as cross-modal classification complement each other, and compose a good OVD detector jointly. We analyze three families of OVD methods with different design emphases. We first propose a vanilla method,i.e., cropping a bounding box obtained by a localizer and resizing it into the CLIP. We next introduce another approach, which combines a standard two-stage object detector with CLIP. A two-stage object detector includes a visual backbone, a region proposal network (RPN), and a region of interest (RoI) head. We decouple RPN and ROI head (DRR) and use RoIAlign to extract meaningful features. In this case, it avoids resizing objects. To further accelerate the training time and reduce the model parameters, we couple RPN and ROI head (CRR) as the third approach. We conduct extensive experiments on these three types of approaches in different settings. On the OVD-COCO benchmark, DRR obtains the best performance and achieves 35.8 Novel AP$_{50}$, an absolute 2.8 gain over the previous state-of-the-art (SOTA). For OVD-LVIS, DRR surpasses the previous SOTA by 1.9 AP$_{50}$ in rare categories. We also provide an object detection dataset called PID and provide a baseline on PID.",
                "authors": "Jincheng Li, Chunyu Xie, Xiaoyu Wu, Bin Wang, Dawei Leng",
                "citations": 2
            },
            {
                "title": "Contrastive Vision-Language Alignment Makes Efficient Instruction Learner",
                "abstract": "We study the task of extending the large language model (LLM) into a vision-language instruction-following model. This task is crucial but challenging since the LLM is trained on text modality only, making it hard to effectively digest the visual modality. To address this, existing methods typically train a visual adapter to align the representation between a pre-trained vision transformer (ViT) and the LLM by a generative image captioning loss. However, we find that the generative objective can only produce weak alignment for vision and language, making the aligned vision-language model very hungry for the instruction fine-tuning data. In this paper, we propose CG-VLM that applies both Contrastive and Generative alignment objectives to effectively align the representation of ViT and LLM. Different from image level and sentence level alignment in common contrastive learning settings, CG-VLM aligns the image-patch level features and text-token level embeddings, which, however, is very hard to achieve as no explicit grounding patch-token relation provided in standard image captioning datasets. To address this issue, we propose to maximize the averaged similarity between pooled image-patch features and text-token embeddings. Extensive experiments demonstrate that the proposed CG-VLM produces strong vision-language alignment and is an efficient instruction learner. For example, using only 10% instruction tuning data, we reach 95% performance of state-of-the-art method LLaVA [29] on the zero-shot ScienceQA-Image benchmark.",
                "authors": "Lizhao Liu, Xinyu Sun, Tianhang Xiang, Zhuangwei Zhuang, Liuren Yin, Mingkui Tan",
                "citations": 2
            },
            {
                "title": "Twenty-minute harvesting of flow-through type vastus lateralis muscle flap significantly reduces the need for a temporary intravascular shunt in the treatment of severe upper extremity trauma in civilian patients",
                "abstract": "For the reconstruction of severe upper extremity trauma involving arterial injury in civilian patients, it is generally recommended that the revascularization time be shortened using a temporary intravascular shunt (TIVS). However, if a flow-through type vastus lateralis muscle (VLm) flap can be harvested in 20 minutes and bypassed at the obstructed ischemic zone within 30 minutes, blood flow can be restored as quickly or more quickly than when using a TIVS, eliminating the need for a TIVS. This procedure was applied in the reconstruction of 3 cases of severe extremity trauma with vascular injury. The mean age was 69.7 years. Surgery was started an average of 2.93 hours from the onset. The average flap harvest time was 0.33 hours. The average time to revascularization from flap harvest was 1.33 hours, the average total operation time was 6.43 hours, and all upper extremities were salvaged. No cases showed ischemia-reperfusion injury or severe muscle contracture. The flow-through-type VLm flap can be applicable as a bypass graft for a 20 cm defect at any region distal to the elbow. In addition, harvesting the flap attached to blood-rich muscle not only controls the infection of contaminated wounds through the filling of dead space, but also has the potential to replace damaged muscle or tendon tissue. Even though TIVS placement is currently used extensively in this field of treatment, its role could be significantly reduced if a flow-through-type VLm flap can be harvested within 20 minutes.",
                "authors": "Masakatsu Hihara, A. Kuro, Toshihito Mitsui, N. Kakudo",
                "citations": 2
            },
            {
                "title": "Offsets in tide-gauge reference levels detected by satellite altimetry: ten case studies",
                "abstract": null,
                "authors": "R. Ray, M. Widlansky, A. Genz, P. Thompson",
                "citations": 2
            },
            {
                "title": "Questionnaire survey of virtual reality experiences of digestive surgery at a rural academic institute: A pilot study for pre-surgical education.",
                "abstract": "We developed a prototype VR platform, VECTORS L&M (VLM), aiming to enhance the understanding of digestive surgery for students, interns, and young surgeons by limiting costs. Its efficacy was assessed via questionnaires before implementation in surgical education. The VLM provides nine-minute VR views of surgeries, from both 180- and 360-degree angles. It was created with L.A.B. Co., Ltd. and incorporates surgery videos from biliary malignancy patients. Following VLM development, a survey was conducted among surgeons who had experienced it. Twenty-eight participants (32% of observers) responded to the survey. A majority (81%) reported positive experiences with the VR content and showed interest in VR video production, though some reported sickness. Most respondents were experienced surgeons, and nearly all believed VR was important for medical education with a mean score of 4.14 on a scale of up to 5. VR was preferred over 3D printed models due to its application versatility. Participants expressed the desire for future VR improvements, such as increased mobility, cloud connectivity, cost reduction, and better resolution. The VLM platform, coupled with this innovative teaching approach, offers experiential learning in intraabdominal surgery, effectively enriching the knowledge of students and surgeons ahead of surgical education and training.",
                "authors": "Atsushi Nanashima, K. Kai, T. Hamada, Shun Munakata, N. Imamura, M. Hiyoshi, Kiyoaki Hamada, Ikko Shimizu, Yuki Tsuchimochi, Isao Tsuneyoshi",
                "citations": 2
            },
            {
                "title": "Enabling Vision-and-Language Navigation for Intelligent Connected Vehicles Using Large Pre-Trained Models",
                "abstract": "In the field of autonomous driving, Visual-and-Language Navigation (VLN) is a typical multimodal task. In the VLN task, an intelligent vehicle needs to find the target location based on user-provided navigation instructions. However, conventional VLN models generally face the problem of limited generalization ability when dealing with a large number of real-world environmental objects and language instructions. This paper proposed a novel VLN system based on large-scale pre-trained models and applied it to intelligent vehicles. The method consists of an Instruction Extraction System, a Vision-Language Association System, and a Navigational Decisions System. Specifically, a pre-trained Large Language Model (LLM) is first used to extract a series of landmark names from the user’s natural language instructions. Then, the landmark name list is inputted into a pre-trained Visual-Language Model (VLM) to infer the joint probability with environmental objects. Therefore, the image nodes that match the landmarks have been selected. Additionally, the selected image nodes are inputted into another VLM to obtain descriptions of the image nodes. Finally, LLM is used to reason navigation actions for the intelligent vehicle. With the reasoning ability of LLM, the intelligent vehicle takes navigation knowledge, visual environment descriptions, and navigation history as inputs to output navigation actions. The simulation results demonstrate that compared to other fully supervised learning methods, this approach exhibits better generalization ability in unknown environments. Based on Google Map’s Street View data, it achieves a 14.6% higher task success rate compared to the baseline model VLN Transformer.",
                "authors": "Yaqi Hu, Dongyuan Ou, Xiaoxu Wang, Rong Yu",
                "citations": 2
            },
            {
                "title": "Gradient constrained sharpness-aware prompt learning for vision-language models",
                "abstract": "This paper targets a novel trade-off problem in generalizable prompt learning for vision-language models (VLM), i.e., improving the performance on unseen classes while maintaining the performance on seen classes. Comparing with existing generalizable methods that neglect the seen classes degradation, the setting of this problem is more strict and fits more closely with practical applications. To solve this problem, we start from the optimization perspective, and leverage the relationship between loss landscape geometry and model generalization ability. By analyzing the loss landscapes of the state-of-the-art method and vanilla Sharpness-aware Minimization (SAM) based method, we conclude that the trade-off performance correlates to both loss value and loss sharpness, while each of them is indispensable. However, we find the optimizing gradient of existing methods cannot maintain high relevance to both loss value and loss sharpness during optimization, which severely affects their trade-off performance. To this end, we propose a novel SAM-based method for prompt learning, denoted as Gradient Constrained Sharpness-aware Context Optimization (GCSCoOp), to dynamically constrain the optimizing gradient, thus achieving above two-fold optimization objective simultaneously. Extensive experiments verify the effectiveness of GCSCoOp in the trade-off problem.",
                "authors": "Liangchen Liu, Nannan Wang, Dawei Zhou, Xinbo Gao, Decheng Liu, Xi Yang, Tongliang Liu",
                "citations": 2
            },
            {
                "title": "Cell Derived Extracellular Matrix Fiber Scaffolds Improve Recovery from Volumetric Muscle Loss.",
                "abstract": "Skeletal muscle is capable of robust self-repair following mild trauma, yet in cases of traumatic volumetric muscle loss (VML), where more than 20% of a muscle's mass is lost, this capacity is overwhelmed. Current soft tissue repair techniques and traditional rehabilitation have not been able to reverse the pathological changes that occur following VML injury. There are currently no surgical guidelines that effectively address the treatment of VML injuries which has motivated the exploration of implantable scaffolding strategies. In this study, the use of an allogenic scaffold fabricated using fibers built from the extracellular matrix (ECM) collected from muscle fibroblast cells during growth in culture was explored as a repair strategy using a lower-limb VLM injury (tibialis anterior muscle) in a rat model. Recovery outcomes (8 weeks) were explored in comparison to unrepaired controls as well previously examined allogenic scaffolds prepared from decellularized skeletal muscle (DSM) tissue (n=9 / sample group). At 8-week follow-up we found that the repair of VML injuries using ECM fiber scaffolds in combination with an autogenic mince muscle (MM) paste significantly increased recovery of peak contractile force (79±13%) of normal contralateral muscle) when compared to unrepaired VML controls (57±13%). Similar significant improvements were measured for muscle mass restoration (93±10%) in response to ECM fiber + MM repair when compared to unrepaired VML controls (73±13%). Of particular note, mass and contractile strength recovery outcomes for ECM fiber scaffolds were not significantly different from DSM + MM repair controls. These in-vivo findings support the further exploration of cell derived ECM fiber scaffolds as promising strategy for the repair of VML injury with recovery outcomes that compare favorably to current state of the field tissue derived ECM scaffolds. Furthermore, while the therapeutic potential of ECM fibers as a treatment strategy for muscle injury was explored in this study, they could be adapted for high throughput fabrication methods developed and routinely used by the textile industry to create a broad range of woven implants (ex. hernia meshes) for even greater impact.",
                "authors": "Cassandra Reed, Tai Huynh, J. Schluns, Payton Phelps, Jamie Hestekin, Jeff Wolchok",
                "citations": 2
            },
            {
                "title": "Parameterization investigation on distributed electric propulsion aircraft aerodynamic characteristics",
                "abstract": "Revealing the principle of distributed electric propulsion (DEP) plane aerodynamic design under the effect of strong aerodynamic coupling of distributed power-wing and the change law of total gain is the key to carrying out the design of distributed electric propulsion plane. For the distributed electric propulsion plane aerodynamic layout, the aerodynamic performance of the DEP configuration is studied based on the VLM-ADT aerodynamic characteristics fast solver method. This paper gives a set of fast parametric methods to research the aerodynamic performance of DEP layout, which mainly includes three comprehensive indexes such as lift-to-drag ratio L/D, power loading T/P, and lift loading L/P—focused on; the factor of propeller number, which affects the aerodynamic performance of DEP aerodynamic layout. As the propeller number increases, the lift-to-drag ratio decreases, but the wing’s lift efficiency increases.",
                "authors": "Z. Cheng, Youxu Yang, Bo Ye",
                "citations": 2
            },
            {
                "title": "Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity",
                "abstract": "This paper presents novel benchmarks for evaluating vision-language models (VLMs) in zero-shot recognition, focusing on granularity and specificity. Although VLMs excel in tasks like image captioning, they face challenges in open-world settings. Our benchmarks test VLMs’ consistency in understanding concepts across semantic granularity levels and their response to varying text specificity. Findings show that VLMs favor moderately fine-grained concepts and struggle with specificity, often misjudging texts that differ from their training data. Extensive evaluations reveal limitations in current VLMs, particularly in distinguishing between correct and subtly incorrect descriptions. While fine-tuning offers some improvements, it doesn’t fully address these issues, highlighting the need for VLMs with enhanced generalization capabilities for real-world applications. This study provides insights into VLM limitations and suggests directions for developing more robust models.",
                "authors": "Zhenlin Xu, Yi Zhu, Tiffany Deng, Abhay Mittal, Yanbei Chen, Manchen Wang, P. Favaro, Joseph Tighe, Davide Modolo",
                "citations": 2
            },
            {
                "title": "Simultaneous Microendoscopic Calcium Imaging and EEG Recording of Mouse Brain during Sleep",
                "abstract": "Sleep is a conserved biological process in the animal kingdom. Understanding the neural mechanisms underlying sleep state transitions is a fundamental goal of neurobiology, important for the development of new treatments for insomnia and other sleep-related disorders. Yet, brain circuits controlling this process remain poorly understood. A key technique in sleep research is to monitor in vivo neuronal activity in sleep-related brain regions across different sleep states. These sleep-related regions are usually located deeply in the brain. Here, we describe technical details and protocols for in vivo calcium imaging in the brainstem of sleeping mice. In this system, sleep-related neuronal activity in the ventrolateral medulla (VLM) is measured using simultaneous microendoscopic calcium imaging and electroencephalogram (EEG) recording. By aligning calcium and EEG signals, we demonstrate that VLM glutamatergic neurons display increased activity during the transition from wakefulness to non-rapid eye movement (NREM) sleep. The protocol described here can be applied to study neuronal activity in other deep brain regions involved in REM or NREM sleep.",
                "authors": "Sasa Teng, Yueqing Peng",
                "citations": 2
            },
            {
                "title": "A Simple Knowledge Distillation Framework for Open-world Object Detection",
                "abstract": "Open World Object Detection (OWOD) is a novel computer vision task with a considerable challenge, bridging the gap between classic object detection (OD) benchmarks and real-world object detection. In addition to detecting and classifying seen/known objects, OWOD algorithms are expected to localize all potential unseen/unknown objects and incrementally learn them. The large pre-trained vision-language grounding models (VLM, e.g. , GLIP) have rich knowledge about the open world, but are limited by text prompts and cannot localize indescribable objects. However, there are many detection scenarios which pre-defined language descriptions are unavailable during inference. In this paper, we attempt to specialize the VLM model for OWOD task by distilling its open-world knowledge into a language-agnostic detector. Surprisingly, we observe that the combination of a simple knowledge distillation approach and the automatic pseudo-labeling mechanism in OWOD can achieve better performance for unknown object detection, even with a small amount of data. Unfortunately, knowledge distillation for unknown objects severely affects the learning of detectors with conventional structures for known objects, leading to catastrophic forgetting. To alleviate these problems, we pro-pose the down-weight loss function for knowledge distillation from vision-language to single vision modality. Meanwhile, we decouple the learning of localization and recognition to reduce the impact of category interactions of known and unknown objects on the localization learning process. Comprehensive experiments performed on MS-COCO and PASCAL VOC demonstrate the effectiveness of our methods.",
                "authors": "Shuailei Ma, Yuefeng Wang, Ying Wei, Jiaqi Fan, Xinyu Sun, Peihao Chen, Enming Zhang",
                "citations": 2
            },
            {
                "title": "Prediction of Aerodynamic Characteristics of the Grid Fins using Low/High Fidelity Methods",
                "abstract": "To predict the aerodynamic characteristics of the grid fins from subsonic to supersonic speeds, low fidelity SW as well as CFD SW were applied. VLM(Vortex Lattice Method) and SE(Shock-Expansion) method were used at subsonic and supersonic speed domain respectively for the rapid prediction of low fidelity SW. For 2 configurations of the grid fins, the CFD computations and tests using the trisonic wind tunnel were also performed to compare the results of the grid fins. The results of low fidelity SW, CFD SW and the wind tunnel tests data were agreed well each other. Through further research on the grid fins, the effective parameters of the grid fin configurations according to the speed regime will be investigated.",
                "authors": "",
                "citations": 0
            },
            {
                "title": "Fine-Grained Visual–Text Prompt-Driven Self-Training for Open-Vocabulary Object Detection",
                "abstract": "Inspired by the success of vision–language methods (VLMs) in zero-shot classification, recent works attempt to extend this line of work into object detection by leveraging the localization ability of pretrained VLMs and generating pseudolabels for unseen classes in a self-training manner. However, since the current VLMs are usually pretrained with aligning sentence embedding with global image embedding, the direct use of them lacks fine-grained alignment for object instances, which is the core of detection. In this article, we propose a simple but effective fine-grained visual-text prompt-driven self-training paradigm for open-vocabulary detection (VTP-OVD) that introduces a fine-grained visual–text prompt adapting stage to enhance the current self-training paradigm with a more powerful fine-grained alignment. During the adapting stage, we enable VLM to obtain fine-grained alignment using learnable text prompts to resolve an auxiliary dense pixelwise prediction task. Furthermore, we propose a visual prompt module to provide the prior task information (i.e., the categories need to be predicted) for the vision branch to better adapt the pretrained VLM to the downstream tasks. Experiments show that our method achieves the state-of-the-art performance for open-vocabulary object detection, e.g., 31.5% mAP on unseen classes of COCO.",
                "authors": "Yanxin Long, Jianhua Han, Runhu Huang, Hang Xu, Yi Zhu, Chunjing Xu, Xiaodan Liang",
                "citations": 15
            },
            {
                "title": "Sea Level Rise Estimation on the Pacific Coast from Southern California to Vancouver Island",
                "abstract": "Previous studies have estimated the sea level rise (SLR) at various locations on the west coast of the USA and Vancouver Island in Canada. Here, we construct an entire SLR profile from Vancouver Island in the Pacific Northwest to San Diego in Southern California. First, we process global navigation satellite system (GNSS) measurements at 405 stations blanketing the whole coast to generate a profile of vertical land motion (VLM) known to bias century-long tide gauge (TG) measurements recording relative SLR (RSLR). We are then able to estimate the absolute SLR (ASLR) by correcting the SLR with the VLM. Our study emphasizes the relationship between the various tectonic movements (i.e., the Cascadia subduction zone, the San Andreas strike-slip fault system) along the Pacific coast which renders it difficult to accurately estimate the SLR. That is why we precisely model the stochastic noise of both GNSS and tide gauge time series using a combination of various models and information criterions (ICs). We also use the latest altimetry products and sea surface height (SSH) to compare it with ASLR at the same location as the TGs. This study supports previous analysis that the power law + white noise and generalized Gauss–Markov + white noise models are the best stochastic noise models for the GNSS time series. The new coastal profile confirms the large variability of VLM estimates in the Pacific Northwest around the Cascadia subduction zone in agreement with previous studies, and a similar result when the San Andreas fault comes onshore in Central California (San Francisco Bay). Negative RSLR values are mostly located in the Pacific Northwest (Vancouver Island and Olympic Peninsula). We also observe a much bigger variation (about 90%–150%) of the ASLR in the Pacific Northwest which is predominantly due to glacial isostatic adjustment (GIA). Moreover, the comparison between the ASLR and the SSH estimates shows similarities in the center of the studied area (South Washington, Oregon planes, and some parts of Southern California) where the tectonic activity does not significantly influence the TG measurements. Finally, the twentieth-century satellite geocentric ocean height rates show a global mean of 1.5 to 1.9 mm/yr. Our estimates based on ASLR and SSH are within this interval.",
                "authors": "Xiaoxing He, J. Montillet, Rui Manuel da Silva Fernandes, T. Melbourne, Weiping Jiang, Zhengkai Huang",
                "citations": 16
            },
            {
                "title": "A reduced PAPR hybrid OFDM visible light communication system",
                "abstract": null,
                "authors": "Basma Taha, H. Fayed, M. Aly, M. Mahmoud",
                "citations": 15
            },
            {
                "title": "A Reinforcement Learning-Based Automatic Video Editing Method Using Pre-trained Vision-Language Model",
                "abstract": "In this era of videos, automatic video editing techniques attract more and more attention from industry and academia since they can reduce workloads and lower the requirements for human editors. Existing automatic editing systems are mainly scene-or event-specific, e.g., soccer game broadcasting, yet the automatic systems for general editing, e.g., movie or vlog editing which covers various scenes and events, were rarely studied before, and converting the event-driven editing method to a general scene is nontrivial. In this paper, we propose a two-stage scheme for general editing. Firstly, unlike previous works that extract scene-specific features, we leverage the pre-trained Vision-Language Model (VLM) to extract the editing-relevant representations as editing context. Moreover, to close the gap between the professional-looking videos and the automatic productions generated with simple guidelines, we propose a Reinforcement Learning (RL)-based editing framework to formulate the editing problem and train the virtual editor to make better sequential editing decisions. Finally, we evaluate the proposed method on a more general editing task with a real movie dataset. Experimental results demonstrate the effectiveness and benefits of the proposed context representation and the learning ability of our RL-based editing framework.",
                "authors": "Panwen Hu, Nan Xiao, Feifei Li, Yongquan Chen, Rui Huang",
                "citations": 1
            },
            {
                "title": "KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization",
                "abstract": "In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema.It enables the model to effectively learn the required knowledge and skills from limited resources in the domain.Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task (Delbrouck et al., 2023), our model benefits from its training across multiple tasks and domains.With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.",
                "authors": "Gangwoo Kim, Hajung Kim, Lei Ji, Seongsu Bae, Chanhwi Kim, Mujeen Sung, Hyunjae Kim, Kun Yan, E. Chang, Jaewoo Kang",
                "citations": 1
            },
            {
                "title": "How to Use Language Expert to Assist Inference for Visual Commonsense Reasoning",
                "abstract": "Visual Commonsense Reasoning (VCR) task requires Vision and Language Model (VLM) to capture cognitive level clues from the visual-language input and give the right answers to questions and their rationales. Recently, although Pretrained Language Model (PLM) has been taken as a powerful in-domain knowledge base to the various tasks like image segmentation and visual question answering, PLM remains unexplored to generalize to the unseen multi-modal data in an out-domain way. In this paper, we explore how to use PLM to assist VLM for the challenging VCR task and propose a framework called Vision and Language Assisted with Expert Language Model (VLAELM). The VLAELM aims to employ a PLM with expert level of commonsense knowledge to assist reasoning, which is difficult for the VLM learning just from scarce multi-modal data. The experiments show that VLAELM achieves significant improvements against the strong baselines. Moreover, we validate credibility for language expert as knowledge base and measure application value between generalization and specialty in PLM.",
                "authors": "Zijie Song, Wenbo Hu, Hao Ye, Richang Hong",
                "citations": 1
            },
            {
                "title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models",
                "abstract": "In our work, we explore the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) on visual commonsense reasoning (VCR) problems. We find that VLMs and LLMs-based decision pipelines are good at different kinds of VCR problems. Pre-trained VLMs exhibit strong performance for problems involving understanding the literal visual content, which we noted as visual commonsense understanding (VCU). For problems where the goal is to infer conclusions beyond image content, which we noted as visual commonsense inference (VCI), VLMs face difficulties, while LLMs, given sufficient visual evidence, can use commonsense to infer the answer well. We empirically validate this by letting LLMs classify VCR problems into these two categories and show the significant difference between VLM and LLM with image caption decision pipelines on two subproblems. Moreover, we identify a challenge with VLMs' passive perception, which may miss crucial context information, leading to incorrect reasoning by LLMs. Based on these, we suggest a collaborative approach, named ViCor, where pre-trained LLMs serve as problem classifiers to analyze the problem category, then either use VLMs to answer the question directly or actively instruct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences. We evaluate our framework on two VCR benchmark datasets and outperform all other methods that do not require in-domain fine-tuning.",
                "authors": "KAI-QING Zhou, Kwonjoon Lee, Teruhisa Misu, X. Wang",
                "citations": 1
            },
            {
                "title": "Numerical Modelling of Static Aeroelastic Deformations of Slender Wing in Aerodynamic Design",
                "abstract": "Abstract The article presents the validation of two methods for analyzing the aerodynamic properties of the aircraft wing concerning aeroelastic effects. The first method is based on low-cost computational models (Euler–Bernoulli Beam Model and Vortex Lattice Method [VLM]). Its primary objective is to estimate the wing’s deformation early in the design stages and during the automatic optimization process. The second one is a method that uses solutions of unsteady Navier–Stokes equations (URANS). This method suits early design, particularly for unconventional designs or flight conditions exceeding lowfidelity method limits. The coupling of the flow and structural models was done by Radial Basis Functions implemented as a user-defined module in the ANSYS Fluent solver. The structural model has variants for linear and nonlinear wing deformations. Features enhancing applicability for real-life applications, such as the definition of deformable and nondeformable mesh zones with smooth transition between them, have been included in this method. A rectangular wing of a high-altitude long-endurance (HALE) aeroplane, built based on the NACA 0012 profile, was used to validate both methods. The resulting deflections and twists of the wing have been compared with reference data for the linear and nonlinear variants of the model.",
                "authors": "Pamela Bugała, Janusz Sznajder, Adam Sieradzki",
                "citations": 1
            },
            {
                "title": "Core content for venous and Lymphatic Medicine: 2022 revision.",
                "abstract": "The core content for a medical specialty outlines the scope of the discipline as well as the categories of knowledge considered essential to practice in the field. It provides a template for the development of curricula for medical school, graduate, and postgraduate education, as well as for creating certification standards. Venous and Lymphatic Medicine (VLM) is a specialty that has benefitted from contributions from specialists from several medical disciplines. Optimally, the societies, boards, and residency review committees representing these disciplines would uniformly recognize the scope of VLM to develop education and assessment standards to allow training and identification of qualified practitioners. In order to inform the standard setting bodies and other stakeholders of the current scope of VLM, a task force of VLM experts from cardiology, dermatology, emergency medicine, general surgery, interventional radiology, vascular medicine, and vascular surgery was formed to revise a 2014 consensus document defining the core content of the specialty of VLM.",
                "authors": "NM Khilnani, SM Wasan, P. Pappas, Z. Deol, JP Schoonover, SF Daugherty, RR Attaran, TV Cartee, TM Straight, J. Fish, JW Granzow, RS Winokur, KR Desai, G. Salazar, J. Stoughton, K. Gibson, AD Jones, JM Lohr, S. Vayuvegula, MH Meissner",
                "citations": 1
            },
            {
                "title": "Chronic Sustained Hypoxia Leads to Brainstem Tauopathy and Declines the Power of Rhythms in the Ventrolateral Medulla: Shedding Light on a Possible Mechanism.",
                "abstract": null,
                "authors": "Jamal Khalilpour, Hamid Soltani Zangbar, M. Alipour, Firouz Qaderi Pakdel, Zohre Zavari, Parviz Shahabi",
                "citations": 1
            },
            {
                "title": "Semantically Enhanced Scene Captions with Physical and Weather Condition Changes",
                "abstract": "Vision-Language models (VLMs), i.e., image-text pairs of CLIP, have boosted image-based Deep Learning (DL). Moreover, Visual-Question-Answer (VQA) tools and open-vocabulary semantic segmentation provide us with more detailed scene descriptions, i.e., qualitative texts, in captions. Images from surveillance, auto-drive, and mobile phone cameras have been used with segmentation and captions. However, unlike indoor scenes, outdoor scenes with uncontrolled illumination and noise can degrade the accuracy of segmented objects. Moreover, unpredictable events such as natural phenomena and accidents can cause dynamic and adverse scene changes over time. This greatly increases unseen objects due to sudden changes. Therefore, only a single state-of-the-art (SOTA) VLM and DL model cannot sufficiently generate and enhance captions. Even one time VQA is limited to generate a good answer. This paper proposes RoadCAP for refined and enriched qualitative and quantitative captions by DL models and VLMs with different tasks in a complementary manner. In particular, 2D-Contrastive Physical-Scale Pretraining (CPP) is also proposed for captions with physical scales. An iterative VQA model is proposed to further refine incomplete segmented images with the prompts. Experimental results outperform SOTA DL models and VLMs using images with adverse conditions. A higher semantic level in captions for real-world scene descriptions is shown as compared with SOTA VLMs.",
                "authors": "Hidetomo Sakaino",
                "citations": 1
            },
            {
                "title": "COCA: Classifier-Oriented Calibration via Textual Prototype for Source-Free Universal Domain Adaptation",
                "abstract": "Universal domain adaptation (UniDA) aims to address domain and category shifts across data sources. Recently, due to more stringent data restrictions, researchers have introduced source-free UniDA (SF-UniDA). SF-UniDA methods eliminate the need for direct access to source samples when performing adaptation to the target domain. However, existing SF-UniDA methods still require an extensive quantity of labeled source samples to train a source model, resulting in significant labeling costs. To tackle this issue, we present a novel plug-and-play classifier-oriented calibration (COCA) method. COCA, which exploits textual prototypes, is designed for the source models based on few-shot learning with vision-language models (VLMs). It endows the VLM-powered few-shot learners, which are built for closed-set classification, with the unknown-aware ability to distinguish common and unknown classes in the SF-UniDA scenario. Crucially, COCA is a new paradigm to tackle SF-UniDA challenges based on VLMs, which focuses on classifier instead of image encoder optimization. Experiments show that COCA outperforms state-of-the-art UniDA and SF-UniDA models.",
                "authors": "Xingxian Liu, Yi Zhou, Tao Zhou, Chun-Mei Feng, Ling Shao",
                "citations": 1
            },
            {
                "title": "Molecular organization of autonomic, respiratory, and spinally-projecting neurons in the mouse ventrolateral medulla",
                "abstract": "The ventrolateral medulla (VLM) is a crucial region in the brain for visceral and somatic control. It also serves as a significant source of synaptic input to the spinal cord. Experimental studies have shown that gene expression in individual VLM neurons is predictive of their function. However, the organizing principles of the VLM have remained uncertain. This study aimed to create a comprehensive dataset of VLM cells using single-cell RNA sequencing. The dataset was enriched with targeted sequencing of spinally-projecting and adrenergic/noradrenergic VLM neurons. Based on differentially expressed genes, the resulting dataset of 114,805 VLM cells identifies 23 subtypes of neurons, excluding those in the inferior olive, and 5 subtypes of astrocytes. Spinally-projecting neurons were found to be abundant in 7 subtypes of neurons, which were validated through in-situ hybridization. These subtypes included adrenergic/noradrenergic neurons, serotonergic neurons, and neurons expressing gene markers associated with pre-motor neurons in the ventromedial medulla. Further analysis of adrenergic/noradrenergic neurons and serotonergic neurons identified 9 and 6 subtypes, respectively, within each class of monoaminergic neurons. Marker genes that identify the neural network responsible for breathing were concentrated in 2 subtypes of neurons, delineated from each other by markers for excitatory and inhibitory neurons. These datasets are available for public download and for analysis with a user-friendly interface. Collectively, this study provides a fine-scale molecular identification of cells in the VLM, forming the foundation for a better understanding of the VLM’s role in vital functions and motor control.",
                "authors": "Dana C Schwalbe, Daniel S. Stornetta, Ruei-Jen Abraham-Fan, George Souza, Maira Jalil, Maisie E. Crook, John N. Campbell, Stephen B. G. Abbott",
                "citations": 1
            },
            {
                "title": "Enhance Reasoning Ability of Visual-Language Models via Large Language Models",
                "abstract": "Pre-trained visual language models (VLM) have shown excellent performance in image caption tasks. However, it sometimes shows insufficient reasoning ability. In contrast, large language models (LLMs) emerge with powerful reasoning capabilities. Therefore, we propose a method called TReE, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios. TReE contains three stages: observation, thinking, and re-thinking. Observation stage indicates that VLM obtains the overall information of the relative image. Thinking stage combines the image information and task description as the prompt of the LLM, inference with the rationals. Re-Thinking stage learns from rationale and then inference the final result through VLM.",
                "authors": "Yueting Yang, Xintong Zhang, Wenjuan Han",
                "citations": 1
            },
            {
                "title": "Successes and Challenges of MT-InSAR Methods in Coastal Regions: A Case Study on the Island of Tutuila, American Samoa",
                "abstract": "Multi-temporal InSAR (MT-InSAR) techniques are powerful time-series analysis methods that enable all-weather measurements of surface deformation. However, even advanced MT-InSAR techniques can produce inaccurate results in challenging environments, including many coastal regions. Here, we describe successes and challenges of applying MT-InSAR techniques for imaging post-seismic vertical land motion (VLM) on Tutuila Island in American Samoa. We tested traditional persistent scatterer (PS) and small baseline subset (SBAS) methods and also designed a customized redundant PS method to address observed inconsistencies in both PS and SBAS analysis. The redundant PS method yielded results with lower noise and more realistic spatial variation compared to PS and SBAS workflows. Our case study emphasizes the importance of careful application of MT-InSAR for measuring VLM along coastal regions, where subsidence and uplift rates may be moderate in the presence of numerous noise sources. In particularly challenging areas, customized approaches may be needed to produce useful solutions.",
                "authors": "Stacey A. Huang, J. Sauber",
                "citations": 1
            },
            {
                "title": "Efficiency Enhancement of Marine Propellers via Reformation of Blade Tip-Rake Distribution",
                "abstract": "This work addresses the effects of blade tip-rake reformation on the performance of marine propellers using a low-cost potential-based vortex-lattice method (VLM) and the high fidelity artificial compressibility CFD-RANS solver MaPFlow. The primary focus lies on determining whether the low-cost VLM, in conjunction with a multidimensional parametric model for the tip-rake and pitch/camber distributions, can produce a propeller geometry with improved efficiency. Due to the availability of experimental and numerical data, the NSRDC 4381-82 propellers were selected as reference geometries. Torque minimization serves as the objective function in the gradient-based optimization procedure under a thrust constraint, which translates into efficiency enhancement at the selected design advance ratio. The optimized 4381 propeller yields a +1.1% improvement in efficiency based on CFD-RANS, whereas for the modified skewed 4382 propeller, the efficiency gain is +0.5%. The performance enhancement is also evident at a region near the design advance ratio. The results suggest that the exploitation of low-cost VLM solvers can significantly reduce the CFD simulations required in the optimization process and thus can be effectively used for the design of propellers with tip-rake reformation.",
                "authors": "D. Anevlavi, Spiros Zafeiris, George Papadakis, K. Belibassakis",
                "citations": 1
            },
            {
                "title": "SKDF: A Simple Knowledge Distillation Framework for Distilling Open-Vocabulary Knowledge to Open-world Object Detector",
                "abstract": "In this paper, we attempt to specialize the VLM model for OWOD tasks by distilling its open-world knowledge into a language-agnostic detector. Surprisingly, we observe that the combination of a simple \\textbf{knowledge distillation} approach and the automatic pseudo-labeling mechanism in OWOD can achieve better performance for unknown object detection, even with a small amount of data. Unfortunately, knowledge distillation for unknown objects severely affects the learning of detectors with conventional structures for known objects, leading to catastrophic forgetting. To alleviate these problems, we propose the \\textbf{down-weight loss function} for knowledge distillation from vision-language to single vision modality. Meanwhile, we propose the \\textbf{cascade decouple decoding structure} that decouples the learning of localization and recognition to reduce the impact of category interactions of known and unknown objects on the localization learning process. Ablation experiments demonstrate that both of them are effective in mitigating the impact of open-world knowledge distillation on the learning of known objects. Additionally, to alleviate the current lack of comprehensive benchmarks for evaluating the ability of the open-world detector to detect unknown objects in the open world, we propose two benchmarks, which we name\"\\textbf{StandardSet}$\\heartsuit$\"and\"\\textbf{IntensiveSet}$\\spadesuit$\"respectively, based on the complexity of their testing scenarios. Comprehensive experiments performed on OWOD, MS-COCO, and our proposed benchmarks demonstrate the effectiveness of our methods. The code and proposed dataset are available at \\url{https://github.com/xiaomabufei/SKDF}.",
                "authors": "Shuailei Ma, Yuefeng Wang, Ying Wei, Jiaqi Fan, Enming Zhang, Xinyu Sun, Peihao Chen",
                "citations": 1
            },
            {
                "title": "Suppression of weed and insect populations by living and straw mulches in sesame (Sesamum indicum L.)",
                "abstract": null,
                "authors": "S. Azimi, R. Amini, Majid Hosseingolizadeh",
                "citations": 1
            },
            {
                "title": "Toxocariosis: From a One Health Perspective",
                "abstract": "Toxocariosis is a neglected zoonotic infection caused by the nematodes Toxocara canis or Toxocara cati. The distribution of the disease is worldwide and mainly affects dogs and cats, and its larval stage can cause human infection with serious repercussions on the health of its hosts. The infection causes a delay in the development, digestive disorders, nonspecific nervous manifestations, and occasionally death of some puppies and kittens associated with hyperparasitosis. In humans, the infection produces clinical syndromes known as visceral larva migrans (VLM), ocular larva migrans (OLM), neurotoxocariosis and covert toxocariosis. The close contact of people with their pets and the environmental conditions that favor the transmission of this diseased place it within the context of one health. The One Health concept is defined as the collaborative efforts of multiple disciplines (medical personnel, veterinarians, researchers, etc.) that work locally, nationally, and globally to achieve optimal health for people, animals, and the environment, from this perspective, toxocariosis is a study model in which classic and recent knowledge of the medical and veterinary area must be combined for its full understanding, with a goal of establishing integrative criteria for its treatment, control, and prevention.",
                "authors": "Muñoz-Guzmán",
                "citations": 1
            },
            {
                "title": "Effectiveness of Reproductive Health Education Through Video Learning Multimedia on Changes in Attitudes About Prevention of Student Sexual Harassment",
                "abstract": "Although not included in the list of countries with the highest rates of sexual violence in the world, Indonesia has recorded a setback in terms of child protection. The National Commission for Child Protection noted that in 2015 there were 218 cases of child sexual violence. Meanwhile, in 2016, the KPAI recorded 120 cases of sexual violence against children. Then in 2017, there were 116 cases recorded. Adolescent age groups, some of which are legally categorized as children, are vulnerable to becoming victims of sexual violence because children are always positioned as weak or considered to have limited ability to protect themselves. This study aims to determine the effectiveness of reproductive health education through VLM on changes in early adolescent attitudes in order to prevent sexual harassment. The research was conducted at the Nurul Fikri Integrated Islamic Elementary School, Makassar City. Using a quantitative research method with a quasi-experimental approach to 50 SDIT Nurul Fikri students. The results showed that there was no difference in the effectiveness of changing attitudes. This is because a short time in imparting education to students (i) is not enough to instantly change one's attitude. This research suggests the need for proper education regarding sexuality according to age stages through counseling or interesting media from the school and is sustainable.",
                "authors": "Nurbaya Nurbaya",
                "citations": 1
            },
            {
                "title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models",
                "abstract": "In our work, we explore the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) on visual commonsense reasoning (VCR) problems. We find that VLMs and LLMs-based decision pipelines are good at different kinds of VCR problems. Pre-trained VLMs exhibit strong performance for problems involving understanding the literal visual content, which we noted as visual commonsense understanding (VCU). For problems where the goal is to infer conclusions beyond image content, which we noted as visual commonsense inference (VCI), VLMs face difficulties, while LLMs, given sufficient visual evidence, can use commonsense to infer the answer well. We empirically validate this by letting LLMs classify VCR problems into these two categories and show the significant difference between VLM and LLM with image caption decision pipelines on two subproblems. Moreover, we identify a challenge with VLMs' passive perception, which may miss crucial context information, leading to incorrect reasoning by LLMs. Based on these, we suggest a collaborative approach, named ViCor, where pre-trained LLMs serve as problem classifiers to analyze the problem category, then either use VLMs to answer the question directly or actively instruct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences. We evaluate our framework on two VCR benchmark datasets and outperform all other methods that do not require in-domain fine-tuning.",
                "authors": "KAI-QING Zhou, Kwonjoon Lee, Teruhisa Misu, X. Wang",
                "citations": 1
            },
            {
                "title": "Chronic Sustained Hypoxia Leads to Brainstem Tauopathy and Declines the Power of Rhythms in the Ventrolateral Medulla: Shedding Light on a Possible Mechanism.",
                "abstract": null,
                "authors": "Jamal Khalilpour, Hamid Soltani Zangbar, M. Alipour, Firouz Qaderi Pakdel, Zohre Zavari, Parviz Shahabi",
                "citations": 1
            },
            {
                "title": "Molecular organization of autonomic, respiratory, and spinally-projecting neurons in the mouse ventrolateral medulla",
                "abstract": "The ventrolateral medulla (VLM) is a crucial region in the brain for visceral and somatic control. It also serves as a significant source of synaptic input to the spinal cord. Experimental studies have shown that gene expression in individual VLM neurons is predictive of their function. However, the organizing principles of the VLM have remained uncertain. This study aimed to create a comprehensive dataset of VLM cells using single-cell RNA sequencing. The dataset was enriched with targeted sequencing of spinally-projecting and adrenergic/noradrenergic VLM neurons. Based on differentially expressed genes, the resulting dataset of 114,805 VLM cells identifies 23 subtypes of neurons, excluding those in the inferior olive, and 5 subtypes of astrocytes. Spinally-projecting neurons were found to be abundant in 7 subtypes of neurons, which were validated through in-situ hybridization. These subtypes included adrenergic/noradrenergic neurons, serotonergic neurons, and neurons expressing gene markers associated with pre-motor neurons in the ventromedial medulla. Further analysis of adrenergic/noradrenergic neurons and serotonergic neurons identified 9 and 6 subtypes, respectively, within each class of monoaminergic neurons. Marker genes that identify the neural network responsible for breathing were concentrated in 2 subtypes of neurons, delineated from each other by markers for excitatory and inhibitory neurons. These datasets are available for public download and for analysis with a user-friendly interface. Collectively, this study provides a fine-scale molecular identification of cells in the VLM, forming the foundation for a better understanding of the VLM’s role in vital functions and motor control.",
                "authors": "Dana C Schwalbe, Daniel S. Stornetta, Ruei-Jen Abraham-Fan, George Souza, Maira Jalil, Maisie E. Crook, John N. Campbell, Stephen B. G. Abbott",
                "citations": 1
            },
            {
                "title": "GPS Imaging of Global Vertical Land Motion for Studies of Sea Level Rise",
                "abstract": "We estimate the rates and patterns of vertical land motion (VLM) on all locations on Earth's land surface using GPS Imaging. The solution is based on a large database of uniformly processed GPS data from solutions that are aligned to the International Terrestrial Reference Frame. We provide global maps and estimates of VLM at all tide gauges of the Permanent Service for Mean Sea Level to better constrain the difference between geocentric and relative sea level rise. To enable critical assessment of the VLM estimate, the temporal and spatial contributions to rate uncertainty and variability are generated and included for every gauge. Seasonality and trends of uplift are assessed and found to be strongly correlated with observations from gravity data suggesting that loading from the terrestrial hydrosphere is a dominant driver of non‐glacial isostatic adjustment (non‐GIA) VLM. Although stations are dominantly concentrated at subsiding parts of continents, GPS Imaging geographically balances VLM signals, correcting for bias associated with network distribution. This allows us to make a global assessment of the budget of uplift and subsidence attributable to GIA and non‐GIA sources. We show that the surface motion of the continents is on average upward, implying that the unobserved areas (composed of the ocean basins and ice‐covered areas) move on average downward with respect to Earth center. However, after correcting for the GIA the reverse is true, and observed areas subside on average implying that the unobserved areas undergo net non‐GIA‐related uplift.",
                "authors": "W. Hammond, G. Blewitt, C. Kreemer, R. Nerem",
                "citations": 30
            },
            {
                "title": "Chemogenetic activation of ventral medullary astrocytes enhances feeding and corticosterone release in response to mild glucoprivation.",
                "abstract": "To investigate the role of glial cells in the regulation of glucoprivic responses in rats, a chemogenetic approach was used to activate astrocytes neighboring catecholamine (CA) neurons in the ventromedial medulla (VLM) where A1 and C1 CA cell groups overlap (A1/C1). Previous results indicate that activation of CA neurons in this region is necessary and sufficient for feeding and corticosterone release in response to glucoprivation. However, it is not known whether astrocyte neighbors of CA neurons contribute to glucoregulatory responses. Hence, we made nano-injections of AAV5-GFAP-hM3D(Gq)-mCherry in order to selectively transfect astrocytes in the A1/C1 region with the excitatory DREADD, hM3D(Gq). After allowing time for DREADD expression we evaluated the rats for increased food intake and corticosterone release in response to low systemic doses of the antiglycolytic agent, 2-deoxy-D-glucose (2DG), alone and in combination with the hM3D(Gq) activator, clozapine-D-oxide (CNO). We found that DREADD transfected rats ate significantly more food when 2DG and CNO were co-administered than when either 2DG or CNO was injected alone. We also found that CNO significantly enhanced 2DG induced FOS expression in the A1/C1 CA neurons, and that corticosterone release also was enhanced when CNO and 2DG were administered together. Importantly, CNO-induced activation of astrocytes in the absence of 2DG did not trigger food intake or corticosterone release. Our results indicate that during glucoprivation, activation of VLM astrocytes cells markedly increases the sensitivity or responsiveness of neighboring A1/C1 CA neurons to glucose deficit, suggesting a potentially important role for VLM astrocytes in glucoregulation.",
                "authors": "Ai-jun Li, Qing Wang, R. Rogers, G. Hermann, R. Ritter, S. Ritter",
                "citations": 0
            },
            {
                "title": "Preconditioned Visual Language Inference with Weak Supervision",
                "abstract": "Humans can infer the affordance of objects by extracting related contextual preconditions for each scenario. For example, upon seeing an image of a broken cup, we can infer that this precondition prevents the cup from being used for drinking. Reasoning with preconditions of commonsense is studied in NLP where the model explicitly gets the contextual precondition. However, it is unclear if SOTA visual language models (VLMs) can extract such preconditions and infer the affordance of objects with them. In this work, we introduce the task of preconditioned visual language inference and rationalization (PVLIR). We propose a learning resource based on three strategies to retrieve weak supervision signals for the task and develop a human-verified test set for evaluation. Our results reveal the shortcomings of SOTA VLM models in the task and draw a road map to address the challenges ahead in improving them.",
                "authors": "Ehsan Qasemi, Amani Maina-Kilaas, Devadutta Dash, Khalid Alsaggaf, Muhao Chen",
                "citations": 0
            },
            {
                "title": "Aeroelasticity Model for Highly Flexible Aircraft Based on the Vortex Lattice Method",
                "abstract": "With the increasing use of composite materials in aviation, structural aircraft design often becomes limited by stiffness, rather than strength. As a consequence, aeroelastic analysis becomes more important to optimize both aircraft structures and control algorithms. A low computational cost aeroelasticity model based on VLM and rigid-body dynamics is proposed in this work. UAV flight testing is performed to evaluate the accuracy of the proposed model. Two flight sections are chosen to be modeled based on recorded aerodynamic surface control data. The calculated accelerations are compared with recorded flight data. It is found that the proposed model adequately captures the general flight profile, with acceleration peak errors between −6.2% and +8.4%. The average relative error during the entire flight section is 39% to 44%, mainly caused by rebounds during the beginning and end of pull-up maneuvers. The model could provide useful results for the initial phases of aircraft control law design when comparing different control algorithms.",
                "authors": "Mindaugas Dagilis, S. Kilikevičius",
                "citations": 0
            },
            {
                "title": "Empowering MultiModal Models’ In-Context Learning Ability through Large Language Models",
                "abstract": "Pretrained visual-language models (VLMs) have made progress in developing multimodal models to improve various tasks. However, they lack reasoning and in-context learning ability. Building on the success of large language models (LLMs) in general-purple NLP tasks, researchers anticipate that the VLM should also have the same strong reasoning and ICL ability through specific techniques, for example benefiting from LLMs. To boost VLMs to solve vision-language problems via few-shot exemplars, we suggest a vision-language model, called MIC1.",
                "authors": "Wenjuan Han, Haozhe Zhao, Zefan Cai",
                "citations": 0
            },
            {
                "title": "Dynamic Stability of a Blended Wing Body Unmanned Aerial Vehicle",
                "abstract": "This research paper focuses on studying the long-term stability of a Blended Wing Body (BWB) Unmanned Aerial System (UAS). The main focus of the analysis is on the effect of the position of the aircraft's center of gravity on its dynamic behavior. The paper also presents the development of a dynamic model that can be used for future analysis and the design of a controller for a BWB aircraft. The research makes use of the open-source software, namely, OpenVSP to create a model of a BWB configuration using a vortex lattice method (VLM) numerical simulation, which provides initial results for the aircraft's longitudinal stability derivatives. These results are then used to model the aircraft's dynamic response to a disturbance using MATLAB and Simulink. Overall, the goal of this research is to improve our understanding of the longitudinal dynamic stability of BWB aircraft and to provide tools for designing controllers and analyzing future design changes to the aircraft.",
                "authors": "Swarna Mayuri Kumar, M. Okasha, M. ElSayed, Ryan Des Cotes, Wan Faris Aizat Wan Aasim, A. Deif, A. Mourad",
                "citations": 0
            },
            {
                "title": "Study on the Influence of Direction of Rotation of Propellers in a Push-Pull Contra-Rotating Propulsor",
                "abstract": "Model Test is a key part of the entire ship design process. The interaction between the propulsor and the hull in these tests is expressed in terms of standard coefficients and forms an important design aspect which affects the final performance of the vessel. Even though these coefficients give important insight for the designers about the overall performance of the propulsor, however, it still lacks detailed information on the sources of these coefficients. The paper studies two design aspects namely the direction of rotation of propellers and the asymmetric strut of a Push-Pull CRP. The study is carried out using a hybrid Reynolds-Averaged Navier-Stokes (RANS) approach where a vortex-lattice code (VLM) is coupled to a RANS solver. The code is validated against model test results before a detailed flow analysis is made. The study shows that the choice and design of the asymmetric strut are very critical in Push-Pull CRP and hold a significant impact on the loading of the pair of propellers. The choice of direction of rotation of propellers is also quite important to ensure a favourable inflow condition to the strut and the load distribution on propellers. The impact of both these aspects on the design of propellers cannot always be determined by propulsive coefficients measured at model tests and needs a detailed CFD analysis.",
                "authors": "Mahish Mohan",
                "citations": 0
            },
            {
                "title": "Deep Video Understanding with Video-Language Model",
                "abstract": "Pre-trained video-language models (VLMs) have shown superior performance in high-level video understanding tasks, analyzing multi-modal information, aligning with Deep Video Understanding Challenge (DVUC) requirements.In this paper, we explore pre-trained VLMs' potential in multimodal question answering for long-form videos. We propose a solution called Dual Branches Video Modeling (DBVM), which combines knowledge graph (KG) and VLMs, leveraging their strengths and addressing shortcomings.The KG branch recognizes and localizes entities, fuses multimodal features at different levels, and constructs KGs with entities as nodes and relationships as edges.The VLM branch applies a selection strategy to adapt input movies into acceptable length and a cross-matching strategy to post-process results providing accurate scene descriptions.Experiments conducted on the DVUC dataset validate the effectiveness of our DBVM.",
                "authors": "Runze Liu, Yaqun Fang, Fan Yu, Ruiqi Tian, Tongwei Ren, Gangshan Wu",
                "citations": 0
            },
            {
                "title": "SBAS-InSAR Analysis of Coastal Subsidence in Kerala, India, to monitor Flood Inundation Risk due to Relative Sea Level Rise",
                "abstract": "The coastal sinking of land in many low-lying regions in the world has a greater potential risk of flooding-inundation hazards due to relative sea level rise. Kerala, being known for its varied geographical features, sharing a long coastline of 590 km in the southern part of India, has been affected by several floods in the past few decades. To develop an effective coastal subsidence monitoring system and predict the hazard risk, this study makes use of the Multi-Temporal Synthetic Aperture Radar (MT-InSAR) technique using Sentinel-1 data to obtain the Vertical Land Motion (VLM) over the Kerala coast and projects till 2100. The VLM results derived from the InSAR technique reveal a large subsidence of more than 20mm/year in the Kuttanad region of Alappuzha district. Further, we incorporate the VLM, the future projection of sea level from the Intergovernmental Panel on Climate Change (IPCC) AR6 report, and the high spatial resolution of the Digital Elevation Model (DEM) to map the low-lying fast subsiding zones that are prone to future flood inundation due to relative sea level rise. We derive future flood inundation maps for the years 2030, 2050, 2070, and 2100 by considering various socio-economic scenarios.",
                "authors": "R. Aparna, Chandrakanta Ojha",
                "citations": 0
            },
            {
                "title": "Spatiotemporal Groundwater Storage Dynamics and Aquifer Mechanical Properties in the Santa Clara Valley Inferred From InSAR Deformation Over 2017–2022",
                "abstract": "We used Interferometric Synthetic Aperture Radar (InSAR)‐derived vertical land motion (VLM) timeseries during 2017–2022 to examine the compounding impacts of natural and anthropogenic processes on groundwater dynamics in the Santa Clara Valley (SCV). VLM strongly correlates (>0.75) with groundwater level in both unconfined and confined aquifers. We show that VLM in SCV is mainly driven by groundwater dynamics in deep aquifer layers below 120 m. Our results show that during the most recent drought from March 2019 to November 2021, Santa Clara County subsided up to 30 mm due to groundwater depletion, three times as large as average seasonal amplitude of VLM. Owing to the managed aquifer recharge, the region has been able to avoid unrecoverable land subsidence. We utilize InSAR data to calibrate storage coefficient and lag time related to delayed response of clay interbeds to groundwater level changes, which further serves to estimate groundwater volume loss in confined aquifer units during drought.",
                "authors": "K. Ghobadi‐Far, S. Werth, M. Shirzaei, R. Bürgmann",
                "citations": 0
            },
            {
                "title": "Mode of Mechanical Ventilation in a Case of Venolymphatic Malformation: Spontaneous-Saves, Positive-Precludes",
                "abstract": "Mediastinal venolymphatic malformations (VLM) are rare tumours, with very few reported cases in the literature. Arising often from the anterior mediastinum, VLM manifests symptoms based on invaded surrounding structures. Masses from the anterior and superior mediastinum pose an anaesthetic challenge for airway and hemodynamic management. A 7-month-old male child presented with a progressively growing mass over the left anterior chest wall for one month, about 4x4 cm, with diffuse margins and now expanded to involve the root of the neck and into the axilla. The patient was free from any apparent systemic illness. The breathing difficulty worsened in the past week with noisy respiration associated with feeding difficulty and hence sought medical admission to the paediatrics emergency unit. In conclusion, such huge mediastinal masses are managed better under spontaneous ventilation with an adequate surgical depth of anaesthesia to maintain appropriate respiratory compliance and necessitate lower peak inspiratory pressure. Given rare cases reported in the literature, similar topics would help choose the modus of ventilation and their safe management.",
                "authors": "Prateek Arora, S. Singha, O. Mujahid, Snigdha Kumari, Abinaya Prakashbabu",
                "citations": 0
            },
            {
                "title": "Rapid, activity-dependent co-release of GABA and glycine in the mouse brainstem",
                "abstract": "Presympathetic neurons in the ventrolateral and ventromedial medulla (VLM/VMM) control the sympathetic tone, and GABA was identified as a primary mechanism for sympathoinhibition. Intriguingly, there is evidence that both GABA and glycine efficiently inhibit sympathetic tone; however, the mechanism of glycine release is unknown. In this study, we tested the hypothesis that GABA and glycine are co-released in the VLM/VMM. Glycine is recycled by glycine transporter 2 (GlyT2), which is a reliable marker of glycinergic neurons. Therefore, we crossed heterozygous GlyT2Cre mice with floxed channelrhodopsin 2 (ChR2-EYFP) mice to generate GlyT2 reporter mice (GlyT2ChR2/EYFP), which were used to determine the mechanism of glycine release and reveal the location of glycinergic neurons. Whole-cell patch-clamp recordings from presympathetic VLM/VMM neurons revealed that GABA mediates most spontaneous inhibitory postsynaptic currents (sIPSCs); whereas increased activity of inhibitory inputs promotes the release of glycine. Light stimulation of glycinergic fibers expressing ChR2 in GlyT2ChR2/EYFP mice evoked IPSCs (eIPSCs) with glycinergic and GABAergic components. Bath application of GABAA receptors antagonist (bicuculline, 10 μM) decreased the amplitude of eIPSCs, suggesting that approximately 70% of the eIPSCs are generated by glycine. Moreover, blockade of GlyT2 partially diminished the glycinergic component of the light-evoked IPSCs. Using GlyT2ChR2/EYFP mice, we found that glycinergic fibers are abundant throughout the rostral brainstem, and GlyT2-expressing neurons were distributed within the ventral gigantocellular nucleus and medial reticular formation. Immunofluorescence labeling of glycine receptor subunits in the VLM/VMM revealed the expression of two prevalent forms of glycine receptors formed by α1 and α3. In summary, we found that GABA and glycine are co-released in the VLM/VMM, and blockage of GlyT2 decreased the amplitude of light-evoked IPSCs, suggesting that GABA and glycine are likely located in separate vesicles. Our data provide evidence for the co-inhibition of presympathetic neurons in the VLM/VMM by GABA and glycine and suggest that GABA controls the threshold excitability, while glycine is released on demand to increase the strength of inhibition. Supported by: NIDDK122842 and Tulane Brain Institute Marko Spark Innovation Research Fund This is the full abstract presented at the American Physiology Summit 2023 meeting and is only available in HTML format. There are no additional versions or additional content available for this abstract. Physiology was not involved in the peer review process.",
                "authors": "Hong Gao, Lucie D. Desmoulins, Adrien J R Molinas, A. Zsombok, A. Derbenev",
                "citations": 0
            },
            {
                "title": "A mathematical model for the optimal configuration of automated storage systems with sliding trays",
                "abstract": "This work aims at contributing to the advancement of Logistics 4.0, focusing on the management of the storage of goods. The goal is to solve the complex problem of efficiently and rapidly configuring Vertical Lift Modules (VLMs) with sliding trays in automated warehouses. This problem is still barely discussed in the related literature and most contributions mainly focus on the optimization of the VLM throughput instead of trays allocation and respective items configuration. To fill this gap, this work proposes a novel mathematical model that allows to properly represent and solve this complex problem, taking into account practical logistic constraints. The problem is defined as a mixed integer non-linear programming model, which is validated on realistic scenarios. Further, a scalability analysis is performed to evaluate its performance even in complex scenarios. The obtained results demonstrate the effectiveness of the model in defining space efficient configurations in short computation time.",
                "authors": "G. Tresca, G. Cavone, Raffaele Carli, M. Dotoli",
                "citations": 0
            },
            {
                "title": "Brain circuits underlying the sympathetic control of the liver",
                "abstract": "The sympathetic nervous system plays an important role in the maintenance of hepatic glucose homeostasis, through stimulating glucose production and glycogenolysis. Pre-sympathetic neurons in the brainstem and hypothalamus govern the sympathetic output to the liver and thus control hepatic metabolism and glycemia. Intriguingly, despite the importance of this central pathway, specific information regarding the neural circuits and properties of liver-related neurons is limited. In this study, we tested the hypothesis that liver-related neurons in the paraventricular nucleus of the hypothalamus (PVN) and ventral lateral medulla of the brainstem (VLM) are part of a pre-sympathetic central circuit that is involved in the control of energy intake and regulation of glucose homeostasis. First, liver-related neurons were identified with a retrograde trans-synaptic viral tracer and their phenotype was revealed. In the VLM, ~23.2% of liver-related neurons expressed Dopamine Beta Hydroxylase (n=5) and ~9.3% Tyrosine Hydroxylase (n=6), which is consistent with previous reports demonstrating that catecholaminergic neurons in the VLM play a critical role in glucose regulation. In the PVN, ~6.8% of liver-related neurons expressed oxytocin (n=7) and ~27.5% expressed Single Minded 1 (Sim1) protein (n=7), two major neuronal populations in the PVN known for their involvement in the regulation of energy metabolism. Next, pre-sympathetic liver-related PVN neurons with projections to the VLM were identified and optogenetic approach was used to reveal functional connections between PVN neurons and pre-sympathetic liver-related neurons in the VLM. Light stimulation of projections of Sim1-expressing PVN neurons resulted in evoked excitatory postsynaptic currents in a subset of liver-related VLM neurons, which suggests the existence of monosynaptic connections between PVN and pre-sympathetic liver-related neurons in the VLM. Then the involvement of pre-sympathetic, VLM-projecting PVN neurons was determined in the regulation of energy homeostasis using chemogenetic approaches. Stimulation of VLM-projecting PVN neurons increased blood glucose levels (basal=137± 6 mg/dl vs. CNO (1mg/kg)=181± 12 mg/dl; paired t-test P=0.01, n=9) and decreased food intake during refeeding (two way ANOVA on paired and repeated measures, P<0.0001, n=9), while the respiratory exchange ratio and locomotor activity were not altered. In summary, our data strongly suggest the existence of a central circuit involving direct connections between pre-sympathetic PVN and catecholaminergic, liver-related VLM neurons and demonstrate that these neurons are involved in the regulation of energy homeostasis. Moreover, these results provide novel information about the pre-sympathetic central circuits involved in the regulation of the liver, and thus energy homeostasis and are crucial for the development of new strategies to improve glucose homeostasis via the autonomic nervous system. NIDDK122842 and Tulane Brain Institute Marko Spark Innovation Research Fund This is the full abstract presented at the American Physiology Summit 2023 meeting and is only available in HTML format. There are no additional versions or additional content available for this abstract. Physiology was not involved in the peer review process.",
                "authors": "Lucie D. Desmoulins, Adrien J R Molinas, Hong Gao, Roslyn Davis, A. Derbenev, A. Zsombok",
                "citations": 0
            },
            {
                "title": "GELDA: A generative language annotation framework to reveal visual biases in datasets",
                "abstract": "Bias analysis is a crucial step in the process of creating fair datasets for training and evaluating computer vision models. The bottleneck in dataset analysis is annotation, which typically requires: (1) specifying a list of attributes relevant to the dataset domain, and (2) classifying each image-attribute pair. While the second step has made rapid progress in automation, the first has remained human-centered, requiring an experimenter to compile lists of in-domain attributes. However, an experimenter may have limited foresight leading to annotation\"blind spots,\"which in turn can lead to flawed downstream dataset analyses. To combat this, we propose GELDA, a nearly automatic framework that leverages large generative language models (LLMs) to propose and label various attributes for a domain. GELDA takes a user-defined domain caption (e.g.,\"a photo of a bird,\"\"a photo of a living room\") and uses an LLM to hierarchically generate attributes. In addition, GELDA uses the LLM to decide which of a set of vision-language models (VLMs) to use to classify each attribute in images. Results on real datasets show that GELDA can generate accurate and diverse visual attribute suggestions, and uncover biases such as confounding between class labels and background features. Results on synthetic datasets demonstrate that GELDA can be used to evaluate the biases of text-to-image diffusion models and generative adversarial networks. Overall, we show that while GELDA is not accurate enough to replace human annotators, it can serve as a complementary tool to help humans analyze datasets in a cheap, low-effort, and flexible manner.",
                "authors": "Krish Kabra, Kathleen M. Lewis, Guha Balakrishnan",
                "citations": 0
            },
            {
                "title": "Diet induced obesity alters the excitability of liver-related PVN neurons",
                "abstract": "Stimulation of hepatic sympathetic nerves increases glucose production and glycogenolysis. Activity of pre-sympathetic neurons in the paraventricular nucleus (PVN) of the hypothalamus and in the ventrolateral and ventromedial medulla (VLM/VMM) largely influence the sympathetic output. Despite the importance of these central circuits, the cellular properties of pre-sympathetic liver-related neurons remain to be determined. Here, we tested the hypotheses that the activity of pre-sympathetic liver-related neurons in the PVN and VLM/VMM is altered in diet induced obese (DIO) mice, as well as their sensitivity to insulin. Whole-cell patch-clamp recordings were conducted from liver-related neurons in male DIO (15–19-week-old) and control mice. During a step protocol, current steps (0-30pA, duration 1s) were applied to reveal the firing activity of neurons. Our data showed increased excitability of liver-related PVN neurons in DIO mice (n=9) compared to Control mice (n=10) (simple linear regression, different elevation, p=0.04). Moreover, in DIO mice, insulin decreased the frequency of action potentials (basal: 0.83±0.28 Hz vs insulin: 0.47±0.22 Hz, n=10; Wilcoxon test, p=0.01), while it had no effect on the firing in control mice (basal: 0.79±0.37 Hz vs insulin: 0.51±0.26 Hz, n=8, Wilcoxon test, p=0.11). Similarly, in DIO mice, insulin decreased the firing of liver-related PVN neurons after current injection (simple linear regression, different elevation, p=0.01, n=9). Next, liver-related PVN neurons were identified based on their projection to the VLM/VMM, and we found that the excitability of pre-sympathetic liver-related PVN neurons (n=7) was increased compared to liver-related PVN neurons (n=10) (simple linear regression, different elevation, p=0.00). Intriguingly, insulin did not alter the excitability of pre-sympathetic liver-related PVN neurons neither in control (n=8) or DIO mice (n=7) (simple linear regression, different elevation (Control: p=0.27; DIO: p=0.21). Since insulin sensitivities of liver-related PVN and pre-sympathetic liver-related PVN neurons were different, recordings were conducted from liver-related neurons in the VLM/VMM, which is well-known for its control over the sympathetic tone. In control mice, during the step protocol insulin had no effect on the excitability (simple linear regression, different elevation, p=0.37 n=7) whereas it decreased the firing rate of liver-related VLM/VMM neurons in DIO mice (nonlinear fit, different curve, p=0.03, n=4). Additionally, insulin receptor expression was confirmed by single cell digital droplet PCR in a subset of liver-related neurons by collecting mRNA from the cytoplasm of the recorded neurons. In summary, these results demonstrate that the firing activity of liver-related neurons are altered in DIO mice as well as their sensitivity to insulin. These data provide further evidence for cellular changes involving neurons regulating the sympathetic output to the liver. This work was supported by the NIH (DK-122842 to AZs and AVD), and Marko Spark Innovation Research Fund to AZs and AVD. We also thank the Tulane Brain Institute Cell and Tissue Imaging Core and the NIH Center for Neuroanatomy and Neurotropic viruses for the PRVs (P40 OD010996). This is the full abstract presented at the American Physiology Summit 2023 meeting and is only available in HTML format. There are no additional versions or additional content available for this abstract. Physiology was not involved in the peer review process.",
                "authors": "Adrien J R Molinas, Lucie D. Desmoulins, Roslyn Davis, Hong Gao, Ryousuke Satou, A. Derbenev, A. Zsombok",
                "citations": 0
            },
            {
                "title": "Visceral larva Migrans in a Young Italian Patient: A Diagnostic Dilemma",
                "abstract": null,
                "authors": "Emanuela Francalanci, T. Manciulli, Giulia Bandini, Pierluigi Blanc, Sara Irene Bonelli, Enrico Brunetti, E. Gotuzzo, C. Crețu, F. Gobbi, A. Bartoloni, L. Zammarchi",
                "citations": 0
            },
            {
                "title": "Prompt Ensemble Self-training for Open-Vocabulary Domain Adaptation",
                "abstract": "Traditional domain adaptation assumes the same vocabulary across source and target domains, which often struggles with limited transfer flexibility and efficiency while handling target domains with different vocabularies. Inspired by recent vision-language models (VLMs) that enable open-vocabulary visual recognition by reasoning on both images and texts, we study open-vocabulary domain adaptation (OVDA), a new unsupervised domain adaptation framework that positions a pre-trained VLM as the source model and transfers it towards arbitrary unlabelled target domains. To this end, we design a Prompt Ensemble Self-training (PEST) technique that exploits the synergy between vision and language to mitigate the domain discrepancies in image and text distributions simultaneously. Specifically, PEST makes use of the complementary property of multiple prompts within and across vision and language modalities, which enables joint exploitation of vision and language information and effective learning of image-text correspondences in the unlabelled target domains. Additionally, PEST captures temporal information via temporal prompt ensemble which helps memorize previously learnt target information. Extensive experiments show that PEST outperforms the state-of-the-art consistently across 10 image recognition tasks.",
                "authors": "Jiaxing Huang, Jingyi Zhang, Han Qiu, Sheng Jin, Shijian Lu",
                "citations": 0
            },
            {
                "title": "Stakeholders’ Perspective on the Quality of Virtual Learning Material in Google Classroom",
                "abstract": "The COVID-19 pandemic stimulated education system worldwide to employ online learning to support learning despite difficult times. To respond to this challenge and to promote Sustainable Development Goal (SDG) 4, advocating quality education, a virtual learning material (VLM) for Biology was articulated in Google Classroom. Accordingly, this study aimed to evaluate its acceptability and conformity to the international standards for online courses using the Open SUNY Course Quality Review (OSCQR) rubric as the questionnaire. Respondents (N=40) involved four stakeholders: Senior High School Students, Pre-service Science Teachers, High School Teachers, and Science Instructors/Professors, with n=10 representatives each group. Their perspectives of the VLM acceptability in terms of Overview and Information, Technology and Tool, Design and Layout, Content and Activities, Interaction, and Assessment and Feedback were obtained through a Google Form by rating the 50-item questionnaire on a 4-point Likert scale together with two open-ended questions. With a grand mean of 3.81(SD=0.40), the findings revealed highly acceptable results. The qualitative responses also substantiated this result. Significant differences in the responses are also discussed, while the Cronbach alpha reliability test is high (α=0.923). Significantly, the VLM conforms with the international standards for online course design, suggesting it can be implemented among target students.",
                "authors": "Mary Rose Briones, Maricar S. Prudente, D. D. Errabo",
                "citations": 0
            },
            {
                "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
                "abstract": "The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, there is a lack of datasets in the academic community that can effectively evaluate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we address this gap by introducing two novel multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K). These datasets incorporate both visual and text-based inputs and outputs. Furthermore, to facilitate the accountability of multimodal systems in rejecting human requests, similar to language-based ChatGPT conversations, we introduce specific rules as supervisory signals within the datasets. This allows the trained VLM to provide a yes or no answer after engaging in visual and textual reasoning, accompanied by a language explanation to clarify the reasons behind the inability to execute the given human instruction. Our proposed method involves a two-stage training procedure, which includes training the image auto-encoder and the auto-regressive transformer from scratch. The first stage employs a discrete variational autoencoder (dVAE) to compress each image into concise tokens, which are then combined with text tokens into a single data stream. This stream is subsequently fed into the decoder-based transformer to generate visual re-creations and textual feedback in the second stage. We conduct comprehensive analyses of experimental results, focusing on re-created image quality, answer accuracy, and the model's behavior when faced with uncertainty and imperfect user queries. Through our explorations and findings, we aim to contribute valuable insights into the accountability of textual-visual generative models.",
                "authors": "Zhiwei Zhang, Yuliang Liu",
                "citations": 0
            },
            {
                "title": "Self Evaluation Using Zero-shot Learning",
                "abstract": "With recent advances in large language models (LLM) and visual language models (VLM), there has been great interest in extending these models to robotics and task execution. Recent studies have bridged the gap between large language models and robotic language processing by providing a real-world grounding to help the robot better understand and execute tasks. A major drawback of existing approaches is that they still require a human to monitor and evaluate the robot’s performance. As robots expand into a wide range of industries from agriculture to healthcare, it is important that they can self-monitor their performance and identify when any mistake occurs. We propose a model that combines large-language models and visual language models to self-evaluate task execution based on video data. We utilize LLMs to generate outcome descriptions for each step and then provide these as input prompts to a VLM to determine whether a step has been completed. Based on video data, our solution identifies if each step in a task has been executed correctly and notices when a mistake has been made. We design a score metric that assesses the status of completion of each step in the task. Upon testing the model on a variety of household-related tasks, it demonstrated successful evaluation of task completion. Our model could be implemented in robots to self-evaluate task execution, leading to more accurate and efficient robotic systems.",
                "authors": "Anuva Banwasi, Xinghua Sun, Rohith Ravindranath, Marc Vazquez",
                "citations": 0
            },
            {
                "title": "Focusing on what to decode and what to train: SOV Decoding with Specific Target Guided DeNoising and Vision Language Advisor",
                "abstract": "Recent transformer-based methods achieve notable gains in the Human-object Interaction Detection (HOID) task by leveraging the detection of DETR and the prior knowledge of Vision-Language Model (VLM). However, these methods suffer from extended training times and complex optimization due to the entanglement of object detection and HOI recognition during the decoding process. Especially, the query embeddings used to predict both labels and boxes suffer from ambiguous representations, and the gap between the prediction of HOI labels and verb labels is not considered. To address these challenges, we introduce SOV-STG-VLA with three key components: Subject-Object-Verb (SOV) decoding, Specific Target Guided (STG) denoising, and a Vision-Language Advisor (VLA). Our SOV decoders disentangle object detection and verb recognition with a novel interaction region representation. The STG denoising strategy learns label embeddings with ground-truth information to guide the training and inference. Our SOV-STG achieves a fast convergence speed and high accuracy and builds a foundation for the VLA to incorporate the prior knowledge of the VLM. We introduce a vision advisor decoder to fuse both the interaction region information and the VLM's vision knowledge and a Verb-HOI prediction bridge to promote interaction representation learning. Our VLA notably improves our SOV-STG and achieves SOTA performance with one-sixth of training epochs compared to recent SOTA. Code and models are available at https://github.com/cjw2021/SOV-STG-VLA",
                "authors": "Junwen Chen, Yingcheng Wang, Keiji Yanai",
                "citations": 0
            },
            {
                "title": "Investigation of very low mass binaries using VLT/NaCo",
                "abstract": "Context: Most stars in the galactic stellar population are low-mass stars. Very low-mass (VLM) stars are a subset of the low-mass stars typically defined in terms of the stellar masses ranging from 0.6 M_sun to the hydrogen-burning limit of about 0.075 M_sun. Aim: The observational studies of VLM binaries can provide effective diagnostics for testing the VLM formation scenarios. The small size of VLMs makes them suitable candidates to detect planets around them in the habitable zone. Methods: In this work, using the high-resolution near-infrared adaptive optics imaging from the NaCo instrument installed on the Very Large Telescope, we report the discovery of a new binary companion to the M-dwarf LP 1033-31 and also confirm the binarity of LP 877-72. We have characterized both stellar systems and estimated the properties of their individual components. Results and Conclusions: We have found that LP 1033-31 AB with the spectral type of M4.5+M4.5 has a projected separation of 6.7+/-1.3 AU. On the other hand, with the spectral type of M1+M4, the projected separation of LP 877-72 AB is estimated to be 45.8+/-0.3 AU. We further investigated the masses, surface gravity, radii, and effective temperature of the detected components. The orbital period of LP 1033-31 and LP 877-72 systems are estimated to be ~28 and ~349 yr, respectively. Our analysis suggests that there is a possibility of finding up to `two' exoplanets around LP 877-72 B. In contrast, the maximum probabilities of hosting exoplanets around LP 877-72 A, LP 1033-31 A, and LP 1033-31 B are estimated to be only ~50%.",
                "authors": "S. Karmakar, A. Rajpurohit, D. Homeier",
                "citations": 0
            },
            {
                "title": "Treatment of Tide Gauge Time Series and Marine GNSS Measurements for Vertical Land Motion with Relevance to the Implementation of the Baltic Sea Chart Datum 2000",
                "abstract": "Tide gauge (TG) time series and GNSS measurements have become standard datasets for various scientific and practical applications. However, the TG and geodetic networks in the Baltic Sea region are deforming due to vertical land motion (VLM), the primary cause of which is the glacial isostatic adjustment. Consequently, a correction for VLM, either obtained from a suitable VLM model or by utilizing space-geodetic techniques, must be applied to ensure compatibility of various data sources. It is common to consider the VLM rate relative to an arbitrary reference epoch, but this also yields that the resulting datasets may not be directly comparable. The common height reference, Baltic Sea Chart Datum 2000 (BSCD2000), has been initiated to facilitate the effective use of GNSS methods for accurate navigation and offshore surveying. The BSCD2000 agrees with the current national height realizations of the Baltic Sea countries. As TGs managed by national authorities are rigorously connected to the national height systems, the TG data can also be used in a common system. Hence, this contribution aims to review the treatment of TG time series for VLM and outline potential error sources for utilizing TG data relative to a common reference. Similar consideration is given for marine GNSS measurements that likewise require VLM correction for some marine applications (such as validating marine geoid models). The described principles are illustrated by analyzing and discussing numerical examples. These include investigations of TG time series and validation of shipborne GNSS determined sea surface heights. The latter employs a high-resolution geoid model and hydrodynamic model-based dynamic topography, which is linked to the height reference using VLM corrected TG data. Validation of the presented VLM corrected marine GNSS measurements yields a 1.7 cm standard deviation and –2.7 cm mean residual. The estimates are 1.9 cm and –10.2 cm, respectively, by neglecting VLM correction. The inclusion of VLM correction thus demonstrates significant improvement toward data consistency. Although the focus is on the Baltic Sea region, the principles described here are also applicable elsewhere.",
                "authors": "Sander Varbla, J. Ågren, A. Ellmann, M. Poutanen",
                "citations": 12
            },
            {
                "title": "Geodetic evidence for a buoyant mantle plume beneath the Eifel volcanic area, NW Europe",
                "abstract": "\n The volcanism of the Eifel volcanic field (EVF), in west-central Germany, is often considered an example of hotspot volcanism given its geochemical signature and the putative mantle plume imaged underneath. EVF's setting in a stable continental area provides a rare natural laboratory to image surface deformation and test the hypothesis of there being a thermally buoyant plume. Here we use Global Positioning System (GPS) data to robustly image vertical land motion (VLM) and horizontal strain rates over most of intraplate Europe. We find a spatially coherent positive VLM anomaly over an area much larger than the EVF and with a maximum uplift of ∼1 mm yr−1 at the EVF (when corrected for glacial isostatic adjustment). This rate is considerably higher than averaged over the Late-Quaternary. Over the same area that uplifts, we find significant horizontal extension surrounded by a radial pattern of shortening, a superposition that strongly suggests a common dynamic cause. Besides the Eifel, no other area in NW Europe shows significant positive VLM coupled with extensional strain rates, except for the much broader region of glacial isostatic adjustment. We refer to this 3-D deformation anomaly as the Eifel Anomaly. We also find an extensional strain rate anomaly near the Massif Central volcanic field surrounded by radial shortening, but we do not detect a significant positive VLM signal there. The fact that the Eifel Anomaly is located above the Eifel plume suggests that the plume causes the anomaly. Indeed, we show that buoyancy forces induced by the plume at the bottom of the lithosphere can explain this remarkable surface deformation. Plume-induced deformation can also explain the relatively high rate of regional seismicity, particularly along the Lower Rhine Embayment.",
                "authors": "C. Kreemer, G. Blewitt, P. Davis",
                "citations": 51
            },
            {
                "title": "Control of non-REM sleep by ventrolateral medulla glutamatergic neurons projecting to the preoptic area",
                "abstract": null,
                "authors": "Sasa Teng, Fenghua Zhen, Li Wang, Jose Canovas Schalchli, Jane Simko, Xinyue Chen, Hao Jin, Christopher D. Makinson, Yueqing Peng",
                "citations": 11
            },
            {
                "title": "Generalizing Multiple Object Tracking to Unseen Domains by Introducing Natural Language Representation",
                "abstract": "Although existing multi-object tracking (MOT) algorithms have obtained competitive performance on various benchmarks, almost all of them train and validate models on the same domain. The domain generalization problem of MOT is hardly studied. To bridge this gap, we first draw the observation that the high-level information contained in natural language is domain invariant to different tracking domains. Based on this observation, we propose to introduce natural language representation into visual MOT models for boosting the domain generalization ability. However, it is infeasible to label every tracking target with a textual description. To tackle this problem, we design two modules, namely visual context prompting (VCP) and visual-language mixing (VLM). Specifically, VCP generates visual prompts based on the input frames. VLM joints the information in the generated visual prompts and the textual prompts from a pre-defined Trackbook to obtain instance-level pseudo textual description, which is domain invariant to different tracking scenes. Through training models on MOT17 and validating them on MOT20, we observe that the pseudo textual descriptions generated by our proposed modules improve the generalization performance of query-based trackers by large margins.",
                "authors": "En Yu, Songtao Liu, Zhuoling Li, Jinrong Yang, Zeming Li, Shoudong Han, Wenbing Tao",
                "citations": 10
            },
            {
                "title": "A Hybrid Unsupervised Approach for Retinal Vessel Segmentation",
                "abstract": "Retinal vessel segmentation (RVS) is a significant source of useful information for monitoring, identification, initial medication, and surgical development of ophthalmic disorders. Most common disorders, i.e., stroke, diabetic retinopathy (DR), and cardiac diseases, often change the normal structure of the retinal vascular network. A lot of research has been committed to building an automatic RVS system. But, it is still an open issue. In this article, a framework is recommended for RVS with fast execution and competing outcomes. An initial binary image is obtained by the application of the MISODATA on the preprocessed image. For vessel structure enhancement, B-COSFIRE filters are utilized along with thresholding to obtain another binary image. These two binary images are combined by logical AND-type operation. Then, it is fused with the enhanced image of B-COSFIRE filters followed by thresholding to obtain the vessel location map (VLM). The methodology is verified on four different datasets: DRIVE, STARE, HRF, and CHASE_DB1, which are publicly accessible for benchmarking and validation. The obtained results are compared with the existing competing methods.",
                "authors": "Khan Bahadar Khan, Muhammad Shahbaz Siddique, M. Ahmad, M. Mazzara",
                "citations": 29
            },
            {
                "title": "A giant planet shaping the disk around the very low-mass star CIDA 1",
                "abstract": "Context. Exoplanetary research has provided us with exciting discoveries of planets around very low mass (VLM) stars (0 . 08M M (cid:46) 0 . 3M ; e.g., TRAPPIST-1 and Proxima Centauri). However, current theoretical models still strive to explain planet formation in these conditions and do not predict the development of giant planets. Recent high-resolution observations from the Atacama Large Millimeter / submillimeter Array (ALMA) of the disk around CIDA 1, a VLM star in Taurus, show substructures hinting at the presence of a massive planet. Aims. We aim to reproduce the dust ring of CIDA 1, observed in the dust continuum emission in ALMA Band 7 (0 . 9mm) and Band 4 (2 . 1mm), along with its 12 CO ( J = 3 − 2) and 13 CO ( J = 3 − 2) channel maps, assuming the structures are shaped by the interaction of the disk with a massive planet. We seek to retrieve the mass and position of the putative planet, through a global simulation assessing planet-disk interaction to quantitatively reproduce protoplanetary disk observations of both dust and gas emission in a self-consistent way. Methods. We model the protoplanetary disk with a set of hydrodynamical simulations, hosting an embedded planet with a starting mass between 0 . 1 and 4 . 0M Jup initially located at a distance between 9 and 11au from the central star. We compute the dust and gas emission using radiative transfer simulations, and, ﬁnally, we obtain the synthetic observations treating the images as the actual ALMA observations. Results. Our models indicate that a planet with a minimum mass of ∼ 1 . 4M Jup orbiting at a distance of ∼ 9 − 10au can explain the morphology and location of the observed dust ring at Band 7 and Band 4. We match the ﬂux of the dust emission observation with a dust-to-gas mass ratio in the disk of ∼ 10 − 2 . We are able to reproduce the low spectral index ( ∼ 2) observed where the dust ring is detected, with a ∼ 40 − 50% fraction of optically thick emission. Assuming a 12 CO abundance of 5 × 10 − 5 and a 13 CO abundance 70 times lower, our synthetic images reproduce the morphology of the 12 CO ( J = 3 − 2) and 13 CO ( J = 3 − 2) observed channel maps where the cloud absorption allowed a detection. From our simulations, we estimate that a stellar mass M (cid:63) = 0 . 2M (cid:12) and a systemic velocity (cid:51) sys = 6 . 25kms − 1 are needed to reproduce the gas rotation as retrieved from molecular line observations. Applying an empirical relation between planet mass and gap width in the dust, we predict a maximum planet mass of ∼ 4 − 8M Jup . Conclusions. Our results suggest the presence of a massive planet orbiting CIDA 1, thus challenging our understanding of planet formation around VLM stars.",
                "authors": "P. Curone, A. Izquierdo, L. Testi, G. Lodato, S. Facchini, A. Natta, P. Pinilla, N. Kurtovic, C. Toci, M. Benisty, M. Tazzari, F. Borsa, M. Lombardi, C. Manara, E. Sanchis, L. Ricci",
                "citations": 7
            },
            {
                "title": "Survey of Multiple Populations in Globular Clusters among Very-low-mass Stars",
                "abstract": "Recent work has shown that near-infrared (NIR) Hubble Space Telescope (HST) photometry allows us to disentangle multiple populations (MPs) among M dwarfs of globular clusters (GCs) and to investigate this phenomenon in very-low-mass (VLM) stars. Here, we present the color–magnitude diagrams of nine GCs and the open cluster NGC 6791 in the F110W and F160W bands of HST, showing that the main sequences (MSs) below the knee are either broadened or split, thus providing evidence of MPs among VLM stars. In contrast, the MS of NGC 6791 is consistent with a single population. The color distribution of M dwarfs dramatically changes between different GCs, and the color width correlates with the cluster mass. We conclude that the MP ubiquity, variety, and dependence on GC mass are properties common to VLM and more-massive stars. We combined UV, optical, and NIR observations of NGC 2808 and NGC 6121 (M4) to identify MPs along with a wide range of stellar masses (∼0.2–0.8 ⊙ ), from the MS turnoff to the VLM regime, and measured, for the first time, their mass functions (MFs). We find that the fraction of MPs does not depend on the stellar mass and that their MFs have similar slopes. These findings indicate that the properties of MPs do not depend on stellar mass. In a scenario where the second generations formed in higher-density environments than the first generations, the possibility that the MPs formed with the same initial MF would suggest that it does not depend on the environment.",
                "authors": "E. Dondoglio, A. Milone, A. Renzini, E. Vesperini, E. Lagioia, A. Marino, A. Bellini, M. Carlos, G. Cordoni, S. Jang, M. V. Legnardi, M. Libralato, A. Mohandasan, F. D’Antona, M. Martorano, F. Muratore, M. Tailo",
                "citations": 6
            },
            {
                "title": "A Snapshot of New Zealand's Dynamic Deformation Field From Envisat InSAR and GNSS Observations Between 2003 and 2011",
                "abstract": "Measuring the deformation at the Earth's surface over a range of spatial and temporal scales is vital for understanding seismic hazard, detecting volcanic unrest, and assessing the effects of vertical land movements (VLMs) on sea level rise. Here, we combine ∼10 years of Interferometric Synthetic Aperture Radar (InSAR) observations from Envisat with interseismic campaign and continuous GNSS velocities to build a high‐resolution velocity field of New Zealand. Exploiting the horizontal GNSS observations, we estimate the vertical component of the deformation to provide the VLM for the entire 15,000‐km‐long coastline. The estimated vertical rates show large variability around the country as a result of volcanic, tectonic, and anthropogenic sources. Interseismic subsidence is observed in Kaikoura region supporting models of at least partial locking of the southern Hikurangi subduction interface. Despite data challenges in the mountainous regions from landslides, sediment compaction, and glaciers, InSAR data shows localized uplift of the Southern Alps.",
                "authors": "I. Hamling, T. Wright, S. Hreinsdóttir, L. Wallace",
                "citations": 19
            },
            {
                "title": "Epoch-Based Height Reference System for Sea Level Rise Impact Assessment on the Coast of Peninsular Malaysia",
                "abstract": "The Peninsular Malaysia Geodetic Vertical Datum 2000 (PMGVD2000) inherited several deficiencies due to offsets between local datums used, levelling error propagations, land subsidence, sea level rise, and sea level slopes along the southern half of the Malacca Strait on the west coast and the South China Sea in the east coast of the Peninsular relative to the Port Klang (PTK) datum point. To cater for a more reliable elevation-based assessment of both sea level rise and coastal flooding exposure, a new epoch-based height reference system PMGVD2022 has been developed. We have undertaken the processing of more than 30 years of sea level data from twelve tide gauge (TG) stations along the Peninsular Malaysia coast for the determination of the relative mean sea level (RMSL) at epoch 2022.0 with their respective trends and incorporates the quantification of the local vertical land motion (VLM) impact. PMGVD2022 is based on a new gravimetric geoid (PMGeoid2022) fitted to the RMSL at PTK. The orthometric height is realised through the GNSS levelling concept H = hGNSS–Nfit_PTK–NRMDT, where NRMDT is a constant offset due to the relative mean dynamic ocean topography (RMDT) between the fitted geoid at PTK and the local MSL datums along the Peninsular Malaysia coast. PMGVD2022 will become a single height reference system with absolute accuracies of better than ±3 cm and ±10 cm across most of the land/coastal area and the continental shelf of Peninsular Malaysia, respectively.",
                "authors": "Sanusi Cob, M. Kadir, R. Forsberg, W. Simons, M. Naeije, Ami Hassan Md Din, Husaini Yacob, A. Amat, Daud Mahdzur, Zuhairy Ibrahim, Kenidi Aziz, Norehan Yaacob, F. Johann, T. Jensen, Hergeir Teitsson, S. Ses, Anim Yahaya, Soeb Nordin, Fadhil Majid",
                "citations": 7
            },
            {
                "title": "Comparison of SaCoVLM™ video laryngeal mask-guided intubation and i-gel combined with flexible bronchoscopy-guided intubation in airway management during general anesthesia: a non-inferiority study",
                "abstract": null,
                "authors": "Chun-ling Yan, Yi-qi-yuan Zhang, Ying-An Chen, Zong-yang Qv, M. Zuo",
                "citations": 6
            },
            {
                "title": "Prediction of Sea Level with Vertical Land Movement Correction Using Deep Learning",
                "abstract": "Sea level rise (SLR) in small island countries such as Kiribati and Tuvalu have been a significant issue for decades. There is an urgent need for more accurate and reliable scientific information regarding SLR and its trend and for more informed decision making. This study uses the tide gauge (TG) dataset obtained from locations in Betio, Kiribati and Funafuti, Tuvalu with sea level corrections for vertical land movement (VLM) at these locations from the data obtained by the Global Navigation Satellite System (GNSS) before the sea level trend and rise predictions. The oceanic feature inputs of water temperature, barometric pressure, wind speed, wind gust, wind direction, air temperature, and three significant lags of sea level are considered in this study for data modeling. A new data decomposition method, namely, successive variational mode decomposition (SVMD), is employed to extract intrinsic modes of each feature that are processed for selection by the Boruta random optimizer (BRO). The study develops a deep learning model, namely, stacked bidirectional long short-term memory (BiLSTM), to make sea level (target variable) predictions that are benchmarked by three other AI models adaptive boosting regressor (AdaBoost), support vector regression (SVR), and multilinear regression (MLR). With a comprehensive evaluation of performance metrics, stacked BiLSTM attains superior results of 0.994207, 0.994079, 0.988219, and 0.899868 for correlation coefficient, Wilmott’s Index, the Nash–Sutcliffe Index, and the Legates–McCabe Index, respectively, for Kiribati, and with values of 0.996806, 0.996272, 0.992316, and 0.919732 for correlation coefficient, Wilmott’s Index, the Nash–Sutcliffe Index, and the Legates–McCabe Index, respectively, for the case of Tuvalu. It also shows the lowest error metrics in prediction for both study locations. Finally, trend analysis and linear projection are provided with the GNSS-VLM-corrected sea level average for the period 2001 to 2040. The analysis shows an average sea level rate rise of 2.1 mm/yr for Kiribati and 3.9 mm/yr for Tuvalu. It is estimated that Kiribati and Tuvalu will have a rise of 80 mm and 150 mm, respectively, by the year 2040 if estimated from year 2001 with the current trend.",
                "authors": "N. Raj",
                "citations": 5
            },
            {
                "title": "Separating GIA signal from surface mass change using GPS and GRACE data",
                "abstract": "\n The visco-elastic response of the solid Earth to the past glacial cycles and the present day surface mass change (PDSMC) are detected by the geodetic observation systems such as global navigation satellite system (GNSS) and satellite gravimetry. Majority of the contemporary PDSMC is driven by climate change and in order to better understand them using the aforementioned geodetic observations, glacial isostatic adjustment (GIA) signal should be accounted first. The default approach is to use forward GIA models that use uncertain ice-load history and approximate Earth rheology to predict GIA, yielding large uncertainties. The proliferation of contemporary, global, geodetic observations and their coverage have therefore enabled estimation of data-driven GIA solutions. A novel framework is presented that uses geophysical relations between the vertical land motion (VLM) and geopotential anomaly due to GIA and PDSMC to express GPS VLM trends and GRACE geopotential trends as a function of either GIA or PDSMC, which can be easily solved using least-squares regression. The GIA estimates are data-driven and differ significantly from forward models over Alaska and Greenland.",
                "authors": "B. Vishwakarma, Y. Ziegler, J. Bamber, S. Royston",
                "citations": 5
            },
            {
                "title": "Land Subsidence Estimation With Tide Gauge and Satellite Radar Altimetry Measurements Along the Texas Gulf Coast, USA",
                "abstract": "A double-difference (DD) method was used to estimate vertical land motion (VLM) at 26 tide gauge (TG) sites with record lengths of at least ten years across the Texas Gulf Coast, USA, between 1993 and 2020. In the method, the first difference was conducted by coupling nearby correlated TG stations to remove sea-level variability for both TG and satellite radar altimetry (SRA) data. Upon completion of the first difference, a second difference was performed by subtracting between TG and SRA data. The results obtained from the DD method were compared against that of: 1) a single-difference (SD) method through subtraction between measurements from TG and SRA and 2) a global navigation satellite system (GNSS) precise point positioning (PPP) method. The results showed that the DD method improved the performance of VLM estimation with an uncertainty below 1.0 mm/yr at most TG stations. Meanwhile, the estimated VLM trends acquired from the DD method correlated better to that of the ground-truth GNSS PPP solutions than the SD method. The DD method possesses great potential to discover VLM knowledge, particularly along coastal regions where other techniques such as GNSS and interferometric synthetic aperture radar (InSAR) are of impaired estimation capability.",
                "authors": "Xiaojun Qiao, Tianxing Chu, P. Tissot, Jason Louis, Ibraheem Ali",
                "citations": 5
            },
            {
                "title": "Black-box Online Aerodynamic Performance Optimization for a Seamless Wing with Distributed Morphing",
                "abstract": "Morphing is a promising bio-inspired technology, with the potential to make aircraft more economical and sustainable through adaptation of the wing shape for best efficiency at any flight condition. This paper proposes an online black-box performance optimization strategy for a seamless wing with distributed morphing control. Pursuing global performance, the presented method integrates a global radial basis function neural network (RBFNN) surrogate model with a derivative-free evolutionary optimization algorithm. The effectiveness of the optimization strategy was validated on a vortex lattice method (VLM) aerodynamic model of an over-actuated morphing wing augmented by wind tunnel experiment data. Simulations show that the proposed method is able to control the morphing shape and angle of attack to achieve various target lift coefficients with better aerodynamic efficiency than the unmorphed wing shape. The global nature of the on-board model allows the presented method to find shape solutions for a wide range of target lift coefficients without the need for additional model excitation maneuvers. Compared to the unmorphed shape, up to 14 . 6 % of lift-to-drag ratio increase is achieved",
                "authors": "Oscar Ruland, T. Mkhoyan, R. De Breuker, Xuerui Wang",
                "citations": 4
            },
            {
                "title": "Aeroelastic method to investigate nonlinear elastic wing structures",
                "abstract": null,
                "authors": "K. Bramsiepe, T. Klimmek, W. Krüger, Lorenz Tichy",
                "citations": 4
            },
            {
                "title": "The significance of vertical land movements at convergent plate boundaries in probabilistic sea-level projections for AR6 scenarios: The New Zealand case",
                "abstract": "Anticipating and managing the impacts of sea-level rise for nations astride active tectonic margins requires rates of sea surface elevation change in relation to coastal land elevation to be understood. Vertical land motion (VLM) can either exacerbate or reduce sea-level changes with impacts varying significantly along a coastline. Determining rate, pattern, and variability of VLM near coasts leads to a direct improvement of location-specific relative sea level (RSL) estimates. Here, we utilise vertical velocity field from interferometric synthetic aperture radar (InSAR) data, calibrated with campaign and continuous Global Navigation Satellite System (GNSS), to determine the VLM for the entire coastline of New Zealand. Guided by existing knowledge of the seismic cycle, the VLM data infer long-term, interseismic rates of land surface deformation. We build probabilistic RSL projections using the Framework for Assessing Changes to Sea-level (FACTS) from IPCC Assessment Report 6 and ingest local VLM data to produce RSL projections at 7435 sites, thereby enhancing spatial coverage that was previously limited to tide gauges. We present ensembles of probability distributions of RSL for medium confidence climatic processes for each scenario to 2150 and low confidence processes to 2300. For regions where land subsidence is occurring at rates >2mm yr-1 VLM makes a significant contribution to RSL projections for all scenarios out 2150. Beyond 2150, for higher emissions scenarios, the land ice contribution to global sea level dominates. We discuss the planning implications of RSL projections, where timing of threshold exceedance for coastal inundation can be brought forward by decades.",
                "authors": "T. Naish, R. Levy, I. Hamling, G. Garner, Sigrún, Hreinsdóttir, Robert E Kopp, N. Golledge, R. Bell, R. Paulik, Judy, Lawrence, P. Denys, T. Gillies, Shannon Bengston, K. Clark, Daniel, King, N. Litchfield, L. Wallace, R. Newnham",
                "citations": 5
            },
            {
                "title": "Grain Growth in the Dust Ring with a Crescent around the Very Low-mass Star ZZ Tau IRS with JVLA",
                "abstract": "The azimuthal asymmetries of dust rings in protoplanetary disks such as a crescent around young stars are often interpreted as dust traps, and thus as ideal locations for planetesimal and planet formations. Whether such dust traps effectively promote planetesimal formation in disks around very low-mass stars (VLM; a mass of ≲0.2 M ☉) is debatable, as the dynamical and grain growth timescales in such systems are long. To investigate grain growth in such systems, we studied the dust ring with a crescent around the VLM star ZZ Tau IRS using the Karl G. Jansky Very Large Array at centimeter wavelengths. Significant signals were detected around ZZ Tau IRS. To estimate the maximum grain size ( amax ) in the crescent, we compared the observed spectral energy distribution (SED) with SEDs for various amax values predicted by radiative transfer calculations. We found amax≳ 1 mm and ≲60 μm in the crescent and ring, respectively, though our modeling efforts rely on uncertain dust properties. Our results suggest that grain growth occurred in the ZZ Tau IRS disk, relative to the sub-micron-sized interstellar medium. Planet formation in a crescent with millimeter-sized pebbles might proceed more efficiently than in other regions with submillimeter-sized pebbles via pebble accretion scenarios.",
                "authors": "J. Hashimoto, H. Liu, R. Dong, Beibei Liu, T. Muto",
                "citations": 5
            },
            {
                "title": "Influence of Aperiodic Non‐Tidal Atmospheric and Oceanic Loading Deformations on the Stochastic Properties of Global GNSS Vertical Land Motion Time Series",
                "abstract": "Monitoring vertical land motions (VLMs) at the level of 0.1 mm/yr remains one of the most challenging scientific applications of global navigation satellite systems (GNSS). Such small rates of change can result from climatic and tectonic phenomena, and their detection is important to many solid Earth‐related studies, including the prediction of coastal sea‐level change and the understanding of intraplate deformation. Reaching a level of precision allowing to detect such small signals requires a thorough understanding of the stochastic variability in GNSS VLM time series. This paper investigates how the aperiodic part of non‐tidal atmospheric and oceanic loading (NTAOL) deformations influences the stochastic properties of VLM time series. Using the time series of over 10,000 stations, we describe the impact of correcting for NTAOL deformation on 5 complementary metrics, namely: the repeatability of position residuals, the power‐spectrum of position residuals, the estimated time‐correlation properties, the corresponding velocity uncertainties, and the spatial correlation of the residuals. We show that NTAOL deformations cause a latitude‐dependent bias in white noise plus power‐law model parameter estimates. This bias is significantly mitigated when correcting for NTAOL deformation, which reduces velocity uncertainties at high latitudes by 70%. Therefore, removing NTAOL deformation before the statistical analysis of VLM time series might help to detect subtle VLM signals in these areas. Our spatial correlation analysis also reveals a seasonality in the spatial correlation of the residuals, which is reduced after removing NTAOL deformation, confirming that NTAOL is a clear source of common‐mode errors in GNSS VLM time series.",
                "authors": "K. Gobron, P. Rebischung, M. Van Camp, A. Demoulin, O. Viron",
                "citations": 15
            },
            {
                "title": "A Study on the Scale Effect According to the Reynolds Number in Propeller Flow Analysis and a Model Experiment",
                "abstract": "The demand for new propeller designs has increased alongside the development of new technology, such as urban aircraft and large unmanned aerial vehicles. In order to experimentally identify the performance of a propeller, a wind tunnel that provides the operating flow is essential. However, in the case of a meter class or larger propeller, a large wind tunnel is required and the related equipment becomes heavy; therefore, it is difficult to implement in reality. For this reason, propeller studies have been conducted via reduced models. In this case, it is necessary to investigate the different performance outputs between the full- and model-scale propellers due to the size difference. In the current study, a method is proposed to investigate the difference in the aerodynamic performance caused by the difference in propeller scale using VLM and RANS calculations, and the differences are analyzed. The wind tunnel test also verified the propeller performance prediction method. The boundary of aerodynamic performance independent of the Reynolds number could be predicted through the VLM based on the ideal fluid assumption. From the RANS calculations, it was possible to present the difference in the aerodynamic performance when propellers of the same geometry with different ratios were operated using different Reynolds numbers. It was confirmed that each numerical method matched well with the wind tunnel test results in the range of the advance ratio that produced the maximum efficiency, and from the results, it was possible to observe the change in aerodynamic performance that differed according to the scale change.",
                "authors": "Yeong-Ju Go, J. Bae, Jaeha Ryi, J. Choi, Chung-Ryeol Lee",
                "citations": 3
            },
            {
                "title": "ILLUME: Rationalizing Vision-Language Models through Human Interactions",
                "abstract": "Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.",
                "authors": "Manuel Brack, P. Schramowski, Bjorn Deiseroth, K. Kersting",
                "citations": 3
            },
            {
                "title": "Golden Retriever: A Real-Time Multi-Modal Text-Image Retrieval System with the Ability to Focus",
                "abstract": "In this work, we present the Golden Retriever, a system leveraging state-of-the-art visio-linguistic models (VLMs) for real-time text-image retrieval. The unique feature of our system is that it can focus on words contained in the textual query, i.e., locate and high-light them within retrieved images. An efficient two-stage process implements real-time capability and the ability to focus. Therefore, we first drastically reduce the number of images processed by a VLM. Then, in the second stage, we rank the images and highlight the focussed word using the outputs of a VLM. Further, we introduce a new and efficient algorithm based on the idea of TF-IDF to retrieve images for short textual queries. One of multiple use cases where we employ the Golden Retriever is a language learner scenario, where visual cues for \"difficult\" words within sentences are provided to improve a user's reading comprehension. However, since the backend is completely decoupled from the frontend, the system can be integrated into any other application where images must be retrieved fast. We demonstrate the Golden Retriever with screenshots of a minimalistic user interface.",
                "authors": "Florian Schneider, Chris Biemann",
                "citations": 3
            },
            {
                "title": "Multidisciplinary Optimisation of an eVTOL UAV With a Hydrogen Fuel Cell",
                "abstract": "To explore the use of hydrogen fuel cells as a feasible alternative on Unmanned Aerial Vehicles (UAVs), a class I concept was designed at the Portuguese Air Force Research Centre (CIAFA). This work focuses on the Multidisciplinary Design Optimisation (MDO) methodology that was used to improve the 3h endurance of the baseline concept that had a Maximum Take-Off Weight of 21.6 kg, using 148 g of hydrogen and a 800 W fuel cell to power conventional flight operations. Another propulsive system comprised of batteries and rotors is used for Vertical Take-Off and Landing (VTOL). MDO was performed with the aid of OpenAeroStruct, a low fidelity software that combines Finite Element Analysis (FEA) and Vortex Lattice Method (VLM) to model lifting surfaces. Initially, a cruise and a load flight conditions were used with structural parameters and geometric twist as design variables. In a second approach, complexity was increased by including taper, wing chord and span as design variables in the problem formulation. Lastly, a third flight condition was introduced to ensure stall requirements were met. The use of MDO led to a 21% increase in endurance with a smaller wing, while satisfying all imposed constraints. This work marks an important milestone in the development of a future prototype at the CIAFA.",
                "authors": "Bernardo Alves, A. Marta, Luís Félix",
                "citations": 3
            },
            {
                "title": "The zone of influence: matching sea level variability from coastal altimetry and tide gauges for vertical land motion estimation",
                "abstract": "Abstract. Vertical land motion (VLM) at the coast is a substantial contributor to relative sea level change. In this work, we present a refined method for its\ndetermination, which is based on the combination of absolute satellite altimetry (SAT) sea level measurements and relative sea level changes\nrecorded by tide gauges (TGs). These measurements complement VLM estimates from the GNSS (Global Navigation Satellite System) by increasing their spatial\ncoverage. Trend estimates from the SAT and TG combination are particularly sensitive to the quality and resolution of applied altimetry data as well as\nto the coupling procedure of altimetry and TGs. Hence, a multi-mission, dedicated coastal along-track altimetry dataset is coupled with\nhigh-frequency TG measurements at 58 stations. To improve the coupling procedure, a so-called “zone of influence” (ZOI) is defined, which confines\ncoherent zones of sea level variability on the basis of relative levels of comparability between TG and altimetry observations. Selecting 20 %\nof the most representative absolute sea level observations in a 300  km radius around the TGs results in the best VLM estimates in terms of\naccuracy and uncertainty. At this threshold, VLM SAT-TG estimates have median formal uncertainties of\n0.58  mm yr−1 . Validation against GNSS VLM estimates yields a root mean square (rms ΔVLM ) of VLM SAT-TG and\nVLM GNSS differences of 1.28  mm yr−1 , demonstrating the level of accuracy of our approach. Compared to a reference\n250  km radius selection, the 300  km zone of influence improves trend accuracies by 15 % and uncertainties by 35 %. With\nincreasing record lengths, the spatial scales of the coherency in coastal sea level trends increase. Therefore, the relevance of the ZOI for\nimproving VLM SAT-TG accuracy decreases. Further individual zone of influence adaptations offer the prospect of bringing the accuracy of\nthe estimates below 1  mm yr−1 .",
                "authors": "J. Oelsmann, M. Passaro, D. Dettmering, C. Schwatke, L. Sánchez, F. Seitz",
                "citations": 16
            },
            {
                "title": "Natural Variability and Vertical Land Motion Contributions in the Mediterranean Sea-Level Records over the Last Two Centuries and Projections for 2100",
                "abstract": "We analyzed a set of geodetic data to investigate the contribution of local factors, namely the sea level natural variability (SLNV) and the vertical land motion (VLM), to the sea-level trend. The SLNV is analyzed through the Empirical Mode Decomposition (EMD) on tidal data (>60 years of recordings) and results are used to evaluate its effects on sea levels. The VLM is measured at a set of continuous GPS (cGPS) stations (>5 years of recordings), located nearby the tide gauges. By combining VLM and SLNV with IPCC-AR5 regional projections of climatic data (Representative Concentration Pathways (RCP) 2.6 and 8.5), we provide relative sea-level rise projections by 2100. Results show that the combined effects of SLNV and VLM are not negligible, contributing between 15% and 65% to the sea-level variability. Expected sea levels for 2100 in the RCP8.5 scenario are between 475 ± 203 (Bakar) and 818 ± 250 mm (Venice). In the Venice Lagoon, the mean land subsidence at 3.3 ± 0.85 mm a−1 (locally up to 8.45 ± 1.69 mm a−1) is driving the local sea-level rise acceleration.",
                "authors": "A. Vecchio, M. Anzidei, E. Serpelloni, F. Florindo",
                "citations": 33
            },
            {
                "title": "Electroacupuncture Involved in Motor Cortex and Hypoglossal Neural Control to Improve Voluntary Swallowing of Poststroke Dysphagia Mice",
                "abstract": "The descending motor nerve conduction of voluntary swallowing is mainly launched by primary motor cortex (M1). M1 can activate and regulate peripheral nerves (hypoglossal) to control the swallowing. Acupuncture at “Lianquan” acupoint (CV23) has a positive effect against poststroke dysphagia (PSD). In previous work, we have demonstrated that electroacupuncture (EA) could regulate swallowing-related motor neurons and promote swallowing activity in the essential part of central pattern generator (CPG), containing nucleus ambiguus (NA), nucleus of the solitary tract (NTS), and ventrolateral medulla (VLM) under the physiological condition. In the present work, we have investigated the effects of EA on the PSD mice in vivo and sought evidence for PSD improvement by electrophysiology recording and laser speckle contrast imaging (LSCI). Four main conclusions can be drawn from our study: (i) EA may enhance the local field potential in noninfarction area of M1, activate the swallowing-related neurons (pyramidal cells), and increase the motor conduction of noninfarction area in voluntary swallowing; (ii) EA may improve the blood flow in both M1 on the healthy side and deglutition muscles and relieve PSD symptoms; (iii) EA could increase the motor conduction velocity (MCV) in hypoglossal nerve, enhance the EMG of mylohyoid muscle, alleviate the paralysis of swallowing muscles, release the substance P, and restore the ability to drink water; and (iv) EA can boost the functional compensation of M1 in the noninfarction side, strengthen the excitatory of hypoglossal nerve, and be involved in the voluntary swallowing neural control to improve PSD. This research provides a timely and necessary experimental evidence of the motor neural regulation in dysphagia after stroke by acupuncture in clinic.",
                "authors": "Shuai Cui, Shuqi Yao, Chunxiao Wu, Lulu Yao, Peidong Huang, Yongjun Chen, C. Tang, Nenggui Xu",
                "citations": 24
            },
            {
                "title": "Aligning MAGMA by Few-Shot Learning and Finetuning",
                "abstract": "The goal of vision-language modeling is to allow models to tie language understanding with visual inputs. The aim of this paper is to evaluate and align the Visual Language Model (VLM) called Multimodal Augmentation of Generative Models through Adapter-based finetuning (MAGMA) with human values. MAGMA is a VLM that is capable of image captioning and visual question-answering. We will evaluate its alignment in three different scenarios. To begin, we assess MAGMA's out-of-the-box alignment through the checkpoint provided by Hugging Face. Then, we measure if few-shot learning manages to improve the results. Finally, we finetune the model on aligned examples and evaluate its behavior.",
                "authors": "Jean-Charles Layoun, Alexis Roger, I. Rish",
                "citations": 2
            },
            {
                "title": "Benchmark of different aerodynamic solvers on wing aero-propulsive interactions",
                "abstract": "Distributed electric propulsion is a fertile research topic aiming to increase the wing aerodynamic efficiency by distributing the thrust over the wing span. The blowing due to distributed propulsors shall increase the wing lift coefficient for a given planform area and flight speed. This should bring several advantages as wing area, drag, and structural weight reduction, which in turn reduce fuel consumption, allowing airplanes to fly more efficiently. However, there are no consolidated preliminary design methods to size a distributed propulsion system. Numerical analysis is then performed at early stage, where many design variables have not been fixed yet. Therefore, the design space is vast and exploring all the possible combinations is unfeasible. For instance, low-fidelity methods (VLM, panel codes) have a low computational time, but usually they do not account for flow separation and hence they are unable to predict the wing maximum lift. Conversely, high-fidelity codes (CFD) provide more realistic results, but a single drag polar sweep can last days. This work provides a benchmark of different aerodynamic solvers for a typical regional turboprop wing with flaps and distributed propulsion, to better understand the limits of each software in the prediction of aero-propulsive effects.",
                "authors": "D. Ciliberti, E. Bénard, F. Nicolosi",
                "citations": 2
            },
            {
                "title": "Vertical land motion trends from GNSS and altimetry at tide gauge stations",
                "abstract": "This study compares eight weighting techniques for Global Navigation Satellite System (GNSS)-derived Vertical Land Motion (VLM) trends at 570 tide gauge (TG) stations. The spread between the methods has a comparable size as the formal uncertainties of the GNSS trends. Taking the median of the surrounding GNSS trends shows the best agreement with differenced altimetry - tide gauge (ALT-TG) trends. An attempt is also made to improve VLM trends from ALT-TG time series. Only using highly correlated along-track altimetry and TG time series, reduces the standard deviation of ALT-TG time series up to 10%. As a result, there are spatially coherent changes in the trends, but the reduction in the RMS of differences between ALT-TG and GNSS trends is insigniﬁcant. However, setting correlation thresholds also acts like a ﬁlter to remove problematic TG stations. This results in sets of ALT-TG VLM trends at 344-663 TG locations, depending on the correlation threshold. Compared to other studies, we decrease the RMS of differences between GNSS and ALT-TG trends (from 1.47 to 1.22 mm/yr ), while we increase the number of locations (from 109 to 155), Depending on the weighting methods the mean of differences between ALT-TG and GNSS trends varies between 0.1-0.2 mm/yr. We reduce the mean of differences by taking into account the effect of elastic deformation due to present-day mass redistribution into account.",
                "authors": "Marcel Kleinherenbrink, R. Riva, T. Frederikse",
                "citations": 42
            },
            {
                "title": "A Vortex Lattice Method for the Hydrodynamic Solution of Lifting Bodies Traveling Close and Across a Free Surface",
                "abstract": "The hydrodynamics performance of submerged and surface-piercing lifting bodies is analyzed by a potential flow model based on a Vortex Lattice Method (VLM). Such a numerical scheme, widely applied in aerodynamics, is particularly suitable to model the lifting effects thanks to the vortex distribution used to discretize the boundaries of the lifting bodies. The method has been developed with specific boundary conditions to account for the development of steady free surface wave patterns. Both submerged bodies, such as flat plates and hydrofoils, as well as planing hulls can be studied. The method is validated by comparison against available experimental data and other Computational Fluid Dynamic (CFD) results from Reynolds Averaged Navier Stokes (RANS) approaches. In all the analyzed cases, namely 2D and 3D flat plates, a NACA hydrofoil, planning flat plates and prismatic planing hulls, results have been found to be consistent with those taken as reference. The obtained hydrodynamic predictionsare discussed highlighting the advantages and the possible improvements of the developed approach.",
                "authors": "Raffaele Solari, P. Bagnerini, G. Vernengo",
                "citations": 2
            },
            {
                "title": "Orbital Venolymphatic Malformation Treated With Sodium Tetradecyl Sulfate: A Case Report",
                "abstract": "Orbital and periorbital venolymphatic malformations (VLMs) are benign congenital vascular lesions and constitute 1%-3% of all orbital masses. Widespread facial venous malformations have a high incidence of associated intracranial developmental venous anomalies (DVAs). In such cases, there can be a sudden increase in proptosis following upper respiratory infection or minor trauma. Numerous percutaneous intralesional sclerosing agents like sodium tetradecyl sulfate (STS), bleomycin, doxycycline, ethanol, and OK-432 (Picibanil) have been used for treating VLMs. We hereby report a rare case of retro-orbital VLM treated successfully with STS injection and an isolated dural arterio-venous (AV) fistula in the same patient.",
                "authors": "Sucharita Das, A. Agrawal, S. Burathoki, K. Nandolia, A. Juneja, R. Samanta",
                "citations": 2
            },
            {
                "title": "GPS data from 2019 and 2020 campaigns in the Chesapeake Bay region towards quantifying vertical land motions",
                "abstract": null,
                "authors": "G. Troia, D. S. Stamps, R. Lotspeich, James Duda, K. J. McCoy, W. Moore, Philippe F. Hensel, R. Hippenstiel, T. McKenna, D. Andreasen, C. Geoghegan, T. Ulizio, Madeline Kronebusch, J. Carr, D. Walters, Neil Winn",
                "citations": 2
            },
            {
                "title": "In vivo absolute quantification of carnosine in the vastus lateralis muscle with 1H MRS using a surface coil and water as internal reference",
                "abstract": null,
                "authors": "Gloria Vega, G. Ricaurte, Mauricio Estrada-Castrillón, H. Reyngoudt, Oscar Cardona, J. Gallo-Villegas, R. Narvaez-Sanchez, J. Calderón",
                "citations": 2
            },
            {
                "title": "Mapping Vertical Land Motion in Challenging Terrain: Six‐Year Trends on Tutuila Island, American Samoa, With PS‐InSAR, GPS, Tide Gauge, and Satellite Altimetry Data",
                "abstract": "Sea level rise is a major challenge facing coastlines worldwide and can be strongly exacerbated by land subsidence. However, detailed characterization of vertical land motion (VLM) is limited for many tectonically active islands, as many remote sensing methods are hindered by dense vegetation and thick cloud cover. In American Samoa, strong post‐seismic deformation from the 2009 Samoa‐Tonga earthquake has increased flooding, but large uncertainties remain in hazard forecasting as only point measurements of VLM have been available. Here, we present novel VLM results over Tutuila, the largest and most populated island in American Samoa, using interferometric synthetic aperture radar, GPS, tide gauge, and satellite altimetry data. Measurements cover populated areas, with subsidence rates of 6–9 mm/yr and uncertainties of <1 mm/yr; the highest rates lie along the coastlines. We find differences in rate changes across the island, suggesting that local processes need to be well‐constrained for effective flood forecasting efforts.",
                "authors": "Stacey A. Huang, J. Sauber, R. Ray",
                "citations": 2
            },
            {
                "title": "Intermediate Points for Missions to Interstellar Objects Using Optimum Interplanetary Trajectory Software",
                "abstract": "This paper explicates the concept of an ‘Intermediate Point’ (IP), its incorporation as a node along an interplanetary trajectory, and how it permits the determination and optimization of trajectories to interstellar objects (ISOs). IPs can be used to model Solar Oberth Manoeuvres (SOM) as well as V ∞ Leveraging Manoeuvres (VLM). The SOM has been established theoretically as an important mechanism by which rapid exit from the solar system and intercept of ISOs can both be achieved; the VLM has been demonstrated practically as a method of reducing overall mission ΔV as well as the Characteristic Energy, C 3 , at launch. Thus via these two applications, the feasibility of missions to interstellar objects (ISOs) such as 1I/’Oumuamua can be analysed. The interplanetary trajectory optimizer tool exploited for this analysis, OITS, permits IP selection as an encounter option along the interplanetary trajectory in question. OITS adopts the assumption of impulsive thrust at discrete points along the trajectory, an assumption which is generally valid for high thrust propulsion scenarios, like chemical or nuclear thermal for instance.",
                "authors": "A. Hibberd",
                "citations": 2
            },
            {
                "title": "Application of a Comprehensive Design Method to Counter-Rotating Propellers",
                "abstract": "A comprehensive design method is applied to design of counter-rotating propellers (CRP). A numerical nonlinear optimization algorithm is first used for design of each propeller. This approach represents a blade by B-spline geometry and the design variables at the location of the vertices of the B-spline polygon determine the optimal blade shape. The nonlinear optimization method aims at minimizing the torque for a given target thrust with constraints, e.g. the minimum pressure constraint for a fully wetted propeller or maximum allowed cavity area for a cavitating propeller. Then the interaction of the designed propellers and a given pod including the viscous flow field around the two propellers is analyzed by coupling a vortex-lattice method (VLM) with a Reynolds-Averaged Navier-Stokes (RANS) solver. The analysis determines a new inflow for a new design of propellers. The procedure of the design and interaction analysis finishes when the propeller thrust converges within a certain tolerance. Finally, the designed propellers are compared with the original propellers.",
                "authors": "K. Cha, S. Kinnas",
                "citations": 2
            },
            {
                "title": "To compare the influence of blind insertion and up-down optimized glottic exposure manoeuvre on oropharyngeal leak pressure using SaCoVLM™ video laryngeal mask among patients undergoing general anesthesia",
                "abstract": null,
                "authors": "Chun-ling Yan, Yi-qi-yuan Zhang, Ying-An Chen, Zong-yang Qv, M. Zuo",
                "citations": 2
            },
            {
                "title": "GPS Vertical Land Motion Corrections to Sea‐Level Rise Estimates in the Pacific Northwest",
                "abstract": "We construct coastal Pacific Northwest profiles of vertical land motion (VLM) known to bias long-term tide-gauge measurements of sea-level rise (SLR) and use them to estimate absolute sea-level rise with respect to Earth’s center of mass. Multidecade GPS measurements at 47 coastal stations along the Cascadia subduction zone show VLM varies regionally but smoothly along the Pacific coast and inland Puget Sound with rates ranging from 1 4.9 to 21.2 mm/yr. Puget Sound VLM is characterized by uniform subsidence at relatively slow rates of 20.1 to 20.3 mm/yr. Uplift rates of 4.5 mm/yr persist along the western Olympic Peninsula of northwestern Washington State and decrease southward becoming nearly 0 mm/yr south of central coastal Washington through Cape Blanco, Oregon. South of Cape Blanco, uplift increases to 122 mm/yr, peaks at 4 mm/yr near Crescent City, California, and returns to zero at Cape Mendocino, California. Using various stochastic noise models, we estimate long-term ( 5",
                "authors": "J. Montillet, T. Melbourne, W. Szeliga",
                "citations": 37
            },
            {
                "title": "A new global GPS data set for testing and improving modelled GIA uplift rates",
                "abstract": "Glacial isostatic adjustment (GIA) is the response of the solid Earth to past ice loading, primarily, since the Last Glacial Maximum, about 20 K yr BP. Modelling GIA is challenging because of large uncertainties in ice loading history and also the viscosity of the upper and lower mantle. GPS data contain the signature of GIA in their uplift rates but these also contain other sources of vertical land motion (VLM) such as tectonics, human and natural influences on water storage that can mask the underlying GIA signal. In this study, we use about 4000 GPS vertical velocities as observational estimates of global GIA uplift rates, after correcting for major elastic deformation effects. A novel fully automatic strategy is developed to postprocess the GPS time-series and to correct for non-GIA artefacts. Before estimating vertical velocities and uncertainties, we detect outliers and jumps and correct for atmospheric mass loading displacements. We correct the resulting velocities for the elastic response of the solid Earth to global changes in ice sheets, glaciers and ocean loading, as well as for changes in the Earth’s rotational pole relative to the 20th century average. We then apply a spatial median filter to remove sites where local effects are dominant to leave approximately 4000 GPS sites. The resulting novel global GPS data set shows a clean GIA signal at all post-processed stations and is therefore suitable to investigate the behavior of global GIA forward models. The results are transformed from a frame with its origin in the centre of mass of the total Earth’s system (CM) into a frame with its origin in the centre of mass of the solid Earth (CE) before comparison with 13 global GIA forward model solutions, with best fits with Pur-6- VM5 and ICE-6G predictions. The largest discrepancies for all models were identified for Antarctica and Greenland, which may be due to either uncertain mantle rheology, ice loading history/magnitude and/or GPS errors.",
                "authors": "M. Schumacher, Matt A. King, J. Rougier, Z. Sha, S. Khan, J. Bamber",
                "citations": 39
            },
            {
                "title": "Mitochondrial DNA population variation is not associated with Alzheimer’s in the Japanese population: A consistent finding across global populations",
                "abstract": "Several mitochondrial DNA (mtDNA) haplogroup association studies have suggested that common mtDNA variants are associated with multifactorial diseases, including Alzheimer’s disease (AD). However, such studies have also produced conflicting results. A new mtDNA association model, the ‘variant load model’ (VLM), has been applied to multiple disease phenotypes. Application of the VLM in a 2017 study failed to find different variant loads in AD patients compared to controls, in two cohorts of European origin. The study also suggested a lower variant load in healthy elderly individuals, but could offer no replicate cohort to support this observation. Here, the VLM is applied to Japanese mtDNA sequences; in doing so, we explored the role of mtDNA variation in AD and ageing in a different global population. Consistent with the previous findings using the VLM in two populations of European origin, we found no evidence for an association between rarer, non-haplogroup associated variation and the development of AD. However, the result in the context of ageing that suggested those with fewer mildly deleterious mutations might undergo healthier ageing, was not replicated. In contrast to our previous study, our present results suggest that those living to advanced old age may harbour more mildly deleterious mtDNA variations. Importantly our analysis showed this finding is not primarily being driven by many rare population variants dispersed across the mtDNA, but by a few more frequent variants with high MutPred scores. It is suggested the variants in question do not exert a mildly deleterious effect in their most frequent haplogroup context.",
                "authors": "Johanna Wong, Jannetta S. Steyn, I. Pienaar, J. Elson",
                "citations": 1
            },
            {
                "title": "Quantifying vertical land motion at tide gauge sites using permanent scatterer interferometric synthetic aperture radar and global navigation satellite system solutions",
                "abstract": null,
                "authors": "R. Reyes, M. D. A. Bauzon, N. Pasaje, Rey Mark Alfante, Pocholo Miguel A. De Lara, Marion Ordillano, Paul Caesar M. Flores, A. Rediang, Patrick Anthony Nota, F. Siringan, A. Blanco, D. Bringas",
                "citations": 1
            },
            {
                "title": "ILLUME: Rationalizing Vision-Language Models by Interacting with their Jabber",
                "abstract": "Bootstrapping from pre-trained language models has been proven to be an efﬁcient approach for building foundation vision-language models (VLM) for tasks such as im- age captioning or visual question answering. However, it is difﬁcult—if not impossible—to utilize it to make the model conform with user’s rationales for speciﬁc answers. To elicit and reinforce commonsense reasons, we propose an iterative sampling and tuning paradigm, called I LLUME , that executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides minimal feedback via preference selec- tion, used for ﬁne-tuning. This loop increases the training data and gradually carves out the VLM’s rationalization capabili- ties. Our exhaustive experiments demonstrate that I LLUME is competitive with standard supervised ﬁne-tuning while using signiﬁcantly fewer training data and only requiring minimal feedback.",
                "authors": "Manuel Brack, P. Schramowski, Bjorn Deiseroth, K. Kersting",
                "citations": 1
            },
            {
                "title": "Tracking Coastal Change in American Samoa by Mapping Local Vertical Land Motion with PS-InSAR",
                "abstract": "Characterizing diverse contributions to coastal land change is a key step to mitigating the effects of rising sea levels that threaten coastal communities. This task is particularly critical for small island communities in tectonically active regions, which are highly vulnerable to the effects of sea level rise. We highlight here a case study to extract regional estimates of vertical land motion (VLM) over American Samoa, which in recent years has observed increased nuisance flooding. We used persistent scatterer Interferometric Synthetic Aperture Radar (PS-InSAR) to derive high-resolution estimates of crustal deformation in populated regions over Tutuila Island from 2016 to 2021. While the area is small and highly vegetated and poses challenges for InSAR, we were able to construct a regional map of the estimated deformation rate in these areas and validate the time-series with a local GPS station. Our preliminary results suggest that PS-InSAR has the ability to capture local and regional deformation patterns. Further work to refine our VLM estimate will include models to account for more complex atmospheric effects and estimates of error margins, as well as integration of our results into models of sealevel change that can be used by local stakeholders.",
                "authors": "Stacey A. Huang, J. Sauber",
                "citations": 1
            },
            {
                "title": "Virtual-stator Loss Model for Synchronous Generators",
                "abstract": "In synchronous generators, end region field can cause additional losses at clamping structures and stator end core. It can be a computational challenge to solve the end region field of synchronous generators with fractional-slot winding. With a minor change of the number of stator slots, three-dimensional (3D) Virtual-stator Loss Model (VLM) can largely reduce computational burden of end region field computation. This paper investigates VLM theory and its implementation. Compared to Real-stator Loss Model, VLM demonstrates excellent accuracy when implemented with two-dimensional finite element method. This indicates that 3D VLM can be used for the accurate computation of losses induced by end region field, while the computational challenge is dramatically reduced. Lab test at a 100kVA salient pole synchronous generator proves that the analysis approach is trustful.",
                "authors": "Zhaoqiang Zhang, A. Nysveen, Børge Johannes Fagermyr, R. Nilssen, H. Ehya",
                "citations": 1
            },
            {
                "title": "Effect of Vernonia amygdalina leaf meal on the reproductive indices of male rabbits",
                "abstract": "Abstract The study evaluated the effect of Vernonia amygdalina leaf meal on semen indices, serum testosterone and sperm reserve of male rabbits. Forty rabbit bucks were randomly assigned into four groups and fed the experimental diets containing Vernonia amygdalina (VLM) at 0, 5, 10 and 15 % levels for 84days. Reproductive indices were evaluated using standard procedures. Data obtained were subjected to analysis of variance at p<0.05. All semen in rabbits fed 0, 5 and 10 % VLM had milky colour while 14.8% light green and 85.2% milky semen colour was observed in bucks fed 15%VLM. Libido score reduced in rabbits fed VLM diets. Bucks fed 15%VLM had significantly higher semen volume (0.47ml). VLM had no significant effect on spermatozoa mass motility, progressive motility and sperm concentration. Live sperm cells significantly increased in bucks fed 5 and 10% VLM diets. VLM had significant (p<0.05) effect on spermatozoa morphology. Vernonia amygdalina leaf meal had no significant (p<0.05) impact on testosterone, testicular and epididymal indices. In conclusion, up to 10%VLM can be adopted as feed ingredient for male rabbit breeder stock without deleterious effect on reproductive indices.",
                "authors": "A. Adeyemi, Christiana Oloyede, Adedamola Adedotun",
                "citations": 1
            },
            {
                "title": "Vision Encoders in Visual Question Answering",
                "abstract": "Most existing methods that apply pretrained Visual Language Models (VLMs) to vision and language tasks do not sufficiently explore the effect of the format of their inputs on downstream performance. We show that utilising appropriate prompt formatting is a simple yet effective approach to improving the few-shot performance of VLMs that use relatively small language models on the Visual Question Answering (VQA) task. We format the inputs used to prompt a VLM using a modified text-only template from a closed-book question answering task that the language-model component of the VLM was pretrained on. By doing this, we explicitly align the VQA task with a task that this language model has already seen, enabling the VLM to leverage the similarities between the tasks, such as the answer-length distribution, when generating answers to the visual questions. In order to test our claims, we implement a simple architecture based on Frozen (Tsimpoukelli et al., 2021) and ClipCap (Mokady et al., 2021), whereby, through image captioning, the VLM learns to integrate powerful pretrained vision-only and language-only models via a relatively simple learnt mapping network. Furthermore, we contextualise our approach relative to existing work by presenting a unified view of VLMs. Our results show that explicit alignment enables our VLMs to achieve a significantly higher zero-shot (34.49% vs 20.89%) and best overall (40.39% vs 30.83%) VQA score on the VQA2.0 dataset (Goyal et al., 2017) than when the prompt template from Frozen (Tsimpoukelli et al., 2021) and Flamingo (Alayrac et al., 2022) is used. Furthermore, our zero-shot and best overall performance is better than Frozen’s (34.49% vs 29.5% and 40.39% vs 38.2%, respectively) despite Frozen using a language model with more than double the number of parameters. Our code is available here.",
                "authors": "Ryan R. Anderson",
                "citations": 1
            },
            {
                "title": "Simplified vortex methods to model wake vortex roll-up in real-time simulations for fuel-saving formation flight",
                "abstract": null,
                "authors": "Henrik Spark, R. Luckner",
                "citations": 1
            },
            {
                "title": "Monitoring Megathrust-Earthquake-Cycle-Induced Relative Sea-Level Changes near Phuket, South Thailand, Using (Space) Geodetic Techniques",
                "abstract": "Temporal changes in vertical land motion (VLM) in and around Phuket Island in southern Thailand following the great 2004 Sumatra–Andaman megathrust earthquake have impacted the relative sea-level change estimates based on tide-gauge (TG) records. To better monitor the VLM, two new continuous global navigation satellite system (GNSS) stations have been installed in the past 5 years, situated on bedrock both near and at the Koh Taphao Noi Island TG in Phuket, which together with older global positioning system (GPS) data provides a clear insight in the VLM of Phuket Island from 1994 onward. In addition, satellite altimetry (SALT) data has been analyzed since 1992. The VLM (GPS) position and relative (TG) and absolute (SALT) sea-level change time series were successfully combined in pairs to validate each independent result (e.g., SALT − GNSS = TG): prior to the 2004 earthquake, the relative sea-level rise in Phuket was 1.0 ± 0.7 mm/yr, lower by 2.4 ± 0.2 mm/yr than the absolute sea-level rise caused by VLM. After the earthquake, nonlinear post-seismic subsidence has caused the VLM to drop by 10 cm in the past 17 years, resulting, by the end of 2020, in a relative sea-level rise by up to 16 cm. During the same period, other TG stations in south Thailand recorded similar sea-level increases. Combination with SALT further suggests that, prior to 2005, uplift (5.3 ± 1.4 mm/yr) of the coastal region of Ranong (north of Phuket) resulted in a relative sea-level fall, but since then, post-seismic-induced negative VLM may have significantly increased coastal erosion along the entire Andaman Sea coastline.",
                "authors": "M. Naeije, W. Simons, Siriporn Pradit, Sommart Niemnil, N. Thongtham, M. A. Mustafar, P. Noppradit",
                "citations": 1
            },
            {
                "title": "VLMbench: A Benchmark for Vision-and-Language Manipulation",
                "abstract": "Recent progress in embodied AI [2, 5–7] pushes intelligent robotic systems to reality closer than any other time before. To achieve the final goal of interacting with unstructured environments to accomplish various daily tasks, the agent needs to learn how to manipulate objects through visual observations and natural languages appropriately. Compared with vision-only systems, natural language possesses two essential properties that enhance robot manipulation task learning: compact and flexible specification of various tasks and natural interactive communication interface with humans. We name such an agent that learns from the combined knowledge of language and vision to accomplish robot manipulation tasks a Vision-and-Language Manipulation (VLM) embodied agent. Given the rapid growth in VLM research and their limitations on generalization across different tasks, we expect an inclusive, modular, and scalable benchmark to evaluate embodied agents for various manipulation tasks. Compared to recent benchmarks developed to evaluate robotics manipulation tasks with language guidance and visual input [1, 3, 8], which lack (1) adaptation to novel objects automatically and (2) categorization for modular and flexible composition to complex tasks, we present VLMbench, a highly categorical robotic manipulation benchmark that generates numerous task demonstrations, as shown in Fig. 1. To generalize across the different object and task settings, we propose AMSolver, an automatic unit task builder that can generate various robot trajectories and grasping poses to accomplish the desired action for a unit task. Compared with previous works [3] that require the user to design a new task from scratch, we propose unit task templates that can generate a new task simply by specifying object properties. For instance, the pick-place task can be automatically generated for different objects provided their geometric models, with free variation in the objects’ colors, shapes, and sizes and 6 degrees of freedom (DoF) poses in Rotation Constraints",
                "authors": "Kai Zheng",
                "citations": 1
            },
            {
                "title": "Vertical Land Motion Monitoring at Tide Gauges in Korean Peninsula using Sequential Sbas-Insar",
                "abstract": "Vertical land motion (VLM) is an essential information for relative sea level changes measured by tide gauges. Corrections to tide gauge records for VLM will provide information to understand the regional and global sea level changes. In this study, time-series SBAS-InSAR algorithm was implemented to measure the ground subsidence/uplift at tide gauges located in the Korean peninsula using Sentinel-1 A/B SAR data. We processed about 10 tide gauges in Korea and observed VLM at Pohang, Incheon and Jeju tide stations.",
                "authors": "S. K. P. Vadivel, Duk‐jin Kim, Jungkyo Jung, Yang‐Ki Cho",
                "citations": 1
            },
            {
                "title": "Ground deformation analysis caused by post-2013 earthquake in Bohol, Philippines",
                "abstract": null,
                "authors": "M. D. A. Bauzon, R. Reyes, A. Blanco, F. Siringan",
                "citations": 1
            },
            {
                "title": "Assessment of Future Flood Hazards for Southeastern Texas: Synthesizing Subsidence, Sea‐Level Rise, and Storm Surge Scenarios",
                "abstract": "Recent hurricanes highlight shortcomings of flood resilience plans in Texas that can worsen with climate change and rising seas. Combining vertical land motion (VLM) with sea‐level rise (SLR) projections and storm surge scenarios for the years 2030, 2050, and 2100, we quantify the extent of flooding hazards. VLM rates are obtained from GNSS data and InSAR imagery from ALOS and Sentinel‐1A/B satellites. VLM is resampled and projected on LIDAR topographic data, then multiple inundation and flooding scenarios are modeled. By the year 2100, over 76 km2 are projected to subside below sea level. Subsidence increases the area of inundation over SLR alone by up to 39%. Under the worst‐case composite scenario of an 8‐m storm surge, subsidence, and the SLR RCP8.5, the total affected area is 1,156 km2. These models enable communities to improve flood resiliency plans.",
                "authors": "Megan M. Miller, M. Shirzaei",
                "citations": 14
            },
            {
                "title": "THE USE OF MOODLE IN ENGLISH LANGUAGE LEARNING DURING THE PANDEMIC: THE STUDENTS VOICE",
                "abstract": "During the emergency remote learning, the role of the Learning Management System is significant to support the teaching and learning process. Many universities utilized the Moodle based LMS to assist students and teachers in the teaching and learning process including Universitas Brawijaya that used Virtual Learning Management (VLM). VLM is highly suggested to be used by the lecturers since it can be accessed free as supported by the University, nonetheless, students claimed that the use of VLM is sometimes problematic. To address the students’ voice toward the use of VLM, this research aims to describe the students' attitude towards the use of VLM as an online learning platform. In this research using quantitative methods with a survey design. Instrument used in this study were questionnaires. The questionnaires were adapted according to the works of Srichanyachon (2014), Munasinghe & Wijewardana (2016), and Putri & Sari (2020). There are 100 students participating in this study consisting of 79 female students and 21 male students. The data was analyzed descriptively. This study has similar results to other studies where the use of platforms such as LMS in the COVID 19 era is indispensable and makes it a very effective platform to use. The result of this students’ attitude was a positive attitude with a mean score of 3.58.",
                "authors": "Nausa Revy Maulana, A. Lintangsari",
                "citations": 13
            },
            {
                "title": "The effects of video-annotated learning and reviewing system with vocabulary learning mechanism on English listening comprehension and technology acceptance",
                "abstract": "Abstract This study is aimed to design a novel vocabulary learning mechanism (VLM) in the previously developed video-annotated learning and reviewing system (VALRS) that allows learners to identify unfamiliar or unknown words when listening to the video and generate personalized input enhancement from English subtitles for vocabulary learning. It is expected that the VALRS with VLM (VALRS-VLM) can help learners improve their English listening comprehension in the recognition of speech sounds of individual vocabulary words and meaning understanding of vocabulary words and oral sentences. To investigate the learning effectiveness and learners’ experiences of the newly developed VLM, this study compared the outcomes of English listening comprehension performance and technology acceptance between the experimental group using the VALRS-VLM and the control group using the VALRS without the VLM (VALRS-NVLM). Analytical results show that the learners using the VALRS-VLM achieved remarkably better in both English listening learning outcomes and learning retention for vocabulary learning and overall English listening comprehension performance than those who used the VALRS-NVLM. Besides, according to the results of the questionnaire surveyed after the experiment, both the VALRS-VLM and VALRS-NVLM groups showed good technology acceptance. This study confirms that the proposed VALRS-VLM could effectively facilitate learners’ English listening learning as well as provide them with a positive learning experience.",
                "authors": "Chih-Ming Chen, Ming-Chaun Li, Mei-Feng Lin",
                "citations": 21
            },
            {
                "title": "The imprints of contemporary mass redistribution on local sea level and vertical land motion observations",
                "abstract": "Abstract. Observations from permanent Global Navigation Satellite System (GNSS) stations are commonly used to correct tide-gauge observations for vertical land motion (VLM). We combine GRACE (Gravity Recovery and Climate Experiment) observations and an ensemble of glacial isostatic adjustment (GIA) predictions to assess and evaluate the impact of solid-Earth deformation (SED) due to contemporary mass redistribution and GIA on VLM trends derived from GNSS stations. This mass redistribution causes relative sea-level (RSL) and SED patterns that not only vary in space but also exhibit large interannual variability signals. We find that for many stations, including stations in coastal locations, this deformation causes VLM trends on the order of 1  mm yr−1 or higher. In multiple regions, including the Amazon Basin and large parts of Australia, the SED trend flips sign between the first half and second half of the 15-year GRACE record. GNSS records often only span a few years, and due to these interannual variations SED causes substantial biases when the linear trends in these short records are extrapolated back in time. We propose a new method to avoid this potential bias in the VLM-corrected tide-gauge record: instead of correcting tide-gauge records for the observed VLM trend, we first remove the effects from GIA and contemporary mass redistributions from the VLM observations before computing the VLM trend. This procedure reduces the extrapolation bias induced by SED, and it also avoids the bias due to sea-floor deformation: SED includes net sea-floor deformation, which is ignored in global-mean sea-level reconstructions based on VLM-corrected tide-gauge data. We apply this method to 8166 GNSS stations. With this separation, we are able to explain a large fraction of the discrepancy between observed sea-level trends at multiple long tide-gauge records and the global-mean sea-level trend from recent reconstructions.",
                "authors": "T. Frederikse, F. Landerer, L. Caron",
                "citations": 29
            },
            {
                "title": "A Rare Case of Hepatic Visceral Larva Migrans",
                "abstract": "A systemic manifestation of nematode second-stage larvae migrating through the tissue of human viscera is termed visceral larva migrans (VLM). Although frequent, it is underdiagnosed in developing nations. Due to its portal venous blood supply, the liver is the organ that is affected the most frequently. The imaging results are subtle, making it challenging to distinguish them from granulomatous disorders, metastases, cystic mesenchymal hamartomas, and hepatocellular carcinoma (HCC). This case report includes clinical and laboratory information that aids in the diagnosis as well as imaging characteristics of hepatic lesions of VLM. It presents as coalescing and or conglomerated lesion(s) in the liver on imaging.",
                "authors": "Dr. Ravinandan Kumar, Dr. Sundareshan G, Dr. Sushmitha Y, Dr. Pankaj Kumar Sah, Dr. Neha Pandey, Dr. Udit Chauhan",
                "citations": 0
            },
            {
                "title": "BDNF expression partially mediates greater verbal learning and memory ability in a cohort enriched with risk for Alzheimer’s disease",
                "abstract": "In cognitively normal adults, women score higher than men in verbal learning ability (VLM)—a cognitive domain sensitive to decline in Alzheimer’s disease (AD). Sex‐specific mechanisms of action in brain‐derived neurotrophic factor (BDNF), a growth factor important for modulating memory and long‐term brain health, may explain this observed sex difference. Therefore, this study examines whether circulating BDNF expression mediates sex differences in VLM scores in a cohort enriched with risk for AD.",
                "authors": "Kyle J. Edmunds, Christine Rogers, Alyssa A Pandos, Gabriella M. Mamlouk, Alice Motovylyak, Sterling C. Johnson, O. Okonkwo",
                "citations": 0
            },
            {
                "title": "Liver abscesses caused by visceral larva migrans: Aimless wanderer separated from the host",
                "abstract": "Visceral larva migrans (VLM) is a systemic zoonotic parasitic disease caused by migration of the second stage larva through viscera of humans. Despite being a foremost public health problem in low- and middle-income countries (LMICs) such as India, larva migrans remains an untended zoonosis. Here, we report two cases of VLM who presented with fever and abdominal pain for a prolonged duration. On further investigation, marked peripheral eosinophilia with multiple confluent necrotizing eosinophilic granulomas were identified on histopathological examination of the liver.",
                "authors": "M. Osama, S. Dhawan, P. Ranjan, S. Singhvi",
                "citations": 0
            },
            {
                "title": "On Trapeze Wing Aerodynamics Calculations Based on Improved Vortex Lattice Method",
                "abstract": "This paper presents, aerodynamics coefficients calculation (Lifting & drag coefficients, pressure central location) of Trapeze wing shape configurations for different aspect ratios (ARs) values by using improved vortex lattice method (VLM), compared with finite-wing and slender body theories. The planar wing was divided into N panels of the size: 6X6 with trapezoid shape panels. As expected, for high ARs the VLM solution for the lifting coefficient is coincided with the finite wing theory whereas for small ARs (<1) it is coincided with the slender body theory (~1). Afterwards, we obtained that the calculated VLM induced drag becomes closer to the finitewing theory as the AR value is increased.",
                "authors": "J. Nagler",
                "citations": 0
            },
            {
                "title": "Immunisation Using Novel DNA Vaccine Encoding Virus Membrane Fusion Complex and Chemokine Genes Shows High Protection from HSV-2",
                "abstract": "Herpes simplex virus 1 and 2 infections cause high unmet disease burdens worldwide. Mainly HSV-2 causes persistent sexually transmitted disease, fatal neonatal disease and increased transmission of HIV/AIDS. Thus, there is an urgent requirement to develop effective vaccines. We developed nucleic acid vaccines encoding a novel virus entry complex stabilising cell membrane fusion, ‘virus-like membranes’, VLM. Two dose intramuscular immunisations using DNA expression plasmids in a guinea pig model gave 100% protection against acute disease and significantly reduced virus replication after virus intravaginal challenge. There was also reduced establishment of latency within the dorsal root ganglia and spinal cord, but recurrent disease and recurrent virus shedding remained. To increase cellular immunity and protect against recurrent disease, cDNA encoding an inhibitor of chemokine receptors on T regulatory cells was added and compared to chemokine CCL5 effects. Immunisation including this novel human chemokine gene, newly defined splice variant from an endogenous virus genome, ‘virokine immune therapeutic’, VIT, protected most guinea pigs from recurrent disease and reduced recurrent virus shedding distinct from a gD protein vaccine similar to that previously evaluated in clinical trials. All DNA vaccines induced significant neutralising antibodies and warrant evaluation for new therapeutic treatments.",
                "authors": "U. Gompels, F. Bravo, Sean Briggs, Shima Ameri, R. Cardin, D. Bernstein",
                "citations": 0
            },
            {
                "title": "FORM AND MODULATION OF METAPHOR TRANSLATION TO INDONESIAN OF VINGT-MILLES LIEUES SOUS LES MERS OF JULES VERNE",
                "abstract": "This research aims to gain an in-depth understanding of the form and modulation of metaphor translation from French to Indonesian, its use in the communicative context of speech events. Furthermore, this study also looks at the relevance of the results of the translation of the form and modulation with mediation, plurilingual and pluricultural competences in the CEFR European standard language proficiency. The data sources used are the Novel Vingt-Milles Lieues Sous Les Mers by Jules Verne and its translated novel 20.000 Mil di bawah Laut by NH Dini. The data in this study is a metaphor in a broad sense. The most dominant form of metaphor translation used is reproduction. Next, there are substitutions and paraphrases. The combined form of reproduction+paraphrasing is found in the VLM metaphor translation. Equivalence of meaning is obtained by using explicit and implicit modulation, special and general meanings, and point of view. The context of speech events contributes to producing an equivalent and natural translation. The results of this translation indicate that translation activities are complex language activities and require precision and accuracy. Translating literary works requires cultural mastery and an advanced level of linguistic mastery. The competencies are described in the Petra Project terms of reference contained in the CEFR 2020 and CECRL 2018 terms of reference. Translating skills require guided practice acquired through a well-oriented education. The ability to understand metaphors and other cultures and be able to compare them by providing analogies in the local culture is a plurilingual and pluricultural competence.",
                "authors": "Yusi Asnidar, S. Sumantri, N. Lustyantie",
                "citations": 0
            },
            {
                "title": "Development and Fidelity Assessment of Potential Flow based Framework for Aerodynamic Modeling of High Lift Devices",
                "abstract": "High lift devices play a vital role in dictating the accelerated performance of an aircraft for different flight phases such as takeoff, landing, and aerobatic maneuvers. The aerodynamic design of high lift devices for any particular aircraft is an iterative process and is achieved through extensive aerodynamic Analysis of the aircraft for various flap configurations. Computational Fluid Dynamics (CFD) and Wind tunnel testing are highly effective techniques for performing the required Analysis, yet they have high computational costs and time. To overcome this shortcoming, a robust framework based on potential flow solver (PFS) and geometry parameterization is required without compromising the fidelity of the Analysis. This research aims to develop a highly robust aerodynamic analysis framework based on the Vortex Lattice Method (VLM) coupled with Polhamus Suction Analogy and parametric modeling of high lift devices. The fidelity of the framework is validated through experimental testing and is quantified by developing a fidelity assessment matrix. It is established that the computational cost of CFD has been reduced three times with only a 10% to 20% loss in accuracy when the developed framework is used. The developed PFS framework gives results from 80% to 90%. The framework results for a reference aircraft are thoroughly compared with CFD analyses. The framework provides values that agree with corresponding CFD analyses in a fraction of the time.",
                "authors": "M. Shahid, M. T. Javaid, Sheharyar Nasir, Umar Sajjad, Faizan Haider, S. S. UlHassan, S. Salamat",
                "citations": 0
            },
            {
                "title": "An Exploration of Lecturer Strategies in Managing Language Skills Courses Using Online Learning During The Pandemic Covid-19: A Case Of Brawijaya University-Indonesia",
                "abstract": "This article describes the online learning strategy for language skills in the English Language Education Study Program of Brawijaya University-Indonesia during the COVID-19 pandemic. This research is exploratory qualitative research that uses observation and interviews as a way to obtain data. The data and information obtained are used as a basis in discussing this research problem with descriptive analysis. All lecturers take advantage of the available learning facilities by implementing synchronous and asynchronous learning sessions. Some information was obtained from interviews with lecturers stating that in using synchronous sessions, the lecturers use the Zoom, Google Meet, and VLM facilities to manage teleconference model lectures. Meanwhile, lecturers use the Google Classroom facility to carry out various activities in using asynchronous sessions, including sending reading assignments, projects, online discussions, and sending asynchronous class schedules. One of the learning strategies in this research is the strategy of managing the online learning process. In the strategy of managing learning, creating student discipline, in this case, so that students actively participate by turning on the camera in a synchronous session, is needed to create meaningful interactions. At last, the researcher suggests further research to develop language skills courses and pedagogy courses.",
                "authors": "S. Adi",
                "citations": 0
            },
            {
                "title": "Dynamic Vertical Larynx Actions Under Prosodic Focus",
                "abstract": "It is well known that there is a positive correlation between fundamental frequency and vertical larynx position. Recently, Lee (2018) observes that one vertical larynx movement (VLM) is associated with an Accentual Phrase (AP) in Seoul Korean. The current study builds on these findings by investigating the effect of prosodic focus on vertical larynx actions. Target sentences were designed to produce four APs (e.g., Joohyun sold six yards of shabby garden field; AP [Joohyun-SUBJ] AP [shabby garden field] AP [six yards-OBJ] AP [sold-DECL], presented in Korean) and were used to elicit focus on the initial word of the object phrase (e.g., six ). Articulatory data on VLM is obtained from five Seoul Korean speakers using real-time MRI. Results indicate that quantifiable VLMs observed for each sentence range from 3 to 6 movements, with 4 movements per sentence being the most frequent. Sentences with focus have more instances of VLM per sentence than those without. Focused sentences exhibit significantly greater vertical larynx displacement around the region of focus than the control. Our findings have implications for prosodic planning and pitch resetting, and ongoing analyses examine how VLMs align with Accentual Phrases in Seoul Korean and correlate with fundamental frequency.",
                "authors": "Mira Oh, Yoonjeong Lee",
                "citations": 0
            },
            {
                "title": "PHÂN TÍCH KHÍ ĐỘNG HỌC CỦA MẪU CÁNH NACA 6409 ỨNG DỤNG TRONG TUABIN ĐIỆN GIÓ VỚI PHƯƠNG PHÁP THANH",
                "abstract": "Bài báo này trình bày cơ sở lý thuyết và khả năng ứng dụng phần mềm XFLR5 để tiến hành phân tích khí động học của mẫu cánh tuabin gió NACA 6409 tại các giá trị số Reynolds thấp. Phần mềm XFLR5 mới được phát triển dựa trên phần mềm XFOIL, đây là phần mềm được sử dụng rất phổ biến trong lĩnh vực thiết kế mẫu cánh trên thế giới. Cơ sở toán học của XFLR5 bao gồm lý thuyết đường nâng (LLT), phương pháp ô mạng xoáy (VLM) và phương pháp thanh (PM). Trong nghiên cứu này, phương pháp PM sẽ được sử dụng để phân tích các thông số khí động học như hệ số lực nâng Cl, hệ số lực cản Cd, hệ số Cl/Cd và hệ số áp lực Cp đặt lên bề mặt của mẫu cánh NACA 6409 khi hoạt động tại các góc tấn công khác nhau. Phương pháp PM có thời gian phân tích nhanh và các kết quả thu được cho thấy sự phù hợp tốt với các dữ liệu thực nghiệm đã được công bố trước đó.",
                "authors": "Đinh Văn Thìn, N. Đức, Lê Quang Sáng",
                "citations": 0
            },
            {
                "title": "Venolymphatic malformation of tongue: a prompt life saving intervention",
                "abstract": "Venolymphatic malformations (VLMs) of the head and neck can have varied clinical presentation with associated complications. Timely diagnosis and treatment is warranted to curb the morbidity and obtain satisfactory outcomes. We present the case of a male patient in his early 20s with VLM of the tongue who received precise interventions which were executed with active contributions from a team of anaesthesiologists, interventional radiologists, oral and maxillofacial surgeons and intensivists. We highlight the importance of an immediate, comprehensive and multimodal treatment approach for VLMs of the maxillofacial region.",
                "authors": "A. Ganesan, Adarsh Ishwar Hegde, T.S. Ghosh, K. Chaudhry",
                "citations": 0
            },
            {
                "title": "Aerodynamic efficiency analysis of variable morphing wings",
                "abstract": "Bu makalede değişken kanat ve/veya kanatçık kavramları incelenmiştir. Çalışmanın amacı, morphing uçaklarının aerodinamik verimliliğini artırmak için kanatları ve kanatçıkların değişim açılarını belirlemek ve optimize etmekti. Analiz, değişen tarama açısına ve kanat ucu bükümüne ve ayrıca eğim açısı biçim değiştirmesine (Γ = 0° - 45° φ = -10°- 10° ve Λ = 0° - 30°) dayanmaktadır. VLM analiz yöntemi kullanılarak farklı kanat varyasyonları denenmiştir. Sonuç olarak, optimum performans faydalarını elde etmek için gerekli açı uyarlanarak uçuş özelliklerinde önemli gelişmeler gözlemlenmiştir",
                "authors": "E. Kaygan, Tugce Koroglu, Melisa Basak",
                "citations": 0
            },
            {
                "title": "Kdelab at ImageCLEFmedical 2022 Caption Prediction Task",
                "abstract": "ImageCLEFmedical 2022 Caption Task is an example of a challenging research problem in the field of image captioning. The goal of this research is to automatically generate accurate captions describing a given medical image. We describe five approaches using image retrieval and Deep Learning . In this paper, we have adopted K-nn as image retrieval, X-VLM and Show, Attend and Tell as Deep Neural Network (DNN). Furthermore, we describe the effectiveness of a method that uses information from the CUI code as an input feature for DNN. We submitted 8 runs to the caption prediction task, and achieved the BLEU score of 0.278 and the ROUGE score of 0.158, which ranked 7th among the participating teams.",
                "authors": "Riku Tsuneda, Tetsuya Asakawa, Kazuki Shimizu, T. Komoda, Masaki Aono",
                "citations": 0
            },
            {
                "title": "Flight dynamics modeling and stability analysis of a tube-launched tandem wing aerial vehicle during the deploying process",
                "abstract": "During the subsequent deploying process after the launch, the vehicle experiences dramatic changes in geometry as well as aerodynamics, which cannot be ignored for the flight stability of the aircraft. The investigation on the flight dynamics modeling and stability analysis during the unsteady deploying process is of great significance for the flight control system design and the launch system design. The nonlinear multibody dynamic model of the vehicle, which mainly consists of the airframe and four wings, is established through the Newton–Euler method. Meanwhile, the unsteady vortex lattice method (VLM), which is suitable for tandem wing aircraft and combined with the viscosity effect in light of the strip theory, is proposed to calculate the aerodynamic loads of the vehicle during the deploying process. A series of simulations about the motion of the vehicle after the launch are calculated to analyze the effects of various parameters, including the deploying forms, the start-up time of the power system along with the launching parameters, on the stability of the aerial vehicle in the deployment process. As results indicated, a small difference in the deploying rates of wings and larger deploying rates of wings on the same side are beneficial to the stability of the vehicle during the deploying process. Additionally, it is advantageous to the pitching stability but not the lateral stability of the vehicle when the power system is started earlier. Besides, the attitude control strategy of the vehicle can be formulated in advance by analyzing the attitude of the vehicle with different launch parameters at the launching moment.",
                "authors": "Hao Cheng, Q. Shi, Hua Wang, W. Shan, Tao Zeng",
                "citations": 9
            },
            {
                "title": "CALM: Survivable Virtual Data Center Allocation in Cloud Networks",
                "abstract": "Cloud data centers have become a popular infrastructure to host diversified application services for tenants. To provide agility and elasticity in resource usage for cloud services, the virtual data center (VDC) is proposed to allocate both virtual machines (VM) and network bandwidth. However, at cloud scale, hardware (e.g., link, server, and switch) failures are inevitable, which may lead to degradation in service performance. To address this challenge, we study the survivable virtual data center allocation problem (SVAP), which aims at allocating survivable virtual data center (SVDC) to each tenant to guarantee resource demands will always be satisfied even after failures. Our objective is to minimize the total bandwidth consumption in order to accommodate more SVDCs. We prove that SVAP is NP-hard and design the Collocation-Aware survivable VM placement and Link Mapping algorithm (CALM). CALM solves the problem in two stages, i.e., VM placement (VMP) and virtual link mapping (VLM). We further find that without an appropriate VMP strategy, VLM cannot lead to the minimum network resource usage. Therefore, we propose a polynomial-time algorithm called collocation-aware survivable placement (CASP) for VMP. For the VLM stage, we formulate a linear programming model to map flows onto the data center network in order to ensure survivability under switch failures. We evaluate the performance via simulations and show that CALM could save up to 42 percent network resource compared to the baseline algorithm. We further show that CALM uses only additional 13 percent network resource to guarantee survivability as compared to a typical VDC strategy.",
                "authors": "Hong-Yen Lo, W. Liao",
                "citations": 9
            },
            {
                "title": "Status of Mean Sea Level Rise around the USA (2020)",
                "abstract": "The potential threats to the USA from current and projected sea level rise are significant, with profound environmental, social and economic consequences. This current study continues the refinement and improvement in analysis techniques for sea level research beyond the Fourth US National Climate Assessment (NCA4) report by incorporating further advancements in the time series analysis of long tide gauge records integrated with an improved vertical land motion (VLM) assessment. This analysis has also been synthesised with an updated regional assessment of satellite altimetry trends in the sea margins fringing the USA. Coastal margins more vulnerable to the threats posed by rising sea levels are those in which subsidence is prevalent, higher satellite altimetry trends are evident and higher ‘geocentric’ velocities in mean sea level are being observed. The evidence from this study highlights key spatial features emerging in 2020, which highlight the northern foreshore of the Gulf Coast and along the east coast of the USA south of the Chesapeake Bay region being more exposed to the range of factors exacerbating threats from sea level rise than other coastlines at present. The findings in this study complement and extend sea level research beyond NCA4 to 2020.",
                "authors": "P. Watson",
                "citations": 9
            },
            {
                "title": "Selective Pharmacogenetic Activation of Catecholamine Subgroups in the Ventrolateral Medulla Elicits Key Glucoregulatory Responses",
                "abstract": "Catecholamine (CA) neurons in the ventrolateral medulla (VLM) contribute importantly to glucoregulation during glucose deficit. However, it is not known which CA neurons elicit different glucoregulatory responses or whether selective activation of CA neurons is sufficient to elicit these responses. Therefore, to selectively activate CA subpopulations, we injected male or female Th-Cre+ transgenic rats with the Cre-dependent DREADD construct, AAV2-DIO-hSyn-hM3D(Gq)-mCherry, at one of four rostrocaudal levels of the VLM: rostral C1 (C1r), middle C1 (C1m), the area of A1 and C1 overlap (A1/C1), and A1. Transfection was highly selective for CA neurons at each site. Systemic injection of the Designer Receptor Exclusively Activated by Designer Drugs (DREADD) receptor agonist, clozapine-N-oxide (CNO), stimulated feeding in rats transfected at C1r, C1m, or A1/C1 but not A1. CNO increased corticosterone secretion in rats transfected at C1m or A1/C1 but not A1. In contrast, CNO did not increase blood glucose or induce c-Fos expression in the spinal cord or adrenal medulla after transfection of any single VLM site but required dual transfection of both C1m and C1r, possibly indicating that CA neurons mediating blood glucose responses are more sparsely distributed in C1r and C1m than those mediating feeding and corticosterone secretion. These results show that selective activation of C1 CA neurons is sufficient to increase feeding, blood glucose levels, and corticosterone secretion and suggest that each of these responses is mediated by CA neurons concentrated at different levels of the C1 cell group.",
                "authors": "Ai-jun Li, Qing Wang, S. Ritter",
                "citations": 31
            },
            {
                "title": "Evaluation of VSPAERO Analysis Capabilities for Conceptual Design of Aircraft with Propeller-Blown Wings",
                "abstract": "Advancements in electric propulsion and the emergence of Advanced Air Mobility are driving the evolution of new aircraft designs. Since electric propulsion enables flexibility in propeller location, there is an increasing need for reliable, quick analyses of propeller-airframe interactions during the conceptual design phase. Many existing analysis tools capable of accurately modeling propeller-airframe interactions are computationally expensive and require a high level of expertise and significant time investment for setup. VSPAERO is a NASA-developed, open source, computational analysis tool that runs a Vortex-Lattice Method (VLM) solver and is targeted at conceptual design. This paper assesses the applicability of the VSPAERO VLM in the conceptual design phase by comparing VSPAERO predictions to predictions by OVERFLOW, a Reynolds-Averaged Navier-Stokes Computational Fluid Dynamics solver, and RoBIN, another VLM tool. The paper details the modeling, meshing, and analysis techniques used within VSPAERO. Analyses were performed for a wing in isolation, a propeller in isolation, and then for two propeller-blown wing configurations: one with a propeller located at the midspan and another with a propeller located at the wingtip. The propeller was modeled both as an actuator disk and as rotating blades.",
                "authors": "Carla N. Sheridan, Dahlia Pham, Siena Whiteside",
                "citations": 8
            },
            {
                "title": "Vertical Land Motion as a Driver of Coastline Changes on a Deltaic System in the Colombian Caribbean",
                "abstract": "To face and properly mitigate coastal changes at a local level, it is necessary to recognize and characterize the specific processes affecting a coastline. Some of these processes are local (e.g., sediment starvation), while others are regional (e.g., relative sea-level change) or global (e.g., eustatic sea-level rise). Long tide gauge records help establish sea-level trends for a region that accounts for global (eustatic, steric) and regional (isostatic) sea-level changes. Local sea-level changes are also the product of vertical land motion (VLM), varying depending on tectonic, sedimentological, and anthropogenic factors. We investigate the role of coastal land subsidence in the present-day dynamics of an abandoned delta in the Colombian Caribbean. Satellite images and synthetic aperture radar acquisitions are used to assess decadal-scale coastline changes and subsidence rates for the period 2007–2021. We found that subsidence rates are highly variable alongshore. Local subsidence rates of up to −1.0 cm/yr correspond with an area of erosion rates of up to −15 m/yr, but coastal erosion also occurs in sectors where subsidence was not detected. The results highlight that local coastline changes are influenced by multiple, interacting drivers, including sand supply, coastline orientation and engineering structures, and that subsidence alone does not explain the high rates of coastal erosion along the study area. By the end of the century, ongoing coastal erosion rates of up to −25 m/yr, annual rates of subsidence of about −1 cm/yr, and current trends of global sea-level rise are expected to increase flooding levels and jeopardize the existence of the deltaic barrier island.",
                "authors": "J. Gómez, E. Kwoll, I. Walker, M. Shirzaei",
                "citations": 8
            },
            {
                "title": "Energy Saving On a Full-Size Wheel Loader Through Variable Load Sense Margin Control",
                "abstract": "\n This paper presents the formulation of a variable load sense control strategy suitable to achieve power savings in hydraulic systems using post-compensated load sensing (LS) hydraulic control architectures. Such architecture is typical in off-road construction machinery. The paper also describes the application of the proposed control strategy referred to as variable load sensing margin (VLM) on a full-size wheel loader. The paper first presents the rationale for the proposed strategy, showing how the state-of-the art LS architecture present in commercial machines has margin for lowering the throttling losses present at the control valves. A feedforward controller, derived from an empirical study on a reference vehicle, is used to control the flow to the front-end loader functions. Test results show improvements of the hydraulic power consumption up to 45%, based on the commanded speed of each front-end loader actuator. The paper also describes a gain scheduling pressure feedback control strategy which is used to allow controlling also functions that include priority. For the case of off-road vehicles this is typically the steering function. The experimental results show how good performances with an error in controlled velocity below 5%, is achieved when the front-end loader functions are used concurrently with the steering.",
                "authors": "Riccardo Madau, A. Vacca, F. Pintore",
                "citations": 8
            },
            {
                "title": "Constraining Interseismic Deformation of the Cascadia Subduction Zone: New Insights From Estimates of Vertical Land Motion Over Different Timescales",
                "abstract": "We determine late Holocene (past 4 kyr) vertical land motion (VLM) rates from relative sea level observations along the coastline of western North America and compare these to contemporary (decadal‐scale) rates inferred from Global Positioning System data. The residual rates (contemporary minus late Holocene) indicate uplift at most locations, which likely reflects short‐term signals associated with the Cascadia subduction earthquake cycle and processes that were recently activated. Regarding the latter, we model and remove the signals associated with ground water extraction and twentieth‐century glacier retreat, which have a significant influence in, respectively, California and southern British Columbia. We interpret the remaining signal as being dominated by interseismic deformation associated with the locking of the Cascadia megathrust. A preliminary comparison of this signal with output from a model of interseismic deformation indicates good agreement at most locations within a range of key model parameter values. This agreement is particularly encouraging as the locking model is based on the inversion of only horizontal land motion observations. Considering the data‐model fit at all locations, preference was found for models featuring nearly full locking to a relatively shallow depth (<30 km), although the locking state preference is variable along the coast and not strong. The best fitting parameter set varies considerably from site to site, and no set of parameter values was able to capture the residual VLM rates in northwestern Vancouver Island. Our study indicates that the residual VLM data provide useful constraints for megathrust locking models of Cascadia subduction.",
                "authors": "M. Yousefi, G. Milne, Shaoyang Li, Kelin Wang, Alan Bartholet",
                "citations": 17
            },
            {
                "title": "The Zone of Influence: Matching sea level variability from coastal\naltimetry and tide gauges for vertical land motion estimation",
                "abstract": "Vertical land motion (VLM) at the coast is a substantial contributor to relative sea level change. In this work, we present a refined method for its determination, which is based on the combination of absolute satellite altimetry (SAT) sea level measurements and relative sea level changes recorded by tide gauges (TGs). These measurements complement VLM estimates from the GNSS (Global Navigation Satellite System) by increasing their spatial coverage. Trend estimates from the SAT and TG combination are particularly sensitive to the quality and resolution of applied altimetry data as well as to the coupling procedure of altimetry and TGs. Hence, a multi-mission, dedicated coastal along-track altimetry dataset is coupled with high-frequency TG measurements at 58 stations. To improve the coupling procedure, a so-called “zone of influence” (ZOI) is defined, which confines coherent zones of sea level variability on the basis of relative levels of comparability between TG and altimetry observations. Selecting 20 % of the most representative absolute sea level observations in a 300 km radius around the TGs results in the best VLM estimates in terms of accuracy and uncertainty. At this threshold, VLMSAT-TG estimates have median formal uncertainties of 0.58 mmyr−1. Validation against GNSS VLM estimates yields a root mean square (rms1VLM) of VLMSAT-TG and VLMGNSS differences of 1.28 mmyr−1, demonstrating the level of accuracy of our approach. Compared to a reference 250 km radius selection, the 300 km zone of influence improves trend accuracies by 15 % and uncertainties by 35 %. With increasing record lengths, the spatial scales of the coherency in coastal sea level trends increase. Therefore, the relevance of the ZOI for improving VLMSAT-TG accuracy decreases. Further individual zone of influence adaptations offer the prospect of bringing the accuracy of the estimates below 1 mmyr−1.",
                "authors": "J. Oelsmann, M. Passaro, D. Dettmering, C. Schwatke, L. Sánchez, F. Seitz",
                "citations": 17
            },
            {
                "title": "Aircraft Aeroservoelastic Modelling of the FLEXOP Unmanned Flying Demonstrator",
                "abstract": "This paper presents the aeroservoelastic modelling toolchain established for the aircraft design exercise within the European research project, FLEXOP. The FLEXOP project aims to develop and apply active flutter suppression and load alleviation techniques on an unmanned flying demonstrator. The developed methods are then to be applied in the design of a commercial-scale wing derivative in a scale-up task. A high-fidelity finite element (FE) structural model is the first block in the modelling process. A condensed FE model together with aerodynamic models based on the doublet-lattice (DLM) and vortexlattice (VLM) methods represent the aeroelastic system. The aerodynamics represented by the afore-mentioned panel methods is complemented by results from higher-fidelity computational fluid dynamics (CFD) simulations. Reduced-order aeroservoelastic models suitable for control-synthesis are then generated using a “bottom-up” modelling approach. The aim of the paper is to present an overview of the different models encountered during such a design process and their domains of application. ∗Research Scientist, Loads Analysis and Aeroelastic Design †Professor, Department Leader Loads Analysis and Aeroelastic Design ‡Research Scientist, Aircraft Systems Dynamics §Research Associate, Institute of Aircraft Design ¶Professor, Head Institute of Aircraft Design ‖Research Associate, Chair of Aerodynamics and Fluid Mechanics ∗∗Professor, Head Chair of Aerodynamics and Fluid Mechanics ††Research Engineer ‡‡Research Fellow, Computer and Automation Research Institute §§Senior Research Fellow, Systems and Control Lab",
                "authors": "Yasser M. Meddaikar, J. Dillinger, T. Klimmek, W. Krueger, Matthias Wuestenhagen, T. Kier, A. Hermanutz, M. Hornung, V. Rozov, C. Breitsamter, J. Alderman, B. Takarics, B. Vanek",
                "citations": 23
            },
            {
                "title": "Sova: A Software-Defined Autonomic Framework for Virtual Network Allocations",
                "abstract": "With the rise of network virtualization, the workloads deployed on data center are dramatically changed to support diverse service-oriented applications, which are in general characterized by the time-bounded service response that in turn puts great burden on the data-center networks. Although there have been numerous techniques proposed to optimize the virtual network allocation in data center, the research on coordinating them in a flexible and effective way to autonomically adapt to the workloads for service time reduction is few and far between. To address these issues, in this article we propose Sova, an autonomic framework that can combine the virtual dynamic SR-IOV (DSR-IOV) and the virtual machine live migration (VLM) for virtual network allocations in data centers. DSR-IOV is a SR-IOV-based virtual network allocation technology, but its operation scope is very limited to a single physical machine, which could lead to the local hotspot issue in the course of computation and communication, likely increasing the service response time. In contrast, VLM is an often-used virtualization technique to optimize global network traffic via VM migration. Sova exploits the software-defined approach to combine these two technologies with reducing the service response time as a goal. To realize the autonomic coordination, the architecture of Sova is designed based on the MAPE-K loop in autonomic computing. With this design, Sova can adaptively optimize the network allocation between different services by coordinating DSR-IOV and VLM in autonomic way, depending on the resource usages of physical servers and the network characteristics of VMs. To this end, Sova needs to monitor the network traffic as well as the workload characteristics in the cluster, whereby the network properties are derived on the fly to direct the coordination between these two technologies. Our experiments show that Sova can exploit the advantages of both techniques to match and even beat the better performance of each individual technology by adapting to the VM workload changes.",
                "authors": "Zhiyong Ye, Yang Wang, Shuibing He, Chengzhong Xu, Xian-He Sun",
                "citations": 6
            },
            {
                "title": "Effects of Health Promotion Model-Based Visual Learning Module on Self-Efficacy and Health Promotion Behavior of Stroke Survivors: A Nonrandomized Controlled Trial",
                "abstract": "Abstract Background and Objectives Globally, stroke is one of the major causes of disability and mortality among adults and old age people. The present study aims to evaluate the effects of the health promotion model-based visual learning module (HPM-VLM) on self-efficacy and behavioral modifications among stroke survivors. Methods This nonrandomized controlled trial was conducted on 70 stroke survivors (intervention group, n = 35, and control group, n = 35). The intervention group was subjected to two sessions of the HPM-VLM and the control group received routine instructions. Data were collected through face-to-face structured interview, and observation using a self-structured self-efficacy questionnaire and health promotion behavior questionnaire. Data were analyzed using descriptive (frequency and percentage) and inferential (Chi-square, independent t-test, mixed model, and ANCOVA) values by IBM Statistical Package for Social Sciences (SPSS; version 23) software. Results Eventually, follow-up could have been completed for 66 participants (intervention group, n = 34, and control group, n = 32). HPM-VLM is found to be effective in the promotion of self-efficacy (19.2 ± 1.6 vs. 16.12 ± 2.5; p = 001) and health promotion behavior of stroke survivors in most of the domains (p < 0.01). Conclusion HPM-VLM is an effective interventional tool for the promotion of self-efficacy and health promotion behavior of stroke survivors.",
                "authors": "S. Mudgal, S. Sharma, Jitender Chaturvedi, D. Chundawat",
                "citations": 5
            },
            {
                "title": "Constraint of glacial isostatic adjustment in the North Sea with geological relative sea level and GNSS vertical land motion data",
                "abstract": "\n In this study, we focus on improved constraint of the glacial isostatic adjustment (GIA) signal at present-day, and its role as a contributor to present-day sea level budgets. The main study area extends from the coastal regions of northwestern Europe to northern Europe. Both Holocene relative sea level (RSL) data as well as vertical land motion (VLM) data are incorporated as constraints in a semi-empirical GIA model. 71 geological rates of GIA-driven RSL change are inferred from Holocene proxy data and 108 rates of vertical land motion from GNSS provide an additional measure of regional GIA deformation. Within the study area, the geological RSL data complement the spatial gaps of the VLM data and vice versa. Both data sets are inverted in a semi-empirical GIA model to yield updated estimates of regional present-day GIA deformations. A regional validation using tide gauges is presented for the North Sea, where the GIA signal may be complicated by lateral variations in Earth structure and existing predictions of regional and global GIA models show discrepancies. The model validation in the North Sea region suggests that geological data are needed to fit independent estimates of GIA-related RSL change inferred from tide gauge rates, indicating that geological rates from Holocene data do provide an important additional constraint for data-driven approaches to GIA estimation.",
                "authors": "K. M. Simon, Riccardo Riva, L. Vermeersen",
                "citations": 5
            },
            {
                "title": "Uplift of the Western Transverse Ranges and Ventura Area of Southern California: A Four‐Technique Geodetic Study Combining GPS, InSAR, Leveling, and Tide Gauges",
                "abstract": "We estimate the rate of vertical land motion (VLM) in the region around the Western Transverse Ranges (WTR), Ventura, and Big Bend of the San Andreas Fault (SAF) of southern California using data from four geodetic techniques: GPS, interferometric synthetic aperture radar (InSAR), leveling, and tide gauges. We use a new analysis technique called GPS Imaging to combine the techniques and leverage the synergy between (1) high geographic resolution of InSAR, (2) precision, stability, and geocentric reference frame of GPS, (3) decades long observation of VLM with respect to the sea surface from tide gauges, and (4) relative VLM along dense leveling lines. The uncertainty in the overall rate field is ~1 mm/yr, though some individual techniques have uncertainties as small as 0.2 mm/yr. The most rapid signals are attributable to subsidence in aquifers and groundwater changes. Uplift of the WTR is geographically continuous, adjacent to the SAF and appears related to active crustal contraction across Pacific/North America plate boundary fault system. Uplift of the WTR and San Gabriel Mountains is ~2 mm/yr and is asymmetrically focused west of the SAF, consistent with interseismic strain accumulation across thrust faults in the Ventura area and Santa Barbara channel that accommodate contraction against the near vertical SAF.",
                "authors": "W. Hammond, R. Burgette, K. Johnson, G. Blewitt",
                "citations": 26
            },
            {
                "title": "Updated Mean Sea-Level Analysis: Australia",
                "abstract": "ABSTRACT Watson, P.J., 2020. Updated mean sea-level analysis: Australia. Journal of Coastal Research, 36(5), 915–931. Coconut Creek (Florida), ISSN 0749-0208. As an island nation with 60,000 km of open coastline and extensive margins of increasingly urbanised intertidal estuarine foreshores, Australia is critically exposed to the global threat posed by rising sea levels into the future. This study provides a contemporary assessment of sea-level rise around Australia to the end of 2018, based on all available tide gauge records and satellite altimetry. The study provides the first national assessment of vertical land motion (VLM) around the coast, identifying margins more prevalent to subsidence, which in turn exacerbate the localised effects of a rising global mean sea level. These areas include coastlines between Townsville and Coffs Harbour, Burnie to Port Pirie, and Fremantle to Wyndham. State-of-the-art time-series analysis techniques applied to all high-quality tide gauge records exceeding 75 years in length (four sites) enabled improved insights into the temporal resolution of current rates of rise and accelerations in mean sea level around Australia than were previously available. Averaged across these four records in 2018, approximately 40% of the “relative” velocity observed (∼2.2 ± 1.8 mm/y, 95% confidence limit [CL]) is attributable to VLM. When corrected for VLM, only the Fort Denison site exhibits “geocentric” mean sea-level velocity in 2018 exceeding 2 mm/y. The average geocentric velocity across all four sites in 2018 equates to 1.3 ± 2.0 mm/y (95% CL). Interestingly, each long record exhibits similar temporal characteristics, whereby a low point in the velocity time series occurs sometime in the period from 1970 to 1990, after which velocity increases over time to a peak occurring sometime after ca. 2010, suggesting the presence of a small acceleration (albeit not statistically different to zero at the 95% CL) in the record.",
                "authors": "P. Watson",
                "citations": 12
            },
            {
                "title": "Biologically Inspired Dynamic Soaring Maneuvers for an Unmanned Air Vehicle Capable of Sweep Morphing",
                "abstract": null,
                "authors": "I. Mir, A. Maqsood, S. Akhtar",
                "citations": 27
            },
            {
                "title": "Vertical Land Motion From Present‐Day Deglaciation in the Wider Arctic",
                "abstract": "Vertical land motion (VLM) from past and ongoing glacial changes can amplify or mitigate ongoing relative sea level change. We present a high‐resolution VLM model for the wider Arctic, that includes both present‐day ice loading (PDIL) and glacial isostatic adjustment (GIA). The study shows that the nonlinear elastic uplift from PDIL is significant (0.5–1 mm yr−1) in most of the wider Arctic and exceeds GIA at 15 of 54 Arctic GNSS sites, including sites in nonglaciated areas of the North Sea region and the east coast of North America. Thereby the sea level change from PDIL (1.85 mm yr−1) is significantly mitigated from VLM caused by PDIL. The combined VLM model was consistent with measured VLM at 85% of the GNSS sites ( R=0.77 ) and outperformed a GIA‐only model ( R=0.64 ). Deviations from GNSS‐measured VLM can be attributed to local circumstances causing VLM.",
                "authors": "C. Ludwigsen, S. Khan, O. Andersen, B. Marzeion",
                "citations": 14
            },
            {
                "title": "Subsidence‐Derived Volumetric Strain Models for Mapping Extensional Fissures and Constraining Rock Mechanical Properties in the San Joaquin Valley, California",
                "abstract": "Large‐scale subsidence due to aquifer‐overdraft is an ongoing hazard in the San Joaquin Valley. Subsidence continues to cause damage to infrastructure and increases the risk of extensional fissures.Here, we use InSAR‐derived vertical land motion (VLM) to model the volumetric strain rate due to groundwater storage change during the 2007–2010 drought in the San Joaquin Valley, Central Valley, California. We then use this volumetric strain rate model to calculate surface tensile stress in order to predict regions that are at the highest risk for hazardous tensile surface fissures. We find a maximum volumetric strain rate of −232 microstrain/yr at a depth of 0 to 200 m in Tulare and Kings County, California. The highest risk of tensile fissure development occurs at the periphery of the largest subsiding zones, particularly in Tulare County and Merced County. Finally, we assume that subsidence is likely due to aquifer pressure change, which is calculated using groundwater level changes observed at 300 wells during this drought. We combine pressure data from selected wells with our volumetric strain maps to estimate the quasi‐static bulk modulus, K, a poroelastic parameter applicable when pressure change within the aquifer is inducing volumetric strain. This parameter is reflective of a slow deformation process and is one to two orders of magnitude lower than typical values for the bulk modulus found using seismic velocity data. The results of this study highlight the importance of large‐scale, high‐resolution VLM measurements in evaluating aquifer system dynamics, hazards associated with overdraft, and in estimating important poroelastic parameters.",
                "authors": "G. Carlson, M. Shirzaei, C. Ojha, S. Werth",
                "citations": 13
            },
            {
                "title": "Sea Level Rise in New Zealand: The Effect of Vertical Land Motion on Century‐Long Tide Gauge Records in a Tectonically Active Region",
                "abstract": "Historically tide gauge (TG) data have been used to estimate global sea level rise. Critical to the analysis of TG records is the assumption that the TG sites are stable and not affected by vertical land motion (VLM). We analyze century‐long TG records from New Zealand that have been affected by VLM due to both major and transient earthquake events at a regional level as well as local instabilities. Using combined GPS and precise leveling, we estimate relative VLM between the GPS and TG of up to 1 mm/year. Based on 15–20 years of GPS data, the effect of seismic activity and slow slip events has uplifted sites by up to 0.8 mm/year on average in Wellington and Dunedin. Updated estimates of long‐term relative sea level (RSL) at four New Zealand TGs, as well as an estimate of RSL at a new fifth TG (New Plymouth), are determined using data through 2013—an additional 13 years compared to the previous study. The VLM corrected RSL rates gives our best estimate absolute sea level of +1.45 ± 0.28 mm/year (1891−2013). It is important that absolute sea level derived from RSL rates include realistic estimates of VLM, especially at TG sites that are close to plate boundaries, located in seismically active regions, affected by glacial isostatic adjustment, land ice loss, or gas/oil/water extraction.",
                "authors": "P. Denys, R. J. Beavan, J. Hannah, C. Pearson, N. Palmer, Mike Denham, S. Hreinsdóttir",
                "citations": 13
            },
            {
                "title": "Late Holocene sea-level changes and vertical land movements in New Zealand",
                "abstract": "ABSTRACT Coasts in tectonically active regions face varying threat levels as land subsides or uplifts relative to rising sea levels. We review the processes influencing relative sea-level change in New Zealand, and the geological context behind ongoing land movements, focussing on major population centres. Whilst Holocene sea levels have been reconstructed using a variety of techniques, recent work uses salt-marsh microfossil assemblages to reconstruct relative sea-level changes over the past few centuries. For the twentieth century, these proxy-based studies often show enhanced rates of sea-level rise relative to tide-gauge observations. The effects of tectonic subsidence must be considered, alongside vertical and dating uncertainties in the sea-level reconstructions. Global Positioning Systems (GPS) observations for the past few decades show that vertical land movement (VLM) may be influencing rates of relative sea-level rise. However, the short period of GPS observations, during which trends and rates have varied at some localities, raises questions over the longer-term contribution of VLM to sea-level change over the past few centuries and for future projections. We argue that high-resolution palaeo-sea-level reconstructions from salt-marsh sedimentary sequences can help to answer these questions regarding the interplay between sea-level change and VLM at key locations.",
                "authors": "D. J. King, R. Newnham, W. Gehrels, K. Clark",
                "citations": 13
            },
            {
                "title": "Estimating Vertical Land Motion from Remote Sensing and In-Situ Observations in the Dubrovnik Area (Croatia): A Multi-Method Case Study",
                "abstract": "Different space-borne geodetic observation methods combined with in-situ measurements enable resolving the single-point vertical land motion (VLM) and/or the VLM of an area. Continuous Global Navigation Satellite System (GNSS) measurements can solely provide very precise VLM trends at specific sites. VLM area monitoring can be performed by Interferometric Synthetic Aperture Radar (InSAR) technology in combination with the GNSS in-situ data. In coastal zones, an effective VLM estimation at tide gauge sites can additionally be derived by comparing the relative sea-level trends computed from tide gauge measurements that are related to the land to which the tide gauges are attached, and absolute trends derived from the radar satellite altimeter data that are independent of the VLM. This study presents the conjoint analysis of VLM of the Dubrovnik area (Croatia) derived from the European Space Agency’s Sentinel-1 InSAR data available from 2014 onwards, continuous GNSS observations at Dubrovnik site obtained from 2000, and differences of the sea-level change obtained from all available satellite altimeter missions for the Dubrovnik area and tide gauge measurements in Dubrovnik from 1992 onwards. The computed VLM estimates for the overlapping period of three observation methods, i.e., from GNSS observations, sea-level differences, and Sentinel-1 InSAR data, are −1.93±0.38 mm/yr, −2.04±0.22 mm/yr, and −2.24±0.46 mm/yr, respectively.",
                "authors": "Marijan Grgić, J. Bender, Tomislav Bašić",
                "citations": 12
            },
            {
                "title": "Monitoring the Vertical Land Motion of Tide Gauges and Its Impact on Relative Sea Level Changes in Korean Peninsula Using Sequential SBAS-InSAR Time-Series Analysis",
                "abstract": "The relative sea-level changes from tide gauges in the Korean peninsula provide essential information to understand the regional and global mean sea-level changes. Several corrections to raw tide gauge records are required to account for coastal vertical land motion (VLM), regional and local coastal variability. However, due to the lack of in-situ measurements such as leveling data and the Global Navigation Satellite System (GNSS), making precise assessments of VLM at the tide gauges is still challenging. This study aims to address the above limitation to assess the VLM in the Korean tide gauges using the time-series Interferometric Synthetic Aperture Radar (InSAR) technique. For 10 tide gauges selected in the Korean peninsula, we applied the Stanford Method for Persistent Scatterers (StaMPS)—Small Baseline Subset (SBAS) method to C-band Sentinel-1 A/B Synthetic Aperture Radar (SAR) data acquired during 2014/10–2020/05, with the novel sequential interferograms pair selection approach to increase the slowly decorrelating filtered phase (SDFP) pixels density near the tide gauges. Our findings show that overall the tide gauges in the Korean peninsula are stable, besides the largest VLM observed at Pohang tide gauge station (East Sea) of about −26.02 mm/year; also, higher rates of uplift (>1 mm/year) were observed along the coast of Yellow Sea (Incheon TG and Boryeong TG) and higher rates of subsidence (<−2 mm/year) were observed at Jeju TG and Seogwipo TG. Our approach estimates the rate of VLM at selected tide gauges with an unprecedented spatial and temporal resolution and is applicable when the in-situ and GNSS observations are not available.",
                "authors": "S. K. P. Vadivel, Duk‐jin Kim, Jungkyo Jung, Yang‐Ki Cho, Ki-Jong Han",
                "citations": 14
            },
            {
                "title": "Vertical deformation and residual altimeter systematic errors around continental Australia inferred from a Kalman-based approach",
                "abstract": null,
                "authors": "Mohammad‐Hadi Rezvani, C. Watson, Matt A. King",
                "citations": 4
            },
            {
                "title": "Optimised electronic patient records to improve clinical monitoring of HIV-positive patients in rural South Africa (MONART trial): study protocol for a cluster-randomised trial",
                "abstract": null,
                "authors": "C. Iwuji, M. Osler, Lusanda Mazibuko, N. Hounsome, N. Ngwenya, R. S. Chimukuche, T. Khoza, Dickman Gareta, H. Sunpath, A. Boulle, K. Herbst",
                "citations": 4
            },
            {
                "title": "MEMS Vaporazing Liquid Microthruster: A Comprehensive Review",
                "abstract": "The interest in developing efficient nano and pico-satellites has grown in the last 20 years. Secondary propulsion systems capable of serving specific maneuvers are an essential part of these small satellites. In particular, Micro-Electro-Mechanical Systems (MEMS) Vaporizing Liquid Microthrusters (VLM), using water as a propellant, represent today a smart choice in terms of simplicity and cost. In this paper, we first propose a review of the international literature focused on MEMS VLM development, reviewing the different geometries and heating solutions proposed in the literature. Then, we focus on a critical aspect of these micro thrusters: the presence of unstable phenomena. In particular, the boiling instabilities and reverse channel flow substantially impact the MEMS VLMs’ performance and limit their applicability. Finally, we review the research focused on the passive and active control of the boiling instabilities, based on VLM geometry optimization and active heating strategies, respectively. Today, these ones represent the two principal research axes followed by the scientific community to mitigate the drawbacks linked to the use of MEMS VLMs.",
                "authors": "D. Fontanarosa, L. Francioso, M. D. De Giorgi, M. Vetrano",
                "citations": 4
            },
            {
                "title": "Ocular Toxocariasis",
                "abstract": "Toxocariasis is a zoonotic infection of global importance. Transmission is typically fecal-oral and results from accidental exposure to food, water, or soil contaminated by either dog or, less commonly, cat roundworms – the helminths Toxocara canis and Toxocara cati, respectively. Infection is common and affects approximately 20% of the world’s population, with seroprevalence rates surpassing 80% in some tropical and subtropical regions in Africa and Asia. While many Toxocara infections are asymptomatic, moderate to severe systemic, neurologic, and/or ocular infections can occur. These are referred to respectively as visceral (VLM), neurologic (NVM), and ocular larva migrans (OLM; also known as ocular toxocariasis OT). Three classic patterns or OT have been described, including posterior granuloma – typically submacular or peripapillary and with pronounced retinal striae or folds; peripheral granuloma, often with a vitreoretinal traction band extending toward the posterior pole; and severe, unilateral intermediate or panuveitis referred to as nematode or Toxocara endophthalmitis. Atypical presentations of OT also exist. One metanalysis, three original articles, and one letter in this issue of Ocular Immunology & Inflammation (OII) address aspects of the prevalence, presentation, treatment and outcome of OT. Badri et al. performed a meta-analysis of five English language database listings (PubMed, Scopus, Science Direct, Web of Science, and Google Scholar) from January 1966 to December 2019 to explore the prevalence of OT in six recognized World Health Organization (WHO)-epidemiological regions. A total of 101 studies with 331,602 cases from 35 countries met the inclusion criteria. Overall, studies that utilized immunologic methods, such as serology for nematodespecific antibodies, showed a higher pooled prevalence than those where the diagnosis was based on ophthalmological examination (9%, 95% CI 6–12% vs 1%, 95% CI 1–2%), male subjects were identified slightly more often than female subjects (OR 1.44, 95% CI 0.94–2.20, nominal p < .01), and the diagnosis of OT was made most often in the second decade of life (7%, 95% CI 1–18%, nominal p < .01) vs roughly half that prevalence for other age groups, and regions of lower-middle class income (6%, 95% CI 2–12%, nominal p < .01). The pooled prevalence of OT in the African WHO-epidemiological region was highest at 10% (95% CI 7–13%), followed by the European region at 8% (95% CI 4–12%), the region of the Americas at 6% (95% CI 2–10%), the Eastern Mediterranean and Western Pacific regions at 3% each (95% CI 0–10% and 1–5%, respectively), and the South-East Asia Region at 1% (95% CI 0–3%). It should be noted, however, that only one study was available from Africa – specifically Sierra Leone (10%, 95% CI 7–13%). Other countries of high prevalence included Peru (39%, 95% CI0-99%) and Slovenia (39%, 95% CI17-36%). Pooled prevalences of near zero were identified in Ireland, Germany, Spain and Nepal, and slightly higher at about 1% in Turkey, Myanmar, and the Czech Republic. Identified risk factors for a higher pooled prevalence of OT included ownership of dogs or cats (18%, 95% CI 0–96%), contact with dogs (17%, 95% CI3-39%), consumption of raw or undercooked meat (12%, 95% CI 0–53%), and exposure to soil (6%, 95% CI 0–27%). The anatomic location of inflammation was described most often as intermediate (6%, 95% CI 1–15%), followed by anterior (3%, 95% CI 0–10%), posterior (2%, 95% CI 1–4%), and panuveitis (1%, 95% CI 0–1%). Granulomas, when described, were most often central (5%, 95% CI 3–7%), followed by peripheral (4%, 95% CI1-8%). The authors concluded that their findings were generally in agreement with those of global epidemiologic studies based on the analyses of anti-Toxocara seroprevalence. They also emphasized the importance of considering OT in endemic areas, and of good hygiene, limiting pet exposure to public places, such as playgrounds and parks, and of monitoring and treating dogs and cats for helminthic infections. Martinez et al retrospectively reviewed the epidemiologic and clinical characteristics, as well as treatment and outcome of 157 patients less than 13 years of age with OT seen in a tertiary referral center in Costa Rica between January 1998 and December 2018. All patients had unilateral disease, the mean age was 6.7 ± 2.8 years, 97 (61.8%) were male, 145 (92.4%) had positive serology for T. canis, and 79 (50.3%) had eosinophilia. The most common reasons for consultation included decreased vision (29.9%), strabismus (26.7%), leukocoria (19.7%) and red eye (17.8%). The vast majority of patients presented with vision between 20/200 and no light perception (NLP, 89.2%). Associated ocular complications included cataract (17.5%), posterior synechiae formation (9.7%), band keratopathy (3.9%), and hypopyon formation (1.3%). The posterior segment could not be visualized in 12.7% of affected eyes. In the remaining eyes, a peripheral granuloma was identified in 57 (54.2%), a posterior granuloma in 27 (25.8%), and no granuloma in 21 (20.0%). Eighty-eight (56.1%) had a retinal detachment (RD). Treatments were tailored to findings and included no therapy in 64 eyes (40.8%), use of prednisone in 32 eyes (20.4%), pars plana vitrectomy (PPV) alone in 23 eyes (14.6%), prednisone plus thiabendazole in 16 eyes (10.2%), cataract surgery in 9 eyes (5.7%), 5 eyes (3.2%) treated with thiabendazole alone, and 3 eyes (1.9%) treated with regional corticosteroids alone. Most eyes with RD failed to show functional improvement despite treatment. The authors concluded that OT is an important cause of moderate-to-severe vision loss in Costa Rica, and that RD, in particular, is a predictor of poor visual outcome. Wang and Tao retrospectively reviewed the clinical characteristics, pattern of inflammatory cytokines, ocular complications and predictors of recurrence of inflammation within 6 months of presentation and treatment in a cohort OCULAR IMMUNOLOGY AND INFLAMMATION 2021, VOL. 29, NOS. 7–8, 1243–1245 https://doi.org/10.1080/09273948.2021.2018825",
                "authors": "E. Cunningham, M. Zierhut",
                "citations": 4
            },
            {
                "title": "Comparative Learning Performance and Mental Involvement in Collaborative Inquiry Learning: Three Modalities of Using Virtual Lever Manipulative",
                "abstract": null,
                "authors": "Cixiao Wang, Yuying Ma, Feng Wu",
                "citations": 11
            },
            {
                "title": "Low-Fidelity Aerostructural Optimization of Aircraft Wings with a Simplified Wingbox Model Using OpenAeroStruct",
                "abstract": null,
                "authors": "Shamsheer S. Chauhan, J. Martins",
                "citations": 23
            },
            {
                "title": "Conditioned haptic perception for 3D localization of nodules in soft tissue palpation with a variable stiffness probe",
                "abstract": "This paper provides a solution for fast haptic information gain during soft tissue palpation using a Variable Lever Mechanism (VLM) probe. More specifically, we investigate the impact of stiffness variation of the probe to condition likelihood functions of the kinesthetic force and tactile sensors measurements during a palpation task for two sweeping directions. Using knowledge obtained from past probing trials or Finite Element (FE) simulations, we implemented this likelihood conditioning in an autonomous palpation control strategy. Based on a recursive Bayesian inferencing framework, this new control strategy adapts the sweeping direction and the stiffness of the probe to detect abnormal stiff inclusions in soft tissues. This original control strategy for compliant palpation probes shows a sub-millimeter accuracy for the 3D localization of the nodules in a soft tissue phantom as well as a 100% reliability detecting the existence of nodules in a soft phantom.",
                "authors": "Nicolas Herzig, Liang He, P. Maiolino, Sara-Adela Abad, Thrishantha Nanayakkara",
                "citations": 10
            },
            {
                "title": "Stability Characteristics of Wing Span and Sweep Morphing for Small Unmanned Air Vehicle: A Mathematical Analysis",
                "abstract": "Morphing aircraft are the flight vehicles that can reconfigure their shape during the flight in order to achieve superior flight performance. However, this promising technology poses cross-disciplinary challenges that encourage widespread design possibilities. This research aims to investigate the flight dynamic characteristics of various morphed wing configurations that can be incorporated in small-scale UAVs. The objective of this study was to analyze the effects of in-flight wing sweep and wingspan morphing on aerodynamic and flight stability characteristics. Longitudinal, lateral, and directional characteristics were evaluated using linearized equations of motion. An open-source code based on Vortex Lattice Method (VLM) assuming quasi-steady flow was used for this purpose. Trim points were identified for a range of angles of attack in prestall regime. The aerodynamic coefficients and flight stability derivatives were compared for the aforementioned morphing schemes with a fixed-wing counterpart. The results indicated that wingspan morphing is better than wing sweep morphing to harness better aerodynamic advantages with favorable flight stability characteristics. However, extension in wingspan beyond certain limits jeopardizes the advantages. Dynamically, wingspan and sweep morphing schemes behave in an exactly opposite way for longitudinal modes, whereas lateral-directional dynamics act in the same fashion for both morphing schemes. The current study provided a baseline to explore the advanced flight dynamic aspects of employed wing morphing schemes.",
                "authors": "Hafiz Muhammad Umer, A. Maqsood, R. Riaz, S. Salamat",
                "citations": 10
            },
            {
                "title": "Estimating trends of the Mediterranean Sea level changes from tide gauge and satellite altimetry data (1993–2015)",
                "abstract": null,
                "authors": "Hebib Taibi., M. Haddad",
                "citations": 18
            },
            {
                "title": "Analysis of the Various Effects of Coating W Tips with Dielectric Epoxylite 478 Resin or UPR-4 Resin Coatings under Similar Operational Conditions",
                "abstract": "Abstract: The objective of this work is to study the differences that occur in behavior and properties of the emitted electron beam from tungsten (W) tips before and after coating these tips with a thin layer of a good proven dielectric material. Core metallic tips have been prepared from a polycrystalline (99.995% purity) tungsten (W) wire. Analysis has been carried out for clean W emitters before and after coating these tips with two differences types of epoxy resins; namely: (Epoxylite 478 and UPR-4). For critical comparison and analysis, several tungsten tips with various apex- radii (very sharp) have been prepared with the use of electrochemical etching techniques. The tips have been coated by dielectric thin films of various thicknesses. Their characteristics have been recorded before and after the process of coating. These measurements have included the current-voltage (I-V) characteristics, Fowler-Nordheim (F-N) plots, visible light microscope (VLM) image and scanning electron microscope (SEM) micrographs to measure the influence of the Epoxylite resin coating’s thickness on the tips after coating. Special distributions have been recorded from the phosphorescent screen of a field electron emission microscope as well. Comparing the two sets of composite systems tested under similar conditions has provided several advantages. Recording highly interesting phenomena has produced a wide opportunity to develop a new type of emitter that includes the most beneficial features of both types.\nKeywords: Cold field emission, Epoxylite 478, Epoxylite UPR-4.",
                "authors": "",
                "citations": 7
            },
            {
                "title": "Retracted: Robust retinal vessel segmentation using vessels location map and Frangi enhancement filter",
                "abstract": "The analysis of retinal vascular is quite important because many diseases including stroke, diabetic retinopathy (DR) and coronary heart diseases can damage retinal vessel structure. In this research, a technique has been proposed using a combination of pre-processing steps, vessel enhancement techniques, segmentation and post-processing. The pre-processing section comprises of adaptive histogram equalisation for dissimilarity enhancement between vessels and background, a morphological top hat filter for macula and optic disc removal and high boost filtering, edges enhancement. Frangi filter is applied at multi-scale for enhancement of different vessel widths. Segmentation has been performed using global Otsu thresholding with some offset applied on difference image and enhanced image separately. A vessel location map (VLM) has been extracted using the post-processing steps of raster to vector transformed area and eccentricity-based threshold to eliminate the exudate/unwanted region from binarised image. Post-processing has been used in a new way to reject misclassified vessel pixels. The final segmented image has been obtained by using pixel-by-pixel AND operation between VLM and Frangi binarised image. The method has been rigorously analysed using STARE and DRIVE datasets.",
                "authors": "Muhammad Shahid, I. A. Taj",
                "citations": 25
            },
            {
                "title": "Practical Considerations before Installing Ground-Based Geodetic Infrastructure for Integrated InSAR and cGNSS Monitoring of Vertical Land Motion",
                "abstract": "Continuously operating Global Navigation Satellite Systems (cGNSS) can be used to convert relative values of vertical land motion (VLM) derived from Interferometric Synthetic Aperture Radar (InSAR) to absolute values in a global or regional reference frame. Artificial trihedral corner reflectors (CRs) provide high-intensity and temporally stable reflections in SAR time series imagery, more so than naturally occurring permanent scatterers. Therefore, it is logical to co-locate CRs with cGNSS as ground-based geodetic infrastructure for the integrated monitoring of VLM. We describe the practical considerations for such co-locations using four case-study examples from Perth, Australia. After basic initial considerations such as land access, sky visibility and security, temporary test deployments of co-located CRs with cGNSS should be analysed together to determine site suitability. Signal to clutter ratios from SAR imagery are used to determine potential sites for placement of the CR. A significant concern is whether the co-location of a deliberately designed reflecting object generates unwanted multipath (reflected signals) in the cGNSS data. To mitigate against this, we located CRs >30 m from the cGNSS with no inter-visibility. Daily RMS values of the zero-difference ionosphere-free carrier-phase residuals, and ellipsoidal heights from static precise point positioning GNSS processing at each co-located site were then used to ascertain that the CR did not generate unwanted cGNSS multipath. These steps form a set of recommendations for the installation of such geodetic ground-infrastructure, which may be of use to others wishing to establish integrated InSAR-cGNSS monitoring of VLM elsewhere.",
                "authors": "A. Parker, W. Featherstone, N. Penna, M. Filmer, M. Garthwaite",
                "citations": 24
            },
            {
                "title": "Recurrence of Distensible Orbital Venous-dominant Venolymphatic Malformations After Sclerotherapy Versus Embolization With Excision",
                "abstract": "Purpose: Treatment for orbital venolymphatic malformations (VLMs) commonly includes 3 major options: sclerotherapy, surgery, and embolization followed by surgical excision. Each has certain advantages, although it is not clear whether all are effective. The authors characterize the clinical course for a series of patients with distensible orbital venous-dominant VLM treated with sclerotherapy and/or embolization with excision. Methods: In this cross-sectional cohort study, patients affected by distensible orbital venous-dominant VLM presenting to the orbital and ophthalmic plastic surgery service from 2014 to 2020 were identified. Patients were included if they presented with a moderate-flow, distensible venous-dominant malformation associated with Valsalva-related symptoms (e.g., pain, proptosis, and diplopia). Results: Six cases were treated with sclerotherapy. Four underwent multiple treatments, with a mean ± SD of 3.5 ± 2.3 (range 1–7). All patients in this group failed to improve or experienced recurrence of symptoms after sclerotherapy. Twelve cases were treated with embolization and excision. Resolution of symptoms in all 12 cases was noted and maintained for a mean of 3.4 ± 2.1 years. There have been no cases of recurrence. Patients treated with sclerotherapy were more likely to experience recurrence of symptoms compared to those treated with embolization and excision (p < 0.001). Conclusions: Treatment of distensible venous-dominant moderate-flow orbital VLM with sclerotherapy may provide temporary improvement in some cases. However, in the medium to long term, recurrence was universal in this series. Embolization with excision appears to provide more definitive management, avoiding recurrence in all cases for a mean follow-up of 3 years. Treatment of distensible orbital venous-dominant venolymphatic malformations with sclerotherapy may provide temporary improvement; however, embolization with excision provides more definitive management and minimizes recurrence.",
                "authors": "L. Cohen, R. Goldberg, D. Rootman",
                "citations": 3
            },
            {
                "title": "Shortest Microlensing Event with a Bound Planet: KMT-2016-BLG-2605",
                "abstract": "With a planet–host mass ratio q = 0.012 ± 0.001, KMT-2016-BLG-2605 has the shortest Einstein timescale, t E = 3.41 ± 0.13 days, of any planetary microlensing event to date. This prompts us to examine the full sample of seven short (t E < 7 days) planetary events with good q measurements. We find that six have clustered Einstein radii θ E = 115 ± 20 μas and lens–source relative proper motions μ rel ≃ 9.5 ± 2.5 mas yr−1. For the seventh, these two quantities could not be measured. These distributions are consistent with a Galactic bulge population of very low mass (VLM) hosts near the hydrogen-burning limit. This conjecture could be verified by imaging at first adaptive optics light on next-generation (30 m) telescopes. Based on a preliminary assessment of the sample, “planetary” companions (i.e., below the deuterium-burning limit) are divided into “genuine planets,” formed in their disks by core accretion, and VLM brown dwarfs, which form like stars. We discuss techniques for expanding the sample, which include taking account of the peculiar “anomaly-dominated” morphology of the KMT-2016-BLG-2605 light curve.",
                "authors": "Y. Ryu, K. Hwang, A. Gould, J. Yee, M. Albrow, Sun-Ju Chung, C. Han, Y. Jung, Hyoun-Woo Kim, I. Shin, Y. Shvartzvald, W. Zang, S. Cha, Dong-Jin Kim, Seung-Lee Kim, Chung-Uk Lee, Dong-Joo Lee, Yongseok Lee, Byeong-Gon Park, R. Pogge",
                "citations": 3
            },
            {
                "title": "The “art” of executive coaching at the top: Using clients’ self-imagery as a tool for high impact.",
                "abstract": "The “ art ” of coaching at the top is considered metaphorically and discussed in this article as a co-creation between client and coach. This co-creation is catalyzed by the client ’ s self-imagery — explicit and highly visual imagery — of how they see themselves as leaders at the beginning, midway, and at the end of coaching. The evocation of this self-imagery is a foundational act at the outset of a coaching engagement and is accomplished by the coach ’ s use of a speci ﬁ c technique, the visual leadership metaphor (VLM). With the use of this technique, client and coach get rapidly aligned in the identi ﬁ cation of client-centered thematic material that not only informs the course of the coaching but also intensi ﬁ es the quality of their connection and ensures traction toward achieving the client ’ s growth objective as a leader. Based on the author ’ s analysis of 180 VLMs, eight overarching coaching themes are identi ﬁ ed. These eight themes and a description of the speci ﬁ c coaching tools used are also provided. Important research questions are raised, and practitioners may ﬁ nd value in considering this conceptualization of co-creation; in doing so, they may evolve their own art in the coaching of top business leaders.",
                "authors": "Karol M. Wasylyshyn",
                "citations": 3
            },
            {
                "title": "Comprehensive Design Method for Open or Ducted Propellers for Underwater Vehicles",
                "abstract": "A comprehensive method which determines the most efficient propeller blade shapes for a given axisymmetric hull to travel at a desired speed, is presented. A nonlinear optimization method is used to design the blade, the shape of which is defined by a 3-D B-spline polygon, with the coordinates of the B-spline control points being the parameters to be optimized for maximum propeller efficiency, for given effective wake and propeller thrust. The performance of the propeller within the optimization scheme is assessed by a vortex-lattice method (VLM). To account fully for the hull/propeller interaction, the effective wake to the propeller and the hull resistance are determined by analyzing the designed propeller geometry by the VLM, coupled with a Reynolds-Averaged Navier-Stokes (RANS) solver. The optimization method re-designs the optimum blade with the updated effective wake and propeller thrust (taken to be equal to the updated hull resistance), and the procedure continues until convergence of the propeller performance. The current approach does not require knowledge of the wake fraction or the thrust deduction factor, both of which must be estimated a priori in traditional propeller design. The method is applied for a given hull to travel at a desired speed, and the optimum blades are designed for various combinations of propeller diameter and RPM, in the case of open and ducted propellers with provided duct shapes. The effects of the propeller diameter and RPM on the designed propeller thrust, torque, propeller efficiency, and required power are presented and compared with each other in the case of open and ducted propellers. The present approach is shown to provide guidance on the design of propulsors for underwater vehicles, and is applicable to the design of propulsors for surface ships.",
                "authors": "S. Kinnas, Kyungjung Cha, Seungnam Kim",
                "citations": 3
            },
            {
                "title": "Enhancement of excitatory transmission in NTS neurons projecting to ventral medulla of rats exposed to sustained hypoxia is blunted by minocycline",
                "abstract": "Rats subjected to sustained hypoxia (SH) present increases in arterial pressure (AP) and in glutamatergic transmission in the nucleus tractus solitarius (NTS) neurons sending projections to ventrolateral medulla (VLM). Treatment with minocycline, a microglial inhibitor, attenuated the increase in AP in response to SH. The increase in the amplitude of glutamatergic postsynaptic currents in the NTS‐VLM neurons, induced by postsynaptic mechanisms, was blunted by minocycline treatment. The number of microglial cells was increased in the NTS of vehicle‐treated SH rats but not in the NTS of minocycline‐treated rats. The data show that microglial recruitment/proliferation induced by SH is associated with the enhancement of excitatory neurotransmission in NTS‐VLM neurons, which may contribute to the observed increase in AP.",
                "authors": "Ludmila Lima-Silveira, D. Accorsi-Mendonça, L. Bonagamba, C. Almado, Melina P da Silva, Polina E Nedoboy, P. Pilowsky, B. Machado",
                "citations": 19
            },
            {
                "title": "3D-Visualization of Neurovascular Compression at the Ventrolateral Medulla in Patients with Arterial Hypertension",
                "abstract": null,
                "authors": "P. Manava, R. Naraghi, R. Schmieder, R. Fahlbusch, A. Doerfler, M. Lell, M. Buchfelder, P. Hastreiter",
                "citations": 9
            },
            {
                "title": "Polymorphic Forms of Valinomycin Investigated by NMR Crystallography",
                "abstract": "A dodecadepsipeptide valinomycin (VLM) has been most recently reported to be a potential anti-coronavirus drug that could be efficiently produced on a large scale. It is thus of importance to study solid-phase forms of VLM in order to be able to ensure its polymorphic purity in drug formulations. The previously available solid-state NMR (SSNMR) data are combined with the plane-wave DFT computations in the NMR crystallography framework. Structural/spectroscopical predictions (the PBE functional/GIPAW method) are obtained to characterize four polymorphs of VLM. Interactions which confer a conformational stability to VLM molecules in these crystalline forms are described in detail. The way how various structural factors affect the values of SSNMR parameters is thoroughly analyzed, and several SSNMR markers of the respective VLM polymorphs are identified. The markers are connected to hydrogen bonding effects upon the corresponding (13C/15N/1H) isotropic chemical shifts of (CO, Namid, Hamid, Hα) VLM backbone nuclei. These results are expected to be crucial for polymorph control of VLM and in probing its interactions in dosage forms.",
                "authors": "J. Czernek, J. Brus",
                "citations": 8
            },
            {
                "title": "Documenting the degradation of animal-tissue residues on experimental stone tools: a multi-analytical approach",
                "abstract": null,
                "authors": "G. Monnier, Kaitlyn May",
                "citations": 15
            },
            {
                "title": "Introducing a vertical land motion model for improving estimates of sea level rates derived from tide gauge records affected by earthquakes",
                "abstract": null,
                "authors": "A. Klos, J. Kusche, L. Fenoglio-Marc, M. Bos, J. Bogusz",
                "citations": 17
            },
            {
                "title": "Stall Cell Prediction Using a Lifting-Surface Model",
                "abstract": "This paper investigates the use of a lifting surface method coupled with a nonlinear lift curve, the nonlinear vortex lattice method (NL-VLM), toward the study of stall cells on wings. First, Spala...",
                "authors": "Frédéric Plante, E. Laurendeau, J. Dandois",
                "citations": 2
            },
            {
                "title": "A new simplified numerical approach for shape design of underwater wings",
                "abstract": "ABSTRACT This paper presents a new simplified numerical approach (SNA) for predicting hydrodynamic data of underwater wings (UWs). Instead of full three-dimensional (3D) flow field analyses, SNA combines Vortex Lattice Method (VLM) and several two-dimensional Reynolds-Average Navier-Stokes (2D RANS) simulations of wing sections to gain results. Hence, SNA shows a remarkably lower computational cost than higher-fidelity methods such as 3D RANS. Moreover, SNA shows good agreements with 3D RANS and experimental data. Thereafter, SNA is applied to shape optimizations of UWs for efficiency and accuracy. Two kinds of optimisation algorithms that include local and global finders are adopted in the optimiser. Searching behaviours of the two algorithms indicate that the problem is not proper to be solved by the local optimiser for its multimodality, while the global optimiser is more efficient. Furthermore, the final optimised wing has few wing-tip vortices and a smooth pressure distribution.",
                "authors": "Siqing Sun, Baowei Song, Peng Wang, Huachao Dong, Xiao Chen",
                "citations": 2
            },
            {
                "title": "Loss of putative GABAergic neurons in the ventrolateral medulla in multiple system atrophy.",
                "abstract": "STUDY OBJECTIVES\nMultiple system atrophy (MSA) is associated with disturbances in cardiovascular, sleep and respiratory control. The lateral paragigantocellular nucleus (LPGi) in the ventrolateral medulla (VLM) contains GABAergic neurons that participate in control of rapid eye movement (REM) sleep and cardiovagal responses. We sought to determine whether there was loss of putative GABAergic neurons in the LPGi and adjacent regions in MSA.\n\n\nMETHODS\nSections of the medulla were processed for GAD65/67 immunoreactivity in eight subjects with clinical and neuropathological diagnosis of MSA and in six control subjects. These putative GABAergic LPGi neurons were mapped based on their relationship to adjacent monoaminergic VLM groups.\n\n\nRESULTS\nThere were markedly decreased numbers of GAD-immunoreactive neurons in the LPGi and adjacent VLM regions in MSA.\n\n\nCONCLUSIONS\nThere is loss of GABAergic neurons in the VLM, including the LPGi in patients with MSA. Whereas these findings provide a possible mechanistic substrate, given the few cases included, further studies are necessary to determine whether they contribute to REM sleep-related cardiovagal and possibly respiratory dysregulation in MSA.",
                "authors": "A. Schmeichel, E. Coon, J. Parisi, W. Singer, P. Low, E. Benarroch",
                "citations": 2
            },
            {
                "title": "Strain Transformation Adjacent to the West Qinling Orogen: Implications for the Growth of the Northeastern Tibetan Plateau",
                "abstract": "The West Qinling orogen has played an important role in accommodating the deformation in the northeastern Tibetan Plateau induced by the India-Eurasia convergence. Here we construct a vertical land motion (VLM) model based on the latest leveling observations adjacent to the West Qinling orogen. Combined with the horizontal deformation field, the crustal deformation pattern in this area is investigated. Additionally, slip rate and coupling coefficients of the West Qinling fault, the longest fault separating the West Qinling orogen from the Lanzhou (Longxi) block, are inverted and constrained with GPS and VLM observations. Results show that the West Qinling fault slips slowly at a rate of 1–2 mm/yr and is strongly coupled with a moment magnitude deficit of Mw7.4. The crustal uplift rates adjacent to the West Qinling orogen are 0–3 mm/yr; which combined with 0–12.5 × 10−9/yr contraction rates, suggests that strain transformation plays a key role in controlling the tectonic uplift in the West Qinling orogen, and furthers our understanding of the contemporary geomorphic and topographic features. We identify a significant deformation transition belt at longitudes of 105°–106°E, which indicates that crustal deformation, induced from the northeastern expansion of the Tibetan Plateau, is mainly constrained to the plateau, rather than accommodated by crustal materials escaping eastward along the Qinling Mountains.",
                "authors": "Zhangjun Li, F. Cheng, M. Hao, Zachary Young, Shangwu Song, Fan Yang, Wenquan Zhuang",
                "citations": 2
            },
            {
                "title": "Preliminary Design and Experimental Investigation of a Distributed Electric Propulsion Aircraft",
                "abstract": "Distributed electric propulsion (DEP) aircraft has several potential advantages, such as improving aerodynamic performance and propulsive efficiency, making this aircraft concept one of the most promising concept for the future aviation. This paper first introduces the conceptual design considerations of DEP aircraft, including configuration, aerodynamics, structure, propulsion, control and safety. Then a 40-kg DEP Short Take-Off and Landing (STOL) demonstrator is designed and developed by using the developed initial sizing method and the aerodynamic analyses method of Vortex Lattice Method (VLM). The aerodynamic characteristics of the demonstrator are investigated and analysed in detail by using the ground mobile testing and numerical calculation methods. Finally, the DEP demonstrator is used for flight testing, and the sizing method and performance of the demonstrator are preliminarily verified. The results showed that due to the influence of the DEP system, the lift coefficient of the wing segment is increased by around 0.2 in the linear phase, and the drag coefficient is also increased due to the propulsors’ incidence angle induced airflow separation.",
                "authors": "Yi-Wei Ma, Wei Zhang, YiZhe Zhang, Yuelong Ma, Zhiliang Bai",
                "citations": 2
            },
            {
                "title": "Cognitive determinants of community functioning and quality of life in homeless adults with mental illness: 6-year follow-up from the At Home/Chez Soi Study Toronto site",
                "abstract": "Abstract Background High rates of physical and mental health comorbidities are associated with functional impairment among persons who are homeless. Cognitive dysfunction is common, but how it contributes to various functional outcomes in this population has not been well investigated. This study examines how cognition covaries with community functioning and subjective quality of life over a 6-year period while accounting for the effects of risk and protective factors. Methods Participants were 349 homeless adults (mean age = 39.8) recruited from the Toronto site of the At Home/Chez Soi study, a large Canadian randomized control trial of Housing First. Participants completed up to four clinical evaluations over 6 years. Factor scores were created to index verbal learning and memory (vLM) and processing speed-cognitive flexibility (PSCF). The primary outcomes were community functioning and subjective quality of life. Risk factors included lifetime homelessness, mental health diagnoses, medical comorbidity, and childhood adversity. Linear mixed-effects models were conducted to examine cognition-functional outcome associations over time, with resilience as a moderator. Results Better vLM (b = 0.787, p = 0.010) and PSCF (b = 1.66, p < 0.001) were associated with better community functioning, but not with quality of life. Resilience conferred a protective effect on subjective quality of life (b = 1.45, p = 0.011) but did not moderate outcomes. Conclusions Our findings suggest a need to consider the unique determinants of community functioning and quality of life among homeless adults. Cognition should be prioritized as a key intervention target within existing service delivery models to optimize long-term functional outcomes.",
                "authors": "K. Gicas, C. Mejia-Lancheros, R. Nisenbaum, R. Wang, S. Hwang, V. Stergiopoulos",
                "citations": 2
            },
            {
                "title": "Nonlinear vortex lattice method for stall prediction",
                "abstract": "The stall behavior of an empennage is a crucial and conditioning factor for its design. Thus, the preliminary design of empennages requires a fast low-order method which reliably computes the stall behavior and which must be sensitive to the design parameters (taper, sweep, dihedral, airfoil, etc.). Handbook or semi-empirical methods typically have a narrow scope and low fidelity, so a more general and unbiased method is desired. This paper presents a nonlinear vortex lattice method (VLM) for the stall prediction of generic fuselage-empennage configurations which is able to compute complete aerodynamic polars up to and beyond stall. The method is a generalized form of the van Dam algorithm, which couples the potential VLM solution with 2.5D viscous data. A novel method for computing 2.5D polars from 2D polars is presented, which extends the traditional infinite swept wing theory to finite wings, relying minimally on empirical data. The method has been compared to CFD and WTT results, showing a satisfactory degree of accuracy for the preliminary design of empennages.",
                "authors": "H. Goitia, R. Llamas",
                "citations": 13
            },
            {
                "title": "Sea level rise scenario for 2100 A.D. for the archaeological site of Motya",
                "abstract": null,
                "authors": "R. Ravanelli, F. Riguzzi, M. Anzidei, A. Vecchio, Lorenzo Nigro, F. Spagnoli, M. Crespi",
                "citations": 13
            },
            {
                "title": "Conceptual Approach to Express Tacit Knowledge by Human–Machine Interactions",
                "abstract": null,
                "authors": "G. Kanygin, O. Kononova",
                "citations": 1
            },
            {
                "title": "Efficient complex activation of Portland cement through processing it in the vortex layer machine",
                "abstract": "Today, more and more studies are devoted to activation of primary components of building materials. This paper is concerned with the viability of activating mineral bonding materials, Portland cement in particular, in the Vortex Layer Machine (VLM) with a purpose of physical modification. By applying modern methods for studying the specific surface, pH change, X‐ray phase and differential‐thermal analysis, and IR spectroscopy, it was found that activation of Portland cement in the VLM constitutes an efficient method for enhancing physical and chemical activity. It was also found that, depending on the content of mineral additives in the VLM, activation of Portland cement leads to significant extension of its specific surface, decrease of particle size, and also to prevalence of processes related to activation of an agent in the surface layer. During activating Portland cement, mineral crystal structure is being destructed, with the most significant changes observed for C3S. Besides, processing Portland cement in the VLM results in polycondensation of SiO4‐tetrahedral units, which reduces the basicity of hydrated calcium silicates during hydration; in the cement stone, ettringite content increases, and so does content of low‐basic hydrated calcium silicates and aluminates, which reinforces strength and chemical resistance of concrete. The acquired results speak for benefits of activating mineral bonding materials in the VLM with the purpose of enhancing physical‐mechanical properties of building materials.",
                "authors": "R. Ibragimov, E. Korolev, T. Deberdeev, V. V. Leksin",
                "citations": 17
            },
            {
                "title": "The Aftertaste you Cannot Erase. Career Histories, Emotions and Emotional Management in Local Newsrooms",
                "abstract": "ABSTRACT Journalists work in a creative profession and often become emotionally attached to “their” medium. Emotions toward their work are an integral part of journalists’ career histories. At the same time, working conditions within contemporary journalism are in a state of constant change that could lead to job precariousness and general insecurity. Emotions are also understood to be part of the journalists’ engagement with the local media environment: though local newsrooms are experiencing similar changes, peripheral newsrooms have different opportunities to respond. The aim of this article is to investigate local journalists’ emotions toward their work during times of change. Drawing on a longitudinal qualitative case study based on interviews, this article focuses on how journalists manage their emotions throughout their career histories in the local media environment. The study focuses on local journalists in the VLM group, the dominant local press owner in the Czech Republic, which has experienced permanent losses and staff layoffs over the past few years. The interviewed journalists manifest deepening contradictions in their feelings towards work, the media organisation which employs them and the local community. The increasing volatility of their emotional responses has led to a general dissatisfaction and the growing importance of emotional management.",
                "authors": "Lenka Waschková Císařová",
                "citations": 1
            },
            {
                "title": "Hepatic larva migrans presenting with upper gastrointestinal haemorrhage: A case report",
                "abstract": "Visceral larva migrans (VLM) occurs because of a host inflammatory response to the migrating larvae of a nematode. Patients usually present with fever, hepatomegaly and abdominal pain; vascular arterial complications are uncommon. A 19-year female presented with fever, jaundice, abdominal discomfort and melena. Computed tomography (CT) revealed multiple discrete, clustered, complex hepatic cystic lesions consistent with VLM, along with an arterial pseudoaneurysm from the right hepatic artery which was managed with endovascular coil embolisation.",
                "authors": "R. Patel, S. Mittal",
                "citations": 1
            },
            {
                "title": "Experimental and Numerical Studies on Static Aeroelastic Behaviours of a Forward-Swept Wing Model",
                "abstract": "The static aeroelastic behaviours of a flat-plate forward-swept wing model in the vicinity of static divergence are investigated by numerical simulations and wind tunnel tests. A medium fidelity model based on the vortex lattice method (VLM) and nonlinear structural analysis is proposed to calculate the displacements of the wing structure with large deformation. Follower forces effect and geometric nonlinearity are considered to calculate the deformation of the wing by finite element method (FEM). In the wind tunnel tests, the divergence dynamic pressure is predicted by the Southwell method, and the static aeroelastic displacement is measured by a photogrammetric method. The results obtained by the medium fidelity model calculations show reasonable agreement with wind tunnel test results. A high fidelity model based on coupled computational fluid dynamics (CFD) and computational structural dynamics (CSD) predicts better results of the wing tip displacement when the freestream dynamic pressure is approaching the divergence dynamic pressure.",
                "authors": "Ouyang Yan, K. Zeng, Xiping Kou, Yingsong Gu, Zhichun Yang",
                "citations": 1
            },
            {
                "title": "Effectiveness of virtual learning module on knowledge and attitude regarding standard days method among public health workers",
                "abstract": "Standard Days Method (SDM) is an effective, inexpensive, natural, affordable, non-hormonal and modern family planning method that is easy to teach and use. The aim of this study was to assess the knowledge and attitude about SDM and preference of CycleBeads over CycleBead mobile application amongst accredited social health activist (ASHA) as a public health worker. A mixed-method study was conducted on 140 ASHA workers, allocated into two groups of 70 members each in experimental and control group. A simple random sampling technique was used for quantitative strand and purposive sampling for qualitative strand. A self-structured knowledge questionnaire, attitude scale and focus group discussion were used for data collection. Self-developed virtual learning module (VLM) on SDM with CycleBeads was used as an intervention. In the experimental group, there was a significant difference between pre-existing (5.74 ± 2.05) and post-test (14.64 ± 2.25) knowledge score, whereas the difference between the mean of pre-test (4.32 ± 0.67) and post-test (4.68 ± 0.46) attitude scores was not significant. There was no statistically significant difference in the knowledge and attitude scores in the control group. In the qualitative interview, participants expressed their preference for CycleBeads over CycleBead mobile application to teach women about SDM. VLM is useful in improving the knowledge regarding SDM of ASHAs as a public health worker. Public health workers (ASHA) preferred teaching SDM with CycleBeads in person instead of a mobile application for educating the beneficiaries.",
                "authors": "R. Kuppuswamy, D. Narayan, Jayanti Tiwari, Himani Koulash, Amandeep Kaur, Indra Gehlot, Jigmet Lamo, Deeksha Saini, Ankita Grewal, Chhewang Dolma, D. Meena",
                "citations": 1
            },
            {
                "title": "A mammalian commensal of the oropharyngeal cavity produces antibiotic and antiviral valinomycin in vivo",
                "abstract": "\n Around weaning, piglets are susceptible to infection by bacterial pathobionts, leading to increased morbidity and mortality. We identified isolates of Rothia nasisuis in the upper respiratory tract of weaned healthy piglets that produce valinomycin in vitro and in vivo via its vlm-encoded non-ribosomal peptide synthase (NRPS) enzyme complex. Valinomycin is an antiviral and antibiotic ionophore that shuttles potassium ions across membranes and is capable of inflammasome activation and apoptosis in LPS-primed macrophages at concentrations of 1 uM. Polarized monolayers of epithelial cells were much less sensitive to valinomycin but concentrations ≥ 10 µM decreased trans-epithelial resistance. R. nasisuis inhibited growth of closely related species of Rothia. Deliberate inoculation of valinomycin-producing R. nasisuis into newborn piglets suggested this species can shape the microbiota post weaning. Our findings support the idea that valinomycin is a competitive niche factor potentially also compromising epithelial integrity to gain access to (micro)nutrients.",
                "authors": "R. Gaiser, M. Ferrando, Alberto Oddo, Milton C. A. Pereira, X. Guan, F. Molist, Marcela M. Fernandez-Gutierrez, Simen Fredriksen, Clare Bryant, D. Petráš, P. Dorrestein, S. Boeren, M. Medema, C. Hill, M. Kleerebezem, P. Baarlen, J. Wells",
                "citations": 1
            },
            {
                "title": "Absolute sea level variability of Arctic Ocean in 1993–2018 from satellite altimetry and tide gauge observations",
                "abstract": null,
                "authors": "Yanguang Fu, Yikai Feng, Dongxu Zhou, Xinghua Zhou",
                "citations": 1
            },
            {
                "title": "A review study for Toxocariasis",
                "abstract": "Many researchers reported rates of infected dogs with T. canis and cats with Toxocara cati all over the world. The degree of tissue damage in the host and the concomitant elicitation of signs and symptoms are varied in different invaded tissues. The liver, lungs and central nervous system, including the eyes, are considered the most sensitive organs. In addition, the number of migrating juveniles and the age of the host. Inflammation manifests as eosinophilic granulomas. The immediate hypersensitivity responses to dying and dead larvae in the viscera, including the lungs, liver and brain, produce symptoms characteristic of VLM. The current review discusses the Toxocara sp. infection from the historical background, taxonomy, lifecycle, pathogeneses, clinical signs, epidemiology, diagnosis, control and treatment. And provides an overview of existing literature and data on the Toxocaraiasis with their references.",
                "authors": "Suhad Yasin Jasim, Afkar Muslim Hadi",
                "citations": 1
            },
            {
                "title": "Toxocara cati larval migration in rats: experimental histopathological study.",
                "abstract": "Toxocara spp. (T. canis and T. cati in particular) are the major etiological nematodes that have contributed to visceral larval migrans (VLM). So to show the ability of T. cati to produce such migration in the rats as experimental model and detection through histopathological observations to detect larval migratory patterns. Adult females T. cati were collected from naturally infected feral cats. Eight rats, Rattus norvegicus had acted as a model for experimental infection, each receiving an infectious dose of about 1000 infective T. cati eggs, while 2 rats served as non-infected control group. Two infected rats were sacrificed and examined at 7, 14, 21, and 28 day post infection (dpi) and tissue samples were taken for digestioning order to recover migrated larvae and for histopathological examination. In vitro embryonation of T. cati eggs was successfully carried out, although the percentage of embryonation was 10%, prepared inoculums were also infective to rats. Larvae recovered from the lungs at 7 and 14 dpi and were also present at 21, and 28 dpi. The larvae of T. cati were present in the intestines at 14, and 21 dpi. There were no larvae or less than one larva per gram found in other studied tissues. Histopathological changes in different organs were observed. Generally speaking, a multi-tissue response can be defined as the histopathological response of T. cati larval migration. The migratory larvae of T. cati can cause severe histopathological alterations in various tissues and organs of infected animals, within the current study shows that the lungs are a favorable site of migration for these larvae. T. cati is a zoonotic parasite that is underestimated.",
                "authors": "N. Marey, M. El-Seify, S. Abou Asa, N. Satour, N. Elhawary, K. Sultan",
                "citations": 1
            },
            {
                "title": "Clinico-radiologic Characteristics of Pulmonary Visceral Larva Migrans Caused by Ascaris suum",
                "abstract": "Objective Visceral larva migrans (VLM) caused by Ascaris suum is a major health problem in pig farming regions. The clinical characteristics of pulmonary VLM caused by A. suum, however, are unclear. We assessed the clinico-radiologic features of this disease. Methods Medical records, including the results of chest radiography and high-resolution computed tomography (HRCT), were retrospectively reviewed from January 2000 through June 2019, at the University of Miyazaki Hospital and Kyoritsuiin Hospital in Miyazaki Prefecture, Japan. Results Seven patients with VLM caused by A. suum were identified. All seven patients had a unique habit of consuming raw foods, such as organic vegetables, chicken, turkey, wild boar, and venison. All but one patient, who had eosinophilic pneumonia with a fever and severe fatigue, had only mild or no respiratory symptoms. All 7 patients had remarkable eosinophilia (median, 1,960 /μL) and high serum IgE levels (median, 1,346 IU/mL). Chest HRCT revealed multiple nodules and multiple nodular ground-glass opacities in 57% and 29% of the patients, respectively. The pulmonary lesions were located predominantly in subpleural areas. All seven patients were treated with albendazole, which led to improvement within two to three months. Neither eggs nor parasites were detected in the feces or sputum of any patient. Conclusion Consumption of raw organic vegetables or raw meat is a possible route of A. suum infection. Infected patients exhibit mild respiratory symptoms, and multiple nodules with a halo in the subpleural area are a common finding on chest HRCT. Treatment with albendazole was effective in these cases.",
                "authors": "N. Matsumoto, Hironobu Tsubouchi, K. Setoguchi, Takanori Horiguchi, Takafumi Shigekusa, Shinpei Tsuchida, A. Matsuo, Yasuharu Oda, S. Yanagi, H. Maruyama, M. Nakazato",
                "citations": 1
            },
            {
                "title": "FUNCTIONAL HEALTH LITERACY IN PATIENTS WITH ACUTE CORONARY SYNDROME",
                "abstract": "HOW TO REFERENCE THIS ARTICLE: Costa FAS da, Pessoa VLM de P, Salles DL, Frota KC da, Sobral MGV, Souza LC de. Functional health literacy in patients with acute coronary syndrome. Cogit. Enferm. [Internet]. 2021 [accessed “insert day, monh and year”]; 26. Available from: http://dx.doi.org/10.5380/ce.v26i0.75415. ABSTRACT Objective: To describe functional health literacy in patients with coronary artery diseases and analyze its correlation with educational level. Methods: Analytical descriptive study with a quantitative approach conducted in a cardiology hospital in Fortaleza, Ceará (CE), Brazil, from January 2015 to December 2018, with 76 participants. The Care Report Form was used for data collection and literacy categorization was performed using the Short Assessment of Health Literacy for PortugueseSpeaking Adults. Statistical analysis was performed in SPSS software using Pearson’s chi-square test (p<0.05) for testing the relationships and cross-tabulation of the variables. Results: It was found that 85.5% of the participants had a low level of education and an inadequate level of functional health literacy. The p-value for the association between level of education and level of literacy was 0.890. Conclusion: Nurses’ knowledge about aspects related to the management of self-care by patients is an important strategy for providing high quality care.",
                "authors": "Francisco A Costa, V. Pessoa, Dafne Lopes Salles, Kairo Cardoso da Frota, M. G. Sobral, Lorena Campos de Souza",
                "citations": 1
            },
            {
                "title": "[Toxocariasis in children: analysis of 85 cases in a paediatric hospital in Argentina].",
                "abstract": "BACKGROUND\nToxocariasis is a widely spread parasitic disease. The most frequent clinical form is asymptomatic (AT) although it may present with visceral larva migrans (VLM), ocular larva migrans (OLM) or covert (TE) involvement.\n\n\nAIMS\nTo describe the clinical presentation, laboratory, evolution and treatment characteristics of the cases and to compare the various clinical forms of presentation.\n\n\nPATIENTS AND METHODS\nRetrospective analysis of all children diagnosed with toxocariasis attended at the Infectology Service of the Pedro de Elizalde Children's General Hospital between 2012-2019.\n\n\nRESULTS\nWe included 85 patients. 63.5% were males and the median age was 60 months. 49 patients presented AT, 14 VLM, 15 OLM and 7 TE. Children with LMV had lower age and higher eosinophil count. All the cases of OLM evolved with a poor visual prognosis. Treatment with albendazole was indicated in all cases of LMV, in active cases of LMO, in 4 TE and in 3 AT.\n\n\nCONCLUSION\nThis study represents one of the largest conducted in our country. The ocular forms had bad prognosis, while the visceral and covert forms had good evolution. It is essential to emphasize the prevention and early diagnosis of the disease in order to establish timely treatment and avoid sequelae.",
                "authors": "X. Juárez, M. Delgado, E. Matteucci, S. Schiavino, M. Pasinovich, Ludmila García-Franco, Aldo D Cancellara",
                "citations": 1
            },
            {
                "title": "Inverse Analysis Method on the Performance Evaluation of Geosynthetic Reinforcements in Highway Pavement on Expansive Soils",
                "abstract": "Sometimes a pavement deflects only because of seasonal volume change of expansive soils in the pavement subgrade. Engineering practitioners expect an implementable and straightforward analysis method for a geosynthetic-reinforced pavement subjected to the swelling/shrinkage issue of expansive clayey subgrade, in an effort to find the bending moment, shear force and tension force distributions through the reinforced pavement, which are induced from the volumetric changes of subgrade soils. The virtual load method (VLM) was proposed in the past following the Timoshenko beam theory to analyze geosynthetic-reinforced pavement on expansive soils. In the VLM, the unknown virtual distributed load was obtained in the way by applying the inverse theory for the identification of material parameters of the pavementfoundation system. It was seen that the selection of the number of material parameters to obtain virtual load significantly affects the accuracy of the structural properties of the pavement and tensile properties of the geosynthetics if the linear least square method is used. In this paper, a unique numerical scheme was proposed in the hopes of solving the issue. After a forward problem was solved numerically, the Timoshenko beam deflection was taken as a start-up for the inverse problem to back analyze the load applied to the pavement. Solutions from forward/backward examples have explicitly shown the accuracy achieved related to the bending moment in the pavement and tension in the geosynthetic reinforcements. The proposed methodology can be applied for an in-depth understanding of the geosynthetic function for the mitigation of longitudinal cracks on pavements caused by heave/shrinkage of expansive soils.",
                "authors": "Debojit Sarker, Jay X. Wang",
                "citations": 1
            },
            {
                "title": "Frequency of Toxocariasis among Patients Clinically Suspected to Have Visceral Toxocariasis: A Retrospective Descriptive Study in Sri Lanka",
                "abstract": "Introduction Human toxocariasis is caused by several species of the nematode Toxocara. Two common clinical syndromes are ocular and visceral larva migrans. Objectives To determine the Toxocara antibody positivity in clinically suspected VLM patients and to describe demographic factors and clinical manifestations of seropositive patients. Methods 522 clinically suspected patients were studied between 1993 and 2014. Relevant data was gathered from referral letters. Serum samples were subjected to Toxocara antigen ELISA. Results Overall, seropositivity was 50.2% (262), of which 109 (40.8%) were positive at high level of Toxocara antibody carriage and 153 (58.4%) were positive at low levels. The seropositives ranged from 3 months to 70 years (mean = 7.8). Younger age group had higher levels of seropositivity and it was statistically significant. Majority of children under 5 years were seropositive (47.7%, n = 125). Seropositivity was common in males (55.3%, n = 145). Clinical manifestations of seropositives include lymphadenopathy (24.1%) skin rash (22.5%), dyspnoea (21.7%), fever (21%), hepatosplenomegaly (9.2%), and abdominal pain (3.8%). 197 (75.2%) seropositive cases had eosinophilia. These symptoms were not statistically significant. Conclusions This study confirms toxocariasis as an important cause of childhood ill health identifying common clinical symptoms recommending preventive measures to limit transmission.",
                "authors": "D. Iddawela, K. Ehambaram, D. Atapattu, K. Pethiyagoda, Lakmalee Bandara",
                "citations": 16
            },
            {
                "title": "Prevalence of Toxocara antibodies among patients clinically suspected to have ocular toxocariasis: A retrospective descriptive study in Sri Lanka",
                "abstract": null,
                "authors": "D. Iddawela, K. Ehambaram, P. Bandara",
                "citations": 16
            },
            {
                "title": "Medullary tyrosine hydroxylase catecholaminergic neuronal populations in sudden unexpected death in epilepsy",
                "abstract": "Sudden unexpected death in epilepsy (SUDEP) is mechanistically complex and one probable cause is seizure‐related respiratory dysfunction. Medullary respiratory regulatory nuclei include the pre‐Bötzinger complex (pre‐BötC) in the ventrolateral medulla (VLM), the medullary raphé nuclei (MR) and nucleus of solitary tract in the dorsomedial medulla (DMM). The region of the VLM also contains intermingled tyrosine hydroxylase (TH) catecholaminergic neurones which directly project to the pre‐BötC and regulate breathing under hypoxic conditions and our aim was to evaluate these neurones in SUDEP cases. In post‐mortem cases from three groups [SUDEP (18), epilepsy controls (8) and non‐epilepsy controls (16)] serial sections of medulla (obex + 2 to + 13 mm) were immunolabeled for TH. Three regions of interest (ROI) were outlined (VLM, DMM and MR) and TH‐immunoreactive (TH‐IR) neurones were evaluated using automated detection for overall labeling index (neurones and processes) and neuronal densities and compared between groups and relative to obex level. C‐fos immunoreactivity was also semi‐quantitatively evaluated in these regions. We found no significant difference in the density of TH‐IR neurones or labeling index between the groups in all regions. Significantly more TH‐IR neurones were present in the DMM region than VLM in non‐epilepsy cases only (P < 0.01). Regional variations in TH‐IR neurones with obex level were seen in all groups except SUDEP. We also identified occasional TH neurones in the MR region in all groups. There was significantly less c‐fos labeling in the VLM and MR in SUDEP than non‐epilepsy controls but no difference with epilepsy controls. In conclusion, in this series we found no evidence for alteration of total medullary TH‐IR neuronal numbers in SUDEP but noted some differences in their relative distribution in the medulla and c‐fos neurones compared to control groups which may be relevant to the mechanism of death.",
                "authors": "S. Patodia, I. Tan, M. Ellis, Alyma Somani, I. Scheffer, S. Sisodiya, M. Thom",
                "citations": 7
            },
            {
                "title": "Virtual-Lattice Based Intrusion Detection Algorithm over Actuator-Assisted Underwater Wireless Sensor Networks",
                "abstract": "Due to the lack of a physical line of defense, intrusion detection becomes one of the key issues in applications of underwater wireless sensor networks (UWSNs), especially when the confidentiality has prime importance. However, the resource-constrained property of UWSNs such as sparse deployment and energy constraint makes intrusion detection a challenging issue. This paper considers a virtual-lattice-based approach to the intrusion detection problem in UWSNs. Different from most existing works, the UWSNs consist of two kinds of nodes, i.e., sensor nodes (SNs), which cannot move autonomously, and actuator nodes (ANs), which can move autonomously according to the performance requirement. With the cooperation of SNs and ANs, the intruder detection probability is defined. Then, a virtual lattice-based monitor (VLM) algorithm is proposed to detect the intruder. In order to reduce the redundancy of communication links and improve detection probability, an optimal and coordinative lattice-based monitor patrolling (OCLMP) algorithm is further provided for UWSNs, wherein an equal price search strategy is given for ANs to find the shortest patrolling path. Under VLM and OCLMP algorithms, the detection probabilities are calculated, while the topology connectivity can be guaranteed. Finally, simulation results are presented to show that the proposed method in this paper can improve the detection accuracy and save the energy consumption compared with the conventional methods.",
                "authors": "Jing Yan, Xiaolei Li, Xiaoyuan Luo, X. Guan",
                "citations": 16
            },
            {
                "title": "Supplement to: A revised acceleration rate from the altimetry-derived global mean sea level record",
                "abstract": "The satellite radar altimetry data for TOPEX and ERS-1&2 are obtained from the Radar Altimetry Database System (RADS). The applied geophysical corrections are listed in Table 1. The orbits solutions used are computed as part of the Sea Level project of the Climate Change Initiative (SLCCI). Note that the pole tide from Desai et al. 2 is used in the crossover analysis, which contains the variations with respect to a linear mean pole. In the tide-gauge comparison, we use the pole tide as described in the IERS2010 conventions, which account for a non-linear mean pole. Mean polar motion is implicity taken into account in a Vertical Land Motion (VLM) correction for the tide gauges (Sect. 3). Furthermore, the tides and dynamic atmosphere correction are not applied for the tide-gauge comparison. The major tidal harmonics are regressed as part of the TOPEX drift estimation (Sect. 3).",
                "authors": "Marcel Kleinherenbrink, R. Riva, R. Scharroo",
                "citations": 10
            },
            {
                "title": "A tubular vaporizing liquid micro-thruster with induction heating",
                "abstract": null,
                "authors": "Bendong Liu, Xu Yang, Yuezong Wang, Desheng Li, Guohua Gao, Jiahui Yang, R. Zhou",
                "citations": 6
            },
            {
                "title": "Bleomycin for orbital and peri-orbital veno-lymphatic malformations – A systematic review",
                "abstract": "Background Orbital and peri-orbital venolymphatic malformations (VLM) are low flow vascular malformations. Intralesional bleomycin is now commonly being used to treat such malformations. Objective The purpose of this systematic review is to synthesize evidence on the safety and efficacy of bleomycin/pingyangmycin sclerotherapy for the treatment of orbital and peri-orbital VLM. Methods We searched Medline, Embase, Scopus and Cochrane database for studies reporting outcomes of bleomycin/pingyangmycin sclerotherapy for orbital and peri-orbital VLM between 1974 to April 5th, 2019. Nine retrospective cohort studies enrolling 132 patients were included. Two reviewers independently screened and extracted data and assessed the risk of bias. Predefined outcome measures were subjective and objective reduction of the lesion and associated complications. Results Subjective reduction of the lesions was seen in 96.2% of the studies. Objective reduction of the lesion and symptomatic improvement were reported in 91.6 and 95% of the studies respectively. Non responders were 9.0%. Minor adverse events were reported in 18.1% of the studies. Major complications like pulmonary toxicity or pulmonary fibrosis was not encountered in any of the included studies. Quality of evidence was generally low. Conclusion Bleomycin/pingyangmycin sclerotherapy is very effective and relatively safe for the treatment of orbital and periorbital VLM and is not associated with any major side effects including pulmonary fibrosis. Limitations: The systematic review is limited mainly due to low quality of the included studies with retrospective design.",
                "authors": "Khunsa Faiz, S. Finitsis, J. Linton, J. Shankar",
                "citations": 6
            },
            {
                "title": "Yorkie and JNK revert syncytial muscles into myoblasts during Org-1–dependent lineage reprogramming",
                "abstract": "Lineage reprogramming has become a prominent focus in research since it was demonstrated that lineage restricted transcription factors can be used in vitro for direct reprogramming [1]. Recently, we reported that the ventral longitudinal musculature (VLM) of the adult Drosophila heart arises in vivo by direct lineage reprogramming from alary muscles (AM), a process which starts with dedifferentiation and fragmentation of syncytial alary muscles into mononucleate myoblasts. Central upstream activators of the genetic program regulating the development of VLMs from alary muscles are the T-box factor Org-1 (Drosophila Tbx1) and the LIM homeodomain factor Tup (Drosophila Islet1) [2]. However, the events downstream of Org-1 and Tup that exert dedifferentiation and fragmentation of alary muscles have been unknown. In the present report, we shed light on the initiation of this first step of transdifferentiation and show that AM lineage specific activation of Yorkie (Yki), the transcriptional co-activator of the transcription factor Scalloped (Sd), has a key role in initiating AM lineage reprogramming. An additional necessary input comes from active dJNK signaling, which contributes to the inactivation of the Hippo kinase cascade upstream of Yki and furthermore activates dJun. The synergistic activities of the Yki/Sd and dJun/dFos (AP-1) transcriptional activator complexes in the absence of Hippo activity initiate AM dedifferentiation and lead to the expression of Myc and piwi, which are crucial for different aspects of AM transdifferentiation. Our results provide new insights into the mechanisms that mediate muscle lineage plasticity during a cellular reprogramming process occurring in vivo. Highlights Direct lineage reprogramming of alary muscles depends on Yorkie and JNK Yorkie and JNK mediate reversal of syncytial muscle cell fate Yki/Sd and AP-1 induce alary muscle dedifferentiation synergistically Yki dependent Myc induces and Piwi mediates reprogramming of alary muscles",
                "authors": "C. Schaub, M. Rose, M. Frasch",
                "citations": 10
            },
            {
                "title": "On the Use of Repeat Leveling for the Determination of Vertical Land Motion: Artifacts, Aliasing, and Extrapolation Errors",
                "abstract": "Leveling remains the most precise technique for measuring changes in heights. However, for the purposes of determining vertical land motion (VLM), a time series of repeat leveling measurements is susceptible to artifacts and aliasing that may arise due to systematic errors, seasonal surface fluctuations, motions occurring during a survey, and any inconsistencies in the observation conditions among epochs. Using measurements from 10 repeat leveling surveys conducted twice yearly along a profile spanning ~40 km across the Perth Basin, Western Australia, we describe the observation, processing, and analysis methods required to mitigate these potential error sources. We also demonstrate how these issues may lead to misinterpretation of the VLM derived from repeat leveling and may contribute to discrepancies between geologically inferred rates of ground motion or those derived from other geodetic measurement techniques. Finally, we employ historical (~40‐year‐old) leveling data in order to highlight the errors that can arise when attempting to extrapolate VLM derived from a geodetic time series, particularly in cases where the long‐term motion may be nonlinear.",
                "authors": "T. J. Lyon, M. Filmer, W. Featherstone",
                "citations": 11
            },
            {
                "title": "Optimizing VNF live migration via para-virtualization driver and QuickAssist technology",
                "abstract": "Live migration of virtual network functions (VNF) is a powerful technique with benefits of server maintenance, resource management and dynamic workload re-balance, among others. Downtime and total migration time are mainly two vital indicators to describe the performance of the VNF live migration (VLM). Modern research has effectively reduced the downtime to zero for some specific VNFs (eg. virtual router). However, for general VNFs predominantly leveraging pre-copy approach, such as firewalls, network address translators (NAT), load balancers, etc., there still remain some intractable problems: inevitable service downtime and long migration time on account of large amount of data transferred during migration, both of which result in a severe performance degradation of VNF services. To resolve these issues, we present a solution called PV-QAT to accelerate the migration process for these general VNFs. The PV-QAT creatively exploits the Para-Virtualization (PV) driver to filter out the useless memory pages in the process of migration, and unprecedentedly applies QuickAssist Technology (QAT) to provide fast compression of memory pages with low overhead, The experimental results show that PV-QAT can significantly reduce 77.5% of downtime and 80.5% of total migration time on average when compared with original pre-copy migration of KVM.",
                "authors": "Jinshi Zhang, Liang Li, Dong Wang",
                "citations": 13
            },
            {
                "title": "The central effects of buspirone on abdominal pain in rats",
                "abstract": "Buspirone, a partial agonist of the 5‐HT1a receptor (5‐HT1aR), owing to potential antinociceptive properties may be useful in treatment of abdominal pain in IBS patients. The pain‐related effects of buspirone are mediated via the 5‐HT1aRs, specifically located within the ventrolateral medulla (VLM). The most animal studies of the 5‐HT1aR involvement in pain control have been carried out with somatic behavioral tests. The 5‐HT1aR contribution in visceral pain transmission within the VLM is unclear. The objective of our study was to evaluate the 5‐HT1aR contribution in abdominal pain transmission within the VLM.",
                "authors": "S. Panteleev, I. Sivachenko, O. Lyubashina",
                "citations": 12
            },
            {
                "title": "BDNF shown to partially mediate greater verbal learning performances in women as compared to men in a cohort enriched with risk for Alzheimer’s disease",
                "abstract": "It is established that the sex‐based prevalence of Alzheimer’s disease (AD) is greater in women, though the mechanisms underlying this difference are not well understood. Furthermore, in cognitively normal adults, women consistently score higher than men in verbal learning ability, a primary domain of cognitive function sensitive to decline from AD progression. The sex‐specific mechanisms of action of brain‐derived neurotrophic factor (BDNF), a growth factor important for memory and long‐term brain health, may explain observed sex‐based differences in cognitive decline. The purpose of this study was to investigate whether sex‐based differences in verbal learning and memory (VLM) are mediated by BDNF expression in a cohort of cognitively normal adults at risk for AD.",
                "authors": "Kyle J. Edmunds, Gabriella M. Mamlouk, O. Okonkwo, Christine Rogers",
                "citations": 0
            },
            {
                "title": "Visceral Larva Migrans: A Rare Encounter by a Cytologist",
                "abstract": "VLM is a zoonotic disease caused by the migration of third-stage larvae of nematodes through the tissue of human viscera. Among various etiological agents such as Baylisascaris procyonis, Capillaria hepatica, Ascaris sum, and some Ancylostoma species, Toxocara is a major cause of VLM. Poor hygiene, contact with dogs and geophagia increases the risk of toxocariasis.Young adults and children who are in close contact with animals are at a higher risk. Here we present a case of 7 years male child presenting with fever, abdominal pain and vomiting. The clinical presentation, biochemical and radiological findings supported the diagnosis of VLM which was corroborated in the cytological examination. Here we report a rarest encounter of VLM in the cytology smear.",
                "authors": "Neeti Nagar, N. Madan, R. Mittal, P. Debata, S. Ranga",
                "citations": 0
            },
            {
                "title": "Constraint of GIA in Northern Europe and the North Sea with Geological RSL and GPS Data",
                "abstract": "<p>This study focusses on improved constraint of the millennial time-scale glacial isostatic adjustment (GIA) signal at present-day, and its role as a contributor to present-day sea-level budgets. The study area extends from the coastal regions of northern Europe to Scandinavia. Both Holocene relative sea level (RSL) data as well as vertical land motion (VLM) data are incorporated as constraints in a semi-empirical GIA model. Specifically, 71 geological rates of GIA-driven RSL change are inferred from Holocene proxy data. Rates of vertical land motion from GNSS at 108 sites provide an additional measure of regional GIA deformation; within the study area, the geological RSL data complement the spatial gaps of the VLM data and vice versa. Both datasets are inverted in a semi-empirical GIA model to yield updated estimates of regional present-day GIA deformations. A regional validation is presented for the North Sea, where the GIA signal may be complicated by lateral variations in Earth structure and existing predictions of regional and global GIA models show discrepancies. The model validation in the North Sea region suggests that geological data are needed to fit independent estimates of GIA-related RSL change inferred from tide gauge rates, indicating that geological rates from Holocene data can provide an important additional constraint for data-driven approaches to GIA estimation. The geological proxy rates therefore provide a unique dataset with which to complement or validate existing data-driven approaches that use satellite era rates of change.</p>",
                "authors": "K. Simon, R. Riva, B. Vermeersen",
                "citations": 0
            },
            {
                "title": "Present-day vertical land movement in San Fernando (La Union) and Currimao (Ilocos Norte), northwest Luzon, Philippines",
                "abstract": "<p>The northwestern coast of Luzon Island is located within the forearc region of the Manila Trench where emergent coral reef platforms have been reported; and an uplift rate of 0.5 m/kyr has been estimated for the past 7,000 years in San Fernando and Currimao. This study examined the present-day vertical land movement (VLM) in both sites using tide gauge records and retracked Jason satellite altimeter missions. Both the tide gauge and satellite data were corrected for tides using the T_Tide algorithm and the difference between the tide gauge sea level (TGSL) and sea surface heights (SSH) from the satellite were calculated. The influence of VLM was inferred from the differences between the TGSL and SSH, then validated using available GNSS data.</p><p>&#160;</p><p>Hourly TGSL for San Fernando is available from 2002 to 2018 with a completeness index (CI) of 37%. The satellite products used were the 20 Hz MLE4 and 1Hz ALES retracked Jason satellite series downloaded from AVISO+ and OpenADB, respectively. The MLE4 product indicates subsidence with a rate of 0.43 &#177; 0.10 mm/yr, while ALES indicates uplift at 1.93 &#177; 0.42 mm/yr. GNSS observations at the San Fernando TG benchmark (TGBM) from 2017 to 2019 shows subsidence at 0.74 &#177; 0.40 mm/yr, which agrees well with the VLM estimate from the difference between TGSL and MLE4 SSH.&#160;&#160;&#160;&#160;</p><p>&#160;</p><p>Currimao TG station has a CI of 90% from 2008 to 2016. Satellite products used were the 20 Hz MLE4 and 20 Hz ALES retracked Jason-2 downloaded from AVISO+, and both indicate uplift with a rate of 7.30 &#177; 0.17 and 6.24 &#177; 0.25 mm/yr, respectively. The present-day uplift agrees with the geological records, however, there are no GNSS data at the TGBM to validate the present-day vertical motion.</p><p>&#160;</p><p>The differences between the present-day vertical motion of San Fernando and Currimao may indicate the influence of other fault systems associated with the Philippine Fault or segmentation of the forearc. Subsidence in San Fernando could imply stress accumulation in the area and the observed uplift in the geological records are cumulative co-seismic vertical displacements.&#160;&#160;</p>",
                "authors": "Paul Caesar M. Flores, A. Rediang, N. Pasaje, Rey Mark Alfante, M. D. A. Bauzon, R. Reyes, F. Siringan, Charina Lyn Amedo-Repollo, D. Bringas, A. Blanco",
                "citations": 0
            },
            {
                "title": "Twist Morphing: A Simulation Approach to Compare Twist Morphed Wing and Flap Configuration",
                "abstract": "Abstract: The aim of the work is to explore and justify an innovative concept in the niche of aerospace industry called as Wing Morphing. To narrow down the study, specifically twist morphing is taken into consideration. Wings with twist and their flap counterparts are compared in similar conditions and their aerodynamic efficiency is observed. The project implementation is done with XFLR5, a VLM solver software. The results show that this concept brings about an improvement in the aerodynamic efficiency without adding much to the drag penalty. Keywords: Wing Morphing, Twist Morphing, Cl (coefficient of lift), Cd (Coefficient of drag), Alpha (angle of attack)",
                "authors": "Aditya Y. Joshi",
                "citations": 0
            },
            {
                "title": "A novel data-driven method to estimate GIA signal from Earth observation data",
                "abstract": "<p>Geophysical inversions are usually solved with the help of a-priori constraints and several assumptions that simplify the physics of the problem. This is true for all the inversion approaches that estimate GIA signal from contemporary datasets such as GNSS vertical land motion (VLM) time-series and GRACE geopotential time-series. One of the assumptions in these GIA inversions is that the change in VLM due to GIA can be written in terms of surface mass change and average mantle density. Furthermore, the surface density change is obtained from GRACE data using the relations derived in Wahr et al., 1998, which actually is only applicable for surface processes (such as hydrology) and not for sub-surface processes such as GIA. This leaves us with a tricky signal-separation problem. Although many studies try to overcome this by constraining the inversion with the help of constrains from a priori GIA models, the output is not free from influence of GIA models that are known to have huge uncertainties. In this presentation, we discuss this problem in detail, then provide a novel mathematical framework that solves for GIA without any a priori GIA model. We validate our method in a synthetic environment first and then estimate a completely data-driven GIA field from contemporary Earth-observation data.</p>",
                "authors": "B. Vishwakarma, Y. Ziegler, S. Royston, J. Bamber",
                "citations": 0
            },
            {
                "title": "A review study for Toxocariasis",
                "abstract": "Many researchers reported rates of infected dogs with T. canis and cats with Toxocara cati all over the world. The degree of tissue damage in the host and the concomitant elicitation of signs and symptoms are varied in different invaded tissues. The liver, lungs and central nervous system, including the eyes, are considered the most sensitive organs. In addition, the number of migrating juveniles and the age of the host. Inflammation manifests as eosinophilic granulomas. The immediate hypersensitivity responses to dying and dead larvae in the viscera, including the lungs, liver and brain, produce symptoms characteristic of VLM. The current review discusses the Toxocara sp. infection from the historical background, taxonomy, lifecycle, pathogeneses, clinical signs, epidemiology, diagnosis, control and treatment. And provides an overview of existing literature and data on the Toxocaraiasis with their references.",
                "authors": "S. Jasim, A. Hadi",
                "citations": 0
            },
            {
                "title": "Sinking land intensifies sea-level rise: a global InSAR analysis of coastal cities",
                "abstract": "\n Coastal land is being lost worldwide at an alarming rate due to relative sea-level rise (RSLR) resulting from vertical land motion (VLM). This problem is understudied at a global scale, due to high spatial variability and difficulties reconciling VLM between regions. Here we provide self-consistent, high spatial resolution VLM observations derived from Interferometric Synthetic Aperture Radar for the 51 largest coastal cities, representing 22% of the global urban population. We show that peak subsidence rates are faster than current global mean sea-level rise rates and VLM contributions to RSLR are greater than IPCC projections in 90% and 53% of the cities respectively. Localized VLM worsens RSLR impacts on land and population in 73-75% of the cities, with Chittagong (Bangladesh), Yangon (Myanmar) and Jakarta (Indonesia) at greatest risk. With this dataset, accurate projections and comparisons of RSLR effects accounting for VLM are now possible for urban areas at a global scale.",
                "authors": "Cheryl W. J. Tay, E. Lindsey, Shi Tong Chin, J. McCaughey, D. Bekaert, Michele Nguyen, H. Hua, G. Manipon, M. Karim, B. Horton, E. Hill",
                "citations": 0
            },
            {
                "title": "Data-driven estimate of past and present surface loading over North America: Bayesian Hierarchical Modelling approach applied to GPS and GRACE observations",
                "abstract": "<p>Glacial Isostatic Adjustment (GIA) and the hydrological cycle are both associated with mass changes, which are observed by GRACE, and vertical land motion (VLM), which is observed by GPS. Hydrology-related VLM results from the instantaneous response of the elastic solid Earth to surface loading by freshwater, whereas GIA-related VLM reveals the long-term response of the visco-elastic Earth mantle to past glacial cycles. Thus, observations of mass changes and VLM are interrelated and GIA and hydrology are difficult to investigate independently. Taking advantage of the differences in the spatio-temporal characteristics of the GIA and hydrology fields, we can separate the respective contributions of each process. In this work, we use a Bayesian Hierarchical Modelling (BHM) approach to provide a new data-driven estimate of GIA and time-evolving hydrology-related VLM for North America. We detail our processing strategy to prepare the input data for the BHM while preserving the content of the original observations. We discuss the separation of GIA and hydrology processes from a statistical and geophysical point of view. Finally, we assess the reliability of our estimates and compare our results to the latest GIA and hydrological models. Specifically, we compare our GIA solution to a forward-model global field, ICE-6G, and a recent GIA estimate developed for North America (Simon et al. 2017). Our time-evolving hydrology field is compared with WaterGAP, a global water balance model. Overall, for both GIA and hydrology, there is a good agreement between our results and the forward models, but we also find differences which possibly highlight deficiencies in these models.</p>",
                "authors": "Y. Ziegler, B. Vishwakarma, A. Brady, S. Chuter, S. Royston, R. Westaway, J. Bamber",
                "citations": 0
            },
            {
                "title": "A-159 Attention and Executive Function Predict Immediate and Delayed Verbal Memory in Healthy Youth: A Structural Equation Modeling Approach",
                "abstract": "\n \n \n Previous neuropsychological assessments have sought to understand the interrelatedness of cognitive functions when interpreting neuropsychological test performance (e.g., Delis-Kaplan Executive Function System; DKEFS), given that they rarely function in isolation. Test results may be misinterpreted if not contextualized within individuals’ overall cognitive profiles; therefore, the present study evaluated the impact of attention and executive function (EF) on verbal learning and memory (VLM) in a sample of healthy youth.\n \n \n \n Participants (n = 166, M age = 12.0 years, 79.5% male, M FSIQ = 109.6) completed a two-hour neuropsychological battery, including the Wide Range Assessment of Learning and Memory, 2nd Edition List Learning (LL), DKEFS Trail Making Test (TMT), and Wechsler Intelligence Scale for Children, Fourth Edition Digit Span (DS) and Letter-Number Sequencing (LNS). Structural equation modeling was used to evaluate the impact of attention and EF on immediate and delayed VLM.\n \n \n \n Latent factors were estimated for attention/EF (DS Forward, DS Backward, LNS, and TMT Condition 4) and VLM (LL Trials 1–4), and the model converged with adequate model fit statistics (RMSEA = 0.064; CFI = 0.975; TLI = 0.963, SRMR = 0.039). Attention and EF predicted performance on both immediate and delayed VLM (ps < 0.001) when accounting for age.\n \n \n \n The results demonstrated that attention and EF predicted immediate and delayed VLM in youth, suggesting that participants’ performance on VLM may be impacted by weaknesses in attention and EF. As a result, clinicians are recommended to examine patterns of performances across multiple cognitive domains when interpreting individual test scores. This is especially important in pediatric populations given the developmental changes the brain undergoes during childhood and adolescence.\n \n",
                "authors": "R. Thompson, A. Deneen, Yelena Markiv, A. Hall, R. Hirst",
                "citations": 0
            },
            {
                "title": "Design For Manufacture And Assembly Analysis of the Tray for Vertical Lift Modules",
                "abstract": "This paper addresses the analysis of the design, manufacture, and assembly (DFMA) of the tray for a vertical lift module (Tray VLM), being a subassembly of this automatic VLM storage system. The method used for DFMA analysis is the Boothroyd Dewhurst method, using their software. Based on the recommendations of the DFMA software, following the study on the current Tray VLM product, several components were redesigned using the Design for Manufacture (DFM) module and certain stages for assembly were optimized, using the design for Assembly module (DFA). The result was reduced assembly cost, reduced assembly process time, and increased assembly design efficiency, which were successfully validated by DFMA software by maximizing the DFA index.",
                "authors": "C. Torcătoru, D. Săvescu, Narcisa Valter",
                "citations": 0
            },
            {
                "title": "Neurovascular Compression in Arterial Hypertension: Correlation of Clinical Data to 3D-Visualizations of MRI-Findings",
                "abstract": "\n \n In this study, we attempted to identify clinical parameters predicting the absence or presence of Neurovascular Compression (NVC) at the Ventrolateral Medulla (VLM) in arterial hypertension (HTN) in MRI findings.\n \n \n \n Cardiovascular and pulmonary afferences are transmitted through the left vagus and glossopharyngeal nerve to the brain stem and vasoactive centers. Evidence supports the association between HTN and NVC at the left VLM. Several independent studies indicate a reduction of HTN after Microvascular Decompression (MVD) of the left. Several independent studies indicate a reduction of HTN after Microvascular Decompression (MVD) of the left VLM. Image processing of MRI provides comprehensible detection of NVC. HTN affects hemodynamic parameters and organs.\n \n \n \n This study analyzes and correlates clinical data and MRI findings in patients with and without NVC at the VLM in treatment resistant HTN to obtain possible selection criteria for neurogenic hypertension.\n \n \n \n In 44 patients with treatment resistant HTN, we compared MRI findings of neurovascular imaging to demographic, clinical and lifestyle data, office and 24-hour ambulatory Blood Pressure (BP), and cardiovascular imaging and parameters.\n \n \n \n Twenty-nine (66%) patients had evidence of NVC at the VLM in MRI. Sixteen patients (36%) had unilateral NVC on the left side, 7 (16%) unilateral right and 6 (14%) bilateral NVC. Fifteen (34%) had no evidence of NVC at the VLM. Patients with left sided NVC were significantly younger, than those without NVC (p=0.034). They showed a statistically significant variance in daytime (p=0.020) and nighttime diastolic BP (p<0.001) as the mean arterial pressure (p=0.020). Other measured parameters did not show significant differences between the two groups.\n \n \n \n We suggest to examine young adults with treatment resistant HTN for the presence of NVC at VLM, before signs of permanent organ damage appear. Clinical and hemodynamic parameters did not emerge as selection criteria to predict NVC. MVD as a surgical treatment of NVC in HTN is not routine yet as a surgical treatment of NVC in HTN is not routine yet. Detection of NVC by imaging and image processing remains the only criteria to suggest MVD, which should be indicated on an individual decision.\n",
                "authors": "P. Manava, P. Hastreiter, R. Schmieder, Susanne Jung, R. Fahlbusch, A. Dörfler, M. Lell, M. Buchfelder, R. Naraghi",
                "citations": 0
            },
            {
                "title": ". Low-order Aeroelastic Modelling of a High Aspect Ratio Wing Aircraft Under Constrained Motion. In AIAA SciTech Forum 2022: Session: Dynamics of Flexible and Hypersonic Aircraft [AIAA 2022-1306] (AIAA",
                "abstract": "The work presented in this paper describes the modelling of a low-order aeroelastic solver, built with the aim of analysing the dynamics of very flexible wing aircraft, with a focus on the coupling between the flight dynamics and the structural dynamics. The model implements a geometrically exact, low-order nonlinear beam solver based on beam shape functions coupled with Vortex Lattice Method (VLM), specifically adapted to account for large deformations. The static solution calculated by implementing the VLM was compared with the one calculated using strip theory as aerodynamic solver, showing good agreement in magnitude, but different load distribution. The aeroelastic solver was then used to analyse the dynamics of a flexible aircraft constrained to a circular trajectory with free pitch (resembling the motion on a Pendulum Rig) and on a spherical motion with free pitch and yaw (resembling the motion on the University of Bristol 5-DOF Manoeuvre Rig), for two different values of pitch stiffness and three different flexible wings (5% deflection, 10% deflection and 15% deflection with respect to the wing semispan). The results show that when the stiffness is high, it suppresses any interaction between the flight dynamics and the structural dynamics. Therefore there is no impact of the wing flexibility on the aircraft motion. On the other hand, when lowering the value of the pitch stiffness, the interaction between the wing dynamics and the pitch dynamics become evident. However, the impact of the wing flexibility was found to be always negligible on the yaw dynamics.",
                "authors": "Alessandro Pontillo, Punsara D. B. Navaratna, James Ascham, M. Lowenberg, D. Rezgui, E. Jonathan, Cooper, S. Neild",
                "citations": 0
            },
            {
                "title": "Benefits of Concurrent Aerobic-Resistance Interval Exercise in Patients with Chronic Obstructive Pulmonary Disease",
                "abstract": "Background: Physical training of lower leg skeletal muscle has been shown to increase exercise tolerance in Chronic Obstructive Pulmonary Disease (COPD) patients. The objective of this study was to measure the effect of concurrent aerobic-resistance interval exercise on lower leg muscle function and local muscle tissue oxygenation in the vastus lateralis muscle (VLM) of COPD patients. Methods: Peripheral muscle oxygenation in the VLM was measured using Near Infrared Spectrometry (NIRS) during the sixminute walk test (6MWT) in 15 COPD patients: Experimental (EXP., n = 9) and control (CTL., n = 6). Both groups trained for 3 weeks (1 hour/day and 5 days/ week). Training sessions consisted of upper body strength exercises for 20 minutes. Next, the CTL. Group performed 30 minutes of aerobic exercise (motorized treadmill or stationary bike), while the EXP. group completed 30 minutes of aerobic-resistance (concurrent training) interval exercise (non-motorized treadmill). Pre and post-intervention NIRS measurements included oxyhemoglobin (O2Hb), deoxyhemoglobin (HHb), hemoglobin difference (HbDif), total hemoglobin (tHb), and tissue saturation index (TSI). Systemic arterial oxygen saturation (SPO2), heart rate and the rating of perceived exertion with the modified Borg scale were also measured pre and post-intervention during the 6MWT. Results: All patients were able to complete three weeks of training. A significant increase in the 6MWT (51.4%, p < 0.05) distance was noted in the EXP. group and in the VLM HbDiff (69%, p < 0.05). No significant differences were observed in the other muscle oxygenation variables. Conclusion: Concurrent aerobic-resistance interval exercise increases the total distance covered at the 6MWT (increased functional capacity) and muscle oxygen extraction.",
                "authors": "C. Aristizabal, Oscar H. Ortiz, Emily J. Walsh, M. Leone, A. Comtois",
                "citations": 0
            },
            {
                "title": "Design and Analysis of a Novel Gravity-Compensating Vertical Linear Motor",
                "abstract": "We propose a vertical linear motor (VLM) for semiconductor manufacturing equipment that compensates for the weight of the moving part by using the balance of magnetic and elastic forces. The developed VLM achieves zero stiffness with a constant upward force over a working range. The magnetic circuit of the VLM is designed to provide a linear negative stiffness over a ±2 mm stroke, with an upward force of 14 N at a 0 mm stroke. The negative stiffness is compensated for by the positive stiffness of elastic component-like springs. The upward force is then constant over the working range and corresponds to the weight of the moving part. The proposed VLM is designed to minimize the coupling effects of stiffness and gravity compensation forces by using magnetic flux saturation and magnetic flux path design. We investigate the effects of each design parameter on VLM operation through finite element analysis. Finally, the simulated forces in the proposed VLM are experimentally verified, indicating a linear negative stiffness of approximately −13.5 N/mm and a gravity compensation force of 14.9 N.",
                "authors": "B. Shin, Kyung-min Lee",
                "citations": 0
            },
            {
                "title": "Pemanfaatan Batu Gunung Pasapak Kecamatan Bambang Kabupaten Mamasa sebagai Agregat Campuran Laston WC",
                "abstract": "PeneIitian ini membahas tentang campuran Iaston WC dengan memanfaatkan sumber daya aIam berupa batu gunung yang berasaI dari Pasapak Kecamatan Bambang Kabupaten Mamasa sebagai agregat daIam campuran. PeneIitian ini diIakukan di Laboratorium Jalan dan Aspal Universitas Kristen Indonesia Paulus Makassar. MetodoIogi pada peneIitian ini yaitu pemeriksaan karakteristik agregat, karakteristik aspaI dan berat jenis FiIler. SeteIah serangkaian pemeriksaan karakteristik diIakukan, maka diIanjutkan dengan rancangan komposisi campuran dan pembuatan benda uji Iaston WC. Benda Uji laston WC kemudian  diuji  meIalui pengujian MarshaIl konvensionaI untuk memperoIeh karakteristik dari campuran meIiputi Stabilitas, FIow, VMA, VlM dan VFB. SeteIah diketahui karakteristik campuran, seIanjutnya adaIah pembuatan benda uji kadar aspaI optimum (KAO) dimana KAO untuk campuran  Iaston WC ditentukan berdasarkan niIai stabiIitas tertinggi.  Benda Uji KAO kemudian  diuji  meIalui pengujian MarshaIl lmmersion untuk memperoIeh niIai StabiIitas MarshaIl Sisa. HasiI dari peneIitian ini diperoIeh karakteristik campuran Iaston WC dengan kadar aspaI 5,50 %, 6,00 %, 6,50 %, 7,00 %, 7,50 % dan StabiIitas MarshalI Sisa (SMS) sebesar 97,24% memenuhi  Spesifikasi Umum Bina Marga 2018.",
                "authors": "Roy Cristian Ponglabba, Rais Rachman, Alpius",
                "citations": 0
            },
            {
                "title": "The Vastus Medialis Oblique Compensates in Current Patellar Dislocation Patients with The Increased Femoral Anteversion",
                "abstract": "\n Purpose The purpose of this study was to investigate whether the vastus medialis oblique (VMO) compensates in patella dislocation patients with the increase of femoral anteversion angle (FAA).Methods From January 2016 to January 2021, a total of 30 knees of 27 patients with recurrent patellar dislocation (RPD Group) were reviewed for this study. Among these patients, 13 patients with patellar dislocation and excessive FAA, >30° were assigned to A Group, and 17 patients with the patellar dislocation without excessive FAA,≤30 were assigned to B Group. And 27 age and sex matched knees without patellofemoral disorders were enrolled as control (C Group). The area of the VMO and vastus lateralis muscle (VLM) was measured 20 mm above the upper edge of the patella, and the ratio of the VMO and VLM area was calculated. The correlation relationship of FAA and VMO, VLM was analyzed.Results Comparing with the C Group, the RPD Group had a significantly larger FAA (30.10± 9.61°vs 15.00± 1.85°, P< 0.05), and smaller VMO/VLM ratio (3.49±1.00/4.18±1.51, P< 0.05), and the VMO/VLM ratio was significantly greater in A Group than the B Group (3.97±1.11 vs3.12±0.70, P< 0.05). There was no statistically significant difference in VMO/VLM ratio among A Group and C Group (3.12±0.70/3.97±1.11, P> 0.05). And the VMO/VLM ratio was decreased in the B Group compared with the C Group (3.12±0.70/4.18±1.51, P< 0.05). The VMO/VLM ratio was positively correlated with the FAA(r=0.42) in the RPD Group.Conclusion Compared with the C Group, the patients with recurrent patellar dislocation had a smaller VMO/VLM ratio. Moreover, the FAA was positively correlated with the VMO/VLM in patients with patella dislocation. This finding explained the compensatory thickening of the VMO in response to excessive FAA in patients with patella dislocation.",
                "authors": "C. Dong, Chao Zhao, Fei Wang",
                "citations": 0
            },
            {
                "title": "Comparison of two rapid numerical methods for predicting the performance of multiple rigid wing-sails",
                "abstract": "The purpose of this study is to compare the accuracy of two cost-effective aerodynamic methods used to predict the performance of a large scale wind propulsion system. The methods are evaluated regarding their ability to predict the performance of a configuration consisting of four rigid wing sails of an approximate height of 80 m and average chord length of 23 m. The distance between the wing sails, from trailing to leading edge, is about one chord length. For a limited number of test cases, it is evaluated how well the methods balance computational cost and accuracy and their potential to predict the performance of multiple rigid wing configurations. Two different types of aerodynamic methods are compared; one method under development based on potential flow/lifting line theory in combination with pre-calculated 2D CFD RANS data (CORR-SILL), and a vortex lattice method (VLM). The results from the two different methods are compared with 3D CFD RANS simulations. The parameters compared are the induced velocities around the sails, system forces and longitudinal center of effort. This paper indicates that both evaluated methods show potential to predict the magnitude and distribution of the forces on multiple wing sail, with a large reduction of computational effort compared to CFD.",
                "authors": "K. Malmek, U. Dhomé, L. Larsson, S. Werner, J. Ringsberg, Christian Finnsgård",
                "citations": 5
            },
            {
                "title": "Laboratory Soft X-Ray Microscopy with an Integrated Visible-Light Microscope—Correlative Workflow for Faster 3D Cell Imaging",
                "abstract": "Abstract Laboratory transmission soft X-ray microscopy (L-TXM) has emerged as a complementary tool to synchrotron-based TXM and high-resolution biomedical 3D imaging in general in recent years. However, two major operational challenges in L-TXM still need to be addressed: a small field of view and a potentially misaligned rotation stage. As it is not possible to alter the magnification during operation, the field of view in L-TXM is usually limited to a few tens of micrometers. This complicates locating areas and objects of interest in the sample. Additionally, if the rotation axis of the sample stage cannot be adjusted prior to the experiments, an efficient workflow for tomographic imaging cannot be established, as refocusing and sample repositioning will become necessary after each recorded projection. Both these limitations have been overcome with the integration of a visible-light microscope (VLM) into the L-TXM system. Here, we describe the calibration procedure of the goniometer sample stage and the integrated VLM and present the resulting 3D imaging of a test sample. In addition, utilizing this newly integrated VLM, the extracellular matrix of cryofixed THP-1 cells (human acute monocytic leukemia cells) was visualized by L-TXM for the first time in the context of an ongoing biomedical research project.",
                "authors": "A. Dehlinger, C. Seim, H. Stiel, S. Twamley, A. Ludwig, M. Kördel, D. Grötzsch, S. Rehbein, B. Kanngießer",
                "citations": 5
            },
            {
                "title": "MOLECULAR SEQUANCING AND PHYLOGENIC ANALYSIS TO VIRULENCE nmuc-1 GENE IN VISCERAL LARVAE MIGRANCE",
                "abstract": "Toxocariasis is a zoonotic parasitic disease caused by Toxocara canis infected egg. Larval stage of this parasite has ability to migrate through intestinal wall and invade all body organs causing a visceral larvae migrant (VLM) syndrome. Diagnosis of VLM is problematic; there were no accurate laboratory test that reveals the presence of larvae infection in paratenic hosts (human, ruminants or poultry). The eggs were isolated from adult T. canis uteri and cultured in 0.2M H2S04 solution for embryonation, mice were experimentally infected with emberyonated eggs. Many hisopathological changes detected in heart and kidney tissues of infected mice but it could not detected encysted larvae compared with molecular detection which confirmed infection within first three day post infection in tissue with accurate diagnosis for the first time in Iraq depend in detected virulence nmuc-1 gene. Phylogenic tree analyses mounted a low genetic variation (0.2) among Iraqi isolate and all other comparison isolates. In conclusion our result indicated that molecular method could diagnosis T. canis larvae infection in any meat or meat products of local or imported from inside or outside Iraq country and used as an accurate microbiological laboratory test used routinely in government",
                "authors": "B. Hade",
                "citations": 5
            },
            {
                "title": "Multi-Step Time Series Forecasting with an Ensemble of Varied Length Mixture Models",
                "abstract": "Many real-world problems require modeling and forecasting of time series, such as weather temperature, electricity demand, stock prices and foreign exchange (FX) rates. Often, the tasks involve predicting over a long-term period, e.g. several weeks or months. Most existing time series models are inheritably for one-step prediction, that is, predicting one time point ahead. Multi-step or long-term prediction is difficult and challenging due to the lack of information and uncertainty or error accumulation. The main existing approaches, iterative and independent, either use one-step model recursively or treat the multi-step task as an independent model. They generally perform poorly in practical applications. In this paper, as an extension of the self-organizing mixture autoregressive (AR) model, the varied length mixture (VLM) models are proposed to model and forecast time series over multi-steps. The key idea is to preserve the dependencies between the time points within the prediction horizon. Training data are segmented to various lengths corresponding to various forecasting horizons, and the VLM models are trained in a self-organizing fashion on these segments to capture these dependencies in its component AR models of various predicting horizons. The VLM models form a probabilistic mixture of these varied length models. A combination of short and long VLM models and an ensemble of them are proposed to further enhance the prediction performance. The effectiveness of the proposed methods and their marked improvements over the existing methods are demonstrated through a number of experiments on synthetic data, real-world FX rates and weather temperatures.",
                "authors": "Yicun Ouyang, Hujun Yin",
                "citations": 12
            },
            {
                "title": "Throughput Models for a Stand-Alone Vertical Lift Module",
                "abstract": "Many companies selling products through e-commerce are on the rise. Their products must be stored, picked and send. Most of them do not need huge storage space and consider using a limited number of Vertical Lift Modules (VLM). These companies experience though that the throughput of a standalone VLM is sometimes too low for efficient picking. This paper presents throughput models for both (i) Existing and (ii) New concepts for standalone VLMs, in order for these companies to project future VLM performance. The models are built starting from already existing throughput models for single bay VLMs and dual bay VLMs. They are extended with new available configurations including multiple lifts in a single lift shaft and buffer systems under the picking bay. The models are parametrized so that the influence of VLM height, vertical transport speed and acceleration of trays, transfer time of trays between lift shaft and storage area can be properly assessed. The paper introduces also a novel VLM configuration based on a combination of existing VLMs which proves to have a considerably higher picking capacity. The models show that mainly the effective availability of trays for picking is increased.",
                "authors": "Vanhauwermeiren Pieter, Juwet Marc, Versteyhe Mark",
                "citations": 4
            },
            {
                "title": "Comparison of Flow-Solvers: Linear Vortex Lattice Method and Higher-Order Panel Method with Analytical and Wind Tunnel Data",
                "abstract": "Two induced drag analysis techniques, Vortex Lattice Method (VLM) and panel method are renowned for inviscid aerodynamic computations and are widely used in the aerospace industry and academia. To demonstrate the applicability of potential flow theory and to establish an extensive correlation of linearized, attached potential flow-solver codes for estimating lift and induced drag, a generic rectangular wing planform is analyzed. Due to a wide range of applicability in conceptual design, the two solvers are compared for accuracy, computational time and input controllability to find an optimum solver that can predict inviscid aerodynamics accurately and efficiently but with the least amount of time. VLM-based code is founded upon the Laplace equation. It approximates a three-dimensional wing into a two-dimensional planform, making it apposite for moderate aspect ratio and thin-airfoil aircraft. A modified VLM is used that takes a suction parameter, calculated analytically, as an input to capture three-dimensional leading-edge thrust and vortex lift effects. On the contrary, the higher-order panel method takes the complete wing surface and changes the wake orientation to model modified flow to better predict the effects of downwash. These codes, allow computation in both subsonic and supersonic regimes, as they include Prandtl-Glauert compressibility correction. The rectangular wing is generated with an identical number of panels and networks for coherent comparison. Distinguishable pre-processing techniques are utilized and similar boundary conditions and flow conditions are maintained over the surfaces that are then given to solvers. The induced drag polar is plotted and compared with wind tunnel and analytical data.",
                "authors": "Tahura Shahid, Faiza Sajjad, Muneeb Ahsan, S. Farhan, S. Salamat, M. Abbas",
                "citations": 4
            },
            {
                "title": "Aero-structural Design of Composite Wings for Airborne Wind Energy Applications",
                "abstract": "In this work we explore the initial design space for composite kites, focusing on the configuration of the bridle line system and its effect on the aeroelastic behaviour of the wing. The computational model utilises a 2D cross sectional model in conjunction with a 1D beam model (2+1D structural model) that captures the complex composite coupling effects exhibited by slender, multi-layered composite structures, while still being computationally efficient for the use at the initial iterative design stage. This structural model is coupled with a non-linear vortex lattice method (VLM) to determine the aerodynamic loading on the wing. In conjunction with the aerodynamic model, a bridle model is utilised to determine the force transfer path between the wing and the bridles connected with the tethers leading to the ground station. The structural model is coupled to the aerodynamic and bridle models in order to obtain the equilibrium aero-structural-bridle state of the kite. This computational model is utilised to perform a design space exploration to assess the effects of varied load introduction to the structure and resulting effects on the kite.",
                "authors": "A. Candade, Maximillian Ranneberg, R. Schmehl",
                "citations": 4
            },
            {
                "title": "The Zoonotic Dog Roundworm Toxocara canis, a Worldwide Burden of Public Health",
                "abstract": null,
                "authors": "Patrick Waindok, M. Raulf, A. Springer, C. Strube",
                "citations": 4
            },
            {
                "title": "Comparative Effectiveness of 2 Diabetes Prevention Lifestyle Programs in the Workplace: The City and County of San Francisco Diabetes Prevention Trial",
                "abstract": "Introduction Data on the comparative effectiveness of Diabetes Prevention Programs (DPPs) in the workplace are limited. Methods Between September 2015 and July 2016, employees of the City and County of San Francisco who were at risk for type 2 diabetes (N = 158) were randomly assigned to one of 2 DPP-derived programs recognized by the Centers for Disease Control and Prevention: an in-person YMCA-DPP (n = 78) or an online virtual lifestyle management DPP (VLM-DPP) offered through Canary Health (n = 80). The primary outcome was change in body weight assessed at 6 and 12 months. Follow-up ended in August 2017. Results Both the YMCA-DPP and VLM-DPP yielded a significant reduction in percentage body weight at 6 months. For the YMCA-DPP, mean percentage change at 6 months was −2.70% (95% confidence interval [CI], −3.91% to −1.48%) and at 12 months was −2.46% (95% CI, −4.24% to −0.68%). For the VLM-DPP, mean percentage change at 6 months was −2.41% (95% CI, −4.07% to −0.77%) and at 12 months was −1.59% (95% CI, −3.51% to 0.33%). The mean between-condition difference at 6 months was −0.25% (95% CI, −2.04% to 1.55%) and at 12 months was −0.84% (95% CI, −3.03% to 1.34%). No significant differences were observed between conditions. The YMCA-DPP had a slightly higher reduction in waist circumference than VLM-DDP at 6 months (mean between-condition difference −2.00 cm [95% CI, −4.24 to 0.25 cm]). Participant engagement, expressed as mean number of completed core program sessions, was significantly higher for the YMCA-DPP than the VLM-DPP. Participants of the YMCA-DPP completed an average of 10.2 sessions (95% CI, 9.0 to 11.4), and participants of the VLM-DPP completed an average of 5.9 sessions (95% CI, 4.7 to 7.1). The adjusted mean between-condition difference was 4.2 sessions (95% CI, 2.54 to 5.99). Conclusion Both the YMCA-DPP and VLM-DPP yielded weight loss at 6 months, which was maintained at 12 months in the YMCA-DPP. The workplace may be an effective setting to offer DPPs.",
                "authors": "A. Ferrara, Julia Mcdonald, S. Brown, Janet G. Alexander, J. Christian-Herman, S. Fisher, C. Quesenberry",
                "citations": 4
            },
            {
                "title": "An Experimental Study on Hydrodynamic Retention of Low and High Molecular Weight Sulfonated Polyacrylamide Polymer",
                "abstract": "Polymers are often added with water as a viscosifier to improve oil recovery from hydrocarbon reservoirs. Polymer might be lost wholly or partially from the injected polymer solution by adsorption on the grain surfaces, mechanical entrapment in pores, and hydrodynamic retention in stagnant zones. Therefore, having a clear picture of polymer losses (and retention) is very important for designing a technically and economically successful polymer flood project. The polymer adsorption and mechanical entrapment are discussed more in depth in the literature, though the effect of hydrodynamic retention can be just as significant. This research investigates the effect of the hydrodynamic retention for low and high molecular weight (AN 113 VLM and AN 113 VHM) sulfonated polyacrylamide polymer. Two high permeability Bentheimer core plugs from outcrops were used to perform polymer corefloods. Polymer retention was first determined by injecting 1 cm3/min, followed by polymer core floods at 3, 5, and 8 cm3/min to determine the hydrodynamic retention (incremental retention). A higher molecular weight polymer (AN 113 VHM) showed higher polymer retention. In contrast, hydrodynamic retention for lower molecular weight (AN 113 VLM) was significantly higher than that of the higher molecular weight polymer. Other important observations were the reversibility of the hydrodynamic retention, no permanent permeability reduction, the shear thinning behavior in a rheometer, and shear thickening behavior in core floods.",
                "authors": "Sameer Al-Hajri, S. M. Mahmood, Ahmed Abdulrahman, H. Abdulelah, S. Akbari, Nabil A. Saraih",
                "citations": 9
            },
            {
                "title": "An Experimental Study on Hydrodynamic Retention of Low and High Molecular Weight Sulfonated Polyacrylamide Polymer",
                "abstract": "Polymers are often added with water as a viscosifier to improve oil recovery from hydrocarbon reservoirs. Polymer might be lost wholly or partially from the injected polymer solution by adsorption on the grain surfaces, mechanical entrapment in pores, and hydrodynamic retention in stagnant zones. Therefore, having a clear picture of polymer losses (and retention) is very important for designing a technically and economically successful polymer flood project. The polymer adsorption and mechanical entrapment are discussed more in depth in the literature, though the effect of hydrodynamic retention can be just as significant. This research investigates the effect of the hydrodynamic retention for low and high molecular weight (AN 113 VLM and AN 113 VHM) sulfonated polyacrylamide polymer. Two high permeability Bentheimer core plugs from outcrops were used to perform polymer corefloods. Polymer retention was first determined by injecting 1 cm3/min, followed by polymer core floods at 3, 5, and 8 cm3/min to determine the hydrodynamic retention (incremental retention). A higher molecular weight polymer (AN 113 VHM) showed higher polymer retention. In contrast, hydrodynamic retention for lower molecular weight (AN 113 VLM) was significantly higher than that of the higher molecular weight polymer. Other important observations were the reversibility of the hydrodynamic retention, no permanent permeability reduction, the shear thinning behavior in a rheometer, and shear thickening behavior in core floods.",
                "authors": "Sameer Al-Hajri, S. M. Mahmood, Ahmed Abdulrahman, H. Abdulelah, S. Akbari, Nabil A. Saraih",
                "citations": 9
            },
            {
                "title": "Static Aeroelastic Characteristics of Morphing Trailing-Edge Wing Using Geometrically Exact Vortex Lattice Method",
                "abstract": "A morphing trailing-edge (TE) wing is an important morphing mode in aircraft design. In order to explore the static aeroelastic characteristics of a morphing TE wing, an efficient and feasible method for static aeroelastic analysis has been developed in this paper. A geometrically exact vortex lattice method (VLM) is applied to calculate the aerodynamic forces. Firstly, a typical model of a morphing TE wing is chosen and built which has an active morphing trailing edge driven by a piezoelectric patch. Then, the paper carries out the static aeroelastic analysis of the morphing TE wing and corresponding simulations were carried out. Finally, the analysis results are compared with those of a traditional wing with a rigid trailing edge using the traditional linearized VLM. The results indicate that the geometrically exact VLM can better describe the aerodynamic nonlinearity of a morphing TE wing in consideration of geometrical deformation in aeroelastic analysis. Moreover, out of consideration of the angle of attack, the deflection angle of the trailing edge, among others, the wing system does not show divergence but bifurcation. Consequently, the aeroelastic analysis method proposed in this paper is more applicable to the analysis and design of a morphing TE wing.",
                "authors": "Sen Mao, Changchuan Xie, Lan Yang, Chao Yang",
                "citations": 8
            },
            {
                "title": "An Attempt to Observe Vertical Land Motion along the Norwegian Coast by CryoSat-2 and Tide Gauges",
                "abstract": "Present-day climate-change-related ice-melting induces elastic glacial isostatic adjustment (GIA) effects, while paleo-GIA effects describe the ongoing viscous response to the melting of late-Pleistocene ice sheets. The unloading initiated an uplift of the crust close to the centers of former ice sheets. Today, vertical land motion (VLM) rates in Fennoscandia reach values up to around 10 mm/year and are dominated by GIA. Uplift signals from GIA can be computed by solving the sea-level equation (SLE), \n \n \n \n S \n ˙ \n \n \n \n = \n \n \n \n N \n ˙ \n \n \n \n − \n \n \n \n U \n ˙ \n \n \n \n . All three quantities can also be determined from geodetic observations: relative sea-level variations ( \n \n \n \n S \n ˙ \n \n \n \n ) are observed by means of tide gauges, while rates of absolute sea-level change ( \n \n \n \n N \n ˙ \n \n \n \n ) can be observed by satellite altimetry; rates of VLM ( \n \n \n \n U \n ˙ \n \n \n \n ) can be determined by GPS (Global Positioning System). Based on the SLE, \n \n \n \n U \n ˙ \n \n \n \n can be derived by combining sea-surface measurements from satellite altimetry and relative sea-level records from tide gauges. In the present study, we have combined 7.5 years of CryoSat-2 satellite altimetry and tide-gauge data to estimate linear VLM rates at 20 tide gauges along the Norwegian coast. Thereby, we made use of monthly averaged tide-gauge data from PSMSL (Permanent Service for Mean Sea Level) and a high-frequency tide-gauge data set with 10-min sampling rate from NMA (Norwegian Mapping Authority). To validate our VLM estimates, we have compared them with the independent semi-empirical land-uplift model NKG2016LU_abs for the Nordic-Baltic region, which is based on GPS, levelling, and geodynamical modeling. Estimated VLM rates from 1 Hz CryoSat-2 and high-frequency tide-gauge data reflect well the amplitude of coastal VLM as provided by NKG2016LU_abs. We find a coastal average of 2.4 mm/year (average over all tide gauges), while NKG2016LU_abs suggests 2.8 mm/year; the spatial correlation is 0.58.",
                "authors": "Martina Idžanović, C. Gerlach, K. Breili, O. Andersen",
                "citations": 8
            },
            {
                "title": "An Assessment of the Utility of Satellite Altimetry and Tide Gauge Data (ALT-TG) as a Proxy for Estimating Vertical Land Motion",
                "abstract": "ABSTRACT Watson, P.J., 2019. An assessment of the utility of satellite altimetry and tide gauge data (ALT-TG) as a proxy for estimating vertical land motion. Journal of Coastal Research, 35(6), 1131–1144. Coconut Creek (Florida), ISSN 0749-0208. Policy, planning, and adaptation responses to sea-level rise are being developed at increasingly localised scales, placing greater emphasis on sea-level studies to more accurately account for all vertical land motions (VLM) at tide gauge sites using technologies such as global navigation satellite systems (GNSSs). Although the spatial coverage and length of GNSS data records continues to increase in proximity to tide gauges, large tracts of the world's coastlines contain limited or no GNSS data. Various studies have investigated proxy methods of estimating VLM by trends from differenced altimetry–tide gauge techniques (ALT-TG). This study has concentrated on investigating the utility of ALT-TG techniques using so-called ‘off the shelf’ gridded satellite altimetry products (U.S. National Aeronautics and Space Administration Jet Propulsion Laboratory [NASA JPL] and Copernicus Climate Change Service [C3S]) for use in regional and local scale sea-level studies to estimate VLM. Twenty locations across the globe were analysed; these locations met specific data constraints designed to ensure the longest overlapping coverage of GNSS, altimetry, and tide gauge data, with the proximity of the GNSS record limited to within 1 km of the tide gauge. The utility of ALT-TG estimates was significantly improved by using gridded altimetry products no closer than 30 km from the open coast. When compared directly with measured GNSS solutions from NASA JPL, Systeme d'Observation du Niveau des Eaux Littorales (SONEL), and Nevada Geodetic Laboratory (NGL), both ALT-TG VLM estimates agree with the NGL GNSS solution for 19 of the 20 locations (95% confidence interval). This is significant given that the NGL estimates are derived over the longest timeframe available from all three key GNSS data repositories considered. If one considers the limitations of altimetry sea surface height measurements in the coastal zone, the results using gridded sea surface height anomaly products for ALT-TG derived estimates of VLM are extremely encouraging for sea-level research.",
                "authors": "P. Watson",
                "citations": 8
            },
            {
                "title": "Visceral larva migrans detection using PCR-RFLP in BALB/c mice infected with Toxocara canis.",
                "abstract": "Toxocara canis is an important zoonotic roundworm distributed worldwide. The infective larvae of T. canis are one of the causes of visceral larva migrans (VLM), a clinical syndrome in humans. Diagnosing VLM is difficult, and the differential diagnosis of the larval development stage is limited. Therefore, this experimental research aimed to diagnose T. canis larvae using a molecular method, not only in liver tissue, which is the most commonly affected tissue, but also in the limb muscles, lungs and brain tissues. For this purpose, 24 BALB/c mice were infected with 1000 embryonated T. canis eggs. Necropsies were performed on the second, fourth, seventh and 14th days post-infection. While a part of the samples were digested with pepsin-HCl, the molecular method was used for the remainder of the samples to replicate the mitochondrial DNA adenosine triphosphate (ATP) synthase subunit-6 gene region of T. canis. BbsI, a restriction endonuclease, was used to determine the specificity of the amplicons obtained from Polymerase chain reaction (PCR). The detection limit for embryonated eggs was recorded. The PCR results showed that the sensitivity of the PCR analysis was 83.3% in the liver (with 88.8% accuracy), 87.5% in the lungs (with 91.6% accuracy) and 75.0% in the brain, forelimb and hindlimb muscles (with 83.3% accuracy). In all tissues, the test specificity was determined to be 100%. In this study, the molecular method was applied to only experimentally infected BALB/c mice tissues; thus, it is suggested that it can be also employed in different paratenic hosts and materials possibly infected with T. canis.",
                "authors": "G. Özbakış, A. Doğanay",
                "citations": 7
            },
            {
                "title": "Development and validation of an enhanced semi-empirical method for estimation of aerodynamic characteristics of light, propeller-driven airplanes",
                "abstract": "This study is intended to introduce an enhanced semi-empirical method for estimation of longitudinal and lateral-directional stability and control derivatives in the preliminary design phase of light airplanes. Specialised for light, single or twin propeller-driven airplanes, available state-of-the-art analytical procedures and design data compendia are combined and modified in a unique compatible method, and automated in NAMAYEH software. In the present study, modified procedures and the software structure are presented. Afterwards, the proposed method is applied to a four-place, low wing, single-engine, propeller-driven general aviation airplane. In order to validate the proposed method, the estimated aerodynamic characteristics are compared with the wind tunnel test data as well as DATCOM and VLM-based method estimations. The results indicate that the proposed method is able to predict the aerodynamic characteristics in an acceptable range of accuracy from zero-lift to stall conditions in all configurations.",
                "authors": "M. Rostami, S. Bagherzadeh",
                "citations": 8
            },
            {
                "title": "Verbal learning and memory in prelingually deaf children with cochlear implants",
                "abstract": "Abstract Objective: Deaf children with cochlear implants (CIs) show poorer verbal working memory compared to normal-hearing (NH) peers, but little is known about their verbal learning and memory (VLM) processes involving multi-trial free recall. Design: Children with CIs were compared to NH peers using the California Verbal Learning Test for Children (CVLT-C). Study sample: Participants were 21 deaf (before age 6 months) children (6–16 years old) implanted prior to age 3 years, and 21 age-IQ matched NH peers. Results: Results revealed no differences between groups in number of words recalled. However, CI users showed a pattern of increasing use of serial clustering strategies across learning trials, whereas NH peers decreased their use of serial clustering strategies. In the CI sample (but not in the NH sample), verbal working memory test scores were related to resistance to the build-up of proactive interference, and sentence recognition was associated with performance on the first exposure to the word list and to the use of recency recall strategies. Conclusions: Children with CIs showed robust evidence of VLM comparable to NH peers. However, their VLM processing (especially recency and proactive interference) was related to speech perception outcomes and verbal WM in different ways from NH peers.",
                "authors": "W. Kronenberger, Shirley C. Henning, Allison M. Ditmars, Adrienne S. Roman, D. Pisoni",
                "citations": 8
            },
            {
                "title": "TASK1 and TASK3 Are Coexpressed With ASIC1 in the Ventrolateral Medulla and Contribute to Central Chemoreception in Rats",
                "abstract": "The ventrolateral medulla (VLM), including the lateral paragigantocellular nucleus (LPGi) and rostral VLM (RVLM), is commonly considered to be a chemosensitive region. However, the specific mechanism of chemoreception in the VLM remains elusive. Acid-sensing ion channels (ASICs), a family of voltage-independent proton-gated cation channels, can be activated by an external pH decrease to cause Na+ entry and induce neuronal excitability. TWIK-related acid-sensitive potassium channels (TASKs) are members of another group of pH-sensitive channels; in contrast to AISICs, they can be stimulated by pH increases and are inhibited by pH decreases in the physiological range. Our previous study demonstrated that ASICs take part in chemoreception. The aims of this study are to explore whether TASKs participate in the acid sensitivity of neurons in the VLM, thereby cooperating with ASICs. Our research demonstrated that TASKs, including TASK1 and TASK3, are colocalized with ASIC1 in VLM neurons. Blocking TASKs by microinjection of the non-selective TASK antagonist bupivacaine (BUP), specific TASK1 antagonist anandamide (AEA) or specific TASK3 antagonist ruthenium red (RR) into the VLM increased the integrated phrenic nerve discharge (iPND), shortened the inspiratory time (Ti) and enhanced the respiratory drive (iPND/Ti). In addition, microinjection of artificial cerebrospinal fluid (ACSF) at a pH of 7.0 or 6.5 prolonged Ti, increased iPND and enhanced respiratory drive, which were inhibited by the ASIC antagonist amiloride (AMI). By contrast, microinjection of alkaline ACSF decreased iPND and respiratory drive, which were inhibited by AEA. Taken together, our data suggest that TASK1 and TASK3 are coexpressed with ASIC1 in the VLM. Moreover, TASK1 and TASK3 contribute to the central regulation of breathing by coordinating with each other to perceive local pH changes; these results indicate a novel chemosensitive mechanism of the VLM.",
                "authors": "Xia Wang, Ruijuan Guan, Xiaomei Zhao, Da-nian Zhu, Nana Song, Lin-lin Shen",
                "citations": 8
            },
            {
                "title": "COMPARATIVE STUDY OF WING LIFT DISTRIBUTION ANALYSIS USING NUMERICAL METHOD",
                "abstract": "This research focuses on calculating the force distribution on the wings of the LSU 05-NG aircraft by several numerical methods. Analysis of the force distribution on the wing is important because the wing has a very important role in producing sufficient lift for the aircraft. The numerical methods used to calculate the lift force distribution on the wings are Computational Flow Dynamics (CFD), Lifting Line Theory, Vortex Lattice Method and 3D Panel Method. The numerical methods used will be compared with each other to determine the accuracy and time required to calculate wing lift distribution. Because CFDs produce more accurate estimates, CFD is used as the main comparison for the other three numerical methods. Based on calculations performed, 3D Panel Method has an accuracy that is close to CFD with a shorter time. 3D Panel Method requires 400 while CFD 1210 seconds with results that are not much different. While LLT and VLM have poor accuracy, however, shorter time is needed. Therefore to analyze the distribution of lift force on the wing it is enough to use the 3D Panel Method due to accurate results and shorter computing time.",
                "authors": "Angga Septiyana, A. Rizaldi, K. Hidayat, Yusuf Giri Wijaya",
                "citations": 3
            },
            {
                "title": "Hand over face segmentation using MPSPNet",
                "abstract": "Accurate hand segmentation is vital in many applications where the hands play a central role. Examples include sign language recognition, action recognition, and gesture recognition. A relatively unexplored obstacle to correct hand segmentation is the case of hands overlapping with the face. Inspired by the hand over face segmentation problem, we have developed the novel Multi-level Pyramid Scene Parsing Network (MPSPNet) for semantic segmentation. We evaluate MPSPNet on two standard object segmentation datasets (NYUDv2, PASCAL VOC) and two recently published and challenging datasets focusing on scenarios in which the hands overlap the face, VLM-HandOverFace and HOF. Additionally, we compare our method against several state-of-the-art hand segmentation methods such as RefineNet and PSPNet. We empirically show that the proposed method achieves a 6% improvement in mIOU compared with RefineNet on the VLM-HandOverFace dataset and a 15% improvement in mIOU compared with PSPNet on the HOF dataset.",
                "authors": "Sakher Ghanem, Alex Dillhoff, Ashiq Imran, V. Athitsos",
                "citations": 3
            },
            {
                "title": "Wing Optimisation for Tractor Propeller Configurations: Validation and Application of Low-Order Numerical Models Adapted to Include Propeller-Induced Velocities",
                "abstract": "Even though propellers are the oldest form of propulsion, they are still a popular choice for unmanned aerial vehicles (UAVs) and passenger aircraft in certain market segments. If the propellers are mounted on the wing, strong propeller-wing interactions alter the aerodynamic efficiency of the aircraft. Using low-order numerical models, it is shown in literature that the wing chord and twist distribution can be changed to maximise this efficiency. These results are only theoretical. This thesis, therefore, aims to validate and apply a numerical model for optimisation of the wing design taking propeller-wing interactions into account. A vortex lattice method (VLM) was adapted to include the effects of propeller-induced velocities. Comparing the results of the adapted VLM with existing experimental data already validates the numerical model for predicting the lift distribution. To validate it for changes in lift distribution due to wing design changes, a wind-tunnel experiment is set up. Two wings are tested in a tractor propeller configuration. The only difference between the wings is in the twist distribution. To find the lift distribution, the circulation is evaluated in the flow around a wing at several stations along the wingspan. Particle-image velocimetry was used to obtain this flow field. Indeed, the lift distributions measured on both wings are matched by predictions from the adapted VLM, which proofs the numerical model is suitable for qualitative optimisation studies. For the wing and operating conditions used for the wind-tunnel experiment, an optimisation study is performed using the adapted VLM. It shows the drag can be reduced with 34% by adopting the optimal chord and twist distribution. Even though the operating conditions are not representative for full-scale aircraft or UAVs, it does show there is a great potential for taking propeller-wing interactions into account for the design of the wing.",
                "authors": "Kitso Epema",
                "citations": 8
            },
            {
                "title": "Seroepidemiological Study of Toxocariasis in Children Aged 6–14 Year Old in Sanandaj, Western Iran",
                "abstract": "Background: Toxocariasis is a disease caused by Toxocara nematodes and occurs from consuming their eggs. The main hosts of these worms are dogs and cats. The disease in humans becomes a visceral larva migrans (VLM). This descriptive cross-sectional study was conducted to determine the prevalence of toxocariasis in children aged 6–14 years. Methods: This cross-sectional descriptive study was conducted from Jun 1 2016 to Dec 1 2017 in Sanandaj, west of Iran. A total of 182 serum samples were collected from children age 6 to14 yr referred to medical diagnostic laboratories. Demographic data (age, sex, and parents’ literacy status), clinical signs (cough, headache, fever, abdominal pain), and the history of contact with dogs and cats was collected by a questionnaire. The presence of anti-Toxocara IgG antibody was detected by T. canis IgG ELISA (IBL, Germany) kit. Results: Of 182 subjects, 97 (53.3%) were male and 85 (46.7%) female. The average age was 9.2 years. Antibodies against T. canis were positive in three cases (1.65%) of all the studied subjects. Conclusions: The results showed a low prevalence of toxocariasis in children studied.",
                "authors": "Y. Maroufi, A. Faridi, M. Khademerfan, F. Bahrami, G. Zamini",
                "citations": 2
            },
            {
                "title": "Aerodynamic Design and Optimization of Bionic Wing Based on Wandering Albatross",
                "abstract": null,
                "authors": "Weigang An, Fuzhen Shi, Shi-xin He, Wen Wang, Hang Zhang, Liu Liu",
                "citations": 2
            },
            {
                "title": "Toxocara canis Mimicking a Metastatic Omental Mass from Sigmoid Colon Cancer: A Case Report",
                "abstract": "Toxocara canis is an important roundworm of canids and a fearsome animal parasite of humans. Human infections can lead to syndromes called visceral larva migrans (VLM), ocular larva migrans, neurotoxocariasis, and covert toxocariasis. VLM is most commonly diagnosed in children younger than 8 years of age, but adult cases are relatively frequent among those infected by ingesting the raw tissue of paratenic hosts in East Asia. This research reports the case of a 59-year-old man with sigmoid colon cancer, who visited our institution for surgery. An intraperitoneal mass was found on preoperative computed tomography, and it was thought to be a metastatic mass from sigmoid colon cancer. A postoperative histologic examination and serum test showed eosinophilic granuloma due to toxocariasis. Diagnosis of VLM is often difficult and highly suspicious in adults. Researchers suggest, although rarely, that VLM be included in the differential diagnosis as a cause of intraperitoneal tumors.",
                "authors": "Han-Gil Kim, J. Yang, Soon-Chan Hong, Young-Joon Lee, Young-Tae Ju, Chi-Young Jeong, Jin-Kwon Lee, Seung-Jin Kwag",
                "citations": 6
            },
            {
                "title": "Space Occupying Lesion in the Liver Caused by Hepatic Visceral Larva Migrans: A Case Report.",
                "abstract": "Visceral larva migrans (VLM) is one of the clinical syndromes of human toxocariasis. We report a case of hepatic VLM presenting preprandial malaise and epigastric discomfort in a 58-year-old woman drinking raw roe deer blood. The imaging studies of the abdomen showed a 74-mm hepatic mass featuring hepatic VLM. Anti-Toxocara canis immunoglobulin G (IgG) was observed in enzyme-linked immunosorbent assay (ELISA) and western blot. Despite anthelmintic treatment, the patient complained of newly developed cough and skin rash with severe eosinophilia. Hepatic lesion increased in size. The patient underwent an open left lobectomy of the liver. After the surgery, the patient was free of symptoms such as preprandial malaise, epigastric discomfort, cough, and skin rash. Laboratory test showed a normal eosinophilic count at postoperative 1 month, 6 months, 1 year, and 4 years. The initial optical density value of 2.55 of anti-T. canis IgG in ELISA was found to be negative (0.684) at postoperative 21 months. Our case report highlights that a high degree of clinical suspicion for hepatic VLM should be considered in a patient with a history of ingestion of raw food in the past, presenting severe eosinophilia and a variety of symptoms which reflect high worm burdens. Symptom remission, eosinophilia remission, and complete radiological resolution of lesions can be complete with surgery.",
                "authors": "Kye-Yeung Park, H. Park, Hwan-Sik Hwang, J. Ryu, K. Lee, K. Jang",
                "citations": 6
            },
            {
                "title": "High-Fidelity Aeroelastic Loads Calculation for a Transport Aircraft Configuration Including Pitch and Roll Maneuvers",
                "abstract": null,
                "authors": "Johan Feldwisch, M. Schulze",
                "citations": 1
            },
            {
                "title": "A Video-Annotated Learning Review System with Vocabulary Learning Mechanism to Facilitate English Listening Comprehension",
                "abstract": "With the rapid development of information technology, computer assisted language learning (CALL) has become a developing trend. This study proposes a novel video- annotated learning and review system with vocabulary learning mechanism (VALRS-VLM), which can assist learners to manually mark any video section that they cannot fully understand, and then clarify the pronunciation and usage of the unfamiliar vocabulary words appearing in the video section through an online dictionary, so as to enhance learners’ English listening performance and perception. To verify the effectiveness of the proposed VALRS-VLM, this study examines the effects of the learners in the experimental group using the VALRS-VLM with those in the control group using the video-annotated learning and review system without vocabulary learning mechanism (VALRS-NVLM) on English listening comprehension performance, learning satisfaction, technology acceptance, cognitive load, and learning retention. Analytical results show that the learners in the experimental group achieved remarkably better-listening comprehension performance and learning retention than those in the control group. Besides, both groups of the learners showed good learning satisfaction and technology acceptance, and low cognitive load. This study confirms that the proposed VALRS-VLM could effectively assist learners in facilitating English listening comprehension performance as well as achieve good learner perceived usefulness and easy of use.",
                "authors": "Zi-Han Yu, Ming-Chaun Li, Chih-Ming Chen, Mei-Feng Lin",
                "citations": 1
            },
            {
                "title": "VERTICAL LAND MOTION AND INUNDATION PROCESSES BASED ON THE INTEGRATION OF REMOTELY SENSED DATA AND IPCC AR5 SCENARIOS IN COASTAL SEMARANG, INDONESIA",
                "abstract": "Vertical land motion (VLM) is an important indicator in obtaining information about relative sea-level rise (SLR) in the coastal environment, but this remains an area of study poorly investigated in Indonesia. The purpose of this study is to investigate the significance of the influence of VLM and SLR on inundation. We address this issue for Semarang, Central Java, by estimating VLM using the small baseline subset time series interferometry SAR method for 24 Sentinel-1 satellite data for the period March 2017 to May 2019. The interferometric synthetic aperture radar (InSAR) method was used to reveal the phase difference between two SAR images with two repetitions of satellite track at different times. The results of this study indicate that the average land subsidence that occurred in Semarang between March 2017 and May 2019 was from (-121) mm/year to + 24 mm/year. Through a combination of VLM and SLR scenario data obtained from the Intergovernmental Panel on Climate Change (IPCC), it was found that the Semarang coastal zone will continue to shrink due to inundation (forecast at 7% in 2065 and 10% in 2100).",
                "authors": "M. R. Nandika, S. B. Susilo, V. Siregar",
                "citations": 1
            },
            {
                "title": "Parotid Gland Venolymphatic Malformation Presentation As Macroglossia.",
                "abstract": "Venolymphatic malformations (VLM) are the rare congenital disorders but the parotid gland VLMs are the rarest. Most of the parotid lesions present with unilateral swellings. Aetiology is unknown. Interestingly, this case came in OPD with the macroglossia and only complaint was cosmetic problem. Diagnosis was confirmed on the basis of Magnetic resonance imaging which is gold standard. Doppler ultrasonography showed low flow. Intra lesion electro cautery was done. There is need to focus on malformations and work to find out the causes.",
                "authors": "Nafeesa Batool Kazmi, Kumail Sajjad, Fouzia Waqar, S. Khan, A. Ghafar",
                "citations": 1
            },
            {
                "title": "Vergleich von Wirbelschleppensimulationen für treibstoffsparenden Formationsflug",
                "abstract": "Eine Moglichkeit den Treibstoffverbrauch zu senken besteht darin, das Aufwindfeld von Wirbelschleppen vorausfliegender Flugzeuge auszunutzen. Dieses Prinzip machen sich Zugvogel im Formationsflug zunutze. Fur Piloten ist der manuelle Formationsflug im treibstoffsparenden Optimalpunkt zu arbeitintensiv. Diese Aufgabe muss ein Flugregler ubernehmen. Die Auslegung eines solchen Formationsflugreglers erfordert ein Wirbelschleppensimulationsmodell, welches die von der Wirbelschleppe induzierten Windfelder berechnet. In diesem Beitrag werden zwei verschiedene Wirbelschleppensimulationsmodelle fur das Szenario eines stationaren Horizontalflugs verglichen und anhand des Ergebnisses des etablierten Stangenwirbelmodells beurteilt. Die zu vergleichenden Simulationsmodelle nutzen einerseits die Prandtlsche Traglinientheorie (engl.: Lifting Line Method, LLM) und andererseits eine instationare Wirbelgittermethode (engl.: Vortex Lattice Method, VLM). Beide Methoden basieren auf der Lagrangeschen Betrachtungsweise der Bewegung von Fluidteilchen, bei der einzelne Fluidteilchen an vordefinierten Tragflachenpositionen in die Nachlaufstromung ubergeben werden und sich dort frei bewegen konnen. Die Bewegung der Fluidteilchen im Nachlauf fuhrt zur Bildung von Streichlinien. Die Streichlinien werden durch abschnittsweise gerade Wirbelfilamente approximiert. Die Wirbelstarke der Wirbelschleppe wird durch Zirkulationswerte der Wirbelfilamente diskretisiert. Aus den Lage der Wirbelfilamente und ihrer Zirkulation kann das Windfeld der Wirbelschleppe berechnet werden. Die LLM-Simulation basiert auf starker vereinfachten Annahmen als die VLM. Die Gegenuberstellung beider Modelle leistet einen Beitrag, Modellierungsfehler zu untersuchen, welche durch diese Vereinfachungen entstehen konnen. Der Vergleich zeigt auch, dass die Ergebnisse beider Berechnungsmethoden in Bezug auf die Wirbelfilamentpositionen und Windfelder bei identischen Parametern ahnlich sind. Die in der LLM umgesetzten Vereinfachungen fuhren zu einer mit der VLM vergleichbaren Wirbelschleppenberechnung im Falle eines stationaren Horizontalfluges. Die Abhangigkeit der Verfahren von ihren Parametern und die Konvergenz hin zu feineren Parametern wird untersucht. Daraus werden Empfehlungen fur Parameterwerte abgeleitet.",
                "authors": "H. Spark, R. Luckner",
                "citations": 1
            },
            {
                "title": "Effect of video learning multimedia toward health belief model of self breast examination",
                "abstract": "Breast cancer is the most commonly diagnosed cancer and the second leading cause of death from cancer for women. The prevalence of cancer in Indonesia is 1.4 per 1000 inhabitants or about 330,000 people. VLM (Video Learning Multimedia) is a learning tool or medium using video or mobile display, using a combination of text, graphics, sound, video and animation. Aim of study was measure effect of VLM toward self breast examination behavior. This type of research is quantitative research with a population of 135. As for the number of samples in this study as many as 60 respondents, which are divided into two groups namely, intervention group and control group. We used paired sample test with 95% confidence level, obtained p-value= 0.001, meaning there was a difference in attitude before and after being educated on breast check behavior (SADARI) in the control group. Factors that facilitate the change in a person's behavior, consisting of knowledge, attitudes, and cultural values.  It is expected that it will be useful for mothers in Buakana Village rappocini sub-district and society especially women who are more at high risk of breast cancer to not only study but perform breast examination themselves (SADARI) as an early detection of breast cancer",
                "authors": "F. Gobel, Nurfitriani Nurfitriani, S. Samsualam",
                "citations": 1
            },
            {
                "title": "The imprints of contemporary mass redistribution on regional sea\nlevel and vertical land motion observations",
                "abstract": "Abstract. We derive trends and monthly anomalies in global and regional sea-level and solid-earth deformation that result from mass redistribution observed by GRACE and an ensemble of GIA models. With this ensemble, we do not only compute mean changes, but we also derive uncertainty estimates of all quantities. We find that over the GRACE era, the trend in land mass change has led to a sea-level trend of 1.28–1.82 mm/yr, which is driven by ice mass loss, while terrestrial water storage has increased over the GRACE period, causing a sea-level drop of 0.11–0.47 mm/yr. This redistribution of mass causes sea-level and deformation patterns that do not only vary in space, but also in time. The temporal variations affect GNSS-derived vertical land motion (VLM) observations, which are now commonly used to correct tide-gauge observations. We find that for many GNSS stations, including GNSS stations in coastal locations, solid-earth deformation resulting from present-day mass redistribution causes trends in the order of 1 mm/yr or higher. Since GNSS records often only span a few years, these trends are generally not representative for the tide-gauge records, which often span multiple decades, and extrapolating them backwards in time could cause substantial biases. To avoid this possible bias, we computed trends and associated uncertainties for 8228 GNSS stations after removing deformation due to GIA and present-day mass redistribution. With this separation, we are able to explain a large fraction of the discrepancy between observed sea-level trends at multiple long tide-gauge records and the reconstructed global-mean sea-level trend from recent reconstructions.\n",
                "authors": "T. Frederikse, F. Landerer, L. Caron",
                "citations": 4
            },
            {
                "title": "Model of a Ducted Axial-Flow Hydrokinetic Turbine – Results of Experimental and Numerical Examination",
                "abstract": "Abstract The article presents the numerical algorithm of the developed computer code which calculates performance characteristics of ducted axial-flow hydrokinetic turbines. The code makes use of the vortex lattice method (VLM), which has been developed and used in IMP PAN for years, to analyse the operation of various fluid-flow machines. To verify the developed software, a series of model tests have been performed in the cavitation tunnel being part of IMP PAN research equipment.",
                "authors": "A. Góralczyk, A. Adamkowski",
                "citations": 5
            },
            {
                "title": "Activation of catecholamine neurons in the ventral medulla reduces CCK-induced hypophagia and c-Fos activation in dorsal medullary catecholamine neurons.",
                "abstract": "Catecholamine (CA) neurons within the A1 and C1 cell groups in the ventrolateral medulla (VLM) potently increase food intake when activated by glucose deficit. In contrast, CA neurons in the A2 cell group of the dorsomedial medulla are required for reduction of food intake by cholecystokinin (CCK), a peptide that promotes satiation. Thus dorsal and ventral medullary CA neurons are activated by divergent metabolic conditions and mediate opposing behavioral responses. Acute glucose deficit is a life-threatening condition, and increased feeding is a key response that facilitates survival of this emergency. Thus, during glucose deficit, responses to satiation signals, like CCK, must be suppressed to ensure glucorestoration. Here we test the hypothesis that activation of VLM CA neurons inhibits dorsomedial CA neurons that participate in satiation. We found that glucose deficit produced by the antiglycolytic glucose analog, 2-deoxy-d-glucose, attenuated reduction of food intake by CCK. Moreover, glucose deficit increased c-Fos expression by A1 and C1 neurons while reducing CCK-induced c-Fos expression in A2 neurons. We also selectively activated A1/C1 neurons in TH-Cre+ transgenic rats in which A1/C1 neurons were transfected with a Cre-dependent designer receptor exclusively activated by a designer drug (DREADD). Selective activation of A1/C1 neurons using the DREADD agonist, clozapine- N-oxide, attenuated reduction of food intake by CCK and prevented CCK-induced c-Fos expression in A2 CA neurons, even under normoglycemic conditions. Results support the hypothesis that activation of ventral CA neurons attenuates satiety by inhibiting dorsal medullary A2 CA neurons. This mechanism may ensure that satiation does not terminate feeding before restoration of normoglycemia.",
                "authors": "Ai-jun Li, Qing Wang, S. Ritter",
                "citations": 5
            },
            {
                "title": "Glycinergic neurotransmission in the rostral ventrolateral medulla controls the time course of baroreflex‐mediated sympathoinhibition",
                "abstract": "To maintain appropriate blood flow to various tissues of the body under a variety of physiological states, autonomic nervous system reflexes regulate regional sympathetic nerve activity and arterial blood pressure. Our data obtained in anaesthetized rats revealed that glycine released in the rostral ventrolateral medulla (RVLM) plays a critical role in maintaining arterial baroreflex sympathoinhibition. Manipulation of brainstem nuclei with known inputs to the RVLM (nucleus tractus solitarius and caudal VLM) unmasked tonic glycinergic inhibition in the RVLM. Whole‐cell, patch clamp recordings demonstrate that both GABA and glycine inhibit RVLM neurons. Potentiation of neurotransmitter release from the active synaptic inputs in the RVLM produced saturation of GABAergic inhibition and emergence of glycinergic inhibition. Our data suggest that GABA controls threshold excitability, wherreas glycine increases the strength of inhibition under conditions of increased synaptic activity within the RVLM.",
                "authors": "Hong Gao, W. Korim, S. Yao, C. Heesch, A. Derbenev",
                "citations": 5
            },
            {
                "title": "Improvement of low temperature impact toughness through flux modification for submerged arc welded low carbon steel E350 plates",
                "abstract": "Objectives: To analyse the effects of rutile (TiO2) and Alumina (Al2O3) on impact toughness properties of Submerged arc welded joints for low temperature applications. Methods: Al2O3 and TiO2 have been added to F7AZ/PZ-EL8 Submerged arc welding (SAW) flux in varying quantities and the base material used for joining is low carbon structural steel having E350 grade. To verify the effect of these elements by flux modification, the Charpy V-Notch impact tests, scanning electron microscope (SEM) fractography and visible light microscopy (VLM) have been conducted. Findings: Charpy V-Notch impact testing and analysis have shown that there is considerable improvement in the impact properties of welds with the addition of 20% TiO2 and 20% Al2O3 in the 80% of flux as separate constituents i.e., when these elements have been added in the flux in separate form or two new modified fluxes were prepared, where one is doped with alumina and other is with rutile. Fractography of welds with addition of 20% alumina in 80% F7AZ/PZ-EL8 SAW fluxes are having best impact properties at lower temperature and addition of 20% rutile also as almost effective as alumina in flux. consequently, mode of failure is ductile fracture with dimples as a result of good amount of acicular ferrite microstructure. Novelty: Analysis of previous studies implicated that there is lack of significant studies in the low temperature impact properties of low carbon steel submerged arc welds. Present study shows that the significant improvement in the toughness properties of welds at lower temperature of -40◦C can be achieved through flux modification, it would ensure the application of Low carbon steel structures at lower working temperature. The applications may be pressure vessels, ships or containers. \nKeywords: Submerged arc welding (SAW); low temperature impact toughness; flux modification; low carbon steels",
                "authors": "Lucky, H. Arya",
                "citations": 0
            },
            {
                "title": "Supporting Transition from Out of Home Care to Adulthood: Linked Data Is Central to The Compass Social Impact Bond",
                "abstract": "The COMPASS program supports young people to successfully transition from Out of Home Care to adulthood. COMPASS is a social impact bond partnership between Victorian government, not-for-profit organisations and investors. COMPASS uses linked data in every phase from design, to implementation, to outcome measurement. \nIntroductionStudies demonstrates that young care leavers experience significantly poorer outcomes than their peers. COMPASS is a preventative program progressivelyproviding 200 care leavers with access to housing and individualised support. The Centre for Victorian Data Linkage (CVDL) developed the Victorian Linkage Map (VLM) in 2016, linking 20 plus health and human services datasets with births and deaths data. Linked data provides a critical evidence base for COMPASS. \nObjectives and ApproachThe presentation describes application of linked data for COMPASS design, implementation and measurement. Linked data cohort analysis of post-care service use of 6000 young people informed the program design. Linked data also provides the basis for a stratification tool to measure the complexity profile of referrals. Health, housing and justice payable outcomes are measured by comparing relative performance of participants with a matched control group using linked data. \nResultsImplementation of COMPASS has highlighted the value of linked data in service design and measurement of payable outcomes. It has also highlighted the challenges of using linked data in a real-world environment, including the need for thorough documentation and testing of specifications, calculations and processes. \nConclusion / ImplicationsThe use of linked data for COMPASS provides a model for evidence-based service design and tests the use of linked data for robust and sustainable outcome measurement. The lessons from COMPASS are applicable to other social impact bonds and service implementation and outcome measurement more broadly.",
                "authors": "Sharon Williams, M. Sipthorp, N. Ivkovic, A. Inglis",
                "citations": 0
            },
            {
                "title": "Pengaruh Promosi Kesehatan Dengan Vidio Learning Multimedia Terhadap Pengetahuan Dengan Sadari Di Kecamatan Rappoccini Kota Makassar",
                "abstract": "Latarbelakang: Kanker payudara merupakan kanker yang paling sering didiagnosis dan penyebab kematian utama kedua akibat kanker bagi wanita. Prevalensi kanker di Indonesia adalah 1,4 per 1000 penduduk atau sekitar 330.000 orang. VLM (Video Learning Multimedia) adalah sebuah alat atau media pembelajaran menggunakan video atau tampilan bergerak, media ini merupakan sebuah alat pembelajaran modern dikalangan masyarakat. Multimedia adalah penyampaian informasi menggunakan gabungan dari teks, grafik, suara, video dan animasi. Metode: jenis penelitian ini adalah penelitian kuantitatif dengan jumlah populasi sebanyak 135. Adapun jumlah sampel pada penelitian ini sebanyak 60 responden, yang terbagi menjadi dua kelompok yakni, kelompok intervensi dan kelompok kontrol. Hasil: Setelah dilakukan uji statistik dengan menggunakan uji Paired Sampel Test dengan tingkat kepercayaan 95%, diperoleh nilai p-value= 0,001, artinya terdapat perbedaan sikap sebelum dan sesudah di berikan edukasi terhadap perilaku periksa payudara sendiri (SADARI) pada kelompok kontrol. Faktor yang mempermudah terjadinya perubahan perilaku seseorang terhadap pengetahuan. Kesimpulan: Diharapkan akan berguna bagi ibu-ibu di Kelurahan Buakana Kecamatan Rappocini dan masyarakat khususnya perempuan yang lebih berisiko tinggi menderita kanker payudara agar tidak hanya mempelajari tetapi melakukan pemeriksaan payudara sendiri (SADARI) sebagai deteksi dini kanker payudara.",
                "authors": "Nurfitriani, F. A. Gobel, Samsualam",
                "citations": 0
            },
            {
                "title": "Aero-elastic tool implemented in a preliminary aircraft design program suite",
                "abstract": "The objective of this project was to build a versatile static aeroelastic tool that can be easily integrated into the software suite CEASIOMpy used for preliminary aircraft design. To meet this objective the partitioned loosely coupled approach is used, which means that separated solvers are used to solve the aerodynamics and the structural mechanics parts of the problem. However, this approach leads to the the problem of mismatched meshes. This problem has been solved during this master thesis by building the solver Aeroframe 2.0 that is based on the theory of virtual work. The aeroelastic (also called Fluid Structure Interaction (FSI)) simulations were made using state of the art validated solvers for both the ﬂuid and the structure problem. The computational ﬂuid dynamics (CFD) solvers used are the well known high ﬁdelity SU2 solver and the vortex lattice method (VLM) solver PyTornado. Concerning the low ﬁdelity three dimensional beam model, the ﬁnite element solver FramAT was chosen.",
                "authors": "J. P. Kuntzer",
                "citations": 0
            },
            {
                "title": "Providing A Person-Centred View: Applying Linked Data to Government Policy Reform and Service Design",
                "abstract": "The application of linked data by government has limited visibility due to complexities in publishing details of the analysis and interventions. The Victorian Department of Health and Human Services (DHHS) acknowledges the value of linked data in the department’s strategic plan, and linked data has been used in many policy reform and service design activities over the past three years. \nIntroductionIn 2016 the Centre for Victorian Data Linkage (CVDL), located in DHHS, developed the Victorian Linkage Map (VLM) of 20 plus health and human services datasets linked with births and deaths data. The VLM has since been expanded to include education, justice and police data. DHHS has been an “early adopter” in applying linked data to policy development, service reforms and departmental operations to improve the health and wellbeing of the Victorian population. \nObjectives and ApproachThe presentation will provide an overview of multi-sector data linkage in Victoria, and the value of frequent collaborations between CVDL and DHHS staff in applying linked data to priority projects and reforms. It will consider the challenges of cross-jurisdictional linkage in Australia, highlighting initiatives which are expanding linked datasets available to DHHS. \nResultsDHHS has undertaken a broad range of linked data projects which have provided an evidence base for departmental activities, including provision of hospital, housing, health surveillance and child protection services. Linked data has also been used to develop an integrated demand model, which forecasts impact of investment in one program area on other parts of the service system. \nConclusion / ImplicationsThe active use of linked data by DHHS provides a model for other government departments to improve service design and delivery to vulnerable populations. DHSS is expanding linked-data use to additional areas, and further imbedding in departmental operations.",
                "authors": "Sharon Williams, K. Cheng, M. Sipthorp",
                "citations": 0
            },
            {
                "title": "TOXOCARIASIS: VISCERAL AND OCULAR LARVA MIGRANS",
                "abstract": "Toxocariasis is a neglected socioeconomically important zoonotic nematode parasite that afflicts millions of the pediatric and adolescent populations worldwide, especially in impoverished communities. This disease is caused by infection with the larvae of Toxocara canis and T. cati, the most ubiquitous intestinal nematode parasite in dogs and cats, respectively. Human infections can lead to syndromes called visceral larva migrans (VLM), ocular larva migrans, neurotoxocariasis, and covert toxocariasis. Infection is not often fatal, but the inflammatory response to migrating larvae is associated withincreased leukocytosis, including generalized lymphadenopathy, endophthalmitis, granulomatous hepatitis, asthma, endomyocarditis, and high eosinophilia (>30%) as well as malignancy",
                "authors": "T. Morsy",
                "citations": 0
            },
            {
                "title": "Sea Level Rise in Macau and Adjacent Southern China Coast: Historical Change and Future Projections",
                "abstract": "\n <p>&#160; &#160; Global warming-related SLR (sea level rise) constitutes a substantial threat to Macau, due to its low elevation, small size and ongoing land reclamation. This study was devised to determine the long-term variation of sea level change in Macau, as well as to develop future projections based on tide gauge and satellite data and GCM simulations, aiming to provide knowledge for SLR mitigation and adaptation.</p><p>&#160; &#160; Based on local tide gauge records, sea level in Macau is now rising at an accelerated rate: 1.35 mm yr<sup>&#8722;1</sup> over 1925&#8211;2010 and jumping to 4.2 mm yr<sup>&#8722;1</sup> over 1970&#8211;2010, reflecting an apparent acceleration of SLR. Furthermore, the sea level near Macau rose 10% faster than the global mean during the period from 1993 to 2012. In addition, the rate of VLM (vertical land movement) at Macau is estimated at -0.153mm yr<sup>-1</sup>, contributing little to local sea level change.</p><p>&#160; &#160; In the future, as projected by a suite of climate models, the rate of SLR in Macau will be about 20% higher than the global average. This is induced primarily by a greater-than-average rate of oceanic thermal expansion in Macau, together with enhanced southerly anomalies that lead to a piling up of sea water. Specifically, the sea level is projected to rise 8&#8211;12, 22&#8211;51 and 35&#8211;118 cm by 2020, 2060 and 2100 with respect to the 1986&#8211;2005 baseline climatology, respectively, depending on the emissions scenario and climate sensitivity. If we consider the medium emissions scenario RCP4.5 along with medium climate sensitivity, Macau can expect to experience an SLR of 10, 34 and 65 cm by 2020, 2060 and 2100. If the worst case happens (RCP8.5 plus high climate sensitivity), the SLR will be far higher than that in the medium case; namely, 12, 51 and 118 cm by 2020, 2060, and 2100, respectively. The SLR under the lower emissions scenario is expected to be less severe than that under the higher emissions scenarios: by 2100, an SLR of 65&#8211;118 cm in Macau under RCP8.5, almost twice as fast as that under RCP2.6. The key source of uncertainty stems from the emissions scenario and poor knowledge of climate sensitivity. By 2020, the uncertainty range is only 4 cm, yet by 2100 the range will be increased to 83 cm.</p>\n",
                "authors": "Lin Wang, G. Huang, Wenxing Zhou, Wen Chen",
                "citations": 0
            },
            {
                "title": "Twist regulates Yorkie to guide lineage reprogramming of syncytial alary muscles",
                "abstract": "The genesis of syncytial muscles is typically considered as a paradigm for an irreversible developmental process. Notably, transdifferentiation of syncytial muscles is naturally occurring during Drosophila development. The ventral longitudinal heart-associated musculature (VLM) arises by a unique mechanism that revokes the differentiated fate from the so-called alary muscles and comprises at least two distinct steps: syncytial muscle cell fragmentation into single myoblasts and direct reprogramming into founder cells of the VLM lineage. Here we provide evidence that the mesodermal master regulator twist plays a key role during this reprogramming process. Acting downstream of Drosophila Tbx1 (Org-1) in the alary muscle lineage, Twist is crucially required for the derepression of the Hippo pathway effector Yki and thus for the initiation of syncytial muscle dedifferentiation and fragmentation. Subsequently, cell-autonomous FGFR-Ras-MAPK signaling in the resulting mono-nucleated myoblasts is maintaining Twist expression, thereby stabilizing nuclear Yki activity and inducing their lineage switch into the founder cells of the VLM.",
                "authors": "M. Rose, Jakob Bartle-Schultheis, K. Domsch, I. Reim, C. Schaub",
                "citations": 0
            },
            {
                "title": "Effect of high-intensity exercise on respiratory and peripheral muscle oxygenation in HF and COPD-HF patients",
                "abstract": "Purpose: To investigate the impact of high-intensity exercise on respiratory (Res) and peripheral muscle oxygenation (Pm) in coexistent chronic obstructive pulmonary disease (COPD)-heart failure (HF) and HF patients. Methods: Eleven COPD-HF and eleven HF patients were underwent evaluation by a lung function test and echocardiography. Another two different days: 1) patients performed cardiopulmonary exercise testing (CPX) and the constant load exercise (CE) (80% of peak work rate of CPX), was performed on a cycle ergometer until the limit of tolerance (Tlim). Relative blood concentrations of oxyhemoglobin ([O2Hb]), deoxyhemoglobin ([HHb]) of Res (intercostal muscles) and VLm (right vastus lateralis) were measured using near infrared spectroscopy. Results: Res, VLm [O2Hb], VLm ([HHb]) and Tlim presented increases in HF in contrast to COPD-HF patients (p Conclusion: COPD when associated to HF produce negative impact on blood flow of ventilatory muscles and exercise intolerance. Support: FAPESP: 2018/18133-0, 2015/26501-1, 2018/03233-0, CNPQ.",
                "authors": "F. Caruso, C. Goulart, José Carlos Bonjorno Júnior, A. Araujo, R. Mendes, A. Borghi‐Silva",
                "citations": 0
            },
            {
                "title": "Seasonal variation in water storage, vertical land motion and well levels: Implications for groundwater storage change in Central Valley",
                "abstract": "\n <p>The establishment of the Inter-Commission Committee on \"Geodesy for Climate Research\" (ICCC) of the International Association of Geodesy (IAG) emphasizes on the usefulness of geodetic sensors for estimating high-resolution water mass variation, which is due to broad applications of geodetic tools ranging from water cycle studies to water resources management. As such, data from both GRACE missions continue to provide insight into the alarming rates of groundwater depletion in large aquifers worldwide. Observations of vertical land motion (VLM) from GPS and InSAR may reflect elastic responses of the Earth's crust to changes in mass load, including those in aquifers. However, above confined aquifers, VLM observations are dominated by poroelastic deformation processes. In previous works, Ojha et al. 2018 and 2019 show that GRACE-based estimates of groundwater storage change in the Central Valley, California, are consistent with those obtained by utilizing measurements of surface deformation. These studies also show that annual variations in VLM correlate well in time with groundwater levels.</p><p>Here, we investigate seasonal variations in groundwater storage by identifying how their effect is manifested in geodetic and hydrological datasets. Groundwater well observations in the Central Valley indicate maximum groundwater levels at the beginning of the year between February to April and lowest water levels in the middle of the year about July to October. Meanwhile, GRACE groundwater storage estimates peak about four months later. To get insight into the mechanisms leading to this discrepancy, we perform a Wavelet multi-resolution analysis of GRACE TWS variations and complementary groundwater, snowcap, soil moisture, and reservoir level variations. We show that the majority of the differences between wavelet spectrums at seasonal frequencies occur during drought periods when there is no supply of precipitation in the high elevations. We employ a 1D diffusion model to demonstrate that the variations in groundwater levels across the Central Valley are due to the propagation of the pressure front at recharge sites due to gradual snowmelt. Such a model could explain the different timing of peaks in groundwater time series based on satellite gravimetry compared to deformation and well observations. We also discuss that winter rains are not able to directly contribute to recharging deep aquifers in the Central Valley, whereas most of the recharge must source from lateral flow caused by differential pressure at the sites of snow-melt in the Sierra Nevada as well as from agricultural return flows.</p><p>This analysis addresses the question of how well the different geodetic signals that reflect groundwater discharge and recharge processes agree with one another and what are the possible causes of disagreements. We emphasize the need for interdisciplinary efforts for the successful integration of available geodetic and hydrological datasets to improve our ability to utilizing geodetic sensors for climate research and water resources management.</p><p>References:</p><p>Ojha, C., Werth, S., & Shirzaei, M. (2019). JGR, https://doi.org/10.1029/2018JB016083.</p><p>Ojha, C., M. Shirzaei, S. Werth, D. F. Argus, and T. G. Farr (2018), WRR, https://doi.org/10.1029/2017WR022250.</p>\n",
                "authors": "S. Werth, M. Shirzaei",
                "citations": 0
            },
            {
                "title": "The occurence of anti-Toxocara IgG to follow visceral larva migrans infections in children attended in basic health care units from peripheral areas of the city of Lages, Santa Catarina, Brazil",
                "abstract": "Visceral larva migrans (VLM) is caused by Toxocara canis larvae infection, which is a small intestine parasite found mainly in dogs, infecting humans occasionally, causing inflammation and damage to several organs with clinical signs ranging from asymptomatic to nonspecific symptoms. The goal of this study was to investigate the occurrence of anti- T. canis IgG in children attended at Basic Health Care Units in the city of Lages, Santa Catarina Brazil. Children’s blood samples were collected by digital puncture, stored in filter papers and later analyzed by the Enzyme-linked Immunosorbent Assay (ELISA) to detect IgG antibodies against T. canis . Laboratory tests were conducted at the Zoology and Parasitology Laboratory of the Planalto Catarinense University (UNIPLAC). The research was conducted from July 2014 to May 2015, in children of both genders aged two to six years old, residing in the peripheral area of the city. Of the 82 children sampled, 43 were boys and 39 girls where 23.17% (19/82) aged five years old. Positivity for anti- T. canis IgG was 15.85% (13/82), i.e., 20.33% of all boys and 10.26% of the girls. Although the seroepidemiological anti- T. canis data in children at the national level present many variations, it is of paramount importance to know the parasitosis data atdifferent regions of Brazil, since the cities harbor an elevated number of urban dogs transiting in parks and schools.",
                "authors": "R. M. Quadros, Rafael de Lima Miguel, M. Ferrari, Thiago Monteiro Fronza, L. Miletti, Carlos José Raupp Ramos",
                "citations": 0
            },
            {
                "title": "Interactive comment on “The Zone of Influence: Matching sea level variability from coastal altimetry and tide gauges for vertical land motion estimation” by Julius Oelsmann et al",
                "abstract": "This paper addresses the methodology of estimating vertical land motion (VLM) from the combination of satellite altimetry (SAT) and tide gauge (TG) observations. The work by J. Oelsmann, M. Passaro et al. builds upon earlier studies concerning the selection of the most suitable SAT observations that show high temporal correlation with high-frequency TG observations. In addition, and contrary to past studies, they use dedicated coastal “retracked” along-track observations to reduce the VLM differences with respect to co-located GNSS VLM estimates, which are taken as ground truth.",
                "authors": "Christopher Watson Referee",
                "citations": 0
            },
            {
                "title": "Co-existing Tubercular Pleural Effusion and Visceral Larva Migrans in a Patient with Hypereosinophilia: A Rare Case",
                "abstract": "Eosinophilic pleural effusions accounts for 5-16% of all the cases of pleural effusion. Here the authors present a case of 21 years old male patient, with right-sided chest pain in whom peripheral blood eosinophilia along with eosinophilic pleural effusion were found after a series of relevant investigations and two causative factors were found for same i.e., Visceral Larva Migrans (VLM) and tubercular pleural effusion. Both of them individually can cause hypereosinophilia, but presence of them together makes this case rare and interesting.",
                "authors": "Taranpreet Kaur, N. Bansal, K. Goyal, Jaykrat Chaudhary",
                "citations": 0
            },
            {
                "title": "A tubular vaporizing liquid micro-thruster with induction heating",
                "abstract": null,
                "authors": "Bendong Liu, Xu Yang, Yuezong Wang, Desheng Li, Guohua Gao, Jiahui Yang, R. Zhou",
                "citations": 0
            },
            {
                "title": "A Buoyant Eifel Mantle Plume Revealed by GPS-Derived Large-Scale 3D Surface Deformation",
                "abstract": "\n <p> The Eifel hotspot is one of the few known active continental hotspots. The evidence is based on volcanism as recent as 11ka and a seismic velocity anomaly that shows a plume-like feature downward to at least the upper transition zone. However, the volcanism lacks a clear space-time progression of activity, and evidence for surface deformation has been ambiguous. Here, we show that the greater area above the Eifel plume shows a distinct and significant surface deformation anomaly not seen anywhere else in intraplate Europe. We use GPS data of thousands of stations in western Europe to image contemporary vertical land motion (VLM) and horizontal strain rates. We show significant surface uplift rates with a maximum of ~1.0 mm/yr (after subtracting the broader-scale VLM predicted by glacial isostatic adjustment) roughly centered on the Eifel Volcanic Field, and above the mantle plume. The same area that uplifts also undergoes significant N-S-oriented extension of ~3 nanostrain/yr, and this area is surrounded by a radial pattern of shortening. River terrace data have revealed tectonic uplift of <span>~</span>150&#8211;250 m of the Eifel since 800 ka, when recent volcanism and uplift reactivated, which would imply an average VLM of <span>0.1</span>&#8211;<span>0.3 mm/yr </span>since that time. Our VLM results suggest that the uplift may have accelerated significantly since Quaternary volcanism commenced. <span>The remarkable superimposition of significant uplift, horizontal extension, and volcanism strongly suggests a causal relationship with the underlying mantle plume. We</span><span> model the plume buoyancy as a half-space vertical force applied to a bi-modal Gaussian areal distribution exerted on a plane at 50 km depth. </span><span>Our modelling shows a good regional fit to the long-wavelength aspects of the surface deformation by applying buoyancy forces related to the plume head at the bottom of the lithosphere. From our spatially integrated force and the first-order assumption that the plume has effectively been buoyant since 250 ka (to explain Quaternary uplift) or 800 ka (at today&#8217;s rate), we estimate that a 360 km high plume requires density reduction of 57-184 kg m</span><sup><span>-3</span></sup><span> (i.e., ~0.7-5.6% of a 3300 kg m</span><sup><span>-3</span></sup><span> dense reference mantle), which is consistent with observed seismic velocity reductions. Finally, we note that the highest extension rates are centred on the Lower Rhine Embayment (LRE), where intraplate seismicity rates are high, and where paleoseismic events increased since 800 ka. We suggest that the surface uplift imposed by the Eifel plume explains the relatively high activity rate on faults along the LRE, particularly since the N-S extension would promote failure on the NW-SE trending faults in the LRE.</span></p>\n",
                "authors": "C. Kreemer, G. Blewitt, P. Davis",
                "citations": 0
            },
            {
                "title": "Importance of Northern Hemisphere Vertical Land Motion for Geodesy and Coastal Sea Levels",
                "abstract": "\n <p>Vertical Land Motion (VLM) is a composite of several earth dynamics caused by changes of earth&#8217;s surface load or tectonics. In most of the Northern Hemisphere mainly two dynamics are causing large scale vertical land motion &#8211; Glacial Isostatic Adjustment (GIA), which is the rebound from the loading of the latest glacial cycle (10-30 kyr ago) and elastic rebound from contemporary land ice changes, that happens immediately when loading is removed from the surface.</p><p>With glacial mass balance data and observations of the Greenland Ice Sheet we have created an Northern Hemisphere ice history from 1996-2015 that is used to make a model for elastic VLM caused by ice mass loss that varies in time.</p><p>It shows that, in most cases, the elastic VLM model is able to close gaps between GIA induced VLM and GNSS-measured VLM, giving confidence that the combined GIA + elastic VLM-model is a better alternative to adjust relative sea level measurements from tide-gauges (where no (reliable) GNSS-data is available) to absolute sea level than 'just' a GIA-model. In particular for Arctic Sea Level, where elastic uplifts are prominent and large coastal regions have limited in-situ data available, the VLM-model is useful for correcting Tide Gauge measurements and thereby validate satellite altimetry observed sea levels, which is challenged by sea ice in the coastal Arctic.</p><p>Furthermore, our elastic VLM-model shows, that the uplift caused by the melt of the Greenland Ice Sheet (GIS) is far-reaching and even in the North Sea region or along the North American coast show uplift rates in the order of 0.4-0.7 mm/yr from 1996-2015. Interestingly, this is roughly equivalent to Greenland&#8217;s sea level contribution in the same period, thereby 'neutralizing' the melt of GIS. As GIS ice mass loss continues to accelerate, the elastic uplift will have increased importance for coastal regions and future relative sea level projections. Unfortunately, the opposite effect is true for the southern hemisphere or vice versa if Antarctic ice sheet mass loss would increase.</p>\n",
                "authors": "C. Ludwigsen, O. Andersen, S. Khan, B. Marzeion",
                "citations": 0
            },
            {
                "title": "Multi-temporal Spaceborne InSAR technique to compensate Vertical Land Motion in Sea Level Change records: A case study of Tide gauges in Korean Peninsula",
                "abstract": "\n <p>Relative sea-level changes observed by tide gauges are commonly corrected for several components including crustal displacement, ocean dynamics, and vertical land motion. Vertical Land Motion (VLM) due to local land hydrology is a crucial component that observed as localized ground motion and varies with each tide gauges. Permanent GNSS stations are used to measure the VLM trend at tide gauges, however, only few tide gauges are equipped with collocated GNSS stations. Multi-temporal InSAR analysis provides ground displacements in both the spatial and temporal domains. Therefore, in our study, we applied the spaceborne Interferometric SAR technique to measure the local ground motion using Sentinel-1 SAR data. The Korean peninsula is surrounded by the East Sea/Sea of Japan, the Yellow Sea and the East China Sea have continuously monitoring tide gauges with a record length of more than 30 years. We acquire C-band Sentinel-1 SAR data (both ascending and descending mode) over the Korean Peninsula during 2014/11 and 2019/04. We estimate the high-resolution (~ 10 m) land motion at tide gauges (mm-level accuracy) over these 21 tide gauges and, compared with available collocated GNSS observations. 2D displacements (vertical and horizontal) are derived from ascending and descending mode InSAR displacements. The linear trend of VLM observed from our InSAR estimates is used to compensate for the relative velocity of sea-level changes observed from tide gauges.</p>\n",
                "authors": "Suresh Krishnan Palanisamy Vadivel, Duk‐jin Kim, Jungkyo Jung, Yang‐Ki Cho",
                "citations": 0
            },
            {
                "title": "Deterministic model construction for flutter characterisation of cantilever flat plates",
                "abstract": "Assessing exactly the flutter speed and frequency is of prior importance when designing an aircraft to ensure a safe flight envelope and to this purpose, experimental and numerical tests have been developed throughout the years. To simplify the computation of the critical speed and frequency, this master thesis proposes to build a mathematical model for flutter characterisation as a function of geometrical parameters of cantilever flat plates namely the Aspect Ratio, the taper ratio and the sweep angle, on the basis of the unsteady Vortex Lattice Method (VLM). The investigation of the hump mode activation as a function of those parameters is carried out in first place to assess its conditions of appearance and it is concluded that a straight discontinuity line can be drawn on the domain to distinguish the zone where the hump is active from the zone where it is not. Then, the linear regression theory is used for building the model and three different polynomial orders are compared. and conclusion is made that a incomplete second order model provides results reliable up to 99%. A comparison is made with experimental results obtained in wind tunnel. To do so, an excitation system is designed so that its eigenfrequencies do not interfere with those of the tested plates. Several issues encountered due to lack of time because of the sanitary crisis prevented to perform more than two tests for which the different problems could not all be checked and solved. In the end, a proper conclusion about the practical validity of the model could not be drawn.",
                "authors": "G. Dimitriadis, V. Terrapon",
                "citations": 0
            },
            {
                "title": "Spatio-temporal decomposition of geophysical signals in North America",
                "abstract": "\n <p>Sea level rise is one of the most significant consequences of projected future changes in climate. One factor which influences sea level rise is vertical land motion (VLM) due to glacial isostatic adjustment (GIA), which changes the elevation of the ocean floor. Typically, GIA forward models are used for this purpose, but these are known to vary with the assumptions made about ice loading history and Earth structure. In this study, we implement a Bayesian hierarchical modelling framework to explore a data-driven VLM solution for North America, with the aim of separating out the overall signal into its GIA and hydrology (mass change) components. A Bayesian spatio-temporal model is implemented in INLA using satellite (GRACE) and in-situ (GPS) data as observations.&#160;Under the assumption that GIA varies in space but is constant in time, and that hydrology is both spatially- and temporally-variable, it is possible to separate the contributions of each component with an associated uncertainty level. Early results will be presented. Extensions to the BHM framework to investigate sea level rise at the global scale, such as the inclusion of additional processes and incorporation of increased volumes of data, will be discussed.</p>\n",
                "authors": "A. Brady, J. Rougier, B. Vishwakarma, Y. Ziegler, R. Westaway, J. Bamber",
                "citations": 0
            },
            {
                "title": "Storage Allocation For Stocked and Buffer Baskets",
                "abstract": "The recent changes in the retailer landscape and the continuous labor shortage in Thailand and have caused many distribution centers to embrace material handling equipment capable of automatically storage and transporting products. This article presents the case study of a home improvement retailer that adopts a vertical lift module (VLM) system into one of its distribution centers. As automatic equipment dedicated for baskets, a VLM system transports two types of baskets, particularly available stock baskets preparing for picking and consolidated baskets waiting for shipping. A preliminary analysis of current operations reveals that the number of baskets allocated for each type is not suitable, resulting in low utilization of the equipment and storage space. After studying the historical data as well as nature of the business, we proposed a simulation to experiment with the effects of the following operating policies (1) the suitable number of baskets allocated for each type (2) storage policy of consolidated baskets and (3) retrieving patterns of consolidated baskets. The results suggest that the suitable level of each factor that reduces the space utilization of 6.67% and vehicles travel time of 37.5%. Nevertheless, travel times of incoming and outgoing lifters are close to similar represented by their utilization.",
                "authors": "Arunrat Walitsarangkul, O. Kittithreerapronchai",
                "citations": 0
            },
            {
                "title": "Scenarios of Twenty-First Century Mean Sea Level Rise at Tide-Gauge Stations Across Canada",
                "abstract": "ABSTRACT Existing scientific literature and international assessments, such as those by the Intergovernmental Panel on Climate Change, provide a wide range of projections for global mean sea level rise (SLR) in the twenty-first century. At the local scale, the ranges or uncertainties of projections are even larger. There is a pressing need to compile plausible local SLR scenarios to aid coastal communities with adaptation. Here we develop three local SLR scenarios for Canadian tide-gauge stations for the twenty-first century (Low, Intermediate, and High). Our Low Scenario is based on projections under the Representative Concentration Pathway 4.5 (RCP4.5) scaled down to the present global SLR rate. Our Intermediate Scenario is based on projections under the Representative Concentration Pathway 8.5 (RCP8.5), and our High Scenario is based on the RCP8.5 projections with an adjusted contribution from the Antarctic ice sheet. For all three scenarios, we use vertical land motion (VLM) from global positioning systems (GPS) data corrected for the present-day melt of glaciers and ice sheets instead of the commonly used VLM from a glacial isostatic adjustment (GIA) model. The GPS data include not only GIA but also other processes affecting VLM. For each scenario, larger SLR is projected along the southeastern Atlantic coast, the Pacific coast, and the Beaufort Sea coast than along other Canadian coasts in the twenty-first century. Under the Low, Intermediate, and High Scenarios, the median relative sea level along the southeastern Atlantic coast may rise by as much as 0.39, 0.82, and 0.96 m, respectively, over 2010–2100. The proposed scenarios allow coastal engineers and managers to consider multiple future conditions and develop multiple response options, as well as choose the most suitable option according to the risk tolerance of infrastructure.",
                "authors": "G. Han, Zhimin Ma, A. Slangen",
                "citations": 0
            },
            {
                "title": "Estimation of Vertical Land Motion at the Tide Gauges in Turkey",
                "abstract": "\n <p>This study aims to estimate vertical land motion (VLM) at tide gauges (TG), located in the Mediterranean, Aegean and the Marmara Sea coasts of Turkey, from differences of multimission satellite altimetry and TG sea level time series. Initially, relative sea level trends are estimated at 7 tide gauges stations operated by the Turkish General Directorate of Mapping over the period 2001-2019. Subsequently, absolute sea level trends independent from VLM are computed from multimission satellite altimetry data over the same period. We have computed estimates of linear trends of difference time series between altimetry and tide gauge sea level after removing seasonal signals by harmonic analysis from each time series to estimate the vertical land motion (VLM) at tide gauges. Traditional way of VLM determination at tide gauges is to use GPS@TG or preferably CGPS@TG data. We therefore, processed these GPS data, collected over the years by several TG-GPS campaigns and by continuous GPS stations close to the TG processed by GAMIT/GLOBK software. Subsequently, the GPS and CGPS vertical coordinate time series are used to estimate VLM. These two different VLM estimates, one from GPS and CGPS coordinate time series and other from altimetry-TG sea level time series differences are compared.</p><p>&#160;</p><p><strong>Keywords: Vertical land motion, Sea Level Changes, Tide gauge, Satellite altimetry, GPS, CGPS </strong></p>\n",
                "authors": "M. H. Erkoç, U. Doğan, S. Özarpacı, H. Yıldız, Erdinç Sezen",
                "citations": 0
            },
            {
                "title": "The Vastus Medialis obliquus and The Vastus Lateralis Muscle Atrophy is Existed in Patients with Patellofemoral Pain Syndrome",
                "abstract": "\n Background: whether the vastus medialis obliquus (VMO) atrophy exists in patients with PFPS and whether the amount of atrophy differs between the VMO and vastus lateralis muscle (VLM) is still obscure. Materials and methods: From June 2016 to March 2019, 61 patients with PFPS were collected into the study group, and an age, sex, and body mass index (BMI) matched cohort of 61 patients with normal knees were randomly selected into the control group. All enrolled subjects had undergone computed Tomography (CT) scans in the supine position. The cross-sectional area of the VMO and VLM in the sections of 0, 5, 10, 15, 20 mm above the upper pole of the patella were measured, and VMO/VLM area ratio were evaluated as well. Results: In the study group and the control group, the VMO area in the section that 0, 5, 10, 15, 20 mm above the upper pole of the patella were 732.64±306.43 mm2 and 941.66±366.83 mm2 (P<0.001), 876.32±341.47 mm2 and 1119.6±405.01 mm2 (P<0.001), 1039.31±410.21 mm2 and 1302.75±425.14 mm2 (P<0.001), 1178.26±449.10 mm2 and 1496.67±474.70 mm2 (P<0.001), 1289.78±487.78 mm2 and 1643.33±507.08 mm2 (P<0.001); the VLM area in the section that 0, 5, 10, 15, 20 mm above the upper pole of the patella were 127.61±66.74 mm2 and 192.2±152.40 mm2 (P=0.003), 183.47±85.41 mm2 and 262.55±187.98 mm2 (P=0.004), 250.66±133.70 mm2 and 352.35±291.96 mm2 (P=0.015), 326.06±139.94 mm2 and 466.27±343.11 mm2 (P=0.013), 574.19±390.00 mm2 (P=0.005); the VMO/ VLM area ratio in the section that 0, 5, 10, 15, 20 mm above the upper pole of the patella were 0.83±0.11 and 7.44±5.13 (P<0.001), 5.37±2.49 and 6.32±4.69 (P=0.168), 4.64±2.43 and 4.15±1.94 (P=0.554), 3.90±1.55 and 3.96±1.66 (P=0.434), 3.42±1.36 and 3.48±1.62 (P=0.826).Conclusion: In patients with PFPS, the VMO and VLM atrophy was existed in the section of 0-20 mm above the upper pole of the patella in comparison with normal people; and the atrophy of the VMO was more evident than that of the VLM in the section that 0-5 mm above the upper pole of the patella. These findings support the rationale for use of general quadriceps exercise combined with VMO strengthening exercise as part of rehabilitation program for patients with PFPS.",
                "authors": "C. Dong, Ming Li, Kuo Hao, Chao Zhao, K. Piao, Wei Lin, Chongyi Fan, Y. Niu, Fei Wang",
                "citations": 0
            },
            {
                "title": "Dynamic Response of a Morphing Wing",
                "abstract": null,
                "authors": "Patrizio Rosatelli, W. Lacarbonara, A. Arena, D. Inman",
                "citations": 0
            },
            {
                "title": "Effective temperature – radius relationship of M dwarfs",
                "abstract": "M-dwarf stars provide very favourable conditions for finding habitable worlds beyond our solar system. The estimation of the fundamental parameters of the transiting exoplanets relies on the accuracy of the theoretical predictions for radius and effective temperature of the host M dwarf, therefore it is important to conduct multiple empirical tests of very low-mass star (VLM) models. These stars are the theoretical counterpart of M dwarfs. Recent determinations of mass, radius, and effective temperature of a sample of M dwarfs of known metallicity have disclosed an apparent discontinuity in the effective temperature-radius diagram that corresponds to a stellar mass of about 0.2 M⊙. This discontinuity has been ascribed to the transition from partially convective to fully convective stars. In this paper we compare existing VLM models to these observations, and find that theory does not predict any discontinuity at around 0.2 M⊙, but a smooth change in slope of the effective temperature-radius relationship around this mass value. The appearance of a discontinuity is due to naively fitting the empirical data with linear segments. Moreover, its origin is not related to the transition to fully convective structures. We find that this feature is instead an empirical signature for the transition to a regime where electron degeneracy provides an important contribution to the stellar equation of state, and it constitutes an additional test of the consistency of the theoretical framework for VLM models.",
                "authors": "S. Cassisi, M. Salaris",
                "citations": 3
            },
            {
                "title": "Mediastinal venolymphatic malformations mimicking thymic carcinoma",
                "abstract": "This report was about a 60‐year‐old asymptomatic female patient who presented to our clinic with an anterior mediastinal mass found on routine chest computed tomography (CT). Chest CT revealed an irregular poorly enhanced anterior mediastinal mass which showed signs of infiltration to adjacent structures with sparse calcifications. The preliminary diagnosis was thymic carcinoma. The patient underwent extended thymectomy via median sternotomy and complete excision of the tumor. A small draining vein to the left brachiocephalic vein and phleboliths were identified in the tumor. A definitive diagnosis was made of mediastinal venolymphatic malformation (VLM). The patient had an uneventful clinical course and was discharged without further complication. This report highlights that it is possible to misdiagnose mediastinal VLM as thymic carcinoma and could serve as a useful reminder to physicians in the future.",
                "authors": "M. Kang, D. Kang, Youn-Ho Hwang, J. Y. Kim",
                "citations": 3
            },
            {
                "title": "Effect of Solid Forms on Physicochemical Properties of Valnemulin",
                "abstract": "To improve the physicochemical properties of valnemulin (VLM), different solid forms formed by VLM and organic acids, including tartaric acid (TAR), fumaric acid (FUM), and oxalic acid (OXA), were successfully prepared and characterized by using differential scanning calorimetry (DSC), scanning electron microscope (SEM), X-ray powder diffraction (XRPD), and Fourier-transform infrared spectroscopy (FT-IR). The excess enthalpy Hex between VLM and other organic acids was calculated by COSMOthermX software and was used to evaluate the probability of forming multi-component solids between VLM and organic acids. By thermal analysis, it was confirmed that multi-component solid forms of VLM were thermodynamically more stable than VLM itself. Through dynamic vapor sorption (DVS) experiments, it was found that three multi-component solid forms of VLM had lower hygroscopicity than VLM itself. Furthermore, the intrinsic dissolution rate of VLM and its multi-component forms was determined in one kind of acidic aqueous medium by using UV-vis spectrometry. It was found that the three multi-component solid forms of VLM dissolved faster than VLM itself.",
                "authors": "Jinbo Ouyang, Jian Chen, Liming Zhou, Fangze Han, Xin Huang",
                "citations": 3
            },
            {
                "title": "No associations between medial temporal lobe volumes and verbal learning/memory in emerging psychosis",
                "abstract": "Grey matter (GM) volume alterations have been repeatedly demonstrated in patients with first episode psychosis (FEP). Some of these neuroanatomical abnormalities are already evident in the at‐risk mental state (ARMS) for psychosis. Not only GM alterations but also neurocognitive impairments predate the onset of frank psychosis with verbal learning and memory (VLM) being among the most impaired domains. Yet, their interconnection with alterations in GM volumes remains ambiguous. Thus, we evaluated associations of different subcortical GM volumes in the medial temporal lobe with VLM performance in antipsychotic‐naïve ARMS and FEP patients. Data from 59 ARMS and 31 FEP patients, collected within the prospective Früherkennung von Psychosen study, were analysed. Structural T1‐weighted images were acquired using a 3 Tesla magnetic resonance imaging scanner. VLM was assessed using the California Verbal Learning Test and its factors Attention Span, Learning Efficiency, Delayed Memory and Inaccurate Memory. FEP patients showed significantly enlarged volumes of hippocampus, pallidum, putamen and thalamus compared to ARMS patients. A significant negative association between amygdala and pallidum volume and Attention Span was found in ARMS and FEP patients combined, which however did not withstand correction for multiple testing. Although we found significant between‐group differences in subcortical volumes and VLM is among the most impaired cognitive domains in emerging psychosis, we could not demonstrate an association between low performance and subcortical GM volumes alterations in antipsychotic‐naïve patients. Hence, deficits in this domain do not appear to stem from alterations in subcortical structures.",
                "authors": "L. Egloff, C. Lenz, E. Studerus, U. Heitz, F. Harrisberger, R. Smieskova, A. Schmidt, L. Leanza, C. Andreou, S. Borgwardt, A. Riecher-Rössler",
                "citations": 3
            },
            {
                "title": "Growth of colorectal liver metastases is not accelerated by intraportal administration of stem cells after portal vein embolization.",
                "abstract": "INTRODUCTION\nFuture liver remnant volume (FLRV) is a crucial factor impacting resectability of colorectal liver metastases (CLM). In case of low FLRV, augmentation can be done by performing portal vein embolization (PVE). However, there is a risk of progression of CLM between PVE and resection. Intraportal application of autologous hematopoietic stem cells (HSC) is a possibility to accelerate the growth of FLRV. The effect of thus applied SC on CLM progression still remains unclear, though.\n\n\nMETHODS\n63 patients underwent PVE between 2003 and 2015. In 20 patients a product with HSC was applied intraportally on the first day after PVE (PVE HSC group). HSC were gained from peripheral blood (10 patients) or bone marrow (10 patients). FLRV and volume of liver metastases (VLM) were evaluated by CT volumetry. The gained data were statistically evaluated in relation to the disease free interval (DFI), overall survival (OS), achievement of CLM resectability and progression of extrahepatic metastases. We compared the PVE HSC group with the group of patient undergoing simple PVE.\n\n\nRESULTS\nNo significant difference in FLRV and VLM growth was observed between the study groups. The percentage of exploratory laparotomies was smaller in the group with PVE and HSC application. Patients with simple PVE had a significantly higher incidence of extrahepatic metastases during follow up. We did not observe any significant differences in DFI and OS between the groups.\n\n\nCONCLUSION\nHSC application did not accelerate CLM growth in comparison with PVE alone. PVE and HSC application had a higher percentage of patients undergoing liver resection and a lower incidence of extrahepatic metastases.",
                "authors": "J. Br̊uha, V. Treska, H. Mírka, P. Hošek, J. Fichtl, T. Skalický, K. Bajcurová, J. Ludvík, P. Duras, D. Lysák, V. Liska",
                "citations": 2
            },
            {
                "title": "Design And Development Of Automated Storage And Retrieval System (ASRS) For Warehouse Using IOT And Wireless Communication",
                "abstract": ": The storage and retrieval system is used to store and retrieve the product, spare part, damaged product, etc. where the storage takes place in the warehouse by utilizing proper space present in a warehouse with the help of conveyor, crane or forklift or vertical lift module (VLM) and in rack or pallet. In this paper the main focus is on Automated Storage and Retrieval system (ASRS). In this paper the design and development of prototype of ASRS has been developed. In prototype storage system and conveyor are at fixed position, where as robot is moving for pick and place arrangement. The developed system is operated with the help of Bluetooth and data is store online with help of IOT device. The command to the system is given by the android app through Bluetooth connected to robot. Data is store on the internet with the help of IOT device connected to conveyor system. The ASRS system is fully automated system there is no manual interference. This system is used to store and retrieve bulk amount of load in warehouse. The large data is handled in this system also the damage of product or load is less due to smooth operation of system. It also reduces the labor cost. Minimize the time required for storage and retrieve. ——————————  ——————————",
                "authors": "Manisha Pingale, H. Kulkarni",
                "citations": 2
            },
            {
                "title": "Lane Change Driver Assistance System for Online Operation Optimization of Connected Vehicles",
                "abstract": "During recent years, all commercial vehicle manufacturers have introduced legally required and advanced predictive functionalities for their on-highway fleets. Telemetry, navigation and electronic horizon systems are the key elements for increasing safety, enabling congestion-free highways, reducing fuel consumption and pollutant emissions under real driving conditions. In this work a novel approach for lane change driver assistance is presented to advise the driver economically to change or keep in lane by using model predictive Adaptive Cruise Control (ACC). The system calculates lane change costs considering detected and connected traffic participants for an 8 km prediction horizon. The novel approach applies velocity loss minimization (VLM) and advanced nonlinear online energy optimization methods. This modular approach can interact with existing conventional and hybrid energy management systems. A final lane change judgment is based on the cost optimization of speed loss, gear selection and fuel consumption trajectories. This approach converges to global optimum fuel consumption under certain traffic flows. Finally, the system is applied to a 40-ton truck in a German macroscopic highway traffic simulation. In a typical highway traffic flow range between 75,000 and 95,000 vehicles per 24 hours on a three-lane highway, an average fuel consumption saving of 6% can be achieved, while the travel time criteria remains below 1%.",
                "authors": "Franz Aubeck, Tobias Oetermann, Georg Birmes, S. Pischinger",
                "citations": 2
            },
            {
                "title": "Evaluation of the Impact of Morphing Horizontal Tail Design of the UAS-S45 Performances",
                "abstract": "owadays, increasingly sensitive to the global warming, the aerospace industry is committed to reduce its toxic gas emissions. To take a part in this global effort, a morphing study on the horizontal tail of an Unmanned Aerial System (UAS) is here presented. This type of morphing consists in changing the shape of the horizontal tail wing during the flight in order to improve aircraft aerodynamical characteristics. The geometry of the horizontal tail was changed as function of 3 parameters: the dihedral (from -80 degrees to +80 degrees), the sweep (from -80 degrees to +80 degrees) or the twist angles (from -50 degrees to 50 degrees). To measure the impact of these types of changes on the horizontal tail, an aerodynamic study was performed using the Vortex-Lattice Method (VLM) implemented in OpenVSP software (distributed by NASA). The methodology consists, in the first place, to develop a reference model that can reproduce the aerodynamic behavior of the reference aircraft with its horizontal tail. Then, morphing models were developed based on the reference model, in which different dihedral, sweep or twist angles were changed for its horizontal tail. Finally, a model able to trim the UAS-S45 for static cruise conditions was used to compute the thrust force required to balance the aircraft for each case (original and morphing). A gain of 6% of thrust has been obtained when the aircraft was trimmed using twist angle instead of elevators deflection.",
                "authors": "M. Segui, R. Botez, Éloise Paper, Jean-Charles Di-Mambro",
                "citations": 2
            },
            {
                "title": "The Vertical Rectus Abdominis Musculocutaneous Flap As a Versatile and Viable Option for Perineal Reconstruction",
                "abstract": "P1: VLM EPLASTY-D-17-00004 eplasty.cls January 9, 2017 Interesting Case Series The Vertical Rectus Abdominis Musculocutaneous Flap As a Versatile and Viable Option for Perineal Reconstruction Demetrius M. Coombs, BS, a Nirav B. Patel, MD, MS, JD, b Matthew R. Zeiderman, MD, b and Michael S. Wong, MD b a Drexel University College of Medicine, Philadelphia, Pa; and b Division of Plastic & Reconstructive Surgery, University of California, Davis, Sacramento Correspondence: dmcoombs@mac.com Keywords: perineal reconstruction, VRAM, squamous cell carcinoma, radiation-induced tissue damage, wound healing Figure 1. Ulcerated right thigh lesion and persistent necrotizing soft-tissue infection upon presentation, despite multiple previous debridements (the patient in lithotomy).",
                "authors": "Demetrius M. Coombs, N. Patel, M. Zeiderman, M. Wong",
                "citations": 5
            },
            {
                "title": "An experimental and numerical study of FSI applied to sail yacht flexible hydrofoil with large deformations",
                "abstract": "The recent use of large aspect ratio and highly loaded composite hydrofoils on sailing boats illustrates the limit of the assumption of rigid body. When flying, the hydrofoil presents large deformations which impact significantly the hydrodynamic loads expected. The present work focuses on an experimental campaign performed on a trapezoidal hydrofoil, made of polyacetate material, in the hydrodynamic tunnel at the Research Institute of French Naval Academy. Large deformations up to 4.5% of the span on the hydrofoil's tip are measured at angle of incidence 10° for Re=0.7x106 calculated at mean chord. Vibration analysis performed on this foil, highlights an increase of its resonance frequencies with bending loading. A coupled approach between the Vortex Lattice Method (VLM) potential flow code, AVL, for inviscid calculations, corrected to consider the viscous component and, an in-house structural code based on beam theory by Finite Element Method (FEM) is developed for this application. The comparisons of simulations show good agreements with experiments in a large range of angles of incidence and flow velocities.",
                "authors": "Vanilla Temtching Temou, O. Fagherazzi, B. Augier, J. Astolfi, David Raison",
                "citations": 3
            },
            {
                "title": "Environmental monitoring of the Amazon basin with a low cost small satellite constellation in equatorial leo",
                "abstract": "The goal of this study is to design a dedicated, low cost satellite mission for environmental monitoring of the Amazon basin. The mission design is based on the unique characteristics of the small Brazilian launcher VLM-1. Launched from the near-equatorial Alcantara Launch Center, the vehicle allows for direct injection of about 150 kg in equatorial LEO, a low Earth orbit with zero (or near-zero) inclination. Such orbital configuration, actually used very seldom because of its limited ground coverage (restricted to equatorial regions only), proves to be ideal for near-continuous monitoring of the Amazon region. We present a possible microsatellite constellation that allows for high quality multispectral imaging of the Amazon basin with near-continuous coverage.",
                "authors": "S. Marcuccio, Rafel Heitkoetter",
                "citations": 3
            },
            {
                "title": "The extremely truncated circumstellar disc of V410 X-ray 1: A precursor to TRAPPIST-1?",
                "abstract": "Protoplanetary discs around brown dwarfs and very low mass (VLM) stars offer some of the best prospects for forming Earth-sized planets in their habitable zones. To this end, we study the nature of the disc around the VLM star V410 X-ray 1, whose spectral energy distribution (SED) is indicative of an optically thick and very truncated dust disc, with our modelling suggesting an outer radius of only 0.6 au. We investigate two scenarios that could lead to such a truncation, and find that the observed SED is compatible with both. The first scenario involves the truncation of both the dust and gas in the disc, perhaps due to a previous dynamical interaction or the presence of an undetected companion. The second scenario involves the fact that a radial location of 0.6 au is close to the expected location of the H2O snowline in the disc. As such, a combination of efficient dust growth, radial migration, and subsequent fragmentation within the snowline leads to an optically thick inner dust disc and larger, optically thin outer dust disc. We find that a firm measurement of the CO J = 2–1 line flux would enable us to distinguish between these two scenarios, by enabling a measurement of the radial extent of gas in the disc. Many models we consider contain at least several Earth-masses of dust interior to 0.6 au, suggesting that V410 X-ray 1 could be a precursor to a system with tightly packed inner planets, such as TRAPPIST-1.",
                "authors": "D. M. Boneberg, S. Facchini, C. J. Clarke, J. Ilee, R. Booth, S. Bruderer",
                "citations": 3
            },
            {
                "title": "Hepatic Toxocariasis with Atypical CT and MR Imaging Findings: a Case Report",
                "abstract": "Hepatic toxocariasis is a form of visceral larva migrans (VLM) caused by migration of second-stage larvae of certain nematodes such as Toxocara canis (T. canis) to the liver. Hepatic toxocariasis typically involves granulomatous lesions containing eosinophils and inflammatory cells (1). Computed tomography (CT) and magnetic resonance imaging (MRI) findings usually show multiple, ill-defined, oval lesions, measuring 1.0-1.5 cm in diameter (2). We have recently experienced a case of hepatic toxocariasis that presented as an appearance different from typical radiologic features. Therefore, we report on a case of hepatic toxocariasis with a radiologic-pathologic correlation.",
                "authors": "Hye Soo Shin, K. Shin, Jeong Eun Lee, J. Min, S. You, B. Shin",
                "citations": 3
            },
            {
                "title": "Hybrid one-cycle control scheme for fault-tolerant modular multilevel rectifiers",
                "abstract": "ABSTRACT This paper customises the classic one-cycle control (OCC) scheme for modular multilevel rectifiers (MMRs) and overcomes the inherent defect of the OCC. To be specific, a hybrid one-cycle control scheme is proposed combining the OCC and the virtual loop mapping (VLM). First, on the basis of the classic OCC and the volt-second equivalence principle, the relationships between the MMR arm equivalent duty cycle and the sub-module (SM) duty cycle are derived. Then, by making use of VLM methods, the switching frequency of the SM is reduced and the dynamic capacitor voltage balance is obtained as well. Further, to achieve the single-line-to-ground fault-tolerance capability and eliminate the second harmonic ripple in the DC voltage, the constant power control is presented using the negative-sequence voltage compensation. Specifically, the whole control scheme only needs one proportional integration controller, which greatly reduces the complexity of system control and the system cost. The validity of the proposed scheme is verified through simulation and experimental studies.",
                "authors": "Jun Mei, Huiyu Miao, Can Huang, Y. Xu, T. Ma, Qinran Hu, Wu Chen",
                "citations": 4
            },
            {
                "title": "A reactive resource defragmentation method for Virtual Links Mapping in software-defined networks",
                "abstract": "Assigning network resources to Virtual Links (VLs) efficiently and on-demand is a challenging problem for any network virtualization solution. Known as the Virtual Link Mapping (VLM) problem, its objective is to compute the appropriate network paths with the required network resources that meet the quality of service expectations of arriving VLs while spreading the load over all nodes to maximise the admissibility of forthcoming VLs. Despite the efficiency of existing VL mapping algorithms, when resources are allocated and released over time due to the arrivals and departures of VLs, the network inevitably drift into a fragmented state (with nodes with very different loads) often causing a VLs request rejection that could have been avoided with a different resource allocation. In practice, defragmentation algorithms are used in complement to VL mapping algorithms to proactively or reactively (on the event of a VLs request refusal) trigger some VLs reallocation (or migration). In this paper, we propose an Integer-Linear program (ILP) based reactive defragmentation and VLs mapping algorithm for an SDN/OpenFlow network. In addition to selecting the VLs that should be migrated to reduce network defragmentation, our algorithm also computes the paths (and the associated resources) that support the previously rejected VLs. Our solution was evaluated on a real network topology and the experiments showed that our proposal outperforms existing approaches from the literature by about 12% in terms of acceptance rate with a gain on migration costs around 40%.",
                "authors": "A. F. S. Tegueu, Slim Abdellatif, T. Villemur, Pascal Berthou",
                "citations": 3
            },
            {
                "title": "Estimating the need of second-line antiretroviral therapy in adults in sub-Saharan Africa up to 2030: a mathematical model",
                "abstract": ", Abstract Background— The number of patients in need of second-line antiretroviral drugs is increasing in sub-Saharan Africa. We aimed to project the need of second-line antiretroviral therapy (ART) in adults in sub-Saharan Africa up to 2030. Methods— We developed a simulation model for HIV and applied it to each sub-Saharan African country. We fitted the number of adult patients on ART to observed estimates, and predicted first-and second-line needs between 2015 and 2030. We present results for sub-Saharan Africa, and 8 selected countries. We present 18 scenarios, combining the availability of viral load monitoring (VLm), speed of ART scale-up, and rates of retention and switching to second-line. HIV transmission was not considered. Findings— Depending on the scenario, 8·7–25·6 million people are expected to receive ART in 2020, of whom 0·5–3·0 million (2·9%–15·6%) will be receiving second-line ART. The percentage was highest (15·6%) in the scenario with perfect retention and immediate switching, no further scale-up, and universal routine VLm. In 2030, the range of patients on ART remained constant, but the number (proportion) of patients on second-line ART increased to 0·8–4·6 million (6·6%– 19·6%). The need of second-line ART was 2–3 times higher if routine VLm was implemented throughout the region, compared with a scenario of no further VLm scale-up. For each monitoring strategy the future proportion of patients on second-line ART differed only minimally between countries. Interpretation— The demand for second-line ART will increase substantially in the future as countries increase access to routine VLm. Funding— World Health Organization",
                "authors": "J. Estill, N. Ford, L. Salazar-Vizcaya, A. Haas, N. Blaser, V. Habiyambere, O. Keiser",
                "citations": 3
            },
            {
                "title": "Hybrid Peak-to-average-power-ratio Reduction Method Based on Filter Bank Multicarrier in Wireless Sensor Networks",
                "abstract": "Filter bank multicarrier (FBMC) technology is considered a suitable solution for replacing orthogonal frequency division multiplexing (OFDM) technology for the transmission of the fifth-generation wireless system (5G) multicarrier. In this study, we investigate the problem of a high peak-to-average-power ratio (PAPR) of FBMC in wireless sensor networks (WSNs). To solve the problem of imaginary interference in the FBMC system, we use an auxiliary pilot. Then, on the basis of the pilot-assisted FBMC system, a hybrid method of the Vandermondelike matrix (VLM) and clipping with filtering is applied to reduce the PAPR. The input data is precoded by the VLM of Chebyshev polynomials to reduce the autocorrelation of the input signal. Then, the signal is sent to the clipping and filtering module to suppress the PAPR of the FBMC signal further. The pilot-assisted FBMC was applied to the multipath channel model. The performance was evaluated using the PAPR and bit error rate (BER) curves. The simulation results indicate that the hybrid method can improve the PAPR suppression performance of the FBMC system compared with the precoding method or the clipping and filtering method alone. From the results, we can also see that the hybrid method improves the BER performance.",
                "authors": "Song Liu, Yue Li, Jianqiang Wang",
                "citations": 1
            },
            {
                "title": "Hacking Jeff Minter’s Virtual Light Machine: Unpacking the Code and Community Behind an Early Software-Based Music Visualizer",
                "abstract": "Associee a la suite logicielle du Jaguar CD (un peripherique associe a une console de jeux video Atari sortie en 1995, mais qui n’eut guere de succes), la Virtual Light Machine (VLM) de Jeff Minter prefigurait les visualiseurs de musique souvent integres aux lecteurs multimedias numeriques du debut des annees 2000. Elle avait ete concue pour lire un CD et generer en temps reel des animations plus ou moins synchronisees avec la musique. En 1996, Jeff Minter publiait en ligne le « Yak’s Quick Intro to VLM Hacking », un guide expliquant comment personnaliser les 81 parametres predefinis du visualiseur, grâce a un menu cache dans le logiciel. Le travail logiciel de Minter, qui est peu connu au-dela de la communaute des historiens et des passionnes de jeux video, merite une place dans l’histoire des technologies grand public de visualisation de la musique. A partir de sources primaires numeriques (pages Web, messages sur les forums de discussion, code d’origine du VLM), cet article cherche a determiner dans quelle mesure les pratiques definies par Minter peuvent etre retrouvees dans les normes contemporaines du hacking.",
                "authors": "Eamonn Bell",
                "citations": 1
            },
            {
                "title": "DAILY AND SEASONAL VARIATION OF SOIL RESPIRATION IN A SEASONAL SEMIDECIDUAL ATLANTIC FOREST FRAGMENT AND A RESTORATION SITE IN SOUTHERN BRAZIL",
                "abstract": null,
                "authors": "Jvc Souza, G. Souza-Gonzaga, J. Melo-Tambani, MF Hertel, Vlm de Paula, Jmd Torezan",
                "citations": 1
            },
            {
                "title": "Virome Analysis Reveals No Association of Head and Neck Vascular Anomalies with an Active Viral Infection",
                "abstract": "Background/Aim: Vascular anomalies encompass different vascular malformations [arteriovenous (AVM), lymphatic (LM), venous lymphatic (VLM), venous (VM)] and vascular tumors such as hemangiomas (HA). The pathogenesis of vascular anomalies is still poorly understood. Viral infection was speculated as a possible underlying cause. Materials and Methods: A total of 13 human vascular anomalies and three human skin control tissues were used for viral analysis. RNA derived from AVM (n=4) and normal skin control (n=3) tissues was evaluated by RNA sequencing. The Virome Capture Sequencing Platform for Vertebrate Viruses (VirCapSeq-VERT) was deployed on 10 tissues with vascular anomalies (2×AVM, 1×HA, 1×LM, 2×VLM, 4×VM). Results: RNA sequencing did not show any correlation of AVM with viral infection. By deploying VirCapSeq-VERT, no consistent viral association was seen in the tested tissues. Conclusion: The analysis does not point to the presence of an active viral infection in vascular anomalies. However, transient earlier viral infections, e.g. during pregnancy, cannot be excluded with this approach.",
                "authors": "N. Franke, M. Bette, André Marquardt, T. Briese, W. Lipkin, C. Kurz, J. Ehrenreich, E. Mack, Bianka Baying, V. Beneš, F. Rodepeter, A. Neff, A. Teymoortash, B. Eivazi, U. Geisthoff, B. Stuck, U. Bakowsky, R. Mandic",
                "citations": 2
            },
            {
                "title": "Toxocariasis Mimicking Lymphoma and Presenting as Multiple Lymphadenopathy: A Case Report",
                "abstract": "Toxocariasis is a zoonotic infestation from parasite Toxocara canis (T. canis) and cati (T. cati) that manifests in variable parts of human body including the liver, lungs, eyes, heart, and the brain. The specific form of toxocariasis involving the systemic organs such as the liver, lungs and the gastrointestinal tract is named visceral larva migrans (VLM) and it presents clinical symptoms such as abdominal pain, fever, cough and wheezing (1). VLM mainly manifests as infiltrations in the liver (2) or as gastroenteritis (3) and ascites (4) in the abdomen, but rarely as generalized lymphadenopathy (1, 5). There have been rare reports of toxocariasis presented as multiple lymphadenopathies in the chest (1) or neck areas (5) but not in the abdomen. Therefore, we report a case of toxocariasis presented as multiple conglomerated lymphadenopathy that was initially misinterpreted as lymphoma on abdomen computed tomography (CT).",
                "authors": "Y. Choi, C. Park, Jeong Woo Kim, Yang Shin Park, Jongmee Lee, J. Choi, K. Kim, Chang Hee Lee",
                "citations": 2
            },
            {
                "title": "A survey for low-mass stellar and substellar members of the Hyades open cluster",
                "abstract": "Unlike young open clusters (with ages <250 Myr), the Hyades cluster (age ~600 Myr) has a clear deficit of very low-mass stars (VLM) and brown dwarfs (BD). Since this open cluster has a low stellar density and covers several tens of square degrees on the sky, extended surveys are required to improve the statistics of the VLM/BD objects in the cluster. We search for new VLM stars and BD candidates in the Hyades to improve the present-day cluster mass function down to substellar masses. An imaging survey of the Hyades with a completeness limit of 21m.5 in the $R$ band and 20m.5 in the $I$ band was carried out with the 2kx2k CCD Schmidt camera at the 2m Alfred Jensch Telescope in Tautenburg. We performed a photometric selection of the cluster member candidates by combining results of our survey with 2MASS JHKs photometry. We present a photometric and proper motion survey covering 23.4 deg$^2$ in the Hyades cluster core region. Using optical/IR colour-magnitude diagrams, we identify 66 photometric cluster member candidates in the magnitude range 14m.7<I<20m.5. The proper motion measurements are based on several all-sky surveys with an epoch difference of 60-70 years for the bright objects. The proper motions allowed us to discriminate the cluster members from field objects and resulted in 14 proper motion members of the Hyades. We rediscover Hy 6 as a proper motion member and classify it as a substellar object candidate (BD) based on the comparison of the observed colour-magnitude diagram with theoretical model isochrones. With our results, the mass function of the Hyades continues to be shallow below 0.15 $M_\\odot$ indicating that the Hyades have probably lost their lowest mass members by means of dynamical evolution. We conclude that the Hyades core represents the `VLM/BD desert' and that most of the substeller objects may have already left the volume of the cluster.",
                "authors": "S. Melnikov, J. Eisloeffel",
                "citations": 2
            },
            {
                "title": "Alternative Design for Anterolateral Thigh Multi-Paddled Flaps: The 3–5 System",
                "abstract": "Background The design and harvest of the anterolateral thigh (ALT) multi-paddled flap is a critical step in reconstructive surgeries. However, limited perforator distribution patterns of traditional design methods have gradually emerged in clinical practice. The aim of this study was to investigate the effect of a new technique (the 3–5 system) on ALT multi-paddled flap design. Material/Methods A total of 151 ALT flaps were harvested from 149 patients over a 26-month period. Among them, 100 ALT flaps were examined preoperatively using a handheld Doppler device to localize vascular perforators. Results By detecting perforator penetration points through the vastus lateral muscle (VLM) or the intermuscular septum and perforator entry points to the deep fascia, precise ALT flap perforator distribution patterns were found. Meanwhile, a 3–5 system was developed to design ALT flaps based on these findings. The remaining 51 ALT flaps from 49 patients during a 9-month period did not require the use of preoperative handheld Doppler. In addition, preoperative handheld Doppler and intraoperative findings demonstrated that all ALT flap penetration points through the VLM or intermuscular septum and the perforator entry point in the deep fascia were closely related based on 3 longitudinal lines and 5 horizontal lines. Conclusions ALT flaps were successfully harvested using a 3–5 system without the need for preoperative handheld Doppler analysis. Moreover, the 3–5 system is a simple and practical approach for preoperative ALT multi-paddled flap design.",
                "authors": "C. Deng, Shusen Chang, Zairong Wei, W. Jin, Hai Li, Kaiyu Nie, Xiu-jun Tang, Dali Wang",
                "citations": 2
            },
            {
                "title": "A comparison of data weighting methods to derive vertical land motion trends from GNSS and altimetry at tide gauge stations",
                "abstract": "This study compares eight weighting techniques for Global Navigation Satellite System (GNSS)-derived Vertical Land Motion (VLM) trends at 570 tide gauge (TG) stations. The spread between the methods has a comparable size as the formal uncertainties of the GNSS trends. Taking the median of the surrounding GNSS trends shows the best agreement with differenced altimetry – tide gauge (ALT-TG) trends. An attempt is also made to improve VLM trends from ALT-TG time series. Only using highly correlated along-track altimetry and TG time series, reduces the standard deviation of ALT-TG time series up to 10 %. As a result, there are spatially coherent changes in the trends, but the reduction in the RMS of differences between ALT-TG and GNSS trends is insignificant. However, setting correlation thresholds also acts like a filter to remove problematic TG stations. This results in sets of ALT-TG VLM trends at 344–663 TG locations, depending on the correlation threshold. Compared to other studies, we decrease the RMS of differences between GNSS and ALT-TG trends (from 1.47 to 1.22 mm/yr), while we increase the number of locations (from 109 to 155), Depending on the weighting methods the mean of differences between ALT-TG and GNSS trends varies between 0.1–0.2 mm/yr. We reduce the mean of differences by taking into account the effect of elastic deformation due to present-day mass redistribution into account.",
                "authors": "Marcel Kleinherenbrink, R. Riva, T. Frederikse",
                "citations": 2
            },
            {
                "title": "Order batching optimization in automated warehouses with metaheuristics",
                "abstract": "Order picking is a cost consuming activity. Before picking the customer demands, consolidating orders into batches can contribute to decrease these costs. In this study, we will focus on order batching optimization in automated warehouses, where Vertical Lift Modules (VLM) are used to store and retrieve products. The treated order batching problem deals with the question of how to combine orders into batches in such a way the total picking time is minimized. This problem is recognized as NP-Hard and its optimal resolution is difficult with large-scale instances and within acceptable computation time. To overtake this issue, metaheuristics are applied: The Tabu Search and the Simulated Annealing algorithm. Their performance is analyzed for different instances and evaluated regarding both computation time and solution quality. We will show that the proposed approaches are able to provide powerful solutions that enable VLMs to operate effectively.",
                "authors": "Emna Laajili, Nicolas Lenoble, Y. Frein",
                "citations": 2
            },
            {
                "title": "Histopathological Changes Due to Interaction of Visceral Larva Migrans and Diabetes Mellitus",
                "abstract": "The study was aimed to investigate the histopathological changes due to interaction of Visceral Larva Migrans (VLM) and diabetes mellitus in Wistar rats (Rattus norvegicus) and its potential zoonotic risk after being consumed accidently. A total of seventy two adult Wistar rats were taken (N=72) and divided into four groups of 18 rats each viz; group I (healthy control), group II (diabetic control), group III (VLM infected healthy rats) and group IV (VLM infected diabetic rats). Experimental rats exhibited haemorrhages in the liver, lungs and brain on 10, 20 and 30 days post infection (dpi). The accumulation of mononuclear cells in the hepatic parenchyma was observed on 10 dpi. Thrombosis was seen in some blood vessels at 20 dpi. Fibrous connective tissue proliferation in triad areas around the biliary tubules were seen at 30 dpi as compared to control group. Massive hyperplasia of the bronchiolar lymphoid tissue, bronchiolar epithelial and sub-mucosal smooth muscle hyperplasia were seen on 20 and 30 dpi. The brain of rat with diabetes and without diabetes showed the degenerative changes on 10, 20 and 30 dpi.",
                "authors": "Sahil Kumar, R. Katoch, N. Nashiruddullah, S. Azmi, Raghubir Singh",
                "citations": 2
            },
            {
                "title": "Conceptual Design of Distributed Propeller Aircraft: Linear Aerodynamic Model Verification of Propeller-Wing Interaction",
                "abstract": "This paper opens an exciting new opportunity of new aircraft concepts specifically targeting to have higher aerodynamic efficiency than conventional designs through the synergistic interaction between the propeller and the wing. Previous literature on propeller and wing performance analysis methods provide a platform on theory description behind the aerodynamic modeling approach selected. The research effort began with attempts to verify previous NASA wind-tunnel tests (TN D-4448) on a medium and short wing span propeller-driven, short take off and landing (STOL) transport aircraft. One prediction is employed on isolated performance of the wing without the propellers and another is to capture one-way coupling, specifically the effect of the propeller slipstream on the wing. For the first decoupled analysis configuration , the vortex lattice model (VLM) technical tools comparison was made with the Athena Vortex Lattice (AVL) and VSPAERO. Next, the propeller and the wing interaction is predicted using three different coupling methods. The first stage of the work is based on VSPAERO solver which combined the actuator disk theory with VLM. The second approach is to compare with the in house code of blade element and lifting line theory (LLT) coupling. Thirdly, to couple the same BET model with VLM to capture the aerodynamics performance and the influence of the propeller on the wing with the aim to determine the lift and induced drag of the configuration. Perhaps the most compelling aspect offered by the technical tools selected is being economic; low computational cost and time. The numerical computation results of the model presented a good correlation with the experimental data. The aerodynamic model developed will be a strong base for more complex analysis and even greater efficiency improvements for a conceptual design studies of future distributed propeller aircraft .",
                "authors": "Baizura Bohari, M. Bronz, E. Bénard, Quentin Borlon",
                "citations": 2
            },
            {
                "title": "Structural wing sizing and planform shape optimization using multidisciplinary CAD-CAE integration process",
                "abstract": "This paper describes a wing mass estimation approach based on a bi-level optimization process and an automated CAD-CAE integration framework. The design framework goes through parametric CAD model generation, aerodynamic load calculation, finite element modeling, structural analysis and sizing. In the lower-level task, the wing box is sized to optimize the structural element thicknesses under a maximum stress constraint for a fixed planform configuration. The upper-level optimization determines the optimal planform shape to minimize the wing mass while maintaining a good lift-to-drag ratio. The process chain involves SIEMENS NX for geometric modeling, VORLAX VLM for aerodynamic calculations and MSC. NASTRAN for FE analysis and sizing. The outer optimization is performed by the multi-objective genetic algorithm on a radial basis function surrogate model.",
                "authors": "Abdelkader Benaouali, S. Kachel",
                "citations": 2
            },
            {
                "title": "Optimizing Treatment Monitoring in Resource Limited Settings in the Era of Routine Viral Load Monitoring",
                "abstract": null,
                "authors": "C. Barbara, S. Reynolds",
                "citations": 2
            },
            {
                "title": "COBISS u Makedoniji: osvrt povodom dva jubileja",
                "abstract": "This paper provides a brief overview of the COBISS organizational model and software usage in the development of the national library information system in Macedonia over the past 30 years. The beginnings of COBISS usage have been running since 1989 within the shared cataloguing system in the SNTIJ / BIS project. Following the breakup of Yugoslavia and the failure of the shared cataloguing system, several Macedonian libraries have again started t use COBISS software due to the need to introduce new technologies and to automate their processes. In 2004, when a great number of libraries showed interest to use COBISS software, the Ministry of Culture of RM has decided to use the COBISS platform for the development of the national library information system of Macedonia, and for this purpose, in the National and University Library “Sv. Kliment Ohridski “ has established centre of the Virtual Library of Macedonia, i.e. VLM Centre. For the past 15 years, COBISS.MK has been operating and evolving according to COBISS.Net development plans. By the end of this anniversary year, more than 70 libraries (national, public, higher education and special) will be included in the system. The decades-long implementation of COBISS has made it possible for online libraries to be seen, to save and to organize the time in the process of the library materials cataloguing, and to animate the public by increasing the interest in the use of libraries.",
                "authors": "Žaklina Gjalevska",
                "citations": 0
            },
            {
                "title": "The effect of nonlinear elastic materials on swept aeroelastic wings",
                "abstract": "Aircraft design under the terms of regulations postulates load cases between 2.5g and ‑1.0g. According to statistical load data, high manoeuvre and gust loads (-1.0g and 2.5g) occur seldom, but they determine aircraft structures. In contrast to linear aircraft materials (e.g. aluminium, carbon), rubber like materials show nonlinear elastic behaviour. \nHence, the aim is to create passive load alleviation with nonlinear elastic materials. The idea is to increase the performance during low load cases around cruise and to decrease the loading for high load cases. In other words, we want to create a wing which is stiff during cruise and which gets more flexible at high loading cases. \nTo examine this, an iterative process computes three trimmed quasi steady manoeuvres (1.0g, 2.5g, -1.0g). In doing so, the process considers the aeroelastic coupling. It iteratively calculates loads and deformations, using the Vortex Lattice Method (VLM) for the aerodynamic analysis and the Nonlinear Solution of MSC Nastran (SOL400) for the structural analysis. Also, Solution 400 computes the deformation due to the loads iteratively. \nThe process utilises a rectangular 30° aft swept wing with a wingspan of 60 m which is close to common long range aircraft. A finite element model with beam elements represents the load carrying structure. A vortex lattice simulates the aerodynamics. The reference wing structure with the linear material is untwisted. To compare two different approaches of nonlinear elastic materials, the corresponding wing structures are pretwisted. This ensures an equal lift distribution of the 1g load cases. \nThe root bending moment decreases of about 3.98% for the material with the stiffer area at cruise conditions.",
                "authors": "K. Bramsiepe, T. Klimmek, W. Krüger, Lorenz Tichy",
                "citations": 0
            },
            {
                "title": "Role of Alpha‐1 Adrenergic Receptors (α1AR) and Norepinephrine Transporter in Parvocellular Neurons of the Hypothalamic Paraventricular Nucleus (PVN) After Chronic Intermittent Hypoxia Exposure",
                "abstract": "The Paraventricular nucleus (PVN) of the hypothalamus is critical for autonomic homeostasis and cardiorespiratory reflex responses to a variety of stimuli and stressors. This is accomplished, in part, via dense catecholaminergic and norepinephrine (NE) projections from nucleus tractus solitarii (nTS) and ventrolateral medulla (VLM). NE signaling is balanced by the uptake of NE by transporters (NET's) and the activation of one or more adrenergic receptor (AR). Within the PVN, stimulation of aAR's increases the frequency of spontaneous excitatory postsynaptic currents (sEPSCs), decreases the frequency of spontaneous inhibitory postsynaptic currents (sIPSCs) and depolarizes resting membrane potential (RMP) which elevates blood pressure (BP) and sympathetic outflow. Chronic intermittent hypoxia (CIH) is a model for obstructive sleep apnea (OSA) that elevates respiration, BP, sympathoexcitation and circulating NE levels. CIH activates not only PVN neurons but also the catecholaminergic neurons in the VLM and nTS. However, the contribution of aAR and NET in PVN neurons after CIH exposure remains elusive. In the present work, we determined the electrophysiological properties of PVN parvocellular neurons in response to a1AR activation and NET inhibition after normoxia (Norm) and CIH. Male Sprague‐Dawley rats (110–150g) were exposed to either 10 days normoxia (Norm, FiO2 = 21%) or CIH (alternating FiO2 = 21% and 6%, 8 hr/day). PVN slices (~280 μm) were generated and cell capacitance (Cm), initial input resistance (Rin), and sEPSCs were examined by whole cell patch clamp. sEPSCs were recorded under aCSF baseline or in the presence of one or more of the following: NE (10–100 μM, general AR agonist), phenylephrine (Phe, a1AR agonist, 100 μM), prazosin (a1AR antagonist, 10 μM), or NET inhibitor (desipramine, 100 μM). The Cm of Norm neurons was smaller than CIH, but CIH decreased Rin. Between Norm and CIH, baseline sEPSC frequency and amplitude were similar. Phe and NE significantly increased sEPSC frequency in Norm but not in CIH. Block of the a1AR with prazosin alone did not alter sEPSC frequency or amplitude in Norm (n=10) or CIH (n=6). NE in the presence of prazosin (n=7) prevented the increase in sEPSC frequency in Norm. In Norm (n=5) and CIH (n=6), NET block with desipramine increased sEPSC frequency but did not alter amplitude. However, the magnitude of increase in sEPSC frequency was comparable. These results demonstrate that a1AR activation participate of neuronal responses in Norm, but are minimized in CIH. NET function may be elevated after CIH exposure. The latter may contribute to reduced neuronal responses evoked by Phe after CIH, as well as compensatory mechanism caused by over sympathetic activity.",
                "authors": "Gean Domingos Silva Souza, D. Kline",
                "citations": 0
            },
            {
                "title": "Tekenbeten en Lyme-borreliose",
                "abstract": null,
                "authors": "Peter Winderickx, Sofie Acke, M. Verbrugghe, Marie-Noëlle Schmickler, Koen De Schrijver",
                "citations": 0
            },
            {
                "title": "Simple universal nonlinear longitudinal flight simulation with avoiding of static and dynamic stability derivatives",
                "abstract": "This paper shows simple method to simulate nonlinear longitudinal flight dynamic of an aircraft in the most direct way. The method is based on physical principles of an analytical-empiric flight dynamics. Non-linearity, as stall or thrust dependent on velocity, can be included. Static and dynamic stability derivatives are not required but can be computed as an output. The model is applicable for various aircraft conceptions. Higher level model, for example, based on flight tests data, can be modified by low-level analytical methods, e.g. for modification of horizontal tail area. No special simulation software is necessary. The model is compared to linear model and flight test experiment. This model, together with valuable analytical-empiric data, might be applied for fast flight dynamic computations. Model is easily accessible and understandable even with basic knowledge of flight dynamic and computer programming. The main application of the method is conceptual design when high precision is not expected, even if VLM, CFD or flight test data can improve the precision.",
                "authors": "Jiří Matějů",
                "citations": 0
            },
            {
                "title": "Multidisciplinary Aircraft Design and Trajectory Control Optimization",
                "abstract": "In the pursuit of increasing aircraft performance, one approach which can yield better results than a conventional design process is a multidisciplinary optimization process. In this paradigm, a design architecture is established so that the analyses for the several disciplines pertinent to the problem are handled simultaneously, rather than sequentially. In this work, a numerical tool was developed in order to perform low-fidelity multidisciplinary optimization upon a commercial airliner the B777-300 considering models for aerodynamics, propulsion, structures and trajectory. For the aerodynamic analysis, a vortex-lattice method (VLM) is employed. The lifting surface structures were modeled by finite elements with the shape of hollow tubular spars. For the propulsion system, a model based on empirical data collected from the target engines was utilized. Finally, the system was optimized for cruise conditions, and then control optimization was performed on the resulting configuration for additional mission phases. The performance metric optimized in this work was be the amount of fuel burnt by the aircraft in order to complete its mission. The described optimization processes were successfully carried out, the former outputting the cruise-optimized wing and tail configurations, and the latter providing the optimized control parameter values for descent flight conditions. These values were validated by means of comparison with those of the original B777-300, including that of the performance metric which improved as more disciplines were considered.",
                "authors": "L. Lúcio",
                "citations": 0
            },
            {
                "title": "Decreased Number of Brainstem Serotonin (5‐HT) Neurons During Chronic Hypercapnia in Goats",
                "abstract": "Thirty days after carotid body denervation (CBD) in goats, the number of neurons expressing the rate limiting enzyme for serotonin (5‐HT) synthesis (tryptophan hydroxylase, TPH), in the medullary raphe nuclei (MRN) and the ventrolateral medulla (VLM), was 50% of that in control goats (J. Appl. Physiol. 115, 1088–1098, 2013). This difference could be due to loss of carotid afferents per se, or it could be secondary to the CBD induced chronic hypercapnia. The objective of the present study was to assess the effect of hypercapnia per se on TPH expressing neurons in the MRN and VLM. To achieve the objective, we constructed environmental chambers to chronically house and study the effects of chronic hypercapnia in adult goats. Following a room air control period, the goats were exposed to either room air or an elevated inspired CO2 (InCO2) of 6% for 30‐days. After 30 days of hypercapnia, the brainstems were harvested and immunohistochemistry and western blots were used to determine whether chronic hypercapnia altered the number of TPH expressing neurons in the MRN and VLM. We found that there was a nearly 50% decrease (P<0.01) in the number of neurons expressing TPH within both the MRN and VLM. Additionally, there was a decline in the number of total neurons within these nuclei, suggesting that the loss of TPH expressing neurons may be from neuronal death. We conclude that chronic hypercapnia per se has a major impact on the 5‐HT neuromodulatory system which is important in the control of breathing.",
                "authors": "Kirstyn J. Buchholz, Nicholas J. Burgraff, J. LeClaire, S. Neumueller, L. Pan, M. Hodges, H. Forster",
                "citations": 0
            },
            {
                "title": "Measuring Device for Controlling a Vaporising Liquid Microthruster",
                "abstract": "The objective of the project is to design a system which can control the temperature of a va-porizing liquid microthruster (VLM). The liquid in a VLM is heated using a heater resistor.This resistor will be used to both heat the liquid and measure the temperature.In this thesis the subsystem responsible for the measurements and the conversion of the mea-sured signals to the digital domain will be discussed. We propose a method where short measure-ment current pulses of a fixed amplitude are applied to the heater resistor. As an optimization,these pulses are omitted when a certain current threshold has been met.Results show that the system can measure temperature with±1◦C accuracy, however more fullsystem measurements are required to ensure functionality as a whole.",
                "authors": "Coen Straathof, R. V. Wijk",
                "citations": 0
            },
            {
                "title": "TIDE GAUGE AND SATELLITE ALTIMETRY DATA FOR POSSIBLE VERTICAL LAND MOTION DETECTION IN SOUTH EAST BOHOL TRENCH AND FAULT",
                "abstract": "Abstract. Coupled with the occurrence of regional/local sea level rise on urbanized coastal cities is the possibility of land subsidence that contaminates the measurement by the tide gauge (TG) sensors. Another technology that could possibly check the in-situ data from tide gauge is satellite altimetry. The sea surface height (SSH) measured from satellite altimeter is compared with the observed tide gauge sea level (TGSL) to detect vertical land motion (VLM). This study used satellite altimeter retracked products near the TG Stations in Tagbilaran, Bohol; Dumaguete, Negros Oriental; and Mambajao, Camiguin located in the vicinity of the South East Bohol Trench and Fault (SEBTF). Based on the results, the TG site in Tagbilaran is undergoing land subsidence. The rate of VLM is around 5 mm/year from 2009 to 2017. The same trend was manifested in the GNSS observed data in the PHIVOLCS monitoring station in Tagbilaran and the geodetic levelling done in the area. After the October 15, 2013 earthquake in Bohol, downward trends of around 27 mm/year and 17 mm/year were observed from GNSS measurements and SSH-TGSL difference respectively. These different rates may be due to the distance between the two sensors. The comparison between SSH and TGSL in Dumaguete showed small difference with a VLM rate of 1.8 mm/year. The difference in SSH-TGSL in Mambajao is quite large with a downward rate of 9.4 mm/year. This result needs to be further investigated for TG or TGBM instability or monitored for a possibility of land uplift.",
                "authors": "R. Reyes, D. P. Noveloso, A. Rediang, M. Passaro, D. Bringas, M. Nagai",
                "citations": 0
            },
            {
                "title": "A novel optimised design using slots for flow control and high-lift performance of UCAVs",
                "abstract": "In this study, two UCAV planforms are considered based around generic 40° edge-aligned configurations. One configuration has a moderate leading and trailing edges sweep of Λ = 40°, while the other configuration is highly swept with a leading-edge sweep of Λ = 60° and trailing edges sweep of Λ = 40°. The objectives of the present study on UCAV configurations are two-fold: first to predict aerodynamic performance particularly the maximum-lift characteristics of two flying wing planforms; second to control the flow by inserting leading-edge and chordwise slots and analysing the viscous flow development over the outboard sections of a flying-wing configuration to maximise the performance of control surfaces. \nThe first part is demonstrated using a variety of inviscid Vortex Lattice Method (VLM) and Euler, and viscous CFD Reynolds Averaged Navier-Stokes (RANS) methods. The computational results are validated against experiment measured in a wind tunnel. The VLM predicts a linear variation of lift and pitching moment with incidence angle, and substantially under-predicts the induced drag. Results obtained from RANS and Euler agree well with experiment. \nFor the second part, a novel optimised design using chordwise slot is implemented on a highly swept Unmanned Combat Air Vehicle (UCAV) configuration to maximise the lift over trailing edge control surfaces. More airflow over the control surfaces will result in enhanced lateral control of the air vehicle at medium to high angles of attack. Four parameters describing the chordwise slot are identified for the numerical optimisation. They are: location, width, length and angle of trajectory of chordwise cavity relative to freestream. The angle of trajectory of chordwise slot is measured with respect to the trailing edge of the air vehicle. The results of CFD optimisation are compared with a clean configuration and verified with experiment. The configuration with chordwise slot has shown higher mass flow rate over the control surfaces of the air vehicle in comparison to baseline clean configuration. It is demonstrated that higher mass flow rate results in higher lift. Leading-edge slot method is considered, but the method improves the flow control for the low angles of attack regime, and is found to be ineffective for a highly-swept UCAV configuration at medium to high angles of attack.",
                "authors": "U Ali",
                "citations": 0
            },
            {
                "title": "A New Design Method for Ship Propellers with Prescribed Circulation Distributions Based on the Vortex Lattice Lifting-Surface Model",
                "abstract": "A new design method has been developed for ship propellers with prescribed circulation distributions based on the vortex lattice lifting-surface model (VLM). For a given set of camber surface geometry and pitch profile of the blade, the circulation distribution in a uniform or a radially non-uniform inflow is computed by means of the VLM, and used to update the camber surface geometry by the Newton-Raphson iterative scheme according to the differences between the computed and the prescribed circulation distributions. Numerical examples are given to confirm that the converged camber surface geometry and pitch profile are practically independent of the initial values, and the method converges well with the number of vortex lattices along the span and the chord. The present method has been applied to design a highly skewed propeller in open water. The hydrodynamic performance and blade-surface pressure distributions of the designed propeller are numerically verified by means of RANS simulation.",
                "authors": "Chen-Jun Yang, Qi Wang, Xiaoqian Dong, Wei Li, F. Noblesse",
                "citations": 0
            },
            {
                "title": "Peak to Average Power Ratio Reduction Technique using LPC Coding in OFDM System",
                "abstract": ". The main challenge in orthogonal frequency division multiplexing (OFDM) is to reduce the high peak to average power ratio (PAPR), in which leads to the nonlinearity and distortion for the different application of high power amplifier (HPA). PAPR is defined as the ratio between the maximum instantaneous power and its average power. In this paper, we have presented new PAPR reduction technique to reduce peak to average power ratio using Linear predicting coding (LPC), Vandermonde-like matrix (VLM) algorithm and selective mapping (SLM) algorithm. These techniques show the significant reduction in PAPR without any harmful degradation in power spectral density (PSD), computational complexity (CC) and performance error of the system. This technique can be applied for any number of subcarrier and independent of modulation scheme under Additive White Gaussian Noise (AWGN) channel.",
                "authors": "S. Verma, Amit Kumar, Ashish Kumar, Rao",
                "citations": 0
            },
            {
                "title": "Your input is a breath of fresh air! A chemosensory microcircuit of medullary raphe and RTN neurons",
                "abstract": "Breathing is our first act upon birth and the last action we complete before death. The first to last breath taken, is in fact, how we define someone’s life. Since it was first reported that the blood concentration of CO2istightly controlled, and provides the dominant drive to breathe, the search for the cells that regulate it began. It took almost 60 years for the identification of the first central chemosensitive areas, regions within the brain that respond to specific chemical stimuli (such as CO2or its proxy H+), found at the ventrolateral surface of the medulla (VLM). Since then the debate over which cells in these areas are responsible for detectingCO2and signalling its fluctuations to the respiratory oscillators, has been extensive and heated. Chemosensitive cells are thought to have cell bodies located in, or close to, the VLM with dendrites in close apposition to blood vessels to better detect changes in blood gases. Several candidates fulfil this criteria, including the retrotrapezoid nucleus (RTN) and medullary raphe.",
                "authors": "Robert T. R. Huckstepp",
                "citations": 0
            },
            {
                "title": "A Laboratory Strain of Leishmania major : Protective Effects on Experimental Leishmaniasis",
                "abstract": "Purpose Leishmaniasis, as one of the most important vector-borne and zoonotic diseases, can be seen in different forms and is more prevalent in developing countries worldwide. Due to the absence of effective strategies in its prevention, treatment, and control, investigation of effective control strategies against the disease is necessary. In this research, we evaluated the immunogenicity of a cold-adapted laboratory strain of Leishmania major (LMC) in the mouse model. Methods Twenty BALB/c mice were divided into two groups. LMC group received 4 × 106 of LMC strain in 0.5 ml DMEM, and VLM group, as the control group, received 0.5 ml Dulbecco’s modified Eagle’s medium. Both groups were challenged with virulent L. major 3 weeks after inoculation. Results The data obtained from the analysis of immune responses and histopathological changes interestingly revealed protection against L. major in immunized mice. Compared with the VLM group, the mice immunized with LMC strain of L. major in the LMC group showed a significant increase in IFN-γ and IgG2a levels (P < 0.05) which are important indexes for Th1-related immune responses. Additionally, significant differences in concentration of IgG1 and IgG total before and after the challenge was observed in LMC group (P < 0.05). Furthermore, the immunized mice showed a significant reduction in mean sizes of skin lesion and liver damage compared to the VLM group. Conclusion Based on the present findings on immunogenicity of LMC strain, it seems this strain is able to induce both humoral and cellular immunity and a significant protection against L. major in the mouse model.",
                "authors": "Namazi, Hosseini, Nazifi, Asadpour",
                "citations": 0
            },
            {
                "title": "A Laboratory Strain of Leishmania major: Protective Effects on Experimental Leishmaniasis",
                "abstract": "Leishmaniasis, as one of the most important vector-borne and zoonotic diseases, can be seen in different forms and is more prevalent in developing countries worldwide. Due to the absence of effective strategies in its prevention, treatment, and control, investigation of effective control strategies against the disease is necessary. In this research, we evaluated the immunogenicity of a cold-adapted laboratory strain of Leishmania major (LMC) in the mouse model. Twenty BALB/c mice were divided into two groups. LMC group received 4 × 106 of LMC strain in 0.5 ml DMEM, and VLM group, as the control group, received 0.5 ml Dulbecco’s modified Eagle’s medium. Both groups were challenged with virulent L. major 3 weeks after inoculation. The data obtained from the analysis of immune responses and histopathological changes interestingly revealed protection against L. major in immunized mice. Compared with the VLM group, the mice immunized with LMC strain of L. major in the LMC group showed a significant increase in IFN-γ and IgG2a levels (P < 0.05) which are important indexes for Th1-related immune responses. Additionally, significant differences in concentration of IgG1 and IgG total before and after the challenge was observed in LMC group (P < 0.05). Furthermore, the immunized mice showed a significant reduction in mean sizes of skin lesion and liver damage compared to the VLM group. Based on the present findings on immunogenicity of LMC strain, it seems this strain is able to induce both humoral and cellular immunity and a significant protection against L. major in the mouse model.",
                "authors": "M. Namavari, F. Namazi, Reza Asadi-Manesh, M. Hosseini, S. Nazifi, M. Asadpour",
                "citations": 0
            },
            {
                "title": "RESEARCH OF THE OPERATION OF THE SCREW PROPELLER ON THE BASIS OF THE NONSTATIONARY THEORY OF THE LIFTING SURFACE",
                "abstract": "Abstract. A numerical model marine propeller at oblique flow, as based on unsteady lifting surface to applied Vortex Lattice Method (VLM) are discussed. The propeller’s blades vortices and free trailing vortex sheet was replaced on vortex loops. Because the propeller’s blades an oblique flow moves unsteady, then result thrust, torque and lateral force are varying in angle of rotation of the propeller. Lateral force has same direction as the lateral velocity component. In order to calculate, estimate degree of influence on thrust, torque and lateral force propeller at oblique flow. According to the proposed mathematical model (MM), has been studied numerically acting oblique flow at various angles flow. There is a noticeable increase propeller thrust coefficient and lateral force coefficient with increasing bevel angle, however, the change propeller torque coefficient is insignificant. The results calculation hydrodynamic forces for different angels flow at every instantaneous time are compared with works others authors. A mathematical model of a propeller operating in oblique flow, based on an unsteady lifting surface, allows one to estimate the magnitude of the variables per revolution of forces acting both on the propeller blade and on the screw as a whole. This MM can be used as a tool to study the hydrody-namics of propellers operating in oblique flow.",
                "authors": "S. Kinnas, Hanseong Lee",
                "citations": 0
            },
            {
                "title": "MEMS Micropropulsion : Design, Modeling and Control of Vaporizing Liquid Microthrusters",
                "abstract": "In recent years, there has been an increase in the number of small multi-mission platforms such as CubeSats, in an attempt to reduce costs of space missions. CubeSats have been used for different purposes including Earth observation, research and technology demonstration. However, a key technology that is still under development is the micropropulsion system that has the potential to significantly increase the capabilities of CubeSat missions. Micropropulsion has been recognized as one of the key development areas for the next generation of highly miniaturized spacecraft such as CubeSats and PocketQubes. It will extend the range of applications of this class of satellites to include missions that require, for example, orbital maneuvering or drag compensation. An interesting option for CubeSats and PocketQubes is the Vaporizing Liquid Microthruster (VLM) which has received increasing attention due to its ability to provide high thrust levels with relatively low power consumption. The thruster uses the vapor generated in the vaporization of the propellant to produce thrust using a nozzle. The vaporization is usually done by applying power to resistive heaters that could be integrated into the device or externally attached to it. The nozzle is usually a convergent-divergent nozzle that can accelerate the propellant to supersonic velocities. This thesis aims to develop modeling and control concepts for micropropulsion systems to allow the spacecraft to perform maneuvers of position and attitude control. The Vaporizing Liquid Microthruster has been selected due to its characteristics that suit the needs of very small spacecraft. The first part of the research is dedicated to an in-depth literature study of the currently available micropropulsion systems. Those that are manufactured with silicon and MEMS (Micro Electro-Mechanical Systems) technologies have been analyzed and compared in terms of their thrust, specific impulse, and power. A classification in terms of complexity is introduced in an attempt to identify the suitability of the devices for the current trend towards simplifying architectures. The analysis of development levels of different types of micropropulsion systems revealed that although the actual thrusters are significantly developed, the interfacing and integration to other components of the system are still to be further developed. The second part of the research focuses on the characterization and modeling of VLM systems. This is an extremely important step in the development of such systems since a proper model, i.e., one that sufficiently represents the dynamics of the system, is required during the design phase to help, for example, in designing controllers, and also during the operational phase to help reproducing the events happening when the satellite is in orbit. A comprehensive model has been developed using theoretical and empirical relations. The third part of the research addresses the problem of controlling multiple redundant devices allowing failures to occur. This is very important to guarantee the successful operation of VLM systems with many thrusters while performing combined attitude-position maneuvers. A fuzzy control system was developed introducing an automatic rule generation algorithm that allows the fuzzy controller to solve control allocation problems. Finally, the last part of the research investigates the possible applications of VLM systems. An example scenario is considered to analyze the performance required to execute different maneuvers and missions. The key contributions of the work presented in this thesis are related to the modeling and control of Vaporizing Liquid Microthrusters. A comprehensive model of the complete system has been proposed and used to develop control algorithms for individual thrusters and for a set of thrusters. A fuzzy control system has been developed to solve the problem of controlling multiple devices with redundant outputs. Finally, an in-depth literature study and an analysis on the possible applications allowed to put VLM systems into perspective offering a glimpse into the future development of such systems.",
                "authors": "M. D. A. C. E. Silva",
                "citations": 1
            },
            {
                "title": "Predictive Control Design Based on System Identification of an Electro-hydraulic Actuator Applied to Brazilian Sounding Rockets and Microsatellite Launchers",
                "abstract": "This paper presents the modeling and control design to new hydraulic actuator being developed for thrust vector assembly applied to Brazilian rockets. Traditionally PID controllers are used for this issue but based on discrete models is proposed a new digital control for best performance. It is based on models obtained from closed-loop system identification of a Brazilian electro-hydraulic actuator using Nitrogen pressure-fed system applied to Sounding Rockets and Microsatellite Launcher (VLM). The vehicles are developed at the Aeronautics and Space Institute (DCTA/IAE) and a new actuator under test is being proposed using Helium gas to the Pressure-Fed-System. Traditionally the Nitrogen gas is used in low pressure operation to feed hydraulically the actuator and a new controller is being implemented to improve the system performance. Simulations developed in AMEsim and Matlab codes show best performance using Helium gas dealing on fast movements to the high pressure hydraulic cylinder, increasing the system bandwidth. The modeling of the hydraulic actuator is presented for linear and nonlinear analysis as well as their influences on system identification algorithms. The fluid flow through the internal pipes and spool are modeled using its nonlinear flow equation while the spool linear dynamics are obtained from Newton’s law and the magnetforce from Biot-Savart law. Discrete models are obtained from system identification using experimental data from the hydraulic closed-loop operation while a digital controller is designed based on that discrete models and finally implemented in the loop. A real-time electronic system with digital-to-analog and analog-to-digital converters performs the digital control, using Labview programming environment. The linear and nonlinear dynamics associated to each sub-system are discussed and simplifications hypotheses are presented in order to obtain the Low Order Equivalent System (LOES) to the entire hydraulic actuator, as well as the influences on the predictive control strategy and linear system identification. According to results in time and frequency domain the performance attends the rocket design requirements.",
                "authors": "Thiago Scharlau Xavier, E. G. Barbosa, L. Góes",
                "citations": 1
            },
            {
                "title": "MULTI-DISCIPLINARY DESIGN OPTIMIZATION OF A BLENDED WINGLET",
                "abstract": "In order to reduce operating costs directly related to fuel burn, it is proposed the development of a blended winglet optimized for a commuter aircraft of 11 passengers. Initially a brief description of the problem is made, as well as the background of the wingtip devices evolution. The optimization method used to model the physical phenomena in question, the drag generated by lift, was a Vortex Lattice Method (VLM) code based on the Trefftz-Plane theory for counting induced drag. After the iterative process the results are analyzed, presenting the comparisons between the aircraft without / with the final winglet. These comparisons cover several areas of aeronautics, such as aerodynamics, loads and performance.",
                "authors": "J. Souza, F. Catalano",
                "citations": 1
            },
            {
                "title": "Neuropsychological and brain structural alterations in emerging psychosis",
                "abstract": "Despite the growing interest in personally tailored interventions in medicine and health care, it is not possible to reach sufficient accuracy in the prediction of psychosis to date. Many factors are associated with transition to psychosis, such as neuropsychological impairments and structural alterations of the brain that have been shown to predate the onset of frank psychosis. \nHowever, the different disease trajectories male and female patients' experience may contribute to the mixed picture representing emerging psychosis. The prospective FePsy (Fruherkennung von Psychosen) study was a project aiming to improve the early detection and intervention of psychosis through multilevel assessment. The in the following described articles are based on data assessed within the FePsy study. \nIn the first article, structural equation modelling and latent growth curve modelling were used to evaluate verbal learning and memory (VLM) performance between at-risk mental state (ARMS) and first episode psychosis (FEP) patients and healthy controls (HC). In line with our hypothesis, results indicated a worse performance of FEP compared to ARMS and HC and a performance of ARMS intermediate to those two groups. Since these differences were more pronounced in the slope than in the intercept of the learning curve, our results indicated that the verbal learning rate tends to be more impaired than attentional processes in both ARMS and FEP patients. \nIn the second article we investigated whether VLM performance is associated with subcortical brain volumes. A significant negative association between amygdala and pallidum volume and attention span was found in ARMS and FEP patients combined, which however did not withstand correction for multiple testing. Although VLM is among the most impaired cognitive domains in emerging psychosis, the deficits in this domain seem not to necessarily stem from alterations in subcortical structures. \nIn the third article, we investigated whether subcortical brain volumes are dependent on sex. Men presented with larger total brain volume and smaller caudate and hippocampus volumes than women independent of diagnostic group. These analyses confirmed previously described patterns of sexual dimorphism in total brain and caudate volume that are equally present in ARMS and FEP patients as well as HC. The only structure affected by reversed sexual dimorphism was the hippocampus (i.e. women showing higher volumes than men). \nIn conclusion, neuropsychological impairments in terms of VLM and subcortical brain structural alterations are present in emerging psychosis. However, subcortical volumes do not seem to be affected by altered sexual dimorphism and may thus not contribute to an effective prediction modelling of transition to psychosis.",
                "authors": "L. Egloff",
                "citations": 1
            },
            {
                "title": "AERODYNAMIC DATABASE CONSTRUCTION FOR FLIGHT DYNAMIC MODEL OF LOW SUBSONIC FIXED-WING VTOL UAV",
                "abstract": "During Unmanned Air Vehicle development, some verification process such as flight simulation and flight test are necessary. These verification process is aimed to verify the compliance of UAV performance with its Design Requirement and Objectives. In order to do accurate flight simulation, the accurate aerodynamic coefficients prediction is required. Wind Tunnel test is the most accurate method among others but it is not always available. Thus, the computational aerodynamics method was implemented. Among well-known Computational Aerodynamics methods to predict the aerodynamic coefficients, Vortex Lattice Method and Computational Fluid Dynamics are frequently used during aircraft conceptual design process. VLM gives less accurate prediction after separation flow occurs, meanwhile CFD method provides options for modeling the turbulent flows but requires high cost and time. In this paper, construction of aerodynamic coefficients data of fixed-wing VTOL UAV was explained. In this case, the aerodynamic coefficients data must be constructed as accurate as possible with limited computational resources and without wind tunnel data. Three different levels of geometry complexity of fixed-wing UAV were used in Vortex Lattice Method and full configuration geometry was used in CFD simulation for longitudinal force and moment analysis. The result is VLM with simplified geometry was considered as the best match with CFD result for longitudinal motion. Thus, the construction of other aerodynamics coefficients for lateral and longitudinal motion was done by the simplified model. The decision of kinds of aerodynamic coefficients was based on open source flight dynamics module JSBSim.",
                "authors": "Febriyan Prayoga, Jae Young Kim, Nwe Tun Wai, D. Ryu, Sangho Kim",
                "citations": 0
            },
            {
                "title": "Identificación de Parámetros de Desempeño de un UAV a Través de Vuelos de Prueba",
                "abstract": "The design and implementation of a autonomous navigation and control system for UAV, mandates simulating the system before in-flight testing.  These simulations require a computational model of the aircraft, that can be obtained from the computation of aerodynamic parameters such as the drag polar. In this paper we present the identification of these parameters using two methods: one by flight testing techniques; the other by a combination of semi-empirical methods and computational fluid dynamics, dubbed the hybrid method. The hybrid method, which produces a parabolic polar drag, had the same qualitative trend compared to experiments. However, a detailed analysis of the parameters that shapes the polar drag, had significant differences, particularly in the parasite drag. This is probably due to propeller effects, low Reynolds Number and limitations of the piloting technique. Keywords: Drag polar, flight testing, parasite drag, UAV, VLM",
                "authors": "Oscar Daniel Garibaldi Castillo, A. Beltrán",
                "citations": 0
            },
            {
                "title": "Development and application of WASPE for conceptual design of HEDiP aircraft",
                "abstract": "In recent years, there has been an increasing emphasis on sustainability in civil aviation. This has motivated new aircraft designs with increased fuel efficiency and reduced emissions. A Hybrid Electric Distributed Propulsion (HEDiP) system offers considerable promise to reduce fuel consumption and emissions. However, the current conceptual design methods are not fully mature for developing optimal HEDiP configurations. Fast and accurate estimation of wing aerodynamic characteristics in the presence of multiple propellers is one of the key needs. This need is fulfilled by the development of WASPE (Wing Aerodynamic Simulation with Propeller Effects), a modified vortex lattice method (VLM). WASPE is coupled with the Pacelab Aircraft Preliminary Design (APD) multidisciplinary design and optimization framework which is then employed to perform conceptual design of a regional transport aircraft using a HEDiP system. The resulting capability offers designers new opportunities of exploring novel designs of more sustainable aircraft.",
                "authors": "R. Ganesh, rvg, P. Raj, S. Choi",
                "citations": 0
            },
            {
                "title": "AERODYNAMIC DATABASE CONSTRUCTION FOR FLIGHT DYNAMIC MODEL OF LOW SUBSONIC FIXED-WING VTOL UAV",
                "abstract": "During Unmanned Air Vehicle development, some verification process such as flight simulation and flight test are necessary. These verification process is aimed to verify the compliance of UAV performance with its Design Requirement and Objectives. In order to do accurate flight simulation, the accurate aerodynamic coefficients prediction is required. Wind Tunnel test is the most accurate method among others but it is not always available. Thus, the computational aerodynamics method was implemented. Among well-known Computational Aerodynamics methods to predict the aerodynamic coefficients, Vortex Lattice Method and Computational Fluid Dynamics are frequently used during aircraft conceptual design process. VLM gives less accurate prediction after separation flow occurs, meanwhile CFD method provides options for modeling the turbulent flows but requires high cost and time. In this paper, construction of aerodynamic coefficients data of fixed-wing VTOL UAV was explained. In this case, the aerodynamic coefficients data must be constructed as accurate as possible with limited computational resources and without wind tunnel data. Three different levels of geometry complexity of fixed-wing UAV were used in Vortex Lattice Method and full configuration geometry was used in CFD simulation for longitudinal force and moment analysis. The result is VLM with simplified geometry was considered as the best match with CFD result for longitudinal motion. Thus, the construction of other aerodynamics coefficients for lateral and longitudinal motion was done by the simplified model. The decision of kinds of aerodynamic coefficients was based on open source flight dynamics module JSBSim.",
                "authors": "Febriyan Prayoga, Jae Young Kim, Nwe Tun Wai, D. Ryu, Sangho Kim",
                "citations": 0
            },
            {
                "title": "Identificación de Parámetros de Desempeño de un UAV a Través de Vuelos de Prueba",
                "abstract": "The design and implementation of a autonomous navigation and control system for UAV, mandates simulating the system before in-flight testing.  These simulations require a computational model of the aircraft, that can be obtained from the computation of aerodynamic parameters such as the drag polar. In this paper we present the identification of these parameters using two methods: one by flight testing techniques; the other by a combination of semi-empirical methods and computational fluid dynamics, dubbed the hybrid method. The hybrid method, which produces a parabolic polar drag, had the same qualitative trend compared to experiments. However, a detailed analysis of the parameters that shapes the polar drag, had significant differences, particularly in the parasite drag. This is probably due to propeller effects, low Reynolds Number and limitations of the piloting technique. Keywords: Drag polar, flight testing, parasite drag, UAV, VLM",
                "authors": "Oscar Daniel Garibaldi Castillo, A. Beltrán",
                "citations": 0
            },
            {
                "title": "Biologically Inspired Dynamic Soaring Maneuvers for an Unmanned Air Vehicle Capable of Sweep Morphing",
                "abstract": null,
                "authors": "I. Mir, A. Maqsood, S. Akhtar",
                "citations": 0
            },
            {
                "title": "Simulation Software to Assess How, Why, Where and When Components will Fail",
                "abstract": "Advanced simulation technology, employing material failure mechanics has been proven effective in diagnostics; prognostics; forensic analysis; design, operation and maintenance optimization; and many other applications. The Virtual Life Management (VLM) simulation software determines, not only “How”, but also “Why, Where, and When” a part, assembly, or a system will fail.",
                "authors": "J. Carter",
                "citations": 0
            },
            {
                "title": "Combining tide-gauge observations with estimates of spatial sea-level variability to improve reconstructions and to close the contemporary global and regional sea-level budget",
                "abstract": "All processes that affect sea level show distinct regional patterns, and therefore, sea-level changes show considerable spatial variability. This spatial variability forms a challenge when reconstructing global and regional sea-level changes from tide gauges, which are only available at a limited number of locations and are mostly located along coasts in the northern hemisphere. We can improve estimates of global and regional sea-level changes from tide-gauge records by explicitly taking the expected spatial variability into account. From estimates of global ice mass loss and land water storage changes, spatial sea-level ﬁngerprints can be computed, which identify whether sea-level changes at speciﬁc tide-gauge locations are expected to be representative for global-mean or basin-mean sea-level changes. Furthermore, developments in altimetry and GPS now allow for reliable estimates of local vertical land motion (VLM) at an increased number of tide gauges. We reconstruct global and regional sea-level changes since 1958 by combining the expected sea-level ﬁngerprints with observations from tide gauges and GPS stations. The spatial sea-level ﬁngerprints associated with glacial isostatic adjustment (GIA) and present-day ice mass loss and land water storage are used to detect and correct possible biases due to the uneven spatial sampling of the tide-gauge observations. The ﬁngerprints that emerge from GIA and present-day mass redistribution have an earth-deformation component",
                "authors": "T. Frederikse, S. Jevrejeva, R. Riva, S. Dangendorf",
                "citations": 0
            },
            {
                "title": "Delft University of Technology Topology Optimization of Heating Chamber of Vaporizing Liquid Microthrusters",
                "abstract": "Vaporizing Liquid Microthrusters (VLM) have recently received attention as promising propulsion technology for highly miniaturized spacecraft due to its high thrust levels and low power consumption. This paper presents the results of numerical optimization of the parameters for the design of the heating chamber of VLMs that use water as the propellant. The optimization is aimed to increase the heat transfer coefficient of the heating chamber in order to maximize the heat convection while minimizing the heat and pressure losses from the inlet to the nozzle as well as the size of the device. The simulations are carried out in a combined environment using Computational Fluid Dynamics (CFD) and an optimization tool to run the algorithms. The results of the optimization are compared to the results of a comprehensive experimental campaign and are intended to be used in the next design of the VLMs produced by TU Delft that will fly on-board of a PocketQube.",
                "authors": "M. D. A. C. E. Silva, D. C. Guerrieri, A. Cervone, E. Gill",
                "citations": 0
            },
            {
                "title": "NOVEL ROLE OF GLYCINE IN CONTROL OF SYMPATHETIC OUTFLOW",
                "abstract": "The arterial baroreflex is one of the essential negative‐feedback mechanisms that compensates rapid changes in blood pressure (BP) through inhibition of presympathetic neurons in the rostral ventrolateral medulla (RVLM). There is general consensus that GABAergic projections originating from the caudal VLM (CVLM) provide the primary inhibitory input that controls the excitability of presympathetic RVLM neurons and sympathetic output. However, glycine, another inhibitory neurotransmitter, is also present in the RVLM. In anesthetized rats, we found that RVLM glycine played a critical role in arterial baroreflex function by controlling the time course of baroreflex mediated inhibition of renal sympathetic nerve activity. Blockade of glycine receptors in the RVLM, following an increase in BP, increased recovery of renal sympathetic nerve activity to baseline levels (5.1 ± 1.0 vs 11.7 ± 1.3 min, N=6, P<0.01) in comparison to control. In addition, tonic glycinergic inhibition of the RVLM was unmasked following blockade of output from the nucleus tractus solitaries and/or during disinhibition of the CVLM. Our whole‐cell, patch‐clamp recordings were obtained from brainstem slices containing presympathetic neurons in the RVLM labeled with PRV‐152. We confirmed that in steady state conditions GABAergic inhibition of RVLM neurons predominated and glycine contributed less than 25% of overall inhibition. In contrast, activation of the network produced saturation of GABAergic inhibition and glycinergic inhibition became prominent. Our findings demonstrated that blockade of glycine receptors in the RVLM decreased duration of baroreflex sympathoinhibition; increase of BP and activation of the network unmasked glycinergic inhibition of the RVLM; and release of glycine in the RVLM depended on the activity of presynaptic inhibitory inputs.",
                "authors": "Gao Hong, W. Korim, S. Yao, C. Heesch, A. Derbenev",
                "citations": 0
            },
            {
                "title": "Eff ect of a single asenapine treatment on Fos expression in the brain catecholamine-synthesizing neurons: impact of a chronic mild stress preconditioning",
                "abstract": "Abstract Objective. Fos protein expression in catecholamine-synthesizing neurons of the substantia nigra (SN) pars compacta (SNC, A8), pars reticulata (SNR, A9), and pars lateralis (SNL), the ventral tegmental area (VTA, A10), the locus coeruleus (LC, A6) and subcoeruleus (sLC), the ventrolateral pons (PON-A5), the nucleus of the solitary tract (NTS-A2), the area postrema (AP), and the ventrolateral medulla (VLM-A1) was quantitatively evaluated aft er a single administration of asenapine (ASE) (designated for schizophrenia treatment) in male Wistar rats preconditioned with a chronic unpredictable variable mild stress (CMS) for 21 days. Th e aim of the present study was to reveal whether a single ASE treatment may 1) activate Fos expression in the brain areas selected; 2) activate tyrosine hydroxylase (TH)-synthesizing cells displaying Fos presence; and 3) be modulated by CMS preconditioning. Methods. Control (CON), ASE, CMS, and CMS+ASE groups were used. CMS included restraint, social isolation, crowding, swimming, and cold. Th e ASE and CMS+ASE groups received a single dose of ASE (0.3 mg/kg, s.c.) and CON and CMS saline (300 μl/rat, s.c.). The animals were sacrificed 90 min aft er the treatments. Fos protein and TH-labeled immunoreactive perikarya were analyzed on double labeled histological sections and enumerated on captured pictures using combined light and fluorescence microscope illumination. Results. Saline or CMS alone did not promote Fos expression in any of the structures investigated. ASE alone or in combination with CMS elicited Fos expression in two parts of the SN (SNC, SNR) and the VTA. Aside from some cells in the central gray tegmental nuclei adjacent to LC, where a small number of Fos profiles occurred, none or negligible Fos occurrence was detected in the other structures investigated including the LC and sLC, PON-A5, NTS-A2, AP, and VLM-A1. CMS preconditioning did not infl uence the level of Fos induction in the SN and VTA elicited by ASE administration. Similarly, the ratio between the amount of free Fos and Fos colocalized with TH was not aff ected by stress preconditioning in the SNC, SNR, and the VTA. Conclusions. Th e present study provides an anatomical/functional knowledge about the nature of the acute ASE treatment on the catecholamine-synthesizing neurons activity in certain brain structures and their missing interplay with the CMS preconditioning.",
                "authors": "J. Osacká, L. Horvathova, Z. Majerčíková, A. Kiss",
                "citations": 1
            },
            {
                "title": "Applications of radial basis functions to fluid-structure coupling",
                "abstract": "The interaction between fluid flows and immersed structural objects is of great interest in engineering and the physical sciences. Such interactions are often highly non-linear and their simulation requires robust methods of information transfer between fluid and structural solvers. One of the most recent and promising developments is a number of methods based around radially symmetric functions known as radial basis functions (RBFs). This paper presents results of a lightweight and robust RBF fluid-structure coupling implementation in based primarily on the works of Rendall and Allen. The coupling is demonstrated in steady and unsteady flows using VLM and CFD codes. Extensions involving localised/partitioned approaches to the method are shown. Analysis of the effects of method parameters (choice of basis functions, model partitioning/partitions of unity) is presented, as well as an exploration of problem-specific parameter selection. Applications of the method to more general interpolation problems (e.g. mesh generation and movement) are also discussed. Full details of the results discussed here can be found in [1].",
                "authors": "A. J. Murray, G. Vio, B. Thornber, J. Geoghegan",
                "citations": 1
            },
            {
                "title": "Closed-loop Thrust Magnitude Control System for Nano- and Pico-Satellite Applications",
                "abstract": "The growing needs of nano- and pico-satellite missions require several enhancements in micro-propulsion capabilities to enable the satellites to perform an increasing variety of orbital maneuvers. Among them, the possibility to accurately control the thrust would open up new scenarios for nano- and pico-satellites applicability to include, for example, missions such as space debris removal and orbit transfer. This thesis presents the design and the implementation of a closed-loop control system for thrust magnitude regulation in micro-resistojets. This is achieved by controlling the propellant mass flow in the micro-valve of the feeding system which is designed to make extensive use of off-the-shelf components. The Vaporizing Liquid Micro-resistojet (VLM) is one of the micro-thrusters under development at TU Delft and, for this reason, it is selected for performing the tests. The outcome of this work is meant to give insights into the design and performance level of future technologies for thrust magnitude control, which will be designed and manufactured at TU Delft. In order to develop appropriate controllers, a non-linear state-space model of the micro-valve system is developed analytically. The model includes different domains, such as electro-magnetic, fluid and mechanical, in order to bring together the complex dynamic behavior of the actuator. The performance of the analytical model is compared to a more sophisticated multi-domain analysis performed with finite element method (FEM) and computational fluid dynamics (CFD). Two controllers, namely PID (proportional-integral-derivative) and SMC (sliding mode control), are designed and tested using the models developed. The ON/OFF micro-valves are controlled with PWM (pulse width modulation) by tuning the operating duty cycle. A hybrid sliding mode control scheme is developed based on the insights gave by the experiments, which enables the thrust magnitude control of the comprehensive system. Finally, the closed-loop control system is implemented in the preliminary hardware design of the micropropulsion system. The experimental set-up comprises the propellant tank, the micro-valve, the micro-thruster, the feeding channel, pressure and temperature sensors and the processing microcontroller. The tests are focused on the performance of the controller, and the fine tuning of its parameters, and also in the validation of the design approach. The controlled micro-propulsion system delivers the commanded input with a response time of ~1.5 s and a resolution of 50 mbar in the chamber pressure. As a first implementation of a control system for micro-propulsion at TU Delft, the results are promising and certainly pave the way for future developments on this research field.",
                "authors": "S. Silvestrini",
                "citations": 1
            },
            {
                "title": "Characterization and Testing of a MEMS-Vaporizing Liquid Microthruster for Small Satellite Propulsion",
                "abstract": "Active propulsion is required in order to further develop the capabilities of small satellites like CubeSats. At TU Delft a micropropulsion system is being developed based on a Vaporizing Liquid Microthruster (VLM), using liquid water as propellant. This research presents the manufacturing, characterization and testing of an existing VLM-design. It focuses on the thruster component of the propulsion system, which is called the MEMS-VLM. The breadboard model consists of an inlet module, multiple heating chamber modules and a nozzle, integrally manufactured using MEMS-technology. The heating chamber modules consist of flow channels etched in silicon, of which the walls support silicon carbide heaters. These heaters are suspended in the center of the cross-section. The nozzle is a two-dimensional convergent-divergent duct. Characterization and testing of the MEMS-VLMis done in a bottom-up approach. First, the geometry and electrical properties of the flow channels and heaters are characterized, after which the performance of the heating chamber and the nozzle are independently studied by models and experiments. In order to facilitate manufacturing, minor changes are implemented to the design of the nozzle, electrical interface and fluidic interface. With these changes, successful manufacturing of theMEMS-VLM is achieved. Microscope images show significant but acceptable deviations between design dimensions and manufacturing results. Furthermore, resistance measurements of the heaters indicate partial mechanical failures in 20 out of 154 measured heaters. The thermal behavior of the heating chamber is studied by means of analytical models for conduction and radiation and a one-dimensional steady state model for convection. The conduction study shows that the chamber wall temperature is homogeneous, while the suspended heaters formhot spots. The convection model is used to study whether sufficient heat transfer can be expected to fully vaporize the propellant flow. It is found that at a chamber wall temperature as little as 10 K above the boiling point, full vaporization can be expected. An experimental setup is made to test the heating chamber, ultimately attempting to validate the design and the performance models. The ability to power the heaters with constant current and measure the mass flow, pressure and dissipated power is demonstrated. Furthermore, amethod to determine the local chamber temperature based on the resistance of the heaters is presented. However, current leaks and partial mechanical failures are detected in the heaters during tests with nitrogen gas, which compromise the ability to operate with liquid water. Hence, full validation of the heating chamber models could not be achieved. With respect to future designs it is recommended to change to metallic heaters, which are placed outside the chamber. The silicon chamber wall then serves as a heat exchanger. It is furthermore recommended to change the capping wafer material to glass, so that the boiling process can be studied optically. A second experimental campaign is done to determine the performance of the nozzle, which is described by the discharge factor CD, the Isp-efficiency ´I sp and the thrust coefficient CF . Nozzle performance tests are carried out in vacuum, using gaseous nitrogen as propellant. The pressure is adapted in order to match the Reynolds number in the nozzle throat to the designed operating conditions. At a measurement Reynolds numbers of Rem ¼ 1400 the performance quality factors are found to be CD ¼ 0.68§0.06 (discharge factor), ´I sp ¼ 0.3§0.05 (Isp-efficiency) and CF ¼ 0.33§0.06 (thrust coefficient). The poor performance is primarily explained by excessive friction and rarefaction losses in the divergent section of the nozzle and are expected to improve when increasing the Reynolds number and when decreasing the expansion ratio. Design validation of the MEMS-VLM could not be achieved, as on various aspects the design is rejected. Nonetheless, the experimental approach has been successful in revealing critical design recommendations and demonstrating the ability to test a microthruster, improving the understanding and advancing the development of micropropulsion at TU Delft.",
                "authors": "T. V. Wees",
                "citations": 1
            },
            {
                "title": "Programación Funcional Multi-Core y Many-Core del Método de Red de Vórtices Bidimensional",
                "abstract": "En los ultimos anos, se ha extendido la utilizacion de tecnicas de programacion paralela que permitieran una disminucion en el tiempo de resolucion de algoritmos numericos y un mejor aprovechamiento de hardware haciendo uso de arquitecturas redundantes en CPU (Central Processing Unit) y GPU (Graphic Processing Unit). En este informe se implementan las tecnicas de programacion secuencial, Multi-Core y OpenCL-GPGPU (General-Purpose Computing on Graphics Processing Units) en el software Wolfram Mathematica, para la solucion de un problema de determinacion de cargas circulatorias sobre una placa plana que se mueve a una velocidad V con un determinado angulo α (de ataque) por un fluido incompresible, no viscoso (irrotacional) y sobre la que se inducen ademas acciones debidas a vortices libres (estela) a traves del tiempo. El metodo utilizado es red de vortices (VLM, Vortice-Lattice Method). En este metodo, la parte computacionalmente mas intensiva se debe a la evaluacion de interaccion de vorticidad, modelada mediante la ley de Biot–Savart. Esta ley, dentro del metodo, resulta altamente paralelizable sobre SIMD (Single Instruction, Multiple Data). De lo expuesto, se implementan rutinas de medicion y comparacion secuenciales, paralelas sobre CPU (con y sin HyperThreading y con granulometria gruesa y fina), paralelas sobre GPU e hibridas y se comparan los resultados obtenidos. Se muestran en detalle las implementaciones realizadas. El codigo desarrollado para GPU muestra reducciones de tiempos de analisis de hasta 15X frente a computos con nucleo simple en el problema completo y mayores si se aisla el modulo de Biot-Savart.",
                "authors": "Mariano P. Ameijeiras",
                "citations": 0
            },
            {
                "title": "Injection of a microsatellite in circular orbits using a three-stage launch vehicle",
                "abstract": "The injection of a satellite into orbit is usually done by a multi-stage launch vehicle. Nowadays, the space market demonstrates a strong tendency towards the use of smaller satellites, because the miniaturization of the systems improve the cost/benefit of a mission. A study to evaluate the capacity of the Brazilian Microsatellite Launch Vehicle (VLM) to inject payloads into Low Earth Orbits is presented in this paper. All launches are selected to be made to the east side of the Alcântara Launch Center (CLA). The dynamical model to calculate the trajectory consists of the three degrees of freedom (3DOF) associated with the translational movement of the rocket. Several simulations are performed according to a set of restrictions imposed to the flight. The altitude reached in the separation of the second stage, the altitude and velocity of injection, the flight path angle at the moment of the activation of the third stage and the duration of the ballistic flight are presented as a function of the payload carried.",
                "authors": "L. O. Marchi, J. Murcia, A. Prado, C. Solórzano",
                "citations": 0
            },
            {
                "title": "Iskustvene spoznaje primjene vertikalnih podiznih modula",
                "abstract": "Kroz ovaj diplomski rad opisana je svrha, uloga i tehnicka rjesenja automatiziranih skladisnih sustava. Naglasak je stavljen na vertikalne podizne module (eng. Vertical Lift Module – VLM) i njihovu primjenu u praksi. Dani su primjeri i iskustva vezana za projektiranje, montažu, rad i eventualne probleme koji se javljaju u ekspoataciji vertikalnih podiznih modula. \nTemeljem provedenih mjerenja radnih parametara vertikalnih podiznih modula i usporedbom izmjerenih vrijednosti s analitickim modelima dan je osvrt na problematiku oblikovanja i upravljanja ovim sustavima u praksi. \nNadalje, prikazana je usporedba vertikalnih podiznih modula s ostalim sustavima automatiziranog skladistenja koje nudi poduzece “Kardex Produktion Deutschland GmbH”, apsolutno najzastupljeniji proizvođaca ovog tipa sustava u Hrvatskoj.",
                "authors": "Stjepan Brozović",
                "citations": 0
            },
            {
                "title": "Investigating the Elastic Performance of Metallic Wings with Piezoelectric Actuators using Smart Triangular Finite Element",
                "abstract": "A Finite Element Model (FEM) for the elastic analysis of aircraft wings with piezoelectric actuators is developed. A smart shell triangular element is established for the FE analysis. The wing aerodynamic loads are obtained using the Vortex Lattice Method (VLM). The model is used to investigate the effect of several piezoelectric voltages on the elastic performance of metallic wings with different configurations. These include as straight rectangular wing, forward swept rectangular wing, backward swept rectangular wing and straight tapered wing, in addition to a 3D wing of an UAV. It is found that the wing displacement can be decreased with the increase in the applied voltage for all wing configurations. However, the rate of decrease in case of backward swept wing is higher than in other cases. Additionally, in case of 3D relatively large wings the piezoelectric effect is found to be insignificant.",
                "authors": "M. Mahran, H. Negm, A. Elsabbagh",
                "citations": 0
            },
            {
                "title": "NIMG-88. RADIONOMIC ANALYSIS OF WHO GRADE 2 AND 3 GLIOMAS WITH GENETIC SUBGROUP PREDICTION.",
                "abstract": "AbstractPURPOSEGenetic alterations found in WHO grade 2 and 3 gliomas include IDH1/2 and TERT promoter mutations and 1p19q co-deletion, which alterations are known to have great impact on the prognosis of the patient. In this research, the authors attempted to test the hypothesis that genetic alterations could contribute to the locations and heterogeneity of the tumor by analyzing 191 WHO grade 2 and 3 gliomas via MR radionomics.METHODS191 WHO grade 2 and 3 gliomas were retrospectively collected and the genomic DNA of the tumor was sequenced for IDH1/2 and TERT promoter mutations. Treatment naive MR images were also collected. T2 weighted, T1 weighted, FLAIR and Gd-enhanced T1 weighted images were collected for analysis. Voxel-based lesion mapping (VLM), and redionomics analysis was performed for all images and were further challenged to construct a model that predicts genetic alterations within the tumor. 126 patients were allocated as training set and 65 for validation set. Patient cohort was divided into the following 3 groups; IDH mt/TERTp wt, IDH mt/TERTp mt and IDH wt. A multi-regression model was constructed using the training set to predict the 3 genetic subgroups and the validation set was used for model validation.RESULTSVLM revealed that IDH mt/TERTp wt gliomas dominated temporal lobe involvement while IDH mt/TERTp mt occupied the frontal lobe. IDH wt tumors, on the other hand, located at much posterior lobes and centered at the deep white matter. 15 radionomic features were identified for model construction to predict 3 genetic subgroups of WHO grade 2 and 3 gliomas. When these 15 texture elements were used to construct a prediction model of the 3 genetic subgroups, AUCs calculated from the training set ranged from 0.7 to 0.75. Accuracy for prediction was 63% for the training set and 60% for the validation set.",
                "authors": "M. Kinoshita, Hideyuki Arita, Masamichi Takahashi, Y. Narita, Y. Terakawa, N. Tsuyuguchi, Y. Okita, M. Nonaka, S. Moriuchi, J. Fukai, S. Izumoto, Kenichi Ishibashi, Y. Kodama, K. Mori, Koh Ichimura, Y. Kanemura",
                "citations": 0
            },
            {
                "title": "Investigation on Natural Frequency and Fuselage Effect for Small UAVs Lateral Motion",
                "abstract": "An accurate mathematical model is necessary for controlling an aircraft. Although the geometrical scale of Unmanned Aerial Vehicles (UAVs) is very small compared to the large aircrafts, they are usually designed by means of the procedure intended for large ones, and stability calculations similarly follow the same formulas. This fact can severely affect the basic assumptions of the formulas and hence it may not be suitable for UAVs. This research validates the dutch roll natural frequency of lateral motion calculated by comparing the usual methods of estimation for the manned aircraft found in references of Roskam and Ostoslavsky, and the numerical Vortex Lattice Method (VLM) program XFLR5 with experimental values of real flight. Also a study is carried out to examine the effect of fuselage on the dutch roll natural frequency to examine the possibility of neglecting it through the calculations. It is found that approximate methods for Roskam procedure is in accordance with the exact solution, and the same for Ostoslavsky. Estimation methods of Roskam (exact), Ostoslavsky and XFLR5 give good results in agreement with the experiment, while the approximate methods of Roskam underestimate the frequency. The contribution of the regular fuselage is found to be very small and it can safely be neglected. NOMENCLATURE ωn dutch roll mode natural frequency",
                "authors": "M. El-Salamony, S. Serokhvostov",
                "citations": 0
            },
            {
                "title": "StopRotor - A new VTOL aircraft configuration",
                "abstract": "The StopRotor Unmanned Aerial Vehicle (UAV) is a new aircraft configuration capable of both rotary and fixed wing flight. The design combines the utility of the open rotor system with fixed wing efficiency by in flight configuration changes. The StopRotor is capable of carrying a 40% payload relative to its empty flying weight. The optimization of the platform for an expanded range, endurance and mission capability relies on the ability to model the platform aerodynamically. Wind tunnel and aerodynamic analysis using Vortex Lattice Methods were used to extract and validate the aerodynamic 6DOF model. This was investigated at RMIT University in the RMIT Industrial Wind Tunnel. The correlation between the VLM and experimental results were favourable and increased confidence in using approximated derivatives that could not be obtained through static wind tunnel tests. Key performance parameter include CD0 = 0.037, L/Dmax = 17, VStall = 15ms-1, CL,max = 1.6.",
                "authors": "M. Marino, J. Ambani, R. Watkins, R. Sabatini",
                "citations": 0
            },
            {
                "title": "Optimisation de la préparation de commandes dans les entrepôts de distribution",
                "abstract": "La preparation de commandes est une activite primordiale dans les entrepots de distribution (pres de 60% des couts operationnels dans les entrepots traditionnels). Un des moyens de reduire ces couts est de collecter plusieurs commandes simultanement, plutot qu’une par une ; cela permet d’eviter des deplacements inutiles des operateurs dans le cas de collecte manuelle des produits, et de moins solliciter les machines, dans le cas de collecte automatisee. Nous considerons dans cette these des entrepots de distribution ou les produits sont stockes dans des machines appelees stockeurs automatises composes de plateaux sur lesquels les produits sont stockes ; nous prenons en compte deux types de stockeurs : les VLMs et Carousels. Ces deux types de machines, en plein developpement, se differencient par le temps necessaire pour passer d’un plateau au plateau suivant a visiter (constant dans le cas d’un VLM, dependant du nombre d’etages entre les 2 plateaux visites dans le cadre des carousels). L’objectif de la these est donc de developper des methodes pour faire des regroupements de commandes en lots dans des entrepots automatises afin de collecter un ensemble donne de commandes le plus efficacement possible selon des criteres que nous allons definir.Nous etudions tout d’abord un premier type de regroupement de commandes en lots, pour lequel chaque lot sera collecte entierement (toutes les commandes composant le lot seront traitees) avant de passer au suivant. Nous parlerons simplement de batching. Nous evaluons le temps de preparation de commandes, dans le cas ou l’operateur effectue la collecte sur une ou sur plusieurs machines. L’interet d’utiliser plusieurs machines est de permettre la recherche de plateaux en temps masque (le stockeur effectue un changement de plateaux pendant que l’operateur est occupe a effectuer la collecte sur d’autres stockeurs). Cette evaluation du temps de preparation de commandes nous permet d’extraire un critere d’optimisation et d’etablir des modeles d’optimisation exacts pour les VLMs puis les carousels. Ces modeles sont ensuite testes avec des donnees reelles d’entreprise, grâce au partenariat avec l’entreprise KLS Logistic, editeur du WMS Gildas. Enfin, nous etendons la resolution aux methodes approchees de type metaheuristiques, afin de garantir de bonnes solutions sous un temps de calcul raisonnable. Des resultats significatifs en termes de reduction du temps de preparation de commandes permettent de justifier l’interet de nos travaux.Nous etudions ensuite un deuxieme type de regroupement, ou un ensemble de commandes est collecte simultanement mais, contrairement au cas precedent, a chaque fois qu’une commande est terminee, elle est immediatement remplacee. Nous parlerons ici de « Rolling batching ». Cette approche est classiquement utilisee dans les « Drive ». Nous nous focalisons sur le cas de collecte effectuee sur des carousels, systeme le plus utilise dans les « Drive ». Nous developpons un algorithme permettant le calcul du temps d’attente de l’operateur. Une resolution approchee couplant l’utilisation d’heuristiques et d’une metaheuristique est proposee afin de resoudre efficacement le sequencement des commandes. Nous notons que des gains significatifs sont obtenus par l’utilisation de la methode proposee.",
                "authors": "Nicolas Lenoble",
                "citations": 0
            },
            {
                "title": "Cálculo de carregamento aerodinâmico em pá de gerador eólico empregando Vortex Lattice Method",
                "abstract": "Para viabilizar a utilizacao de microgeradores eolicos e adequado fazer o controle do ângulo de arfagem de forma passiva. Para fazer esse controle utilizando materiais compositos atraves do aeroelastic tailoring e necessario conhecer o carregamento aerodinâmico. No presente trabalho foi empregado o Vortex Lattice Method (VLM) para calculo do carregamento aerodinâmico em uma pa de gerador eolico. Para este desenvolvimento, foram abordados topicos do metodo de elemento de pa (BEM) e da teoria aerodinâmica. Para uso do VLM, a pa e dividida em uma malha, onde cada elemento recebe um ponto de controle e uma ferradura de vortices. Com a condicao de nao permeabilidade da parede da pa, e possivel calcular o carregamento em cada ponto. Os resultados obtidos estao proximos com os obtidos pelo metodo BEM, presente na literatura. Isso mostra a confiabilidade do VLM e seu potencial para ser utilizado no dimensionamento das pas do gerador eolico.",
                "authors": "Carlos Eduardo Kruppa, Suelen Baggio",
                "citations": 0
            },
            {
                "title": "Latent Dirichlet Allocation Based Image Retrieval",
                "abstract": null,
                "authors": "Jing Hao, Hongxi Wei",
                "citations": 0
            },
            {
                "title": "Programming Efficiency in the Creation of ADaM BDS Datasets",
                "abstract": "The ADaM Basic Data Structure (BDS) has become one of the most prominent and widely implemented dataset structures in the industry since the CDISC ADaM Implementation Guide V1.0 was published in 2009. The strictly vertical data design of the BDS brings two common challenges to statistical programming: (1) BDS datasets often quickly grow very large, especially in larger clinical trials. (2) Metadata for BDS datasets become more difficult to develop and understand due to the use of valuelevel metadata (VLM) for describing variables by PARAM/CD and the use of multiple BASETYPEs and other derived data records within the same dataset. This paper will describe the programming challenges specific to BDS and illustrate with examples how to achieve better programming efficiency and quality. The approaches include: 1) designing streamlined programming steps to maximize data processing efficiency; 2) reading metadata (e.g., VLM and Controlled Terminology) directly into dataset creation to ensure consistency and avoid error-prone hardcoding; and 3) using modular macros to standardize common data computations and imputations.",
                "authors": "Ellen O. Lin, T. Oaks",
                "citations": 0
            },
            {
                "title": "Real-time diffuse indirect illumination with virtual light meshes",
                "abstract": "Indirect illumination on a rendered scene can add a great deal to its visual quality, but it is also a costly operation. Therefore, a lot of research targets how to render indirect illumination in real-time. While powerful techniques for real-time indirect illumination currently exist, they provide only coarse-grained artistic control over the trade-off between quality and speed. We propose a Virtual Light Mesh to compute the scene’s diffuse indirect illumination, inspired by the use of other current auxiliary meshes such as Navigation Meshes and Collision Meshes. A Virtual Light Mesh (VLM) is a simplified mesh of polygonal lights used to approximate the light bounced by the real geometry. Together with the VLM, we design an acceleration data structure for efficient indirect illumination performance with a complex VLM. The use of VLM presents some positive properties: greater artistic control of the indirect illumination characteristics; the possibility of integration with existing techniques such as skeletal animation and procedural generation; and simple integration into existing asset production tools and pipelines. Our experimental results show that artist controlled indirect illumination is a viable alternative to existing methods.",
                "authors": "F. A. Aquotte",
                "citations": 0
            },
            {
                "title": "Coastal Vertical Land motion in the German Bight",
                "abstract": "In the framework of the ESA Sea Level Climate Change Initiative (CCI) we analyse a set of GNSS equipped tide gauges at the German Bight. Main goals are the determination of tropospheric zenith delay corrections for altimetric observations, precise coordinates in ITRF2008 and vertical land motion (VLM) rates of the tide gauge stations. These are to be used for georeferencing the tide gauges and the correction of tide gauge observations for VLM.",
                "authors": "M. Becker, L. Fenoglio, Florian Reckeweg",
                "citations": 0
            },
            {
                "title": "Toxocariasis in Brazilian Children : A Case-Control Study",
                "abstract": "Introduction: The identification of epidemiological, clinical, behavioral, and ultrasonographic and laboratory features could aid in the diagnosis of visceral larva migrans (VLM) in children and adolescents in Brazil. Methods: A case-control study was conducted in ambulatory patients aged 6 months to 16 years cared for 4 years at a pediatric infectious diseases outpatient service in Belo Horizonte, Brazil. Patients with serum ELISA (enzyme-linked immunosorbent assay) anti-T. canis IgG antibody titers>1:640 were assigned to the case group, and the remaining subjects were assigned to the control group. The statistical significance of univariate associations between the outcome variable and exposure (i.e., epidemiological, clinical, behavioral, laboratory and ultrasonographic findings) was assessed using Pearson’s chi-square (χ2) test or Fisher’s exact test. Multiple logistic regression analyses were used to assess the independent effect of each variable on the odds of the serologic status. Results: Thirty-seven cases and 31 controls were studied. In multiple analyses, residence in a rural area (OR = 4.23; 95% CI = 0.66-27.06), keeping a dog at home (OR = 9.71; 95% CI = 1.02-92.67), and total serum immunoglobulin E (IgE)>1,000 IU/ mL (OR = 2.05; 95% CI = 0.67-6.30) were the most important explanatory variables for VLM. Age, gender, hepatomegaly, splenomegaly, serum isohemagglutinins, total serum immunoglobulinG, immunoglobulin A (IgA) and immunoglobulinM, hypereosinophilia and abdominal ultrasonographic findings were not different in cases and controls. Conclusions: Keeping dogs at home and residing in rural areas are important epidemiological risk factors for VLM. In the presence of one or more of these risk factors, patients should undergo anti-Toxocara ELISA. However; with larger sample studies should be performed.",
                "authors": "E. Carvalho, Regina Lunardi, Rocha",
                "citations": 0
            },
            {
                "title": "Aerodynamic Analysis of Nonplanar Wings Mondher Yahyaoui",
                "abstract": "In this work four nonplanar wing configurations were studied using the VLM method: the wing-winglet, the Cwing, the biplane, and the box wing. It has been shown that linear twist, which is more practical in aeronautical construction, is more than adequate when it comes to achieving the higher values of span efficiency factor obtained by a completely optimized, but highly varying, twist distribution along the span camber. It has also been shown that moderate sweep can slightly increase the span efficiency factor, and further reduce vortex drag. But the most important finding is that the wing-winglet configuration and the C-wing, when both have a ratio of winglet/fin bottom chord to wing tip chord equal to 0.5, give the highest reduction in induced drag, clearly outperforming what is known as Prandtl’s best wing system. Empirical laws for induced drag ratio all the configurations, including the cases of the wing-winglet and C-wing with a chord ratio of 0.5 were obtained. The biplane and then the box wing configurations have the higher values of lift-to-drag ratio at higher aspect ratios whereas the wing-winglet configuration and C-wing with a chord ratio of 0.5 surpass the biplane and the box wing at lower aspect ratios. Reducing vortex drag can be viewed as more important reducing the overall lift-to-drag ratio of the wing configuration since vortex drag represents a much higher percentage of the overall drag of an aircraft than does the profile drag of the wings alone. From this perspective, the wing-winglet and C-wing with a chord ratio of 0.5 are a better choice than the Prandtl’s best wing system.",
                "authors": "Borj El Amri",
                "citations": 0
            },
            {
                "title": "United States Patent ( 19 ) Kato et al . ( 54 CHITOSAN-IODINE ADDUCT 75 Inventors :",
                "abstract": "(57) ABSTRACT (22) Filed: Mar. 14, 2012 This invention relates to the construction of a rocket motor Related U.S. Application Data and fuel system thereof and, in particular to a new and useful (60) Provisional application No. 61/452,559, filed on Mar. Viscous Liquid Monopropellant (VLM). rocket motor con 14, 2011 taining a liquid propellant that is pumped into the combustion s chamber, atomised and then ignited. The atomisation step O O significantly increases the Surface area of the propellant, Publication Classification delivering faster burn rates and smoother combustion. VLM (51) Int. Cl. is a non-Newtonian fluid containing both oxidisers and fuels. F42B (5/10 (2006.01) These monopropellants are comprised of a variety of liquid C06B 25/34 (2006.01) and Solid components, mixed together to form a homogenous C06B 3L/28 (2006.01) fluid, although heterogeneous in composition. The Solid con C06B 29/22 (2006.01) stituents are retained within the liquid phase by dispersion, FO2K 9/42 (2006.01) Suspension, bonding or chemical emulsification techniques, C06B 43/00 (2006.01) So as when a motive force is applied to the propellant, all the C06B 47/O (2006.01) constituents are also transported, and held in correct propor F42B I5/0 (2006.01) tion whilst doing so.",
                "authors": "M. Kato",
                "citations": 0
            },
            {
                "title": "Early Treatment with a Peptide Derived from the Human Heat-Shock 60 Protein Avoids Progression to Severe Stages of COVID-19",
                "abstract": null,
                "authors": "Hernández Jeb, Martín Adr, Cruz Ldr, Rodríguez Rv, Cedeño Mh, Diaz As, Ruiz Rp, Moynelo Ie, Sordo Tf, Adan Am, Mircevski Jg, Mulet Ds, Villanueva Fs, Rosabal Rd, Núñez Zr, Salina-Caballero Y, Paz Av, M. Om, Donato Gm, González Vlm, Nieto Gg, H. Mcd",
                "citations": 1
            },
            {
                "title": "Renal Cell Carcinoma: 10-Year Clinical-Pathological Behaviours Report in Cuban Hospital",
                "abstract": "Background: Introduction: Renal cell carcinoma (RCC) represents about 4% of the total adult malignant tumors and 90% of all renal tumors, all with different clinical evolution. Surgery is a gold standard of treatment in all stage and almost 95% of overall survival at 5 years in localized illness. Objective: to characterize the clinical-epidemiological behavior of the patients, and identify the possible associated variables. Methods: was conducted an observational, retrospective and longitudinal study to collected all the information recorded of the 336 surgical patients with RCC at the Cuban Hospital, over a 10 year period (2006-2015). A Cox regression model was adjusted to study the influence of demographic, anatomical, clinical-histological and laboratory variables on survival, with the consequent estimation of the risk ratios with their 95% confidence interval. Results: The patients was represent to males in 67.9% and white skin color predominated in 77.1% of them, with a median age of 57 years. The most frequent associated comorbidities were arterial hypertension (59.2%) and diabetes mellitus (36%). The majority of patients (64%) were asymptomatic. The classic RCC had the highest frequency (72.9%). High erythrocyte sedimentation and anemia were the laboratory parameters associated with more than 75%. In multivariate analyses survival according to the presence of thrombus in the renal vein, high grade, and capsule infiltration was significantly lower (p < 0.01), 95% confidence interval. Conclusions: unilateral disease, male and young patients characterized the main study populations. Overall survival was high and the risk of individual stratification established from laboratory variables as haemoglobin and erythrocyte sedimentation is an independent and significant predictor of survival in Cuban patients with RCC.",
                "authors": "Aguirrechu Ci, Oca Md, Yera Edc, Rojas Si, Lopez Ac, García Ag, Matos E, Rodriguez Vlm, Perez Drm, Carvajal M",
                "citations": 0
            },
            {
                "title": "Snake River Salmon and the National Forests : The Struggle for Habitat Conservation , Resource Development , and Ecosystem Management in the Pacific Northwest",
                "abstract": null,
                "authors": "Eda, Vlm",
                "citations": 0
            },
            {
                "title": "O CONTRATO LÚDICO NA PRÁTICA DO FUTEBOL LAZER – ESTUDO DA REPRESENTAÇÃO SOCIAL COM PRATICANTES MORADORES DE COMUNIDADES DE BAIXA RENDA NO MUNICÍPIO DE VIÇOSA EM MINAS GERAIS",
                "abstract": "Introducao: O  futebol  e  o  principal  pilar  de  sustentacao  da  cultura  esportiva  brasileira.  A modalidade esta inserida em todas as regioes do pais e absorve a atencao de diversas camadas sociais nas mais distintas faixas etarias. Na condicao de lazer ocupa um papel central seja na pratica cotidiana ou na relacao estabelecida com o clube pelo qual o individuo torce e dedica sua  atencao  emocional.  O  brasileiro  estabeleceu  com  a  modalidade  uma  rica  rede  de interacoes que pode ser observada naturalmente na estrutura social. Objetivo: Compreender  como  e  estabelecido  o  contrato  ludico  na  pratica  do  futebol  lazer  e tambem como o praticante de futebol representa esta pratica. Metodologia: A  investigacao  foi  do tipo  exploratorio,  com  analise  desenvolvida  baseada  na abordagem  qualitativa.  Os  instrumentos utilizados  foram  a  observacao  direta  e  a  entrevista semiestruturada,  em  que  utilizou-se  o  recorte  das  falas  dos  informantes  para  dar  suporte  a tessitura  do  texto.  A  amostra  foi  constituida  por  praticantes  de  futebol-lazer  de  um  campo localizado em uma favela (Escorpiao) no municipio de Vicosa, localizado no estado de Minas Gerais. O referencial teorico teve suporte nos apontamentos de Parlebas, Simmel e Elias. Resultados:  Pelos  relatos  dos  informantes  o  grupo  se  formou  voluntariamente  (amigos, vizinhos,  parentes,  convidados)  ao  final  dos  anos  de  1990. Sao  aproximadamente  100 integrantes,  que  usufruem do  espaco  fisico.  Os  adultos  tem  prioridade  sobre  as  criancas  e adolescentes que utilizam o mesmo espaco nos encontros que ocorrem aos finais de semana e em  feriados.  Nestes  dias as  criancas  jogam  e brincam  nos  espacos  marginais  do  campo, enquanto  as  mulheres  estabelecem  relacoes sociais  de  cunho verbais  sem  participacao  no campo esportivo. O contrato ludico e estabelecido sobre a forma de organizacao, os vinculos afetivos e a permanencia no grupo. Entretanto, o que funda o contrato e a confraternizacao que os encontros esportivos proporcionam. Conclusoes:  O  futebol  sugere  ser  o  eixo  basal  dos  encontros  sociais  da  comunidade. Entretanto, como justificar a presenca das mulheres e criancas que nao jogam? Parecem existir outros  fatores  socializantes  que  surgem  desta  modalidade,  ocasionando  diferentes  quadros nas relacoes sociais. O futebol lazer aparece como um espaco de transgressao da rotina diaria das  pessoas.  No  campo,  ainda  que  por  pouco  tempo,  ha  uma  tentativa  de descomprometimento  com  as  imposicoes  sociais,  promovendo  um  espaco  de  festa  e confraternizacao dos amigos e da familia.",
                "authors": "J. Salles, I. Costa, Vlm Costa",
                "citations": 0
            },
            {
                "title": "G379 ‘departmental induction…less mandatory and more opportunity’",
                "abstract": "Aim To consider if a comprehensive clinical induction improves paediatric trainee anxiety and confidence about starting a tertiary neonatal intensive care unit (NICU) placement. Working in a tertiary NICU is compulsory training for paediatric doctors. Feeling competent and confident to manage common neonatal scenarios is often a cause of trainee anxiety. Departmental inductions deliver a wealth of information to ensure the process of safe handover between rotations of junior doctors. Rarely, however, are they used to cover the topics most needed by the new doctors or used as a teaching experience. Methods A standardised clinical induction was implemented to a tertiary NICU departmental programme. Every trainee rotated through simulated sessions covering airway management, central access, neonatal life support and an emergency simulation scenario. This allowed trainees to practice essential skills for the job in a safe environment with the department’s equipment and resources including permanent NICU staff. Results Qualitative and quantitative feedback was obtained from a total of 14 trainees over a one year period at two separate NICU clinical inductions. Trainees completed questionnaires rating their confidence overall and in relation to NICU specific situations, before and after the induction [1 (least) to 10 (most)]. These responses were analysed using the Mann-Whitney-U test. In all situations, there was an overall improvement in trainee confidence post induction. This was statistically significant (p≤0.05) in trainee confidence to manage a neonatal airway problem with airway manoeuvres or non-invasive respiratory support and trainee confidence to initiate management in situations requiring neonatal resuscitation. Other positive factors trainees reported as a result of the clinical induction included feeling part of the NICU team (13/14 trainees scored ≥7/10) and feeling inspired (11/14≥8/10). Conclusions Empowering trainees through a good clinical induction improves confidence and experience of working in a tertiary NICU, potentially improving patient care and management. The departmental induction is an excellent education opportunity for all involved.",
                "authors": "Vlm Leith",
                "citations": 0
            },
            {
                "title": "MMBench: Is Your Multi-modal Model an All-around Player?",
                "abstract": "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. The evalutation code of MMBench has been integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.",
                "authors": "Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin",
                "citations": 613
            },
            {
                "title": "3D-LLM: Injecting the 3D World into Large Language Models",
                "abstract": "Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 1M 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on held-out evaluation dataset, ScanQA, SQA3D and 3DMV-VQA, outperform state-of-the-art baselines. In particular, experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin ( e.g. , the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/ .",
                "authors": "Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan",
                "citations": 180
            },
            {
                "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning",
                "abstract": "High-quality instructions and responses are essential for the zero-shot performance of large language models on interactive natural language tasks. For interactive vision-language tasks involving intricate visual scenes, a large quantity of diverse and creative instruction-response pairs should be imperative to tune vision-language models (VLMs). Nevertheless, the current availability of vision-language instruction-response pairs in terms of quantity, diversity, and creativity remains limited, posing challenges to the generalization of interactive VLMs. Here we present MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos. Each pair is accompanied by multi-modal in-context information, forming conversational contexts aimed at empowering VLMs in perception, reasoning, and planning. The instruction-response collection process, dubbed as Syphus, is scaled using an automatic annotation pipeline that combines human expertise with GPT's capabilities. Using the MIMIC-IT dataset, we train a large VLM named Otter. Based on extensive evaluations conducted on vision-language benchmarks, it has been observed that Otter demonstrates remarkable proficiency in multi-modal perception, reasoning, and in-context learning. Human evaluation reveals it effectively aligns with the user's intentions. We release the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and the Otter model.",
                "authors": "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, C. Li, Ziwei Liu",
                "citations": 190
            },
            {
                "title": "Med-Flamingo: a Multimodal Medical Few-shot Learner",
                "abstract": "Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models (VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering (VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20\\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app under https://github.com/snap-stanford/med-flamingo.",
                "authors": "Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Cyril Zakka, Yashodhara Dalmia, E. Reis, P. Rajpurkar, J. Leskovec",
                "citations": 160
            },
            {
                "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
                "abstract": "In this work, we present a novel method to tackle the token generation challenge in Vision Language Models (VLMs) for video and image understanding, called LLaMA-VID. Current VLMs, while proficient in tasks like image captioning and visual question answering, face computational burdens when processing long videos due to the excessive visual tokens. LLaMA-VID addresses this issue by representing each frame with two distinct tokens, namely context token and content token. The context token encodes the overall image context based on user input, whereas the content token encapsulates visual cues in each frame. This dual-token strategy significantly reduces the overload of long videos while preserving critical information. Generally, LLaMA-VID empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token. It is proved to surpass previous methods on most of video- or image-based benchmarks. Code is available https://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID",
                "authors": "Yanwei Li, Chengyao Wang, Jiaya Jia",
                "citations": 139
            },
            {
                "title": "Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review",
                "abstract": "This comprehensive review delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). The development of Artificial Intelligence (AI), from its inception in the 1950s to the emergence of advanced neural networks and deep learning architectures, has made a breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in Vision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt engineering is the process of structuring inputs, which has emerged as a crucial technique to maximize the utility and accuracy of these models. This paper explores both foundational and advanced methodologies of prompt engineering, including techniques such as self-consistency, chain-of-thought, and generated knowledge, which significantly enhance model performance. Additionally, it examines the prompt method of VLMs through innovative approaches such as Context Optimization (CoOp), Conditional Context Optimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this discussion is the aspect of AI security, particularly adversarial attacks that exploit vulnerabilities in prompt engineering. Strategies to mitigate these risks and enhance model robustness are thoroughly reviewed. The evaluation of prompt methods is also addressed, through both subjective and objective metrics, ensuring a robust analysis of their efficacy. This review also reflects the essential role of prompt engineering in advancing AI capabilities, providing a structured framework for future research and application.",
                "authors": "Banghao Chen, Zhaofeng Zhang, Nicolas Langren'e, Shengxin Zhu",
                "citations": 136
            },
            {
                "title": "What does CLIP know about a red circle? Visual prompt engineering for VLMs",
                "abstract": "Large-scale Vision-Language Models, such as CLIP, learn powerful image-text representations that have found numerous applications, from zero-shot classification to text-to-image generation. Despite that, their capabilities for solving novel discriminative tasks via prompting fall behind those of large language models, such as GPT-3. Here we explore the idea of visual prompt engineering for solving computer vision tasks beyond classification by editing in image space instead of text. In particular, we discover an emergent ability of CLIP, where, by simply drawing a red circle around an object, we can direct the model’s attention to that region, while also maintaining global information. We show the power of this simple approach by achieving state-of-the-art in zero-shot referring expressions comprehension and strong performance in keypoint localization tasks. Finally, we draw attention to some potential ethical concerns of large language-vision models.",
                "authors": "Aleksandar Shtedritski, C. Rupprecht, A. Vedaldi",
                "citations": 101
            },
            {
                "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
                "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
                "authors": "Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, Prateek Mittal",
                "citations": 98
            },
            {
                "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
                "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
                "authors": "Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin",
                "citations": 115
            },
            {
                "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
                "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
                "authors": "Andy Zeng, Adrian S. Wong, Stefan Welker, K. Choromanski, F. Tombari, Aveek Purohit, M. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Peter R. Florence",
                "citations": 523
            },
            {
                "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
                "abstract": "Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 11 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.",
                "authors": "Wenbo Hu, Y. Xu, Y. Li, W. Li, Z. Chen, Z. Tu",
                "citations": 93
            },
            {
                "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
                "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
                "authors": "Erfan Shayegani, Yue Dong, Nael B. Abu-Ghazaleh",
                "citations": 80
            },
            {
                "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
                "abstract": "In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.",
                "authors": "Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao",
                "citations": 74
            },
            {
                "title": "Vision-Language Foundation Models as Effective Robot Imitators",
                "abstract": "Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.",
                "authors": "Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chi-Hou Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong",
                "citations": 80
            },
            {
                "title": "RSGPT: A Remote Sensing Vision Language Model and Benchmark",
                "abstract": "The emergence of large-scale large language models, with GPT-4 as a prominent example, has significantly propelled the rapid advancement of artificial general intelligence and sparked the revolution of Artificial Intelligence 2.0. In the realm of remote sensing (RS), there is a growing interest in developing large vision language models (VLMs) specifically tailored for data analysis in this domain. However, current research predominantly revolves around visual recognition tasks, lacking comprehensive, large-scale image-text datasets that are aligned and suitable for training large VLMs, which poses significant challenges to effectively training such models for RS applications. In computer vision, recent research has demonstrated that fine-tuning large vision language models on small-scale, high-quality datasets can yield impressive performance in visual and language understanding. These results are comparable to state-of-the-art VLMs trained from scratch on massive amounts of data, such as GPT-4. Inspired by this captivating idea, in this work, we build a high-quality Remote Sensing Image Captioning dataset (RSICap) that facilitates the development of large VLMs in the RS field. Unlike previous RS datasets that either employ model-generated captions or short descriptions, RSICap comprises 2,585 human-annotated captions with rich and high-quality information. This dataset offers detailed descriptions for each image, encompassing scene descriptions (e.g., residential area, airport, or farmland) as well as object information (e.g., color, shape, quantity, absolute position, etc). To facilitate the evaluation of VLMs in the field of RS, we also provide a benchmark evaluation dataset called RSIEval. This dataset consists of human-annotated captions and visual question-answer pairs, allowing for a comprehensive assessment of VLMs in the context of RS.",
                "authors": "Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, Xiang Li",
                "citations": 68
            },
            {
                "title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection",
                "abstract": "Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, \\eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.",
                "authors": "Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen",
                "citations": 66
            },
            {
                "title": "When and why vision-language models behave like bags-of-words, and what to do about it?",
                "abstract": "Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO&Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on retrieval over existing datasets without using the composition and order information. Given that contrastive pretraining optimizes for retrieval on datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality.",
                "authors": "Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Y. Zou",
                "citations": 286
            },
            {
                "title": "CLIP-Count: Towards Text-Guided Zero-Shot Object Counting",
                "abstract": "Recent advances in visual-language models have shown remarkable zero-shot text-image matching ability that is transferable to downstream tasks such as object detection and segmentation. Adapting these models for object counting, however, remains a formidable challenge. In this study, we first investigate transferring vision-language models (VLMs) for class-agnostic object counting. Specifically, we propose CLIP-Count, the first end-to-end pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner. To align the text embedding with dense visual features, we introduce a patch-text contrastive loss that guides the model to learn informative patch-level visual representations for dense prediction. Moreover, we design a hierarchical patch-text interaction module to propagate semantic information across different resolution levels of visual features. Benefiting from the full exploitation of the rich image-text alignment knowledge of pretrained VLMs, our method effectively generates high-quality density maps for objects-of-interest. Extensive experiments on FSC-147, CARPK, and ShanghaiTech crowd counting datasets demonstrate state-of-the-art accuracy and generalizability of the proposed method. Code is available: https://github.com/songrise/CLIP-Count. https://github.com/songrise/CLIP-Count.",
                "authors": "Ruixia Jiang, Lin Liu, Changan Chen",
                "citations": 45
            },
            {
                "title": "DeAR: Debiasing Vision-Language Models with Additive Residuals",
                "abstract": "Large pre-trained vision-language models (VLMs) reduce the time for developing predictive models for various vision-grounded language downstream tasks by providing rich, adaptable image and text representations. However, these models suffer from societal biases owing to the skewed distribution of various identity groups in the training data. These biases manifest as the skewed similarity between the representations for specific text concepts and images of people of different identity groups and, therefore, limit the usefulness of such models in real-world high-stakes applications. In this work, we present Dear(Debiasing with Additive Residuals), a novel debiasing method that learns additive residual image representations to offset the original representations, ensuring fair output representations. In doing so, it reduces the ability of the representations to distinguish between the different identity groups. Further, we observe that the current fairness tests are performed on limited face image datasets that fail to indicate why a specific text concept should/should not apply to them. To bridge this gap and better evaluate Dear,we introduce the Protected Attribute Tag Association (pata)dataset - a new context-based bias benchmarking dataset for evaluating the fairness of large pre-trained VLMs. Additionally, Pataprovides visual context for a diverse human population in different scenarios with both positive and negative connotations. Experimental results for fairness and zero-shot performance preservation using multiple datasets demonstrate the efficacy of our framework. The dataset is released here.",
                "authors": "Ashish Seth, Mayur Hemani, Chirag Agarwal",
                "citations": 43
            },
            {
                "title": "Raising the Bar of AI-generated Image Detection with CLIP",
                "abstract": "The aim of this work is to explore the potential of pre-trained vision-language models (VLMs) for universal detection of AI-generated images. We develop a lightweight detection strategy based on CLIP features and study its performance in a wide variety of challenging scenarios. We find that, contrary to previous beliefs, it is neither necessary nor convenient to use a large domain-specific dataset for training. On the contrary, by using only a handful of example images from a single generative model, a CLIP-based detector exhibits surprising generalization ability and high robustness across different architectures, including recent commercial tools such as Dalle-3, Midjourney v5, and Firefly. We match the state-of-the-art (SoTA) on in-distribution data and significantly improve upon it in terms of generalization to out-of-distribution data (+6% AUC) and robustness to impaired/laundered data (+13%). Our project is available at https://grip-unina.github.io/ClipBased-SyntheticImageDetection/",
                "authors": "D. Cozzolino, G. Poggi, Riccardo Corvi, Matthias Nießner, L. Verdoliva",
                "citations": 43
            },
            {
                "title": "RoboCLIP: One Demonstration is Enough to Learn Robot Policies",
                "abstract": "Reward specification is a notoriously difficult problem in reinforcement learning, requiring extensive expert supervision to design robust reward functions. Imitation learning (IL) methods attempt to circumvent these problems by utilizing expert demonstrations but typically require a large number of in-domain expert demonstrations. Inspired by advances in the field of Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design. Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains. RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation. Reinforcement learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher zero-shot performance than competing imitation learning methods on downstream robot manipulation tasks, doing so using only one video/text demonstration.",
                "authors": "S. Sontakke, Jesse Zhang, S'ebastien M. R. Arnold, Karl Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, Laurent Itti",
                "citations": 43
            },
            {
                "title": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts",
                "abstract": "Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~ 7%), SUN397 (~ 4.6%), and CUB ( ~3.3%) when compared to CLIP’s default prompt. We also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized fine-grained datasets. The code, prompts, and auxiliary text dataset is available at github.com/mayug/VDT-Adapter.",
                "authors": "Mayug Maniparambil, Chris Vorster, D. Molloy, N. Murphy, Kevin McGuinness, Noel E. O'Connor",
                "citations": 39
            },
            {
                "title": "Equivariant Similarity for Vision-Language Foundation Models",
                "abstract": "This study explores the concept of equivariance in vision-language foundation models (VLMs), focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. Unlike the existing image-text similarity objective which only categorizes matched pairs as similar and unmatched pairs as dissimilar, equivariance also requires similarity to vary faithfully according to the semantic changes. This allows VLMs to generalize better to nuanced and unseen multimodal compositions. However, modeling equivariance is challenging as the ground truth of semantic change is difficult to collect. For example, given an image-text pair about a dog, it is unclear to what extent the similarity changes when the pixel is changed from dog to cat? To this end, we propose EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we present a new challenging benchmark EqBen. Compared to the existing evaluation sets, EqBen is the first to focus on \"visual-minimal change\". Extensive experiments show the lack of equivariance in current VLMs1 and validate the effectiveness of EqSim2.",
                "authors": "Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, Lijuan Wang",
                "citations": 41
            },
            {
                "title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models",
                "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising billions of image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-explored. While few works have recently explored LLMs-based conversational medical models, they mainly focus on text-based analysis. In this paper, we introduce XrayGPT, a conversational medical vision-language (VLMs) model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder with a fine-tuned LLM to possess visual conversation abilities, grounded in an understanding of radiographs and medical knowledge. For improved alignment of chest radiograph data, we generate ~217k interactive and high-quality summaries from free-text radiology reports. Extensive experiments are conducted to validate the merits of XrayGPT. To conduct an expert evaluation, certified medical doctors evaluated the output of our XrayGPT on a test subset and the results reveal that more than 70% of the responses are scientifically accurate, with an average score of 4/5. We hope our simple and effective method establishes a solid baseline, facilitating future research toward automated analysis and summarization of chest radiographs. Code, models, and instruction sets will be publicly released.",
                "authors": "Omkar Thawakar, Abdelrahman M. Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, R. Anwer, Salman Siddique Khan, J. Laaksonen, F. Khan",
                "citations": 43
            },
            {
                "title": "Fine-Grained Visual Prompting",
                "abstract": "Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive zero-shot transfer capabilities in image-level visual perception. However, these models have shown limited performance in instance-level tasks that demand precise localization and recognition. Previous works have suggested that incorporating visual prompts, such as colorful boxes or circles, can improve the ability of models to recognize objects of interest. Nonetheless, compared to language prompting, visual prompting designs are rarely explored. Existing approaches, which employ coarse visual cues such as colorful boxes or circles, often result in sub-optimal performance due to the inclusion of irrelevant and noisy pixels. In this paper, we carefully study the visual prompting designs by exploring more fine-grained markings, such as segmentation masks and their variations. In addition, we introduce a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting. Consequently, our investigation reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting strategy leverages the precise mask annotations to reduce focus on weakly related regions while retaining spatial coherence between the target and the surrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates superior performance in zero-shot comprehension of referring expressions on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an average margin of 3.0% to 4.6%, with a maximum improvement of 12.5% on the RefCOCO+ testA subset. Code is available at https://github.com/ylingfeng/FGVP.",
                "authors": "Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, Jian Yang",
                "citations": 43
            },
            {
                "title": "Scaling Vision-Language Models with Sparse Mixture of Experts",
                "abstract": "The field of natural language processing (NLP) has made significant strides in recent years, particularly in the development of large-scale vision-language models (VLMs). These models aim to bridge the gap between text and visual information, enabling a more comprehensive understanding of multimedia data. However, as these models become larger and more complex, they also become more challenging to train and deploy. One approach to addressing this challenge is the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the model into smaller, specialized sub-models that can jointly solve a task. In this paper, we explore the effectiveness of MoE in scaling vision-language models, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost. Our research offers valuable insights into stabilizing the training of MoE models, understanding the impact of MoE on model interpretability, and balancing the trade-offs between compute performance when scaling VLMs. We hope our work will inspire further research into the use of MoE for scaling large-scale vision-language models and other multimodal machine learning applications.",
                "authors": "Sheng Shen, Z. Yao, Chunyuan Li, Trevor Darrell, K. Keutzer, Yuxiong He",
                "citations": 49
            },
            {
                "title": "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph",
                "abstract": "Adapter-style efficient transfer learning (ETL) has shown excellent performance in the tuning of vision-language models (VLMs) under the low-data regime, where only a few additional parameters are introduced to excavate the task-specific knowledge based on the general and powerful representation of VLMs. However, most adapter-style works face two limitations: (i) modeling task-specific knowledge with a single modality only; and (ii) overlooking the exploitation of the inter-class relationships in downstream tasks, thereby leading to sub-optimal solutions. To mitigate that, we propose an effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dual-modality structure knowledge (i.e., the correlation of different semantics/classes in textual and visual modalities) with a dual knowledge graph. In particular, the dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph, where the nodes and edges represent the semantics/classes and their correlations in two modalities, respectively. This enables the textual feature of each prompt to leverage the task-specific structure knowledge from both textual and visual modalities, yielding a more effective classifier for downstream tasks. Extensive experimental results on 11 benchmark datasets reveal that our GraphAdapter significantly outperforms previous adapter-based methods. The code will be released at https://github.com/lixinustc/GraphAdapter",
                "authors": "Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang",
                "citations": 40
            },
            {
                "title": "Visual Classification via Description from Large Language Models",
                "abstract": "Vision-language models (VLMs) such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what features the model uses to construct its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.",
                "authors": "Sachit Menon, Carl Vondrick",
                "citations": 236
            },
            {
                "title": "Large Language Models are Visual Reasoning Coordinators",
                "abstract": "Visual reasoning requires multimodal perception and commonsense cognition of the world. Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains. However, how to harness the collective power of these complementary VLMs is rarely explored. Existing methods like ensemble still struggle to aggregate these models with the desired higher-order communications. In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning. Our key insight is that a large language model (LLM) can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities. Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning. Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable impressive visual reasoning capabilities.",
                "authors": "Liangyu Chen, Boyi Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, Ziwei Liu",
                "citations": 36
            },
            {
                "title": "Vision-Language Models in Remote Sensing: Current progress and future trends",
                "abstract": "The remarkable achievements of ChatGPT and Generative Pre-trained Transformer 4 (GPT-4) have sparked a wave of interest and research in the field of large language models (LLMs) for artificial general intelligence (AGI). These models provide intelligent solutions that are closer to human thinking, enabling us to use general artificial intelligence (AI) to solve problems in various applications. However, in the field of remote sensing (RS), the scientific literature on the implementation of AGI remains relatively scant. Existing AI-related research in RS focuses primarily on visual-understanding tasks while neglecting the semantic understanding of the objects and their relationships. This is where vision-LMs (VLMs) excel as they enable reasoning about images and their associated textual descriptions, allowing for a deeper understanding of the underlying semantics. VLMs can go beyond visual recognition of RS images and can model semantic relationships as well as generate natural language descriptions of the image. This makes them better suited for tasks that require both visual and textual understanding, such as image captioning and visual question answering (VQA). This article provides a comprehensive review of the research on VLMs in RS, summarizing the latest progress, highlighting current challenges, and identifying potential research opportunities. Specifically, we review the application of VLMs in mainstream RS tasks, including image captioning, text-based image generation, text-based image retrieval (TBIR), VQA, scene classification, semantic segmentation, and object detection. For each task, we analyze representative works and discuss research progress. Finally, we summarize the limitations of existing works and provide possible directions for future development. This review aims to provide a comprehensive overview of the current research progress of VLMs in RS (see Figure 1), and to inspire further research in this exciting and promising field.",
                "authors": "Congcong Wen, Yuan Hu, Xiang Li, Zhenghang Yuan, Xiao Xiang Zhu",
                "citations": 47
            },
            {
                "title": "Vision Language Models in Autonomous Driving and Intelligent Transportation Systems",
                "abstract": "—The applications of Vision-Language Models (VLMs) in the fields of Autonomous Driving (AD) and Intelligent Transportation Systems (ITS) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By integrating language data, the vehicles, and transportation systems are able to deeply understand real-world environments, improving driving safety and efficiency. In this work, we present a comprehensive survey of the advances in language models in this domain, encompassing current models and datasets. Additionally, we explore the potential applications and emerging research directions. Finally, we thoroughly discuss the challenges and research gap. The paper aims to provide researchers with the current work and future trends of VLMs in AD and ITS.",
                "authors": "Xingcheng Zhou, Mingyu Liu, B. L. Žagar, Ekim Yurtsever, Alois C. Knoll",
                "citations": 44
            },
            {
                "title": "CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning",
                "abstract": "Nowadays, the research on Large Vision-Language Models (LVLMs) has been significantly promoted thanks to the success of Large Language Models (LLM). Nevertheless, these Vision-Language Models (VLMs) are suffering from the drawback of hallucination -- due to insufficient understanding of vision and language modalities, VLMs may generate incorrect perception information when doing downstream applications, for example, captioning a non-existent entity. To address the hallucination phenomenon, on the one hand, we introduce a Contrastive Instruction Evaluation Method (CIEM), which is an automatic pipeline that leverages an annotated image-text dataset coupled with an LLM to generate factual/contrastive question-answer pairs for the evaluation of the hallucination of VLMs. On the other hand, based on CIEM, we further propose a new instruction tuning method called CIT (the abbreviation of Contrastive Instruction Tuning) to alleviate the hallucination of VLMs by automatically producing high-quality factual/contrastive question-answer pairs and corresponding justifications for model tuning. Through extensive experiments on CIEM and CIT, we pinpoint the hallucination issues commonly present in existing VLMs, the disability of the current instruction-tuning dataset to handle the hallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM and public datasets.",
                "authors": "Hongyu Hu, Jiyuan Zhang, Minyi Zhao, Zhenbang Sun",
                "citations": 30
            },
            {
                "title": "DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment",
                "abstract": "Large language models (LLMs) encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous work has explored how to ground LLMs in robotic tasks to generate feasible and executable textual plans. However, low-level execution in the physical world may deviate from the high-level textual plan due to environmental perturbations or imperfect controller design. In this paper, we propose DoReMi, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, we leverage LLMs to play a dual role, aiding not only in high-level planning but also generating constraints that can indicate misalignment during execution. Then vision language models (VLMs) are utilized to detect constraint violations continuously. Our pipeline can monitor the low-level execution and enable timely recovery if certain plan-execution misalignment occurs. Experiments on various complex tasks including robot arms and humanoid robots demonstrate that our method can lead to higher task success rates and shorter task completion times.",
                "authors": "Yanjiang Guo, Yen-Jen Wang, Lihan Zha, Zheyuan Jiang, Jianyu Chen",
                "citations": 30
            },
            {
                "title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning",
                "abstract": "Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge. We release the dataset for further research in this area.",
                "authors": "Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, Radu Soricut",
                "citations": 34
            },
            {
                "title": "Semantic Scene Understanding with Large Language Models on Unmanned Aerial Vehicles",
                "abstract": "Unmanned Aerial Vehicles (UAVs) are able to provide instantaneous visual cues and a high-level data throughput that could be further leveraged to address complex tasks, such as semantically rich scene understanding. In this work, we built on the use of Large Language Models (LLMs) and Visual Language Models (VLMs), together with a state-of-the-art detection pipeline, to provide thorough zero-shot UAV scene literary text descriptions. The generated texts achieve a GUNNING Fog median grade level in the range of 7–12. Applications of this framework could be found in the filming industry and could enhance user experience in theme parks or in the advertisement sector. We demonstrate a low-cost highly efficient state-of-the-art practical implementation of microdrones in a well-controlled and challenging setting, in addition to proposing the use of standardized readability metrics to assess LLM-enhanced descriptions.",
                "authors": "Federico Tombari, J. Curtò, I. D. Zarzà, C. Calafate",
                "citations": 34
            },
            {
                "title": "Can I Trust Your Answer? Visually Grounded Video Question Answering",
                "abstract": "We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA - an extension of NExT-QA with 10.5K temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are extremely weak in substantiating the answers despite their strong QA performance. This exposes the limitation of current VLMs in making reliable predictions. As a remedy, we further explore and propose a grounded-QA method via Gaussian mask optimization and cross-modal learning. Experiments with different backbones demonstrate that this grounding mechanism improves both grounding and QA. With these efforts, we aim to push towards trustworthy VLMs in VQA systems. Our dataset and code are available at https://github.com/doc-doc/NExT-GQA.",
                "authors": "Junbin Xiao, Angela Yao, Yicong Li, Tat-Seng Chua",
                "citations": 24
            },
            {
                "title": "A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions",
                "abstract": "Curation methods for massive vision-language datasets trade off between dataset size and quality. However, even the highest quality of available curated captions are far too short to capture the rich visual detail in an image. To show the value of dense and highly-aligned image-text pairs, we collect the Densely Captioned Images (DCI) dataset, containing 7805 natural images human-annotated with mask-aligned descriptions averaging above 1000 words each. With precise and reliable captions associated with specific parts of an image, we can evaluate vision-language models' (VLMs) understanding of image content with a novel task that matches each caption with its corresponding subcrop. As current models are often limited to 77 text tokens, we also introduce a summarized version (sDCI) in which each caption length is limited. We show that modern techniques that make progress on standard benchmarks do not correspond with significant improvement on our sDCI based benchmark. Lastly, we finetune CLIP using sDCI and show significant improvements over the baseline despite a small training set. By releasing the first human annotated dense image captioning dataset, we hope to enable the development of new benchmarks or finetuning recipes for the next generation of VLMs to come.",
                "authors": "Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, Adriana Romero-Soriano",
                "citations": 27
            },
            {
                "title": "RoboVQA: Multimodal Long-Horizon Reasoning for Robotics",
                "abstract": "We present a scalable, bottom-up and intrinsically diverse data collection scheme that can be used for high-level reasoning with long and medium horizons and that has 2.2x higher throughput compared to traditional narrow top-down step-by-step collection. We collect realistic data by performing any user requests within the entirety of 3 office buildings and using multiple embodiments (robot, human, human with grasping tool). With this data, we show that models trained on all embodiments perform better than ones trained on the robot data only, even when evaluated solely on robot episodes. We explore the economics of collection costs and find that for a fixed budget it is beneficial to take advantage of the cheaper human collection along with robot collection. We release a large and highly diverse (29,520 unique instructions) dataset dubbed RoboVQA containing 829,502 (video, text) pairs for robotics-focused visual question answering. We also demonstrate how evaluating real robot experiments with an intervention mechanism enables performing tasks to completion, making it deployable with human oversight even if imperfect while also providing a single performance metric. We demonstrate a single video-conditioned model named RoboVQA-VideoCoCa trained on our dataset that is capable of performing a variety of grounded high-level reasoning tasks in broad realistic settings with a cognitive intervention rate 46% lower than the zeroshot state of the art visual language model (VLM) baseline and is able to guide real robots through long-horizon tasks. The performance gap with zero-shot state-of-the-art models indicates that a lot of grounded data remains to be collected for real-world deployment, emphasizing the critical need for scalable data collection approaches. Finally, we show that video VLMs significantly outperform single-image VLMs with an average error rate reduction of 19% across all VQA tasks. Thanks to video conditioning and dataset diversity, the model can be used as general video value functions (e.g. success and affordance) in situations where actions needs to be recognized rather than states, expanding capabilities and environment understanding for robots. Data and videos are available at robovqa.github.io",
                "authors": "P. Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, K. Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J. Joshi, Pete Florence, Wei Han, Robert Baruch, Yao Lu, Suvir Mirchandani, Peng Xu, Pannag R. Sanketi, Karol Hausman, Izhak Shafran, Brian Ichter, Yuan Cao",
                "citations": 27
            },
            {
                "title": "Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving",
                "abstract": "Large vision-language models (VLMs) have garnered increasing interest in autonomous driving areas, due to their advanced capabilities in complex reasoning tasks essential for highly autonomous vehicle behavior. Despite their potential, research in autonomous systems is hindered by the lack of datasets with annotated reasoning chains that explain the decision-making processes in driving. To bridge this gap, we present Reason2Drive, a benchmark dataset with over 600K video-text pairs, aimed at facilitating the study of interpretable reasoning in complex driving environments. We distinctly characterize the autonomous driving process as a sequential combination of perception, prediction, and reasoning steps, and the question-answer pairs are automatically collected from a diverse range of open-source outdoor driving datasets, including nuScenes, Waymo and ONCE. Moreover, we introduce a novel aggregated evaluation metric to assess chain-based reasoning performance in autonomous systems, addressing the semantic ambiguities of existing metrics such as BLEU and CIDEr. Based on the proposed benchmark, we conduct experiments to assess various existing VLMs, revealing insights into their reasoning capabilities. Additionally, we develop an efficient approach to empower VLMs to leverage object-level perceptual elements in both feature extraction and prediction, further enhancing their reasoning accuracy. The code and dataset will be released.",
                "authors": "Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu, Li Zhang",
                "citations": 25
            },
            {
                "title": "CLIP4STR: A Simple Baseline for Scene Text Recognition With Pre-Trained Vision-Language Model",
                "abstract": "Pre-trained vision-language models (VLMs) are the de-facto foundation models for various downstream tasks. However, scene text recognition methods still prefer backbones pre-trained on a single modality, namely, the visual modality, despite the potential of VLMs to serve as powerful scene text readers. For example, CLIP can robustly identify regular (horizontal) and irregular (rotated, curved, blurred, or occluded) text in images. With such merits, we transform CLIP into a scene text reader and introduce CLIP4STR, a simple yet effective STR method built upon image and text encoders of CLIP. It has two encoder-decoder branches: a visual branch and a cross-modal branch. The visual branch provides an initial prediction based on the visual feature, and the cross-modal branch refines this prediction by addressing the discrepancy between the visual feature and text semantics. To fully leverage the capabilities of both branches, we design a dual predict-and-refine decoding scheme for inference. We scale CLIP4STR in terms of the model size, pre-training data, and training data, achieving state-of-the-art performance on 13 STR benchmarks. Additionally, a comprehensive empirical study is provided to enhance the understanding of the adaptation of CLIP to STR. Our method establishes a simple yet strong baseline for future STR research with VLMs.",
                "authors": "Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yezhou Yang",
                "citations": 25
            },
            {
                "title": "A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis",
                "abstract": "Zero-shot medical image classification is a critical process in real-world scenarios where we have limited access to all possible diseases or large-scale annotated data. It involves computing similarity scores between a query medical image and possible disease categories to determine the diagnostic result. Recent advances in pretrained vision-language models (VLMs) such as CLIP have shown great performance for zero-shot natural image recognition and exhibit benefits in medical applications. However, an explainable zero-shot medical image recognition framework with promising performance is yet under development. In this paper, we propose a novel CLIP-based zero-shot medical image classification framework supplemented with ChatGPT for explainable diagnosis, mimicking the diagnostic process performed by human experts. The key idea is to query large language models (LLMs) with category names to automatically generate additional cues and knowledge, such as disease symptoms or descriptions other than a single category name, to help provide more accurate and explainable diagnosis in CLIP. We further design specific prompts to enhance the quality of generated texts by ChatGPT that describe visual medical features. Extensive results on one private dataset and four public datasets along with detailed analysis demonstrate the effectiveness and explainability of our training-free zero-shot diagnosis pipeline, corroborating the great potential of VLMs and LLMs for medical applications.",
                "authors": "Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, Zuozhu Liu",
                "citations": 25
            },
            {
                "title": "DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback",
                "abstract": "Despite their wide-spread success, Text-to-Image models (T2I) still struggle to produce images that are both aesthetically pleasing and faithful to the user's input text. We introduce DreamSync, a model-agnostic training algorithm by design that improves T2I models to be faithful to the text input. DreamSync builds off a recent insight from TIFA's evaluation framework -- that large vision-language models (VLMs) can effectively identify the fine-grained discrepancies between generated images and the text inputs. DreamSync uses this insight to train T2I models without any labeled data; it improves T2I models using its own generations. First, it prompts the model to generate several candidate images for a given input text. Then, it uses two VLMs to select the best generation: a Visual Question Answering model that measures the alignment of generated images to the text, and another that measures the generation's aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I model to guide its generation towards the selected best generations. DreamSync does not need any additional human annotation. model architecture changes, or reinforcement learning. Despite its simplicity, DreamSync improves both the semantic alignment and aesthetic appeal of two diffusion-based T2I models, evidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA aesthetic) and human evaluation.",
                "authors": "Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, Cyrus Rashtchian",
                "citations": 25
            },
            {
                "title": "Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs",
                "abstract": "Vision and language models (VLMs) have demonstrated remarkable zero-shot (ZS) performance in a variety of tasks. However, recent works have shown that even the best VLMs struggle to capture aspects of compositional scene understanding, such as object attributes, relations, and action states. In contrast, obtaining structured annotations, such as scene graphs (SGs), that could improve these models is time-consuming and costly, and thus cannot be used on a large scale. Here we ask whether small SG datasets can provide sufficient information for enhancing structured understanding of pretrained VLMs. We show that it is indeed possible to improve VLMs when learning from SGs by integrating components that incorporate structured information into both visual and textual representations. For the visual side, we incorporate a special\"SG Component\"in the image transformer trained to predict SG information, while for the textual side, we utilize SGs to generate fine-grained captions that highlight different compositional aspects of the scene. Our method improves the performance of several popular VLMs on multiple VL datasets with only a mild degradation in ZS capabilities.",
                "authors": "Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, R. Feris, Trevor Darrell, A. Globerson",
                "citations": 25
            },
            {
                "title": "LLM Multimodal Traffic Accident Forecasting",
                "abstract": "With the rise in traffic congestion in urban centers, predicting accidents has become paramount for city planning and public safety. This work comprehensively studied the efficacy of modern deep learning (DL) methods in forecasting traffic accidents and enhancing Level-4 and Level-5 (L-4 and L-5) driving assistants with actionable visual and language cues. Using a rich dataset detailing accident occurrences, we juxtaposed the Transformer model against traditional time series models like ARIMA and the more recent Prophet model. Additionally, through detailed analysis, we delved deep into feature importance using principal component analysis (PCA) loadings, uncovering key factors contributing to accidents. We introduce the idea of using real-time interventions with large language models (LLMs) in autonomous driving with the use of lightweight compact LLMs like LLaMA-2 and Zephyr-7b-α. Our exploration extends to the realm of multimodality, through the use of Large Language-and-Vision Assistant (LLaVA)—a bridge between visual and linguistic cues by means of a Visual Language Model (VLM)—in conjunction with deep probabilistic reasoning, enhancing the real-time responsiveness of autonomous driving systems. In this study, we elucidate the advantages of employing large multimodal models within DL and deep probabilistic programming for enhancing the performance and usability of time series forecasting and feature weight importance, particularly in a self-driving scenario. This work paves the way for safer, smarter cities, underpinned by data-driven decision making.",
                "authors": "I. D. Zarzà, J. Curtò, Gemma Roig, C. Calafate",
                "citations": 24
            },
            {
                "title": "Vision-Language Models as a Source of Rewards",
                "abstract": "Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.",
                "authors": "Kate Baumli, Satinder Baveja, Feryal M. P. Behbahani, Harris Chan, Gheorghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin, Clare Lyle, Hussain Masoom, Kay McKinney, Volodymyr Mnih, Alexander Neitz, Fabio Pardo, Jack Parker-Holder, John Quan, Tim Rocktaschel, Himanshu Sahni, T. Schaul, Yannick Schroecker, Stephen Spencer, Richie Steigerwald, Luyu Wang, Lei Zhang",
                "citations": 22
            },
            {
                "title": "From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design",
                "abstract": "Engineering design is undergoing a transformative shift with the advent of AI, marking a new era in how we approach product, system, and service planning. Large language models have demonstrated impressive capabilities in enabling this shift. Yet, with text as their only input modality, they cannot leverage the large body of visual artifacts that engineers have used for centuries and are accustomed to. This gap is addressed with the release of multimodal vision-language models (VLMs), such as GPT-4V, enabling AI to impact many more types of tasks. Our work presents a comprehensive evaluation of VLMs across a spectrum of engineering design tasks, categorized into four main areas: Conceptual Design, System-Level and Detailed Design, Manufacturing and Inspection, and Engineering Education Tasks. Specifically in this paper, we assess the capabilities of two VLMs, GPT-4V and LLaVA 1.6 34B, in design tasks such as sketch similarity analysis, CAD generation, topology optimization, manufacturability assessment, and engineering textbook problems. Through this structured evaluation, we not only explore VLMs' proficiency in handling complex design challenges but also identify their limitations in complex engineering design applications. Our research establishes a foundation for future assessments of vision language models. It also contributes a set of benchmark testing datasets, with more than 1000 queries, for ongoing advancements and applications in this field.",
                "authors": "Cyril Picard, Kristen M. Edwards, Anna C. Doris, Brandon Man, Giorgio Giannone, Md Ferdous Alam, Faez Ahmed",
                "citations": 22
            },
            {
                "title": "Prompt-based Distribution Alignment for Unsupervised Domain Adaptation",
                "abstract": "Recently, despite the unprecedented success of large pre-trained visual-language models (VLMs) on a wide range of downstream tasks, the real-world unsupervised domain adaptation (UDA) problem is still not well explored. Therefore, in this paper, we first experimentally demonstrate that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA. However, a major challenge for directly deploying such models on downstream UDA tasks is prompt engineering, which requires aligning the domain knowledge of source and target domains, since the performance of UDA is severely influenced by a good domain-invariant representation. We further propose a Prompt-based Distribution Alignment (PDA) method to incorporate the domain knowledge into prompt learning. Specifically, PDA employs a two-branch prompt-tuning paradigm, namely base branch and alignment branch. The base branch focuses on integrating class-related representation into prompts, ensuring discrimination among different classes. To further minimize domain discrepancy, for the alignment branch, we construct feature banks for both the source and target domains and propose image-guided feature tuning (IFT) to make the input attend to feature banks, which effectively integrates self-enhanced and cross-domain features into the model. In this way, these two branches can be mutually promoted to enhance the adaptation of VLMs for UDA. We conduct extensive experiments on three benchmarks to demonstrate that our proposed PDA achieves state-of-the-art performance. The code is available at https://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.",
                "authors": "Shuanghao Bai, Min Zhang, Wanqi Zhou, Siteng Huang, Zhirong Luan, Donglin Wang, Badong Chen",
                "citations": 22
            },
            {
                "title": "Open-vocabulary Queryable Scene Representations for Real World Planning",
                "abstract": "Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io.",
                "authors": "Boyuan Chen, F. Xia, Brian Ichter, Kanishka Rao, K. Gopalakrishnan, M. Ryoo, Austin Stone, Daniel Kappler",
                "citations": 162
            },
            {
                "title": "Transferable Decoding with Visual Entities for Zero-Shot Image Captioning",
                "abstract": "Image-to-text generation aims to describe images using natural language. Recently, zero-shot image captioning based on pre-trained vision-language models (VLMs) and large language models (LLMs) has made significant progress. However, we have observed and empirically demonstrated that these methods are susceptible to modality bias induced by LLMs and tend to generate descriptions containing objects (entities) that do not actually exist in the image but frequently appear during training (i.e., object hallucination). In this paper, we propose ViECap, a transferable decoding model that leverages entity-aware decoding to generate descriptions in both seen and unseen scenarios. ViECap incorporates entity-aware hard prompts to guide LLMs’ attention toward the visual entities present in the image, enabling coherent caption generation across diverse scenes. With entity-aware hard prompts, ViECap is capable of maintaining performance when transferring from in-domain to out-of-domain scenarios. Extensive experiments demonstrate that ViECap sets a new state-of-the-art cross-domain (transferable) captioning and performs competitively in-domain captioning compared to previous VLMs-based zero-shot methods. Our code is available at: https://github.com/FeiElysia/ViECap",
                "authors": "Junjie Fei, Teng Wang, Jinrui Zhang, Zhenyu He, Chengjie Wang, Feng Zheng",
                "citations": 21
            },
            {
                "title": "Exploring Vision-Language Models for Imbalanced Learning",
                "abstract": "Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%, and 6.17%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. We further analyze the influence of pre-training data size, backbones, and training cost. Our study highlights the significance of imbalanced learning algorithms in face of VLMs pre-trained by huge data. We release our code at https://github.com/Imbalance-VLM/Imbalance-VLM.",
                "authors": "Yidong Wang, Zhuohao Yu, Jindong Wang, Qiang Heng, Haoxing Chen, Wei Ye, Rui Xie, Xingxu Xie, Shi-Bo Zhang",
                "citations": 21
            },
            {
                "title": "Language Models as Black-Box Optimizers for Vision-Language Models",
                "abstract": "Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities on downstream tasks when fine-tuned with minimal data. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. As such, we aim to develop a black-box approach to optimize VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or even output logits. We propose employing chat-based LLMs to search for the best text prompt for VLMs. Specifically, we adopt an automatic “hill-climbing” procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop. In a challenging 1-shot image classification setup, our simple approach surpasses the white-box continuous prompting method (CoOp) by an average of1.5% across 11 datasets including ImageNet. Our approach also outperforms both human-engineered and LLM-generated prompts. We high-light the advantage of conversational feedback that incor-porates both positive and negative prompts, suggesting that LLMs can utilize the implicit “gradient” direction in textual feedback for a more efficient search. In addition, we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across different VLM architectures in a black-box manner. Lastly, we demonstrate our framework on a state-of-the-art black-box VLM (DALL-E 3) for text-to-image optimization.",
                "authors": "Samuel Yu, Shihong Liu, Zhiqiu Lin, Deepak Pathak, Deva Ramanan",
                "citations": 19
            },
            {
                "title": "APPLeNet: Visual Attention Parameterized Prompt Learning for Few-Shot Remote Sensing Image Generalization using CLIP",
                "abstract": "In recent years, the success of large-scale vision-language models (VLMs) such as CLIP has led to their increased usage in various computer vision tasks. These models enable zero-shot inference through carefully crafted instructional text prompts without task-specific supervision. However, the potential of VLMs for generalization tasks in remote sensing (RS) has not been fully realized. To address this research gap, we propose a novel image-conditioned prompt learning strategy called the Visual Attention Parameterized Prompts Learning Network (APPLeNet). APPLeNet emphasizes the importance of multi-scale feature learning in RS scene classification and disentangles visual style and content primitives for domain generalization tasks. To achieve this, APPLeNet combines visual content features obtained from different layers of the vision encoder and style properties obtained from feature statistics of domain-specific batches. An attention-driven injection module is further introduced to generate visual tokens from this information. We also introduce an anti-correlation regularizer to ensure discrimination among the token embeddings, as this visual information is combined with the textual tokens. To validate APPLeNet, we curated four available RS benchmarks and introduced experimental protocols and datasets for three domain generalization tasks. Our results consistently outperform the relevant literature and code is available at https://github.com/mainaksingha01/APPLeNet",
                "authors": "M. Singha, Ankit Jha, Bhupendra S. Solanki, Shirsha Bose, Biplab Banerjee",
                "citations": 19
            },
            {
                "title": "Visual Spatial Reasoning",
                "abstract": "Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1",
                "authors": "Fangyu Liu, Guy Edward Toh Emerson, Nigel Collier",
                "citations": 125
            },
            {
                "title": "AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors",
                "abstract": "Deep generative models can create remarkably photorealistic fake images while raising concerns about misinformation and copyright infringement, known as deepfake threats. Deepfake detection technique is developed to distinguish between real and fake images, where the existing methods typically learn classifiers in the image domain or various feature domains. However, the generalizability of deepfake detection against emerging and more advanced generative models remains challenging. In this paper, being inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach using VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve the deepfake detection accuracy over unseen data. We formulate deepfake detection as a visual question answering problem, and tune soft prompts for InstructBLIP to answer the real/fake information of a query image. We conduct full-spectrum experiments on datasets from 3 held-in and 13 held-out generative models, covering modern text-to-image generation, image editing and image attacks. Results demonstrate that (1) the deepfake detection accuracy can be significantly and consistently improved (from 58.8% to 91.31%, in average accuracy over unseen data) using pretrained vision-language models with prompt tuning; (2) our superior performance is at less cost of trainable parameters, resulting in an effective and efficient solution for deepfake detection. Code and models can be found at https://github.com/nctu-eva-lab/AntifakePrompt.",
                "authors": "You-Ming Chang, Chen Yeh, Wei-Chen Chiu, Ning Yu",
                "citations": 15
            },
            {
                "title": "Can Language Models Understand Physical Concepts?",
                "abstract": "Language models~(LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is not yet clear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85\\% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x. Our dataset is available at \\url{https://github.com/TobiasLee/VEC}",
                "authors": "Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Qi Liu, Lingpeng Kong, Xu Sun",
                "citations": 15
            },
            {
                "title": "Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning",
                "abstract": "Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is often necessary to optimize their performance. However, a major obstacle is the limited availability of labeled data. We study the use of pseudolabels, i.e., heuristic labels for unlabeled data, to enhance CLIP via prompt tuning. Conventional pseudolabeling trains a model on labeled data and then generates labels for unlabeled data. VLMs' zero-shot capabilities enable a\"second generation\"of pseudolabeling approaches that do not require task-specific training on labeled data. By using zero-shot pseudolabels as a source of supervision, we observe that learning paradigms such as semi-supervised, transductive zero-shot, and unsupervised learning can all be seen as optimizing the same loss function. This unified view enables the development of versatile training strategies that are applicable across learning paradigms. We investigate them on image classification tasks where CLIP exhibits limitations, by varying prompt modalities, e.g., textual or visual prompts, and learning paradigms. We find that (1) unexplored prompt tuning strategies that iteratively refine pseudolabels consistently improve CLIP accuracy, by 19.5 points in semi-supervised learning, by 28.4 points in transductive zero-shot learning, and by 15.2 points in unsupervised learning, and (2) unlike conventional semi-supervised pseudolabeling, which exacerbates model biases toward classes with higher-quality pseudolabels, prompt tuning leads to a more equitable distribution of per-class accuracy. The code to reproduce the experiments is at https://github.com/BatsResearch/menghini-neurips23-code.",
                "authors": "Cristina Menghini, Andrew T. Delworth, Stephen H. Bach",
                "citations": 14
            },
            {
                "title": "Sieve: Multimodal Dataset Pruning Using Image Captioning Models",
                "abstract": "Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy web-crawled datasets. This underscores the critical need for dataset pruning, as the quality of these datasets is strongly correlated with the performance of VLMs on downstream tasks. Using CLIPScore from a pretrained model to only train models using highly-aligned samples is one of the most successful methods for pruning. We argue that this approach suffers from multiple limitations including: false positives and negatives due to CLIP's pretraining on noisy labels. We propose a pruning signal, Sieve, that employs synthetic captions generated by image-captioning models pretrained on small, diverse, and well-aligned image-text pairs to evaluate the alignment of noisy image-text pairs. To bridge the gap between the limited diversity of generated captions and the high diversity of alternative text (alt-text), we estimate the semantic textual similarity in the embedding space of a language model pretrained on unlabeled text corpus. Using DataComp, a multimodal dataset filtering benchmark, when evaluating on 38 downstream tasks, our pruning approach, surpasses CLIPScore by 2.6% and 1.7% on medium and large scale respectively. In addition, on retrieval tasks, Sieve leads to a significant improvement of 2.7% and 4.5% on medium and large scale respectively.",
                "authors": "Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, Ari S. Morcos",
                "citations": 14
            },
            {
                "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models",
                "abstract": "Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.",
                "authors": "Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran",
                "citations": 17
            },
            {
                "title": "Black-box Prompt Tuning for Vision-Language Model as a Service",
                "abstract": "In the scenario of Model-as-a-Service (MaaS), pre-trained models are usually released as inference APIs. Users are allowed to query those models with manually crafted prompts. Without accessing the network structure and gradient information, it's tricky to perform continuous prompt tuning on MaaS, especially for vision-language models (VLMs) considering cross-modal interaction. In this paper, we propose a black-box prompt tuning framework for VLMs to learn task-relevant prompts without back-propagation. In particular, the vision and language prompts are jointly optimized in the intrinsic parameter subspace with various evolution strategies. Different prompt variants are also explored to enhance the cross-model interaction. Experimental results show that our proposed black-box prompt tuning framework outperforms both hand-crafted prompt engineering and gradient-based prompt learning methods, which serves as evidence of its capability to train task-relevant prompts in a derivative-free manner.",
                "authors": "Lang-Chi Yu, Qin Chen, Jiaju Lin, Liang He",
                "citations": 15
            },
            {
                "title": "RadOcc: Learning Cross-Modality Occupancy Knowledge through Rendering Assisted Distillation",
                "abstract": "3D occupancy prediction is an emerging task that aims to estimate the occupancy states and semantics of 3D scenes using multi-view images. However, image-based scene perception encounters significant challenges in achieving accurate prediction due to the absence of geometric priors. In this paper, we address this issue by exploring cross-modal knowledge distillation in this task, i.e., we leverage a stronger multi-modal model to guide the visual model during training. In practice, we observe that directly applying features or logits alignment, proposed and widely used in bird's-eye-view (BEV) perception, does not yield satisfactory results. To overcome this problem, we introduce RadOcc, a Rendering assisted distillation paradigm for 3D Occupancy prediction. By employing differentiable volume rendering, we generate depth and semantic maps in perspective views and propose two novel consistency criteria between the rendered outputs of teacher and student models. Specifically, the depth consistency loss aligns the termination distributions of the rendered rays, while the semantic consistency loss mimics the intra-segment similarity guided by vision foundation models (VLMs). Experimental results on the nuScenes dataset demonstrate the effectiveness of our proposed method in improving various 3D occupancy prediction approaches, e.g., our proposed methodology enhances our baseline by 2.2% in the metric of mIoU and achieves 50% in Occ3D benchmark.",
                "authors": "Haiming Zhang, Xu Yan, Dongfeng Bai, Jiantao Gao, Pan Wang, Bingbing Liu, Shuguang Cui, Zhen Li",
                "citations": 15
            },
            {
                "title": "Revisiting the Role of Language Priors in Vision-Language Models",
                "abstract": "Vision-language models (VLMs) are impactful in part because they can be applied to a variety of visual understanding tasks in a zero-shot fashion, without any fine-tuning. We study $\\textit{generative VLMs}$ that are trained for next-word generation given an image. We explore their zero-shot performance on the illustrative task of image-text retrieval across 8 popular vision-language benchmarks. Our first observation is that they can be repurposed for discriminative tasks (such as image-text retrieval) by simply computing the match score of generating a particular text string given an image. We call this probabilistic score the $\\textit{Visual Generative Pre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on others. We analyze this behavior through a probabilistic lens, pointing out that some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions. In fact, we demonstrate that even a\"blind\"language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago. We derive a probabilistic post-processing scheme that controls for the amount of linguistic bias in generative VLMs at test time without having to retrain or fine-tune the model. We show that the VisualGPTScore, when appropriately debiased, is a strong zero-shot baseline for vision-language understanding, oftentimes producing state-of-the-art accuracy.",
                "authors": "Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, Deva Ramanan",
                "citations": 15
            },
            {
                "title": "Open-Vocabulary Object Detection using Pseudo Caption Labels",
                "abstract": "Recent open-vocabulary detection methods aim to detect novel objects by distilling knowledge from vision-language models (VLMs) trained on a vast amount of image-text pairs. To improve the effectiveness of these methods, researchers have utilized datasets with a large vocabulary that contains a large number of object classes, under the assumption that such data will enable models to extract comprehensive knowledge on the relationships between various objects and better generalize to unseen object classes. In this study, we argue that more fine-grained labels are necessary to extract richer knowledge about novel objects, including object attributes and relationships, in addition to their names. To address this challenge, we propose a simple and effective method named Pseudo Caption Labeling (PCL), which utilizes an image captioning model to generate captions that describe object instances from diverse perspectives. The resulting pseudo caption labels offer dense samples for knowledge distillation. On the LVIS benchmark, our best model trained on the de-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6, comparable to the state-of-the-art performance. PCL's simplicity and flexibility are other notable features, as it is a straightforward pre-processing technique that can be used with any image captioning model without imposing any restrictions on model architecture or training process.",
                "authors": "Han-Cheol Cho, Won Young Jhoo, Woohyun Kang, Byungseok Roh",
                "citations": 16
            },
            {
                "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
                "abstract": "This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and 1552.5 in MME-perception.",
                "authors": "Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia",
                "citations": 14
            },
            {
                "title": "DRPT: Disentangled and Recurrent Prompt Tuning for Compositional Zero-Shot Learning",
                "abstract": "Compositional Zero-shot Learning (CZSL) aims to recognize novel concepts composed of known knowledge without training samples. Standard CZSL either identifies visual primitives or enhances unseen composed entities, and as a result, entanglement between state and object primitives cannot be fully utilized. Admittedly, vision- language models (VLMs) could naturally cope with CZSL through tuning prompts, while uneven entanglement leads prompts to be dragged into local optimum. In this paper, we take a further step to introduce a novel Disentangled and Recurrent Prompt Tuning framework termed DRPT to better tap the potential of VLMs in CZSL. Specifically, the state and object primitives are deemed as learnable tokens of vocabulary embedded in prompts and tuned on seen compositions. Instead of jointly tuning state and object, we devise a disentangled and recurrent tuning strategy to suppress the traction force caused by entanglement and gradually optimize the token parameters, leading to a better prompt space. Notably, we develop a progressive fine-tuning procedure that allows for incremental updates to the prompts, optimizing the object first, then the state, and vice versa. Meanwhile, the optimization of state and object is independent, thus clearer features can be learned to further alleviate the issue of entangling misleading optimization. Moreover, we quantify and analyze the entanglement in CZSL and supplement entanglement rebalancing optimization schemes. DRPT surpasses representative state-of-the-art methods on extensive benchmark datasets, demonstrating superiority in both accuracy and efficiency.",
                "authors": "Xiaocheng Lu, Ziming Liu, Song Guo, Jingcai Guo, Fushuo Huo, Sikai Bai, Tao Han",
                "citations": 14
            },
            {
                "title": "Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models",
                "abstract": "We study open-world 3D scene understanding, a family of tasks that require agents to reason about their 3D environment with an open-set vocabulary and out-of-domain visual inputs - a critical skill for robots to operate in the unstructured 3D world. Towards this end, we propose Semantic Abstraction (SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D spatial capabilities, while maintaining their zero-shot robustness. We achieve this abstraction using relevancy maps extracted from CLIP, and learn 3D spatial and geometric reasoning skills on top of those abstractions in a semantic-agnostic manner. We demonstrate the usefulness of SemAbs on two open-world 3D scene understanding tasks: 1) completing partially observed objects and 2) localizing hidden objects from language descriptions. Experiments show that SemAbs can generalize to novel vocabulary, materials/lighting, classes, and domains (i.e., real-world scans) from training on limited 3D synthetic data. Code and data is available at https://semantic-abstraction.cs.columbia.edu/",
                "authors": "Huy Ha, Shuran Song",
                "citations": 88
            },
            {
                "title": "EventCLIP: Adapting CLIP for Event-based Object Recognition",
                "abstract": "Recent advances in zero-shot and few-shot classification heavily rely on the success of pre-trained vision-language models (VLMs) such as CLIP. Due to a shortage of large-scale datasets, training such models for event camera data remains infeasible. Thus, adapting existing VLMs across modalities to event vision is an important research challenge. In this work, we introduce EventCLIP, a novel approach that utilizes CLIP for zero-shot and few-shot event-based object recognition. We first generalize CLIP's image encoder to event data by converting raw events to 2D grid-based representations. To further enhance performance, we propose a feature adapter to aggregate temporal information over event frames and refine text embeddings to better align with the visual inputs. We evaluate EventCLIP on N-Caltech, N-Cars, and N-ImageNet datasets, achieving state-of-the-art few-shot performance. When fine-tuned on the entire dataset, our method outperforms all existing event classifiers. Moreover, we explore practical applications of EventCLIP including robust event classification and label-free event recognition, where our approach surpasses previous baselines designed specifically for these tasks.",
                "authors": "Ziyi Wu, Xudong Liu, Igor Gilitschenski",
                "citations": 12
            },
            {
                "title": "Adversarial Prompt Tuning for Vision-Language Models",
                "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
                "authors": "Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang",
                "citations": 13
            },
            {
                "title": "Distribution-Aware Prompt Tuning for Vision-Language Models",
                "abstract": "Pre-trained vision-language models (VLMs) have shown impressive performance on various downstream tasks by utilizing knowledge learned from large data. In general, the performance of VLMs on target tasks can be further improved by prompt tuning, which adds context to the input image or text. By leveraging data from target tasks, various prompt-tuning methods have been studied in the literature. A key to prompt tuning is the feature space alignment between two modalities via learnable vectors with model parameters fixed. We observed that the alignment becomes more effective when embeddings of each modality are ‘well-arranged’ in the latent space. Inspired by this observation, we proposed distribution-aware prompt tuning (DAPT) for vision-language models, which is simple yet effective. Specifically, the prompts are learned by maximizing inter-dispersion, the distance between classes, as well as minimizing the intra-dispersion measured by the distance between embeddings from the same class. Our extensive experiments on 11 benchmark datasets demonstrate that our method significantly improves generalizability. The code is available at https://github.com/mlvlab/DAPT.",
                "authors": "Eulrang Cho, Jooyeon Kim, Hyunwoo J. Kim",
                "citations": 12
            },
            {
                "title": "VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores",
                "abstract": "Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as P ( match | text , image ) have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the V isual G enerative P re-T raining Score ( Visu-alGPTScore ) of P ( text | image ) , a multimodal generative score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the marginal P(text) and the Pointwise Mutual Information (PMI). This helps to (a) diagnose datasets with strong language bias, and (b) debias results on other benchmarks like Winoground using an information-theoretic framework. Visual-GPTScore provides valuable insights and serves as a strong baseline for future evaluation of visio-linguistic compositionality.",
                "authors": "Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, Deva Ramanan",
                "citations": 13
            },
            {
                "title": "GIPCOL: Graph-Injected Soft Prompting for Compositional Zero-Shot Learning",
                "abstract": "Pre-trained vision-language models (VLMs) have achieved promising success in many fields, especially with prompt learning paradigm. In this work, we propose GIPCOL (Graph-Injected Soft Prompting for Compositional Learning) to better explore the compositional zero-shot learning (CZSL) ability of VLMs within the prompt-based learning framework. The soft prompt in GIPCOL is structured and consists of the prefix learnable vectors, attribute label and object label. In addition, the attribute and object labels in the soft prompt are designated as nodes in a compositional graph. The compositional graph is constructed based on the compositional structure of the objects and attributes extracted from the training data and consequently feeds the updated concept representation into the soft prompt to capture this compositional structure for a better prompting for CZSL. With the new prompting strategy, GIPCOL achieves state-of-the-art AUC results on all three CZSL benchmarks, including MIT-States, UT-Zappos, and C-GQA datasets in both closed and open settings compared to previous non-CLIP as well as CLIP-based methods. We analyze when and why GIPCOL operates well given the CLIP backbone and its training data limitations, and our findings shed light on designing more effective prompts for CZSL.",
                "authors": "Guangyue Xu, Joyce Chai, Parisa Kordjamshidi",
                "citations": 12
            },
            {
                "title": "VicTR: Video-conditioned Text Representations for Activity Recognition",
                "abstract": "Vision-Language models (VLMs) have excelled in the image-domain- especially in zero-shot settings- thanks to the availability of vast pretraining data (i.e., paired image-text samples). However for videos, such paired data is not as abundant. Therefore, video- VLMs are usually designed by adapting pretrained image- VLMs to the video-domain, instead of training from scratch. All such recipes rely on aug-menting visual embeddings with temporal information (i.e., image -+ video), often keeping text embeddings unchanged or even being discarded. In this paper, we argue the contrary, that better video- VLMs can be designed by focusing more on augmenting text, rather than visual information. More specifically, we introduce Video-conditioned Text Representations (Vi c TR): a form of text embeddings optimized w.r.t. vi-sual embeddings, creating a more-flexible contrastive latent space. Our model canfurther make use offreely-available semantic information, in the form of visually- grounded aux-iliary text (e.g. object or scene information). We evaluate our model on few-shot, zero-shot (HMDB-51, UCF-10l), short-form (Kinetics-400) and long-form (Charades) activ-ity recognition benchmarks, showing strong performance among video-VLMs.",
                "authors": "Kumara Kahatapitiya, Anurag Arnab, Arsha Nagrani, M. Ryoo",
                "citations": 13
            },
            {
                "title": "ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models",
                "abstract": "Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model. Code is available at https://github.com/ExplainableML/ProbVLM",
                "authors": "Uddeshya Upadhyay, Shyamgopal Karthik, Massimiliano Mancini, Zeynep Akata",
                "citations": 12
            },
            {
                "title": "Grounding Classical Task Planners via Vision-Language Models",
                "abstract": "Classical planning systems have shown great advances in utilizing rule-based human knowledge to compute accurate plans for service robots, but they face challenges due to the strong assumptions of perfect perception and action executions. To tackle these challenges, one solution is to connect the symbolic states and actions generated by classical planners to the robot's sensory observations, thus closing the perception-action loop. This research proposes a visually-grounded planning framework, named TPVQA, which leverages Vision-Language Models (VLMs) to detect action failures and verify action affordances towards enabling successful plan execution. Results from quantitative experiments show that TPVQA surpasses competitive baselines from previous studies in task completion rate.",
                "authors": "Xiaohan Zhang, Yan Ding, S. Amiri, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang",
                "citations": 13
            },
            {
                "title": "FunQA: Towards Surprising Video Comprehension",
                "abstract": "Surprising videos, such as funny clips, creative performances, or visual illusions, attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA, a challenging video question-answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter-intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts, e.g., cooking or instructional videos, FunQA covers three previously unexplored types of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous QA tasks designed to assess the model's capability in counter-intuitive timestamp localization, detailed video description, and reasoning around counter-intuitiveness. We also pose higher-level tasks, such as attributing a fitting and vivid title to the video and scoring the video creativity. In total, the FunQA benchmark consists of 312K free-text QA pairs derived from 4.3K video clips, spanning a total of 24 video hours. Moreover, we propose FunMentor, an agent designed for Vision-Language Models (VLMs) that uses multi-turn dialogues to enhance models' understanding of counter-intuitiveness. Extensive experiments with existing VLMs demonstrate the effectiveness of FunMentor and reveal significant performance gaps for the FunQA videos across spatial-temporal reasoning, visual-centered reasoning, and free-text generation.",
                "authors": "Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu",
                "citations": 12
            },
            {
                "title": "Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?",
                "abstract": "\n The field of multimedia research has witnessed significant interest in leveraging multimodal pretrained neural network models to perceive and represent the physical world. Among these models, vision-language pretraining (VLP) has emerged as a captivating topic. Currently, the prevalent approach in VLP involves supervising the training process with paired image-text data. However, limited efforts have been dedicated to exploring the extraction of essential linguistic knowledge, such as semantics and syntax, during VLP and understanding its impact on multimodal alignment. In response, our study aims to shed light on the influence of comprehensive linguistic knowledge encompassing semantic expression and syntactic structure on multimodal alignment. To achieve this, we introduce SNARE, a large-scale multimodal alignment probing benchmark designed specifically for the detection of vital linguistic components, including lexical, semantic, and syntax knowledge. SNARE offers four distinct tasks: Semantic Structure, Negation Logic, Attribute Ownership, and Relationship Composition. Leveraging SNARE, we conduct holistic analyses of six advanced VLP models (BLIP, CLIP, Flava, X-VLM, BLIP2, and GPT-4), along with human performance, revealing key characteristics of the VLP model:\n i)\n Insensitivity to complex syntax structures, relying primarily on content words for sentence comprehension.\n ii)\n Limited comprehension of sentence combinations and negations.\n iii)\n Challenges in determining actions or spatial relations within visual information, as well as difficulties in verifying the correctness of ternary relationships. Based on these findings, we propose the following strategies to enhance multimodal alignment in VLP: 1) Utilize a large generative language model as the language backbone in VLP to facilitate the understanding of complex sentences. 2) Establish high-quality datasets that emphasize content words and employ simple syntax, such as short-distance semantic composition, to improve multimodal alignment. 3) Incorporate more fine-grained visual knowledge, such as spatial relationships, into pretraining objectives.\n \n 1\n \n",
                "authors": "Fei Wang, Liang Ding, Jun Rao, Ye Liu, Li-juan Shen, Changxing Ding",
                "citations": 13
            },
            {
                "title": "A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues",
                "abstract": "Conditional inference on joint textual and visual clues is a multi-modal reasoning task that textual clues provide prior permutation or external knowledge, which are complementary with visual content and pivotal to deducing the correct option. Previous methods utilizing pretrained vision-language models (VLMs) have achieved impressive performances, yet they show a lack of multimodal context reasoning capability, especially for text-modal information. To address this issue, we propose a Multi-modal Context Reasoning approach, named ModCR. Compared to VLMs performing reasoning via cross modal semantic alignment, it regards the given textual abstract semantic and objective image information as the pre-context information and embeds them into the language model to perform context reasoning. Different from recent vision-aided language models used in natural language processing, ModCR incorporates the multi-view semantic alignment information between language and vision by introducing the learnable alignment prefix between image and text in the pretrained language model. This makes the language model well-suitable for such multi-modal reasoning scenario on joint textual and visual clues. We conduct extensive experiments on two corresponding data sets and experimental results show significantly improved performance (exact gain by 4.8% on PMR test set) compared to previous strong baselines.",
                "authors": "Yunxin Li, Baotian Hu, Xinyu Chen, Yuxin Ding, Lin Ma, Min Zhang",
                "citations": 12
            },
            {
                "title": "Probing Conceptual Understanding of Large Visual-Language Models",
                "abstract": "In recent years large visual-language (V+L) models have achieved great success in various downstream tasks. However, it is not well studied whether these models have a conceptual grasp of the visual content. In this work we focus on conceptual understanding of these large V+L models. To facilitate this study, we propose novel benchmarking datasets for probing three different aspects of content understanding, 1) relations, 2) composition, and 3) context. Our probes are grounded in cognitive science and help determine if a V+L model can, for example, determine if snow garnished with a man is implausible, or if it can identify beach furniture by knowing it is located on a beach. We experimented with many recent state-of-the-art V+L models and observe that these models mostly fail to demonstrate a conceptual understanding. This study reveals several interesting insights such as that cross-attention helps learning conceptual understanding, and that CNNs are better with texture and patterns, while Transformers are better at color and shape. We further utilize some of these insights and investigate a simple finetuning technique that rewards the three conceptual understanding measures with promising initial results. The proposed benchmarks will drive the community to delve deeper into conceptual understanding and foster advancements in the capabilities of large V+L models. The code and dataset is available at: https://tinyurl.com/vlm-robustness",
                "authors": "Madeline Chantry Schiappa, Michael Cogswell, Ajay Divakaran, Y. Rawat",
                "citations": 12
            },
            {
                "title": "Task Residual for Tuning Vision-Language Models",
                "abstract": "Large-scale vision-language models (VLMs) pre-trained on billion-level data have learned general visual representations and broad visual concepts. In principle, the welllearned knowledge structure of the VLMs should be inherited appropriately when being transferred to downstream tasks with limited data. However, most existing efficient transfer learning (ETL) approaches for VLMs either damage or are excessively biased towards the prior knowledge, e.g., prompt tuning (PT) discards the pre-trained text-based classifier and builds a new one while adapter-style tuning (AT) fully relies on the pre-trained features. To address this, we propose a new efficient tuning approach for VLMs named Task Residual Tuning (TaskRes), which performs directly on the text-based classifier and explicitly decouples the prior knowledge of the pre-trained models and new knowledge regarding a target task. Specifically, TaskRes keeps the original classifier weights from the VLMs frozen and obtains a new classifier for the target task by tuning a set of prior-independent parameters as a residual to the original one, which enables reliable prior knowledge preservation and flexible task-specific knowledge exploration. The proposed TaskRes is simple yet effective, which significantly outperforms previous ETL methods (e.g., PT and AT) on 11 benchmark datasets while requiring minimal effort for the implementation. Our code is available at https://github.com/geekyutao/TaskRes.",
                "authors": "Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, Xinchao Wang",
                "citations": 58
            },
            {
                "title": "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning",
                "abstract": "We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP also outperforms CoOp, a soft prompting method that fine-tunes the prefix context tokens, by an average of 5.8 percentage points on AUC. We perform additional experiments to show that CSP improves generalization to higher-order attribute-attribute-object compositions (e.g., old white cat) and combinations of pretrained attributes and fine-tuned objects. The code is available at https://github.com/BatsResearch/csp.",
                "authors": "Nihal V. Nayak, Peilin Yu, Stephen H. Bach",
                "citations": 48
            },
            {
                "title": "Robot Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning",
                "abstract": "The pre-train and fine-tune paradigm in machine learning has had dramatic success in a wide range of domains because the use of existing data or pre-trained models on the internet enables quick and easy learning of new tasks. We aim to enable this paradigm in robotic reinforcement learning, allowing a robot to learn a new task with little human effort by leveraging data and models from the Internet. However, reinforcement learning often requires significant human effort in the form of manual reward specification or environment resets, even if the policy is pre-trained. We introduce RoboFuME, a reset-free fine-tuning system that pre-trains a multi-task manipulation policy from diverse datasets of prior experiences and self-improves online to learn a target task with minimal human intervention. Our insights are to utilize calibrated offline reinforcement learning techniques to ensure efficient online fine-tuning of a pre-trained policy in the presence of distribution shifts and leverage pre-trained vision language models (VLMs) to build a robust reward classifier for autonomously providing reward signals during the online fine-tuning process. In a diverse set of five real robot manipulation tasks, we show that our method can incorporate data from an existing robot dataset collected at a different institution and improve on a target task within as little as 3 hours of autonomous real-world experience. We also demonstrate in simulation experiments that our method outperforms prior works that use different RL algorithms or different approaches for predicting rewards. Project website: https://robofume.github.io",
                "authors": "Jingyun Yang, Max Sobol Mark, Brandon Vu, Archit Sharma, Jeannette Bohg, Chelsea Finn",
                "citations": 11
            },
            {
                "title": "Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?",
                "abstract": "Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are available at https://github.com/vl-illusion/dataset.",
                "authors": "Yichi Zhang, Jiayi Pan, Yuchen Zhou, Rui Pan, Joyce Chai",
                "citations": 11
            },
            {
                "title": "GenZI: Zero-Shot 3D Human-Scene Interaction Generation",
                "abstract": "Can we synthesize 3D humans interacting with scenes without learning from any 3D human-scene interaction data? We propose GenZI11Project page: craigleili.github.io/projects/genzi, the first zero-shot approach to generating 3D human-scene interactions. Key to GenZI is our distillation of interaction priors from large vision-language models (VLMs), which have learned a rich semantic space of 2D human-scene compositions. Given a natural language description and a coarse point location of the desired interaction in a 3D scene, we first leverage VLMs to imagine plausible 2D human interactions inpainted into multiple rendered views of the scene. We then formulate a robust iterative optimization to synthesize the pose and shape of a 3D human model in the scene, guided by consistency with the 2D interaction hypotheses. In contrast to existing learning-based approaches, GenZI circumvents the conventional need for captured 3D interaction data, and allows for flexible control of the 3D interaction synthesis with easy-to-use text prompts. Extensive experiments show that our zero-shot approach has high flexibility and generality, making it applicable to diverse scene types, including both indoor and outdoor environments.",
                "authors": "Lei Li, Angela Dai",
                "citations": 10
            },
            {
                "title": "Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models",
                "abstract": "Medical image segmentation allows quantifying target structure size and shape, aiding in disease diagnosis, prognosis, surgery planning, and comprehension.Building upon recent advancements in foundation Vision-Language Models (VLMs) from natural image-text pairs, several studies have proposed adapting them to Vision-Language Segmentation Models (VLSMs) that allow using language text as an additional input to segmentation models. Introducing auxiliary information via text with human-in-the-loop prompting during inference opens up unique opportunities, such as open vocabulary segmentation and potentially more robust segmentation models against out-of-distribution data. Although transfer learning from natural to medical images has been explored for image-only segmentation models, the joint representation of vision-language in segmentation problems remains underexplored. This study introduces the first systematic study on transferring VLSMs to 2D medical images, using carefully curated $11$ datasets encompassing diverse modalities and insightful language prompts and experiments. Our findings demonstrate that although VLSMs show competitive performance compared to image-only models for segmentation after finetuning in limited medical image datasets, not all VLSMs utilize the additional information from language prompts, with image features playing a dominant role. While VLSMs exhibit enhanced performance in handling pooled datasets with diverse modalities and show potential robustness to domain shifts compared to conventional segmentation models, our results suggest that novel approaches are required to enable VLSMs to leverage the various auxiliary information available through language prompts. The code and datasets are available at https://github.com/naamiinepal/medvlsm.",
                "authors": "K. Poudel, Manish Dhakal, Prasiddha Bhandari, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal",
                "citations": 10
            },
            {
                "title": "Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation",
                "abstract": "Pre-trained Vision-Language Models (VLMs), such as CLIP, have shown enhanced performance across a range of tasks that involve the integration of visual and linguistic modalities. When CLIP is used for depth estimation tasks, the patches, divided from the input images, can be combined with a series of semantic descriptions of the depth information to obtain similarity results. The coarse estimation of depth is then achieved by weighting and summing the depth values, called depth bins, corresponding to the predefined semantic descriptions. The zero-shot approach circumvents the computational and time-intensive nature of traditional fully-supervised depth estimation methods. However, this method, utilizing fixed depth bins, may not effectively generalize as images from different scenes may exhibit distinct depth distributions. To address this challenge, we propose a few-shot-based method which learns to adapt the VLMs for monocular depth estimation to balance training costs and generalization capabilities. Specifically, it assigns different depth bins for different scenes, which can be selected by the model during inference. Additionally, we incorporate learnable prompts to preprocess the input text to convert the easily human-understood text into easily model-understood vectors and further enhance the performance. With only one image per scene for training, our extensive experiment results on the NYU V2 and KITTI dataset demonstrate that our method outperforms the previous state-of-the-art method by up to 10.6% in terms of MARE1.",
                "authors": "Xue-mei Hu, Ce Zhang, Yi Zhang, Bowen Hai, Ke Yu, Zhihai He",
                "citations": 10
            },
            {
                "title": "BDC-Adapter: Brownian Distance Covariance for Better Vision-Language Reasoning",
                "abstract": "Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP and ALIGN, have introduced a new paradigm for learning transferable visual representations. Recently, there has been a surge of interest among researchers in developing lightweight fine-tuning techniques to adapt these models to downstream visual tasks. We recognize that current state-of-the-art fine-tuning methods, such as Tip-Adapter, simply consider the covariance between the query image feature and features of support few-shot training samples, which only captures linear relations and potentially instigates a deceptive perception of independence. To address this issue, in this work, we innovatively introduce Brownian Distance Covariance (BDC) to the field of vision-language reasoning. The BDC metric can model all possible relations, providing a robust metric for measuring feature dependence. Based on this, we present a novel method called BDC-Adapter, which integrates BDC prototype similarity reasoning and multi-modal reasoning network prediction to perform classification tasks. Our extensive experimental results show that the proposed BDC-Adapter can freely handle non-linear relations and fully characterize independence, outperforming the current state-of-the-art methods by large margins.",
                "authors": "Yi Zhang, Ce Zhang, Zihan Liao, Yushun Tang, Zhihai He",
                "citations": 10
            },
            {
                "title": "Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling",
                "abstract": "Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both text-only and multimodal tasks, with experiments that account for both parameter-count scaling and training regime (with and without instruction tuning).",
                "authors": "Yaqing Wang, Jialin Wu, T. Dabral, Jiageng Zhang, Geoff Brown, Chun-Ta Lu, Frederick Liu, Yi Liang, Bo Pang, Michael Bendersky, Radu Soricut",
                "citations": 11
            },
            {
                "title": "Cross-Modal Concept Learning and Inference for Vision-Language Models",
                "abstract": "Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP, establish the correlation between texts and images, achieving remarkable success on various downstream tasks with fine-tuning. In existing fine-tuning methods, the class-specific text description is matched against the whole image. We recognize that this whole image matching is not effective since images from the same class often contain a set of different semantic objects, and an object further consists of a set of semantic parts or concepts. Individual semantic parts or concepts may appear in image samples from different classes. To address this issue, in this paper, we develop a new method called cross-model concept learning and inference (CCLI). Using the powerful text-image correlation capability of CLIP, our method automatically learns a large set of distinctive visual concepts from images using a set of semantic text concepts. Based on these visual concepts, we construct a discriminative representation of images and learn a concept inference network to perform downstream image classification tasks, such as few-shot learning and domain generalization. Extensive experimental results demonstrate that our CCLI method is able to improve the performance upon the current state-of-the-art methods by large margins, for example, by up to 8.0% improvement on few-shot learning and by up to 1.3% for domain generalization.",
                "authors": "Yi Zhang, Ce Zhang, Yushun Tang, Z. He",
                "citations": 10
            },
            {
                "title": "Bridging Language and Action: A Survey of Language-Conditioned Robot Manipulation",
                "abstract": "Language-conditioned robot manipulation is an emerging field aimed at enabling seamless communication and cooperation between humans and robotic agents by teaching robots to comprehend and execute instructions conveyed in natural language. This interdisciplinary area integrates scene understanding, language processing, and policy learning to bridge the gap between human instructions and robotic actions. In this comprehensive survey, we systematically explore recent advancements in language-conditioned robotic manipulation. We categorize existing methods into language-conditioned reward shaping, language-conditioned policy learning, neuro-symbolic artificial intelligence, and the utilization of foundational models (FMs) such as large language models (LLMs) and vision-language models (VLMs). Specifically, we analyze state-of-the-art techniques concerning semantic information extraction, environment and evaluation, auxiliary tasks, and task representation strategies. By conducting a comparative analysis, we highlight the strengths and limitations of current approaches in bridging language instructions with robot actions. Finally, we discuss open challenges and future research directions, focusing on potentially enhancing generalization capabilities and addressing safety issues in language-conditioned robot manipulators. The GitHub repository of this paper can be found at https://github.com/hk-zh/language-conditioned-robot-manipulation-models.",
                "authors": "Hongkuan Zhou, Xiangtong Yao, Oier Mees, Yuan Meng, Ted Xiao, Yonatan Bisk, Jean Oh, Edward Johns, Mohit Shridhar, Dhruv Shah, Jesse Thomason, Kai Huang, Joyce Chai, Zhenshan Bing, Alois Knoll",
                "citations": 10
            },
            {
                "title": "SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples",
                "abstract": "While vision-language models (VLMs) have achieved re-markable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of imagetext pairs for various combinations of social attributes. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intersectional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender). Through our over-generate-then-filter methodology, we produce SocialCounterfactuals, a high-quality dataset containing 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics. We conduct extensive experiments to demonstrate the usefulness of our generated dataset for probing and mitigating intersectional social biases in state-of-the-art VLMs.",
                "authors": "Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Anahita Bhiwandiwalla, Vasudev Lal",
                "citations": 10
            },
            {
                "title": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities",
                "abstract": "The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of vision and language understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capacity. In this work, we introduce Acoustic Prompt Turning (APT), a new adapter extending LLMs and VLMs to the audio domain by soft prompting only. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as language model inputs. To mitigate the data scarcity in the audio domain, a multi-task learning strategy is proposed by formulating diverse audio tasks in a sequence-to-sequence manner. Moreover, we improve the framework of audio language model by using interleaved audio-text embeddings as the input sequence. This improved framework imposes zero constraints on the input format and thus is capable of tackling more understanding tasks, such as few-shot audio classification and audio reasoning. To further evaluate the reasoning ability of audio networks, we propose natural language audio reasoning (NLAR), a new task that analyses across two audio clips by comparison and summarization. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the targeted datasets) across various tasks. We finally demonstrate the APT's ability in extending frozen VLMs to the audio domain without finetuning, achieving promising results in the audio-visual question and answering task. Our code and model weights are released at https://github.com/JinhuaLiang/APT.",
                "authors": "Jinhua Liang, Xubo Liu, Wenwu Wang, Mark D. Plumbley, Huy Phan, Emmanouil Benetos",
                "citations": 9
            },
            {
                "title": "Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models",
                "abstract": "Vision-language models (VLMs) pre-trained on large- scale image-text pairs have demonstrated impressive transferability on various visual tasks. Transferring knowledge from such powerful VLMs is a promising direction for building effective video recognition models. However, current exploration in this field is still limited. We believe that the greatest value of pre-trained VLMs lies in building a bridge between visual and textual domains. In this paper, we propose a novel framework called BIKE, which utilizes the cross-modal bridge to explore bidirectional knowledge: i) We introduce the Video Attribute Association mechanism, which leverages the Video-to-Text knowledge to generate textual auxiliary attributes for complementing video recognition. ii) We also present a Temporal Concept Spotting mechanism that uses the Text-to-Video expertise to capture temporal saliency in a parameter-free manner, leading to enhanced video representation. Extensive studies on six popular video datasets, including Kinetics-400 & 600, UCF-101, HMDB-51, ActivityNet and Charades, show that our method achieves state-of-the-art performance in various recognition scenarios, such as general, zero-shot, and few-shot video recognition. Our best model achieves a state-of-the-art accuracy of 88.6% on the challenging Kinetics-400 using the released CLIP model. The code is available at https://github.com/whwu95/BIKE.",
                "authors": "Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, Wanli Ouyang",
                "citations": 43
            },
            {
                "title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
                "abstract": "In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.",
                "authors": "Ted Xiao, Harris Chan, P. Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, S. Levine, Jonathan Tompson",
                "citations": 54
            },
            {
                "title": "Reassessment of 20th century global mean sea level rise",
                "abstract": "Significance Estimates of global mean sea level (GMSL) before the advent of satellite altimetry vary widely, mainly because of the uneven coverage and limited temporal sampling of tide gauge records, which track local sea level rather than the global mean. Here we introduce an approach that combines recent advances in solid Earth and geoid corrections for individual tide gauges with improved knowledge about their geographical representation of ocean internal variability. Our assessment yields smaller trends before 1990 than previously reported, leading to a larger overall acceleration; identifies three major explanations for differences with previous estimates; and reconciles observational GMSL estimates with the sum of individually modeled contributions from the Coupled Model Intercomparison Project 5 database for the entire 20th century. The rate at which global mean sea level (GMSL) rose during the 20th century is uncertain, with little consensus between various reconstructions that indicate rates of rise ranging from 1.3 to 2 mm⋅y−1. Here we present a 20th-century GMSL reconstruction computed using an area-weighting technique for averaging tide gauge records that both incorporates up-to-date observations of vertical land motion (VLM) and corrections for local geoid changes resulting from ice melting and terrestrial freshwater storage and allows for the identification of possible differences compared with earlier attempts. Our reconstructed GMSL trend of 1.1 ± 0.3 mm⋅y−1 (1σ) before 1990 falls below previous estimates, whereas our estimate of 3.1 ± 1.4 mm⋅y−1 from 1993 to 2012 is consistent with independent estimates from satellite altimetry, leading to overall acceleration larger than previously suggested. This feature is geographically dominated by the Indian Ocean–Southern Pacific region, marking a transition from lower-than-average rates before 1990 toward unprecedented high rates in recent decades. We demonstrate that VLM corrections, area weighting, and our use of a common reference datum for tide gauges may explain the lower rates compared with earlier GMSL estimates in approximately equal proportion. The trends and multidecadal variability of our GMSL curve also compare well to the sum of individual contributions obtained from historical outputs of the Coupled Model Intercomparison Project Phase 5. This, in turn, increases our confidence in process-based projections presented in the Fifth Assessment Report of the Intergovernmental Panel on Climate Change.",
                "authors": "S. Dangendorf, M. Marcos, G. Wöppelmann, C. Conrad, T. Frederikse, R. Riva",
                "citations": 291
            },
            {
                "title": "ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation",
                "abstract": "State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are incorporated. First, we propose to leverage the inherent structure of programming language to depict visual structural information. This approach enables explicit and consistent representation of visual structural information of multiple granularities, such as concepts, relations, and events, in a well-organized structured format. Second, we introduce curriculum-based learning for VLMs to progressively comprehend visual structures, from fundamental visual concepts to intricate event structures. Our intuition is that lower-level knowledge may contribute to complex visual structure understanding. Furthermore, we compile and release a collection of datasets tailored for visual structural knowledge extraction. We adopt a weakly-supervised approach to directly generate visual event structures from captions for ViStruct training, capitalizing on abundant image-caption pairs from the web. In experiments, we evaluate ViStruct on visual structure prediction tasks, demonstrating its effectiveness in improving the understanding of visual structures. The code is public at \\url{https://github.com/Yangyi-Chen/vi-struct}.",
                "authors": "Yangyi Chen, Xingyao Wang, Manling Li, Derek Hoiem, Heng Ji",
                "citations": 9
            },
            {
                "title": "Towards General Purpose Medical AI: Continual Learning Medical Foundation Model",
                "abstract": "Inevitable domain and task discrepancies in real-world scenarios can impair the generalization performance of the pre-trained deep models for medical data. Therefore, we audaciously propose that we should build a general-purpose medical AI system that can be seamlessly adapted to downstream domains/tasks. Since the domain/task adaption procedures usually involve additional labeling work for the target data, designing a data-efficient adaption algorithm is desired to save the cost of transferring the learned knowledge. Our recent work found that vision-language models (VLMs) are efficient learners with extraordinary cross-domain ability. Therefore, in this work, we further explore the possibility of leveraging pre-trained VLMs as medical foundation models for building general-purpose medical AI, where we thoroughly investigate three machine-learning paradigms, i.e., domain/task-specialized learning, joint learning, and continual learning, for training the VLMs and evaluate their generalization performance on cross-domain and cross-task test sets. To alleviate the catastrophic forgetting during sequential training, we employ rehearsal learning and receive a sharp boost in terms of generalization capability. In a nutshell, our empirical evidence suggests that continual learning may be a practical and efficient learning paradigm for the medical foundation model. And we hope researchers can use our empirical evidence as basement to further explore the path toward medical foundation model.",
                "authors": "Huahui Yi, Ziyuan Qin, Qicheng Lao, Wei Xu, Zekun Jiang, Dequan Wang, Shaoting Zhang, Kang Li",
                "citations": 9
            },
            {
                "title": "Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering",
                "abstract": "In this paper, we explore effective prompting techniques to enhance zero- and few-shot Visual Question Answering (VQA) performance in contemporary Vision-Language Models (VLMs). Central to our investigation is the role of question templates in guiding VLMs to generate accurate answers. We identify that specific templates significantly influence VQA outcomes, underscoring the need for strategic template selection. Another pivotal aspect of our study is augmenting VLMs with image captions, providing them with additional visual cues alongside direct image features in VQA tasks. Surprisingly, this augmentation significantly improves the VLMs' performance in many cases, even though VLMs\"see\"the image directly! We explore chain-of-thought (CoT) reasoning and find that while standard CoT reasoning causes drops in performance, advanced methods like self-consistency can help recover it. Furthermore, we find that text-only few-shot examples enhance VLMs' alignment with the task format, particularly benefiting models prone to verbose zero-shot answers. Lastly, to mitigate the challenges associated with evaluating free-form open-ended VQA responses using string-matching based VQA metrics, we introduce a straightforward LLM-guided pre-processing technique to adapt the model responses to the expected ground-truth answer distribution. In summary, our research sheds light on the intricacies of prompting strategies in VLMs for VQA, emphasizing the synergistic use of captions, templates, and pre-processing to enhance model efficacy.",
                "authors": "Rabiul Awal, Le Zhang, Aishwarya Agrawal",
                "citations": 9
            },
            {
                "title": "Understanding and Mitigating Overfitting in Prompt Tuning for Vision-Language Models",
                "abstract": "Pretrained vision-language models (VLMs) such as CLIP have shown impressive generalization capability in downstream vision tasks with appropriate text prompts. Instead of designing prompts manually, Context Optimization (CoOp) has been recently proposed to learn continuous prompts using task-specific training data. Despite the performance improvements on downstream tasks, several studies have reported that CoOp suffers from the overfitting issue in two aspects: (i) the test accuracy on base classes first improves and then worsens during training; (ii) the test accuracy on novel classes keeps decreasing. However, none of the existing studies can understand and mitigate such overfitting problems. In this study, we first explore the cause of overfitting by analyzing the gradient flow. Comparative experiments reveal that CoOp favors generalizable and spurious features in the early and later training stages, respectively, leading to the non-overfitting and overfitting phenomena. Given those observations, we propose Subspace Prompt Tuning (Sub PT) to project the gradients in back-propagation onto the low-rank subspace spanned by the early-stage gradient flow eigenvectors during the entire training process and successfully eliminate the overfitting problem. In addition, we equip CoOp with a Novel Feature Learner (NFL) to enhance the generalization ability of the learned prompts onto novel categories beyond the training set, needless of image training data. Extensive experiments on 11 classification datasets demonstrate that Sub PT+NFL consistently boost the performance of CoOp and outperform the state-of-the-art CoCoOp approach. Experiments on more challenging vision downstream tasks, including open-vocabulary object detection and zero-shot semantic segmentation, also verify the effectiveness of the proposed method. Codes can be found at https://tinyurl.com/mpe64f89.",
                "authors": "Cheng Ma, Yang Liu, Jiankang Deng, Lingxi Xie, Weiming Dong, Changsheng Xu",
                "citations": 37
            },
            {
                "title": "Detecting and Correcting Hate Speech in Multimodal Memes with Large Visual Language Model",
                "abstract": "Recently, large language models (LLMs) have taken the spotlight in natural language processing. Further, integrating LLMs with vision enables the users to explore more emergent abilities in multimodality. Visual language models (VLMs), such as LLaVA, Flamingo, or GPT-4, have demonstrated impressive performance on various visio-linguistic tasks. Consequently, there are enormous applications of large models that could be potentially used on social media platforms. Despite that, there is a lack of related work on detecting or correcting hateful memes with VLMs. In this work, we study the ability of VLMs on hateful meme detection and hateful meme correction tasks with zero-shot prompting. From our empirical experiments, we show the effectiveness of the pretrained LLaVA model and discuss its strengths and weaknesses in these tasks.",
                "authors": "Minh-Hao Van, Xintao Wu",
                "citations": 8
            },
            {
                "title": "CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding",
                "abstract": "A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make\"infinite use of finite means\". However, current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their\"bag-of-words\"behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose CoVLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering.",
                "authors": "Junyan Li, Delin Chen, Yining Hong, Zhenfang Chen, Peihao Chen, Yikang Shen, Chuang Gan",
                "citations": 8
            },
            {
                "title": "LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions",
                "abstract": "Vision-language models (VLMs) offer a promising paradigm for image classification by comparing the similarity between images and class embeddings. A critical challenge lies in crafting precise textual representations for class names. While previous studies have leveraged recent advancements in large language models (LLMs) to enhance these descriptors, their outputs often suffer from ambiguity and inaccuracy. We attribute this to two primary factors: 1) the reliance on single-turn textual interactions with LLMs, leading to a mismatch between generated text and visual concepts for VLMs; 2) the oversight of the inter-class relationships, resulting in descriptors that fail to differentiate similar classes effectively. In this paper, we propose a novel framework that integrates LLMs and VLMs to find the optimal class descriptors. Our training-free approach develops an LLM-based agent with an evolutionary optimization strategy to iteratively refine class descriptors. We demonstrate our optimized descriptors are of high quality which effectively improves classification accuracy on a wide range of benchmarks. Additionally, these descriptors offer explainable and robust features, boosting performance across various backbone models and complementing fine-tuning-based methods.",
                "authors": "Songhao Han, Le Zhuo, Yue Liao, Si Liu",
                "citations": 8
            },
            {
                "title": "Prompting Scientific Names for Zero-Shot Species Recognition",
                "abstract": "Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g.,\"a photo of Lepus Timidus\"(which is a scientific name in Latin). Because these names are usually not included in CLIP's training set. To improve performance, prior works propose to use large-language models (LLMs) to generate descriptions (e.g., of species color and shape) and additionally use them in prompts. We find that they bring only marginal gains. Differently, we are motivated to translate scientific names (e.g., Lepus Timidus) to common English names (e.g., mountain hare) and use such in the prompts. We find that common names are more likely to be included in CLIP's training set, and prompting them achieves 2$\\sim$5 times higher accuracy on benchmarking datasets of fine-grained species recognition.",
                "authors": "Shubham Parashar, Zhiqiu Lin, Yanan Li, Shu Kong",
                "citations": 8
            },
            {
                "title": "EventBind: Learning a Unified Representation to Bind Them All for Event-Based Open-World Understanding",
                "abstract": null,
                "authors": "Jiazhou Zhou, Xueye Zheng, Yuanhuiyi Lyu, Lin Wang",
                "citations": 8
            },
            {
                "title": "Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models",
                "abstract": "Prompt tuning and adapter tuning have shown great potential in transferring pre-trained vision-language models (VLMs) to various downstream tasks. In this work, we design a new type of tuning method, termed as regularized mask tuning, which masks the network parameters through a learnable selection. Inspired by neural pathways, we argue that the knowledge required by a downstream task already exists in the pre-trained weights but just gets concealed in the upstream pre-training stage. To bring the useful knowledge back into light, we first identify a set of parameters that are important to a given downstream task, then attach a binary mask to each parameter, and finally optimize these masks on the downstream data with the parameters frozen. When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, in order to prevent the model from forgetting old knowledge and overfitting the downstream data. Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives. It is noteworthy that we manage to deliver 18.73% performance improvement compared to the zero-shot CLIP via masking an average of only 2.56% parameters. Furthermore, our method is synergistic with most existing parameter-efficient tuning methods and can boost the performance on top of them. Project page can be found here.",
                "authors": "Kecheng Zheng, Wei Wu, Ruili Feng, Kai Zhu, Jiawei Liu, Deli Zhao, Zhengjun Zha, Wei Chen, Yujun Shen",
                "citations": 8
            },
            {
                "title": "Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models",
                "abstract": "Fine-tuning pre-trained vision-language models (VLMs), e.g., CLIP, for the open-world generalization has gained increasing popularity due to its practical value. However, performance advancements are limited when relying solely on intricate algorithmic designs for a single model, even one exhibiting strong performance, e.g., CLIP-ViT-B/16. This paper, for the first time, explores the collaborative potential of leveraging much weaker VLMs to enhance the generalization of a robust single model. The affirmative findings motivate us to address the generalization problem from a novel perspective, i.e., ensemble of pre-trained VLMs. We introduce three customized ensemble strategies, each tailored to one specific scenario. Firstly, we introduce the zero-shot ensemble, automatically adjusting the logits of different models based on their confidence when only pre-trained VLMs are available. Furthermore, for scenarios with extra few-shot samples, we propose the training-free and tuning ensemble, offering flexibility based on the availability of computing resources. The proposed ensemble strategies are evaluated on zero-shot, base-to-new, and cross-dataset generalization, achieving new state-of-the-art performance. Notably, this work represents an initial stride toward enhancing the generalization performance of VLMs via ensemble. The code is available at https://github.com/zhiheLu/Ensemble_VLM.git.",
                "authors": "Zhihe Lu, Jiawang Bai, Xin Li, Zeyu Xiao, Xinchao Wang",
                "citations": 8
            },
            {
                "title": "Tackling Vision Language Tasks Through Learning Inner Monologues",
                "abstract": "Visual language tasks such as Visual Question Answering (VQA) or Visual Entailment (VE) require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. \nTo tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating Inner Monologue, a cognitive process in which an individual engages in silent verbal communication with themselves. More specifically, we enable LLMs and VLMs to interact through natural language conversation (i.e., Inner Monologue) and propose to use a two-stage training process to learn how to do Inner Monologue (self-asking questions and answering questions). IMMO is evaluated on two popular tasks and achieves competitive performance with less training data when compared with state-of-the-art models while concurrently keeping the interpretability. The results suggest that by emulating the cognitive phenomenon of internal dialogue, our approach can enhance reasoning and explanation abilities, contributing to the more effective fusion of vision and language models. More importantly, instead of using predefined human-crafted monologues, IMMO learns this process within the deep learning models, broadening its potential applications across various AI challenges beyond vision and language tasks.",
                "authors": "Diji Yang, Kezhen Chen, Jinmeng Rao, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Y. Zhang",
                "citations": 8
            },
            {
                "title": "Simple Image-level Classification Improves Open-vocabulary Object Detection",
                "abstract": "Open-Vocabulary Object Detection (OVOD) aims to detect novel objects beyond a given set of base categories on which the detection model is trained. Recent OVOD methods focus on adapting the image-level pre-trained vision-language models (VLMs), such as CLIP, to a region-level object detection task via, eg., region-level knowledge distillation, regional prompt learning, or region-text pre-training, to expand the detection vocabulary. These methods have demonstrated remarkable performance in recognizing regional visual concepts, but they are weak in exploiting the VLMs' powerful global scene understanding ability learned from the billion-scale image-level text descriptions. This limits their capability in detecting hard objects of small, blurred, or occluded appearance from novel/base categories, whose detection heavily relies on contextual information. To address this, we propose a novel approach, namely Simple Image-level Classification for Context-Aware Detection Scoring (SIC-CADS), to leverage the superior global knowledge yielded from CLIP for complementing the current OVOD models from a global perspective. The core of SIC-CADS is a multi-modal multi-label recognition (MLR) module that learns the object co-occurrence-based contextual information from CLIP to recognize all possible object categories in the scene. These image-level MLR scores can then be utilized to refine the instance-level detection scores of the current OVOD models in detecting those hard objects. This is verified by extensive empirical results on two popular benchmarks, OV-LVIS and OV-COCO, which show that SIC-CADS achieves significant and consistent improvement when combined with different types of OVOD models. Further, SIC-CADS also improves the cross-dataset generalization ability on Objects365 and OpenImages. Code is available at https://github.com/mala-lab/SIC-CADS.",
                "authors": "Ru Fang, Guansong Pang, Xiaolong Bai",
                "citations": 8
            },
            {
                "title": "Mas receptor: a potential strategy in the management of ischemic cardiovascular diseases",
                "abstract": "ABSTRACT MasR is a critical element in the RAS accessory pathway that protects the heart against myocardial infarction, ischemia-reperfusion injury, and pathological remodeling by counteracting the effects of AT1R. This receptor is mainly stimulated by Ang 1–7, which is a bioactive metabolite of the angiotensin produced by ACE2. MasR activation attenuates ischemia-related myocardial damage by facilitating vasorelaxation, improving cell metabolism, reducing inflammation and oxidative stress, inhibiting thrombosis, and stabilizing atherosclerotic plaque. It also prevents pathological cardiac remodeling by suppressing hypertrophy- and fibrosis-inducing signals. In addition, the potential of MasR in lowering blood pressure, improving blood glucose and lipid profiles, and weight loss has made it effective in modulating risk factors for coronary artery disease including hypertension, diabetes, dyslipidemia, and obesity. Considering these properties, the administration of MasR agonists offers a promising approach to the prevention and treatment of ischemic heart disease. Abbreviations: Acetylcholine (Ach); AMP-activated protein kinase (AMPK); Angiotensin (Ang); Angiotensin receptor (ATR); Angiotensin receptor blocker (ARB); Angiotensin-converting enzyme (ACE); Angiotensin-converting enzyme inhibitor (ACEI); Anti-PRD1-BF1-RIZ1 homologous domain containing 16 (PRDM16); bradykinin (BK); Calcineurin (CaN); cAMP-response element binding protein (CREB); Catalase (CAT); C-C Motif Chemokine Ligand 2 (CCL2); Chloride channel 3 (CIC3); c-Jun N-terminal kinases (JNK); Cluster of differentiation 36 (CD36); Cocaine- and amphetamine-regulated transcript (CART); Connective tissue growth factor (CTGF); Coronary artery disease (CAD); Creatine phosphokinase (CPK); C-X-C motif chemokine ligand 10 (CXCL10); Cystic fibrosis transmembrane conductance regulator (CFTR); Endothelial nitric oxide synthase (eNOS); Extracellular signal-regulated kinase 1/2 (ERK 1/2); Fatty acid transport protein (FATP); Fibroblast growth factor 21 (FGF21); Forkhead box protein O1 (FoxO1); Glucokinase (Gk); Glucose transporter (GLUT); Glycogen synthase kinase 3β (GSK3β); High density lipoprotein (HDL); High sensitive C-reactive protein (hs-CRP); Inositol trisphosphate (IP3); Interleukin (IL); Ischemic heart disease (IHD); Janus kinase (JAK); Kruppel-like factor 4 (KLF4); Lactate dehydrogenase (LDH); Left ventricular end-diastolic pressure (LVEDP); Left ventricular end-systolic pressure (LVESP); Lipoprotein lipase (LPL); L-NG-Nitro arginine methyl ester (L-NAME); Low density lipoprotein (LDL); Mammalian target of rapamycin (mTOR); Mas-related G protein-coupled receptors (Mrgpr); Matrix metalloproteinase (MMP); MAPK phosphatase-1 (MKP-1); Mitogen-activated protein kinase (MAPK); Monocyte chemoattractant protein-1 (MCP-1); NADPH oxidase (NOX); Neuropeptide FF (NPFF); Neutral endopeptidase (NEP); Nitric oxide (NO); Nuclear factor κ-light-chain-enhancer of activated B cells (NF-κB); Nuclear-factor of activated T-cells (NFAT); Pancreatic and duodenal homeobox 1 (Pdx1); Peroxisome proliferator- activated receptor γ (PPARγ); Phosphoinositide 3-kinases (PI3k); Phospholipase C (PLC); Prepro-orexin (PPO); Prolyl-endopeptidase (PEP); Prostacyclin (PGI2); Protein kinase B (Akt); Reactive oxygen species (ROS); Renin-angiotensin system (RAS); Rho-associated protein kinase (ROCK); Serum amyloid A (SAA); Signal transducer and activator of transcription (STAT); Sirtuin 1 (Sirt1); Slit guidance ligand 3 (Slit3); Smooth muscle 22α (SM22α); Sterol regulatory element-binding protein 1 (SREBP-1c); Stromal-derived factor-1a (SDF); Superoxide dismutase (SOD); Thiobarbituric acid reactive substances (TBARS); Tissue factor (TF); Toll-like receptor 4 (TLR4); Transforming growth factor β1 (TGF-β1); Tumor necrosis factor α (TNF-α); Uncoupling protein 1 (UCP1); Ventrolateral medulla (VLM)",
                "authors": "A. Molaei, Emad Molaei, A. Hayes, G. Karimi",
                "citations": 8
            },
            {
                "title": "Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models",
                "abstract": "Large vision-and-language models (VLMs) trained to match images with text on large-scale datasets of image-text pairs have shown impressive generalization ability on several vision and language tasks. Several recent works, however, showed that these models lack fine-grained understanding, such as the ability to count and recognize verbs, attributes, or relationships. The focus of this work is to study the understanding of spatial relations. This has been tackled previously using image-text matching (e.g., Visual Spatial Reasoning benchmark) or visual question answering (e.g., GQA or VQAv2), both showing poor performance and a large gap compared to human performance. In this work, we show qualitatively (using explainability tools) and quantitatively (using object detectors) that the poor object localization\"grounding\"ability of the models is a contributing factor to the poor image-text matching performance. We propose an alternative fine-grained, compositional approach for recognizing and ranking spatial clauses that combines the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative VLMs (such as LXMERT, GPV, and MDETR) and compare and highlight their abilities to reason about spatial relationships.",
                "authors": "Navid Rajabi, J. Kosecka",
                "citations": 8
            },
            {
                "title": "CLIP Is Also a Good Teacher: A New Learning Framework for Inductive Zero-shot Semantic Segmentation",
                "abstract": "Generalized Zero-shot Semantic Segmentation aims to segment both seen and unseen categories only under the supervision of the seen ones. To tackle this, existing methods adopt the large-scale Vision Language Models (VLMs) which obtain outstanding zero-shot performance. However, as the VLMs are designed for classification tasks, directly adapting the VLMs may lead to sub-optimal performance. Consequently, we propose CLIP-ZSS (Zero-shot Semantic Segmentation), a simple but effective training framework that enables any image encoder designed for closed-set segmentation applied in zero-shot and open-vocabulary tasks in testing without combining with VLMs or inserting new modules. CLIP-ZSS consists of two key modules: Global Learning Module (GLM) and Pixel Learning Module (PLM). GLM is proposed to probe the knowledge from the CLIP visual encoder by pulling the CLS token and the dense features from the image encoder of the same image and pushing others apart. Moreover, to enhance the ability to discriminate unseen categories, PLM consisting of pseudo labels and weight generation is designed. To generate semantically discriminated pseudo labels, a multi-scale K-Means with mask fusion working on the dense tokens is proposed. In pseudo weight generation, a synthesizer generating pseudo semantic features for the unannotated area is introduced. Experiments on three benchmarks show large performance gains compared with SOTA methods.",
                "authors": "Jialei Chen, Daisuke Deguchi, Chenkai Zhang, Xu Zheng, Hiroshi Murase",
                "citations": 8
            },
            {
                "title": "Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World",
                "abstract": "We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even conceived a neuro-symbolic reasoning approach that reconciles LLMs&VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities.",
                "authors": "Rujie Wu, Xiaojian Ma, Qing Li, Wei Wang, Zhenliang Zhang, Song-Chun Zhu, Yizhou Wang",
                "citations": 6
            },
            {
                "title": "Unified Visual Relationship Detection with Vision and Language Models",
                "abstract": "This work focuses on training a single visual relationship detector predicting over the union of label spaces from multiple datasets. Merging labels spanning different datasets could be challenging due to inconsistent taxonomies. The issue is exacerbated in visual relationship detection when second-order visual semantics are introduced between pairs of objects. To address this challenge, we propose UniVRD, a novel bottom-up method for Unified Visual Relationship Detection by leveraging vision and language models (VLMs). VLMs provide well-aligned image and text embeddings, where similar relationships are optimized to be close to each other for semantic unification. Our bottom-up design enables the model to enjoy the benefit of training with both object detection and visual relationship datasets. Empirical results on both human-object interaction detection and scene-graph generation demonstrate the competitive performance of our model. UniVRD achieves 38.07 mAP on HICO-DET, outperforming the current best bottom-up HOI detector by 14.26 mAP. More importantly, we show that our unified detector performs as well as dataset-specific models in mAP, and achieves further improvements when we scale up the model. Our code will be made publicly available on GitHub1.",
                "authors": "Long Zhao, Liangzhe Yuan, Boqing Gong, Yin Cui, Florian Schroff, Ming Yang, Hartwig Adam, Ting Liu",
                "citations": 7
            },
            {
                "title": "Large Language Models as Automated Aligners for benchmarking Vision-Language Models",
                "abstract": "With the advancements in Large Language Models (LLMs), Vision-Language Models (VLMs) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence. In this work, we address the limitations via Auto-Bench, which delves into exploring LLMs as proficient aligners, measuring the alignment between VLMs and human intelligence and value through automatic data curation and assessment. Specifically, for data curation, Auto-Bench utilizes LLMs (e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning triplets via prompting on visual symbolic representations (e.g., captions, object locations, instance relationships, and etc.). The curated data closely matches human intent, owing to the extensive world knowledge embedded in LLMs. Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered question-answer-reasoning triplets have been curated, covering 4 primary abilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to serve as judges, implementing the quantitative and qualitative automated assessments to facilitate a comprehensive evaluation of VLMs. Our validation results reveal that LLMs are proficient in both evaluation data curation and model assessment, achieving an average agreement rate of 85%. We envision Auto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating the evolving sophisticated VLMs.",
                "authors": "Yuanfeng Ji, Chongjian Ge, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, Ping Luo",
                "citations": 6
            },
            {
                "title": "Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models",
                "abstract": "Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \\textit{data-types}, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual data-types through scaling. By analyzing the pre-training distributions of these models and incorporating data-type information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released at https://github.com/bethgelab/DataTypeIdentification.",
                "authors": "Vishaal Udandarao, Max F. Burg, Samuel Albanie, Matthias Bethge",
                "citations": 6
            },
            {
                "title": "Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning",
                "abstract": "Recent compositional zero-shot learning (CZSL) methods adapt pre-trained vision-language models (VLMs) by constructing trainable prompts only for composed state-object pairs. Relying on learning the joint representation of seen compositions, these methods ignore the explicit modeling of the state and object, thus limiting the exploitation of pre-trained knowledge and generalization to unseen compositions. With a particular focus on the universality of the solution, in this work, we propose a novel paradigm for CZSL models that establishes three identification branches (i.e., Multi-Path) to jointly model the state, object, and composition. The presented Troika is an outstanding implementation that aligns the branch-specific prompt representations with decomposed visual features. To calibrate the bias between semantically similar multi-modal representations, we further devise a Cross-Modal Traction module into Troika that shifts the prompt representation towards the current visual content. We conduct extensive experiments on three popular benchmarks, where our method significantly outperforms existing methods in both closed-world and open-world settings. The code will be available at https://github.com/bighuang624/Troika.",
                "authors": "Siteng Huang, Biao Gong, Yutong Feng, Yiliang Lv, Donglin Wang",
                "citations": 7
            },
            {
                "title": "Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining",
                "abstract": "Contemporary large-scale visual language models (VLMs) exhibit strong representation capacities, making them ubiquitous for enhancing image and text understanding tasks. They are often trained in a contrastive manner on a large and diverse corpus of images and corresponding text captions scraped from the internet. Despite this, VLMs often struggle with compositional reasoning tasks which require a fine-grained understanding of the complex interactions of objects and their attributes. This failure can be attributed to two main factors: 1) Contrastive approaches have traditionally focused on mining negative examples from existing datasets. However, the mined negative examples might not be difficult for the model to discriminate from the positive. An alternative to mining would be negative sample generation 2) But existing generative approaches primarily focus on generating hard negative texts associated with a given image. Mining in the other direction, i.e., generating negative image samples associated with a given text has been ignored. To overcome both these limitations, we propose a framework that not only mines in both directions but also generates challenging negative samples in both modalities, i.e., images and texts. Leveraging these generative hard negative samples, we significantly enhance VLMs’ performance in tasks involving multimodal compositional reasoning. Our code and dataset are released at https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html.",
                "authors": "U. Sahin, Hang Li, Qadeer Ahmad Khan, Daniel Cremers, Volker Tresp",
                "citations": 7
            },
            {
                "title": "Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning",
                "abstract": "Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even surpass human capability in reasoning times and location. To address this question, we propose a two-stage Recognition & Reasoning probing task applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the studies, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In extensive evaluation experiments, we find that although VLMs can effectively retain times and location-relevant features in visual encoders, they still fail to make perfect reasoning with context-conditioned visual features. The dataset is available at https://github.com/gengyuanmax/WikiTiLo.",
                "authors": "Gengyuan Zhang, Yurui Zhang, Kerui Zhang, Volker Tresp",
                "citations": 7
            },
            {
                "title": "Towards Robust Prompts on Vision-Language Models",
                "abstract": "With the advent of vision-language models (VLMs) that can perform in-context and prompt-based learning, how can we design prompting approaches that robustly generalize to distribution shift and can be used on novel classes outside the support set of the prompts? In this work, we first define two types of robustness to distribution shift on VLMs, namely, robustness on base classes (the classes included in the support set of prompts) and robustness on novel classes. Then, we study the robustness of existing in-context learning and prompt learning approaches, where we find that prompt learning performs robustly on test images from base classes, while it does not generalize well on images from novel classes. We propose robust prompt learning by integrating multiple-scale image features into the prompt, which improves both types of robustness. Comprehensive experiments are conducted to study the defined robustness on six benchmarks and show the effectiveness of our proposal.",
                "authors": "Jindong Gu, Ahmad Beirami, Xuezhi Wang, Alex Beutel, Philip H. S. Torr, Yao Qin",
                "citations": 6
            },
            {
                "title": "OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data",
                "abstract": "In the era of big data and large models, automatic annotating functions for multi-modal data are of great significance for real-world AI-driven applications, such as autonomous driving and embodied AI. Unlike traditional closed-set annotation, open-vocabulary annotation is essential to achieve human-level cognition capability. However, there are few open-vocabulary auto-labeling systems for multi-modal 3D data. In this paper, we introduce OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can automatically generate 2D masks, 3D masks, and 3D bounding box annotations for vision and point cloud data. Our system integrates the chain-of-thought capabilities of Large Language Models (LLMs) and the cross-modality capabilities of vision-language models (VLMs). To the best of our knowledge, OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal 3D auto-labeling. We conduct comprehensive evaluations on both public and in-house real-world datasets, which demonstrate that the system significantly improves annotation efficiency compared to manual annotation while providing accurate open-vocabulary auto-annotating results.",
                "authors": "Yijie Zhou, Likun Cai, Xianhui Cheng, Zhongxue Gan, Xiangyang Xue, Wenchao Ding",
                "citations": 7
            },
            {
                "title": "HGCLIP: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding",
                "abstract": "Object categories are typically organized into a multi-granularity taxonomic hierarchy. When classifying categories at different hierarchy levels, traditional uni-modal approaches focus primarily on image features, revealing limitations in complex scenarios. Recent studies integrating Vision-Language Models (VLMs) with class hierarchies have shown promise, yet they fall short of fully exploiting the hierarchical relationships. These efforts are constrained by their inability to perform effectively across varied granularity of categories. To tackle this issue, we propose a novel framework (HGCLIP) that effectively combines CLIP with a deeper exploitation of the Hierarchical class structure via Graph representation learning. We explore constructing the class hierarchy into a graph, with its nodes representing the textual or image features of each category. After passing through a graph encoder, the textual features incorporate hierarchical structure information, while the image features emphasize class-aware features derived from prototypes through the attention mechanism. Our approach demonstrates significant improvements on 11 diverse visual recognition benchmarks. Our codes are fully available at https://github.com/richard-peng-xia/HGCLIP.",
                "authors": "Peng Xia, Xingtong Yu, Ming Hu, Lie Ju, Zhiyong Wang, Peibo Duan, Zongyuan Ge",
                "citations": 7
            },
            {
                "title": "Taming Self-Training for Open-Vocabulary Object Detection",
                "abstract": "Recent studies have shown promising performance in open-vocabulary object detection (OVD) by utilizing pseudo labels (PLs) from pre-trained vision and language models (VLMs). However, teacher-student self-training, a powerful and widely used paradigm to leverage PLs, is rarely explored for OVD. This work identifies two challenges of using self-training in OVD: noisy PLs from VLMs and frequent distribution changes of PLs. To address these challenges, we propose SAS-Det that tames self-training for OVD from two key perspectives. First, we present a split-and-fusion (SAF) head that splits a standard detection into an open-branch and a closed-branch. This design can reduce noisy supervision from pseudo boxes. More-over, the two branches learn complementary knowledge from different training data, significantly enhancing performance when fused together. Second, in our view, un-like in closed-set tasks, the PL distributions in OVD are solely determined by the teacher model. We introduce a periodic update strategy to decrease the number of up-dates to the teacher, thereby decreasing the frequency of changes in PL distributions, which stabilizes the training process. Extensive experiments demonstrate SAS-Det is both efficient and effective. SAS-Det outperforms recent models of the same scale by a clear margin and achieves 37.4 AP50 and 29.1 APr on novel categories of the COCO and LVIS benchmarks, respectively. Code is available at https://github.com/xiaofeng94/SAS-Det.",
                "authors": "Shiyu Zhao, S. Schulter, Long Zhao, Zhixing Zhang, Vijay Kumar B.G, Yumin Suh, Manmohan Chandraker, Dimitris N. Metaxas",
                "citations": 6
            },
            {
                "title": "The autonomic nervous system: A potential link to the efficacy of acupuncture",
                "abstract": "The autonomic nervous system (ANS) is a diffuse network that regulates physiological systems to maintain body homeostasis by integrating inputs from the internal and external environment, including the sympathetic, parasympathetic, and enteric nervous systems (ENS). Recent evidence suggests that ANS is one of the key neural pathways for acupuncture signal transduction, which has attracted worldwide attention in the acupuncture field. Here, we reviewed the basic and clinical research published in PubMed over the past 20 years on the effects of acupuncture on ANS regulation and homeostasis maintenance. It was found that acupuncture effectively alleviates ANS dysfunction-associated symptoms in its indications, such as migraine, depression, insomnia, functional dyspepsia, functional constipation. Acupuncture stimulation on some specific acupoints activates sensory nerve fibers, the spinal cord, and the brain. Using information integration and efferents from a complex network of autonomic nuclei of the brain, such as the insular cortex (IC), prefrontal cortex, anterior cingulate cortex (ACC), amygdala (AMG), hypothalamus, periaqueductal gray (PAG), nucleus tractus solitarius (NTS), ventrolateral medulla (VLM), nucleus ambiguus (AMB), acupuncture alleviates visceral dysfunction, inflammation via efferent autonomic nerves, and relieves pain and pain affect. The modulating pattern of sympathetic and parasympathetic nerves is associated with acupuncture stimulation on specific acupoints, intervention parameters, and disease models, and the relationships among them require further exploration. In conclusion, ANS is one of the therapeutic targets for acupuncture and mediates acupuncture’s actions, which restores homeostasis. A systemic study is needed to determine the rules and mechanisms underlying the effects of acupoint stimulation on corresponding organs mediated by specific central nervous networks and the efferent ANS.",
                "authors": "Yan-wei Li, Wei Li, Songtao Wang, Yinan Gong, Baomin Dou, Zhongxi Lyu, Luis Ulloa, Shenjun Wang, Zhifang Xu, Yi-song Guo",
                "citations": 29
            },
            {
                "title": "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text",
                "abstract": "Pretrained Vision-Language Models (VLMs) have achieved remarkable performance in image retrieval from text. However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend. Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this paper, we regard linguistically complex texts as compound proposition texts composed of multiple simple proposition sentences and propose an end-to-end Neural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three main components: 1) Divide: a proposition generator divides the compound proposition text into simple proposition sentences and produces their corresponding representations, 2) Conquer: a pretrained VLMs-based visual-linguistic interactor achieves the interaction between decomposed proposition sentences and images, 3) Combine: a neural-symbolic reasoner combines the above reasoning states to obtain the final solution via a neural logic reasoning approach. According to the dual-process theory, the visual-linguistic interactor and neural-symbolic reasoner could be regarded as analogical reasoning System 1 and logical reasoning System 2. We conduct extensive experiments on a challenging image retrieval from contextual descriptions data set. Experimental results and analyses indicate NDCR significantly improves performance in the complex image-text reasoning problem.",
                "authors": "Yunxin Li, Baotian Hu, Yunxin Ding, Lin Ma, M. Zhang",
                "citations": 5
            },
            {
                "title": "KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models",
                "abstract": "Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLMs. We benchmark and reveal practical challenges in adapting these VLMs to image ad understanding. We propose a simple feature adaptation strategy to effectively fuse multimodal information for image ads and further empower it with knowledge of real-world entities. We hope our study draws more attention to image ad understanding which is broadly relevant to the advertising industry.",
                "authors": "Zhiwei Jia, P. Narayana, Arjun Reddy Akula, G. Pruthi, Haoran Su, Sugato Basu, Varun Jampani",
                "citations": 4
            },
            {
                "title": "Strong but Simple: A Baseline for Domain Generalized Dense Perception by CLIP-Based Transfer Learning",
                "abstract": null,
                "authors": "Christoph Hümmer, Manuel Schwonberg, Liangwei Zhou, Hu Cao, Alois Knoll, Hanno Gottschalk",
                "citations": 4
            },
            {
                "title": "Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment",
                "abstract": "Learning to ground natural language queries to target objects or regions in 3D point clouds is quite essential for 3D scene understanding. Nevertheless, existing 3D visual grounding approaches require a substantial number of bounding box annotations for text queries, which is time-consuming and labor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly supervised approach for 3D visual grounding based on Visual Linguistic Alignment. Our 3D-VLA exploits the superior ability of current large-scale vision-language models (VLMs) on aligning the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds with no need for fine-grained box annotations in the training procedure. During the inference stage, the learned text-3D correspondence will help us ground the text queries to the 3D target objects even without 2D images. To the best of our knowledge, this is the first work to investigate 3D visual grounding in a weakly supervised manner by involving large scale vision-language models, and extensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even superior results over the fully supervised methods.",
                "authors": "Xiaoxu Xu, Yitian Yuan, Qiudan Zhang, Wen-Bin Wu, Zequn Jie, Lin Ma, Xu Wang",
                "citations": 4
            },
            {
                "title": "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties",
                "abstract": "A major reason behind the recent success of large language models (LLMs) is their \\textit{in-context learning} capability, which makes it possible to rapidly adapt them to downstream text-based tasks by prompting them with a small number of relevant demonstrations. While large vision-language models (VLMs) have recently been developed for tasks requiring both text and images, they largely lack in-context learning over visual information, especially in understanding and generating text about videos. In this work, we implement \\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos (\\eilev{}), a novel training paradigm that induces in-context learning over video and text by capturing key properties of pre-training data found by prior work to be essential for in-context learning in transformers. In our experiments, we show that \\eilev-trained models outperform other off-the-shelf VLMs in few-shot video narration for novel, rare actions. Furthermore, we demonstrate that these key properties of bursty distributions, skewed marginal distributions, and dynamic meaning each contribute to varying degrees to VLMs' in-context learning capability in narrating procedural videos. Our results, analysis, and \\eilev{}-trained models yield numerous insights about the emergence of in-context learning over video and text, creating a foundation for future work to optimize and scale VLMs for open-domain video understanding and reasoning. Our code and demo are available at \\url{https://github.com/yukw777/EILEV}.",
                "authors": "Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Joyce Chai",
                "citations": 5
            },
            {
                "title": "Zelda: Video Analytics using Vision-Language Models",
                "abstract": "Advances in ML have motivated the design of video analytics systems that allow for structured queries over video datasets. However, existing systems limit query expressivity, require users to specify an ML model per predicate, rely on complex optimizations that trade off accuracy for performance, and return large amounts of redundant and low-quality results. This paper focuses on the recently developed Vision-Language Models (VLMs) that allow users to query images using natural language like\"cars during daytime at traffic intersections.\"Through an in-depth analysis, we show VLMs address three limitations of current video analytics systems: general expressivity, a single general purpose model to query many predicates, and are both simple and fast. However, VLMs still return large numbers of redundant and low-quality results, which can overwhelm and burden users. We present Zelda: a video analytics system that uses VLMs to return both relevant and semantically diverse results for top-K queries on large video datasets. Zelda prompts the VLM with the user's query in natural language and additional terms to improve accuracy and identify low-quality frames. Zelda improves result diversity by leveraging the rich semantic information encoded in VLM embeddings. We evaluate Zelda across five datasets and 19 queries and quantitatively show it achieves higher mean average precision (up to 1.15$\\times$) and improves average pairwise similarity (up to 1.16$\\times$) compared to using VLMs out-of-the-box. We also compare Zelda to a state-of-the-art video analytics engine and show that Zelda retrieves results 7.5$\\times$ (up to 10.4$\\times$) faster for the same accuracy and frame diversity.",
                "authors": "Francisco Romero, Caleb Winston, Johann Hauswald, M. Zaharia, Christos Kozyrakis",
                "citations": 5
            },
            {
                "title": "EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models",
                "abstract": "Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person per-spective. However, the capability of VLMs to “think” from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from ego-centric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate twenty-one popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics.",
                "authors": "Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, Yang Liu",
                "citations": 5
            },
            {
                "title": "Domain Prompt Learning with Quaternion Networks",
                "abstract": "Prompt learning has emerged as a potent and resource-efficient technique in large Vision-Language Models (VLMs). However, its application in adapting VLMs to specialized domains like remote sensing and medical imaging, termed domain prompt learning, remains relatively unexplored. Although large-scale domain-specific foundation models offer a potential solution, their focus on a singular vision level presents challenges in prompting both vision and language modalities. To address this limitation, we propose leveraging domain-specific knowledge from these foundation models to transfer the robust recognition abilities of VLMs from generalized to specialized domains, employing quaternion networks. Our method entails utilizing domain-specific vision features from domain-specific foundation models to guide the transformation of generalized contextual embeddings from the language branch into a specialized space within quaternion networks. Furthermore, we introduce a hierarchical approach that derives vision prompt features by analyzing intermodal relationships between hierarchical language prompt features and domain-specific vision features. Through this mechanism, quaternion networks can effectively explore intermodal relationships in specific domains, facilitating domain-specific vision-language contrastive learning. Extensive experiments conducted on domain-specific datasets demonstrate that our proposed method achieves new state-of-the-art results in prompt learning. Codes are available at https://github.com/caoq198/DPLQ.",
                "authors": "Qinglong Cao, Zhengqin Xu, Yuntian Chen, Chao Ma, Xiaokang Yang",
                "citations": 5
            },
            {
                "title": "Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Models",
                "abstract": "Vision-Language Large Models (VLMs) have become primary backbone of AI, due to the impressive performance. However, their expensive computation costs, i.e., throughput and delay, impede potentials in real-world scenarios. To achieve acceleration for VLMs, most existing methods focus on the model perspective: pruning, distillation, quantification, but completely overlook the data-perspective redundancy. To fill the overlook, this paper pioneers the severity of data redundancy, and designs one plug-and-play Turbo module guided by information degree to prune inefficient tokens from visual or textual data. In pursuit of efficiency-performance trade-offs, information degree takes two key factors into consideration: mutual redundancy and semantic value. Concretely, the former evaluates the data duplication between sequential tokens; while the latter evaluates each token by its contribution to the overall semantics. As a result, tokens with high information degree carry less redundancy and stronger semantics. For VLMs' calculation, Turbo works as a user-friendly plug-in that sorts data referring to information degree, utilizing only top-level ones to save costs. Its advantages are multifaceted, e.g., being generally compatible to various VLMs across understanding and generation, simple use without retraining and trivial engineering efforts. On multiple public VLMs benchmarks, we conduct extensive experiments to reveal the gratifying acceleration of Turbo, under negligible performance drop.",
                "authors": "Chen Ju, Haicheng Wang, Zeqian Li, Xu Chen, Zhonghua Zhai, Weilin Huang, Shuai Xiao",
                "citations": 5
            },
            {
                "title": "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering",
                "abstract": "Recently, finetuning pretrained vision-language models (VLMs) has been a prevailing paradigm for achieving state-of-the-art performance in VQA. However, as VLMs scale, it becomes computationally expensive, storage inefficient, and prone to overfitting when tuning full model parameters for a specific task in low-resource settings. Although current parameter-efficient tuning methods dramatically reduce the number of tunable parameters, there still exists a significant performance gap with full finetuning. In this paper, we propose MixPHM, a redundancy-aware parameter-efficient tuning method that outperforms full finetuning in low-resource VQA. Specifically, MixPHM is a lightweight module implemented by multiple PHM-experts in a mixture-of-experts manner. To reduce parameter redundancy, we reparameterize expert weights in a low-rank subspace and share part of the weights inside and across MixPHM. Moreover, based on our quantitative analysis of representation redundancy, we propose Redundancy Regularization, which facilitates MixPHM to reduce task-irrelevant redundancy while promoting task-relevant correlation. Experiments conducted on VQA v2, GQA, and OK-VQA with different low-resource settings show that our MixPHM outperforms state-of-the-art parameter-efficient methods and is the only one consistently surpassing full finetuning.",
                "authors": "Jingjing Jiang, Nanning Zheng",
                "citations": 5
            },
            {
                "title": "Improving Pseudo Labels for Open-Vocabulary Object Detection",
                "abstract": "Recent studies show promising performance in open-vocabulary object detection (OVD) using pseudo labels (PLs) from pretrained vision and language models (VLMs). However, PLs generated by VLMs are extremely noisy due to the gap between the pretraining objective of VLMs and OVD, which blocks further advances on PLs. In this paper, we aim to reduce the noise in PLs and propose a method called online Self-training And a Split-and-fusion head for OVD (SAS-Det). First, the self-training finetunes VLMs to generate high quality PLs while prevents forgetting the knowledge learned in the pretraining. Second, a split-and-fusion (SAF) head is designed to remove the noise in localization of PLs, which is usually ignored in existing methods. It also fuses complementary knowledge learned from both precise ground truth and noisy pseudo labels to boost the performance. Extensive experiments demonstrate SAS-Det is both efficient and effective. Our pseudo labeling is 3 times faster than prior methods. SAS-Det outperforms prior state-of-the-art models of the same scale by a clear margin and achieves 37.4 AP 50 and 27.3 AP r on novel categories of the COCO and LVIS benchmarks, respectively.",
                "authors": "Shiyu Zhao, S. Schulter, Long Zhao, Zhixing Zhang, Vijay Kumar, Yumin Suh, M. Chandraker, Dimitris N. Metaxas",
                "citations": 4
            },
            {
                "title": "Few-shot medical image classification with simple shape and texture text descriptors using vision-language models",
                "abstract": "In this work, we investigate the usefulness of vision-language models (VLMs) and large language models for binary few-shot classification of medical images. We utilize the GPT-4 model to generate text descriptors that encapsulate the shape and texture characteristics of objects in medical images. Subsequently, these GPT-4 generated descriptors, alongside VLMs pre-trained on natural images, are employed to classify chest X-rays and breast ultrasound images. Our results indicate that few-shot classification of medical images using VLMs and GPT-4 generated descriptors is a viable approach. However, accurate classification requires to exclude certain descriptors from the calculations of the classification scores. Moreover, we assess the ability of VLMs to evaluate shape features in breast mass ultrasound images. We further investigate the degree of variability among the sets of text descriptors produced by GPT-4. Our work provides several important insights about the application of VLMs for medical image analysis.",
                "authors": "Michal Byra, M. F. Rachmadi, Henrik Skibbe",
                "citations": 4
            },
            {
                "title": "Distilling Functional Rearrangement Priors from Large Models",
                "abstract": "—Object rearrangement, a fundamental challenge in robotics, demands versatile strategies to handle diverse objects, configurations, and functional needs. To achieve this, the AI robot needs to learn functional rearrangement priors in order to specify precise goals that meet the functional requirements. Previous methods typically learn such priors from either laborious human annotations or manually designed heuristics, which limits scalability and generalization. In this work, we propose a novel approach that leverages large models to distill functional rearrangement priors. Specifically, our approach collects diverse arrangement examples using both LLMs and VLMs and then distills the examples into a diffusion model. During test time, the learned diffusion model is conditioned on the initial configuration and guides the positioning of objects to meet functional requirements. In this manner, we create a handshaking point that combines the strengths of conditional generative models and large models. Extensive experiments on multiple domains, including real-world scenarios, demonstrate the effectiveness of our approach in generating compatible goals for object rearrangement tasks, significantly outperforming baseline methods. Our real-world results can be seen on https://sites.google.com/view/lvdiffusion .",
                "authors": "Yiming Zeng, Mingdong Wu, Long Yang, Jiyao Zhang, Hao Ding, Hui Cheng, Hao Dong",
                "citations": 4
            },
            {
                "title": "SAGE: Bridging Semantic and Actionable Parts for GEneralizable Manipulation of Articulated Objects",
                "abstract": "To interact with daily-life articulated objects of diverse structures and functionalities, understanding the object parts plays a central role in both user instruction comprehension and task execution. However, the possible discordance between the semantic meaning and physics functionalities of the parts poses a challenge for designing a general system. To address this problem, we propose SAGE, a novel framework that bridges semantic and actionable parts of articulated objects to achieve generalizable manipulation under natural language instructions. More concretely, given an articulated object, we first observe all the semantic parts on it, conditioned on which an instruction interpreter proposes possible action programs that concretize the natural language instruction. Then, a part-grounding module maps the semantic parts into so-called Generalizable Actionable Parts (GAParts), which inherently carry information about part motion. End-effector trajectories are predicted on the GAParts, which, together with the action program, form an executable policy. Additionally, an interactive feedback module is incorporated to respond to failures, which closes the loop and increases the robustness of the overall framework. Key to the success of our framework is the joint proposal and knowledge fusion between a large vision-language model (VLM) and a small domain-specific model for both context comprehension and part perception, with the former providing general intuitions and the latter serving as expert facts. Both simulation and real-robot experiments show our effectiveness in handling a large variety of articulated objects with diverse language-instructed goals.",
                "authors": "Haoran Geng, Songlin Wei, Congyue Deng, Bokui Shen, He Wang, Leonidas Guibas",
                "citations": 3
            },
            {
                "title": "Revisiting Few-Shot Object Detection with Vision-Language Models",
                "abstract": "The era of vision-language models (VLMs) trained on web-scale datasets challenges conventional formulations of\"open-world\"perception. In this work, we revisit the task of few-shot object detection (FSOD) in the context of recent foundational VLMs. First, we point out that zero-shot predictions from VLMs such as GroundingDINO significantly outperform state-of-the-art few-shot detectors (48 vs. 33 AP) on COCO. Despite their strong zero-shot performance, such foundation models may still be sub-optimal. For example, trucks on the web may be defined differently from trucks for a target application such as autonomous vehicle perception. We argue that the task of few-shot recognition can be reformulated as aligning foundation models to target concepts using a few examples. Interestingly, such examples can be multi-modal, using both text and visual cues, mimicking instructions that are often given to human annotators when defining a target concept of interest. Concretely, we propose Foundational FSOD, a new benchmark protocol that evaluates detectors pre-trained on any external data and fine-tuned on multi-modal (text and visual) K-shot examples per target class. We repurpose nuImages for Foundational FSOD, benchmark several popular open-source VLMs, and provide an empirical analysis of state-of-the-art methods. Lastly, we discuss our recent CVPR 2024 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 23.3 mAP! Our code and dataset splits are available at https://github.com/anishmadan23/foundational_fsod",
                "authors": "Anish Madan, Neehar Peri, Shu Kong, Deva Ramanan",
                "citations": 3
            },
            {
                "title": "Compositional Zero-shot Learning via Progressive Language-based Observations",
                "abstract": "Compositional zero-shot learning aims to recognize unseen state-object compositions by leveraging known primitives (state and object) during training. However, effectively modeling interactions between primitives and generalizing knowledge to novel compositions remains a perennial challenge. There are two key factors: object-conditioned and state-conditioned variance, i.e., the appearance of states (or objects) can vary significantly when combined with different objects (or states). For instance, the state\"old\"can signify a vintage design for a\"car\"or an advanced age for a\"cat\". In this paper, we argue that these variances can be mitigated by predicting composition categories based on pre-observed primitive. To this end, we propose Progressive Language-based Observations (PLO), which can dynamically determine a better observation order of primitives. These observations comprise a series of concepts or languages that allow the model to understand image content in a step-by-step manner. Specifically, PLO adopts pre-trained vision-language models (VLMs) to empower the model with observation capabilities. We further devise two variants: 1) PLO-VLM: a two-step method, where a pre-observing classifier dynamically determines the observation order of two primitives. 2) PLO-LLM: a multi-step scheme, which utilizes large language models (LLMs) to craft composition-specific prompts for step-by-step observing. Extensive ablations on three challenging datasets demonstrate the superiority of PLO compared with state-of-the-art methods, affirming its abilities in compositional recognition.",
                "authors": "Lin Li, Guikun Chen, Jun Xiao, Long Chen",
                "citations": 3
            },
            {
                "title": "CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction",
                "abstract": "In this paper, we explore the potential of Vision-Language Models (VLMs), specifically CLIP, in predicting visual object relationships, which involves interpreting visual features from images into language-based relations. Current state-of-the-art methods use complex graphical models that utilize language cues and visual features to address this challenge. We hypothesize that the strong language priors in CLIP embeddings can simplify these graphical models paving for a simpler approach. We adopt the UVTransE relation prediction framework, which learns the relation as a translational embedding with subject, object, and union box embeddings from a scene. We systematically explore the design of CLIP-based subject, object, and union-box representations within the UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate Estimation). CREPE utilizes text-based representations for all three bounding boxes and introduces a novel contrastive training strategy to automatically infer the text prompt for union-box. Our approach achieves state-of-the-art performance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the Visual Genome benchmark, achieving a 15.3\\% gain in performance over recent state-of-the-art at mR@20. This work demonstrates CLIP's effectiveness in object relation prediction and encourages further research on VLMs in this challenging domain.",
                "authors": "Rakshith Subramanyam, T. S. Jayram, Rushil Anirudh, J. Thiagarajan",
                "citations": 3
            },
            {
                "title": "Voila-A: Aligning Vision-Language Models with User's Gaze Attention",
                "abstract": "In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VLMs while preserving their pretrained knowledge. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE Testset, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications.",
                "authors": "Kun Yan, Lei Ji, Zeyu Wang, Yuntao Wang, Nan Duan, Shuai Ma",
                "citations": 3
            },
            {
                "title": "Linear Spaces of Meanings: the Compositional Language of VLMs",
                "abstract": ",",
                "authors": "Matthew Trager, Pramuditha Perera, L. Zancato, A. Achille, Parminder Bhatia, Bing Xiang, S. Soatto",
                "citations": 3
            },
            {
                "title": "3VL: Using Trees to Improve Vision-Language Models’ Interpretability",
                "abstract": "Vision-Language models (VLMs) have proven to be effective at aligning image and text representations, producing superior zero-shot results when transferred to many downstream tasks. However, these representations suffer from some key shortcomings in understanding Compositional Language Concepts (CLC), such as recognizing objects’ attributes, states, and relations between different objects. Moreover, VLMs typically have poor interpretability, making it challenging to debug and mitigate compositional-understanding failures. In this work, we introduce the architecture and training technique of Tree-augmented Vision-Language (3VL) model accompanied by our proposed Anchor inference method and Differential Relevance (DiRe) interpretability tool. By expanding the text of an arbitrary image-text pair into a hierarchical tree structure using language analysis tools, 3VL allows the induction of this structure into the visual representation learned by the model, enhancing its interpretability and compositional reasoning. Additionally, we show how Anchor, a simple technique for text unification, can be used to filter nuisance factors while increasing CLC understanding performance, e.g., on the fundamental VL-Checklist benchmark. We also show how DiRe, which performs a differential comparison between VLM relevancy maps, enables us to generate compelling visualizations of the reasons for a model’s success or failure.",
                "authors": "Nir Yellinek, Leonid Karlinsky, Raja Giryes",
                "citations": 3
            },
            {
                "title": "Semantic Mechanical Search with Large Vision and Language Models",
                "abstract": "Moving objects to find a fully-occluded target object, known as mechanical search, is a challenging problem in robotics. As objects are often organized semantically, we conjecture that semantic information about object relationships can facilitate mechanical search and reduce search time. Large pretrained vision and language models (VLMs and LLMs) have shown promise in generalizing to uncommon objects and previously unseen real-world environments. In this work, we propose a novel framework called Semantic Mechanical Search (SMS). SMS conducts scene understanding and generates a semantic occupancy distribution explicitly using LLMs. Compared to methods that rely on visual similarities offered by CLIP embeddings, SMS leverages the deep reasoning capabilities of LLMs. Unlike prior work that uses VLMs and LLMs as end-to-end planners, which may not integrate well with specialized geometric planners, SMS can serve as a plug-in semantic module for downstream manipulation or navigation policies. For mechanical search in closed-world settings such as shelves, we compare with a geometric-based planner and show that SMS improves mechanical search performance by 24% across the pharmacy, kitchen, and office domains in simulation and 47.1% in physical experiments. For open-world real environments, SMS can produce better semantic distributions compared to CLIP-based methods, with the potential to be integrated with downstream navigation policies to improve object navigation tasks. Code, data, videos, and the appendix are available: https://sites.google.com/view/semantic-mechanical-search",
                "authors": "Satvik Sharma, K. Shivakumar, Huang Huang, Ryan Hoque, A. Imran, Brian Ichter, Ken Goldberg",
                "citations": 3
            },
            {
                "title": "ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data",
                "abstract": "Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that describes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as exhibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and textual attributes when trained on such data has not been previously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relationships, exhibiting performance degradations of up to 37% on retrieval tasks. In order to address this issue, we introduce ViLLA as our second key contribution. ViLLA, which is trained to capture fine-grained region-attribute relationships from complex datasets, involves two components: (a) a lightweight, self-supervised mapping model to decompose image-text samples into region-attribute pairs, and (b) a contrastive VLM to learn representations from generated region-attribute pairs. We demonstrate with experiments across four domains (synthetic, product, medical, and natural images) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks, such as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP points on LVIS) and retrieval (up to 14.2 R-Precision points1",
                "authors": "M. Varma, Jean-Benoit Delbrouck, Sarah Hooper, Akshay Chaudhari, C. Langlotz",
                "citations": 3
            },
            {
                "title": "Self-PT: Adaptive Self-Prompt Tuning for Low-Resource Visual Question Answering",
                "abstract": "Pretraining and finetuning large vision-language models (VLMs) have achieved remarkable success in visual question answering (VQA). However, finetuning VLMs requires heavy computation, expensive storage costs, and is prone to overfitting for VQA in low-resource settings. Existing prompt tuning methods have reduced the number of tunable parameters, but they cannot capture valid context-aware information during prompt encoding, resulting in 1) poor generalization of unseen answers and 2) lower improvements with more parameters. To address these issues, we propose a prompt tuning method for low-resource VQA named Adaptive Self-Prompt Tuning (Self-PT), which utilizes representations of question-image pairs as conditions to obtain context-aware prompts. To enhance the generalization of unseen answers, Self-PT uses dynamic instance-level prompts to avoid overfitting the correlations between static prompts and seen answers observed during training. To reduce parameters, we utilize hyper-networks and low-rank parameter factorization to make Self-PT more flexible and efficient. The hyper-network decouples the number of parameters and prompt length to generate flexible-length prompts by the fixed number of parameters. While the low-rank parameter factorization decomposes and reparameterizes the weights of the prompt encoder into a low-rank subspace for better parameter efficiency. Experiments conducted on VQA v2, GQA, and OK-VQA with different low-resource settings show that our Self-PT outperforms the state-of-the-art parameter-efficient methods, especially in lower-shot settings, e.g., 6% average improvements cross three datasets in 16-shot. Code is available at https://github.com/NJUPT-MCC/Self-PT.",
                "authors": "Bowen Yuan, Sisi You, Bing-Kun Bao",
                "citations": 3
            },
            {
                "title": "Orthogonal Temporal Interpolation for Zero-Shot Video Recognition",
                "abstract": "Zero-shot video recognition (ZSVR) is a task that aims to recognize video categories that have not been seen during the model training process. Recently, vision-language models (VLMs) pre-trained on large-scale image-text pairs have demonstrated impressive transferability for ZSVR. To make VLMs applicable to the video domain, existing methods often use an additional temporal learning module after the image-level encoder to learn the temporal relationships among video frames. Unfortunately, for video from unseen categories, we observe an abnormal phenomenon where the model that uses spatial-temporal feature performs much worse than the model that removes temporal learning module and uses only spatial feature. We conjecture that improper temporal modeling on video disrupts the spatial feature of the video. To verify our hypothesis, we propose Feature Factorization to retain the orthogonal temporal feature of the video and use interpolation to construct refined spatial-temporal feature. The model using appropriately refined spatial-temporal feature performs better than the one using only spatial feature, which verifies the effectiveness of the orthogonal temporal feature for the ZSVR task. Therefore, an Orthogonal Temporal Interpolation module is designed to learn a better refined spatial-temporal video feature during training. Additionally, a Matching Loss is introduced to improve the quality of the orthogonal temporal feature. We propose a model called OTI for ZSVR by employing orthogonal temporal interpolation and the matching loss based on VLMs. The ZSVR accuracies on popular video datasets (i.e., Kinetics-600, UCF101 and HMDB51) show that OTI outperforms the previous state-of-the-art method by a clear margin.Our codes are publicly available at https://github.com/yanzhu/mm2023_oti.",
                "authors": "Yan Zhu, Junbao Zhuo, B. Ma, Jiajia Geng, Xiaoming Wei, Xiaolin Wei, Shuhui Wang",
                "citations": 3
            },
            {
                "title": "Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning",
                "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel concepts formed by known states and objects during training. Existing methods either learn the combined state-object representation, challenging the generalization of unseen compositions, or design two classifiers to identify state and object separately from image features, ignoring the intrinsic relationship between them. To jointly eliminate the above issues and construct a more robust CZSL system, we propose a novel framework termed Decomposed Fusion with Soft Prompt (DFSP)11Code is available at: https://github.corn/Forest-art/DFSP.git, by involving vision-language models (VLMs)for unseen composition recognition. Specifically, DFSP constructs a vector combination of learnable soft prompts with state and object to establish the joint representation of them. In addition, a cross-modal decomposed fusion module is designed between the language and image branches, which decomposes state and object among language features instead of image features. Notably, being fused with the decomposed features, the image features can be more expressive for learning the relationship with states and objects, respectively, to improve the response of unseen compositions in the pair space, hence narrowing the domain gap between seen and unseen sets. Experimental results on three challenging benchmarks demonstrate that our approach significantly outperforms other state-of-the-art methods by large margins.",
                "authors": "Xiaocheng Lu, Ziming Liu, Song Guo, Jingcai Guo",
                "citations": 24
            },
            {
                "title": "The Impact of Future Sea-Level Rise on Low-Lying Subsiding Coasts: A Case Study of Tavoliere Delle Puglie (Southern Italy)",
                "abstract": "Low-lying coastal zones are highly subject to coastal hazards as a result of sea-level rise enhanced by natural or anthropogenic land subsidence. A combined analysis using sea-level data and remote sensing techniques allows the estimation of the current rates of land subsidence and shoreline retreat, supporting the development of quantified relative sea-level projections and flood maps, which are appropriate for specific areas. This study focuses on the coastal plain of Tavoliere delle Puglie (Apulia, Southern Italy), facing the Adriatic Sea. In this area, land subsidence is mainly caused by long-term tectonic movements and sediment compaction driven by high anthropogenic pressure, such as groundwater exploitation and constructions of buildings. To assess the expected effects of relative sea-level rise for the next decades, we considered the following multidisciplinary source data: (i) sea-level-rise projections for different climatic scenarios, as reported in the Sixth Assessment Report of the Intergovernmental Panel on Climate Change, (ii) coastal topography from airborne and terrestrial LiDAR data, (iii) Vertical Land Movement (VLM) from the analysis of InSAR and GNSS data, and (iv) shoreline changes obtained from the analysis of orthophotos, historic maps, and satellite images. To assess the expected evolution of the coastal belt, the topographic data were corrected for VLM values, assuming that the rates of land subsidence will remain constant up to 2150. The sea-level-rise projections and expected flooded areas were estimated for the Shared Socioeconomic Pathways SSP1-2.6 and SSP5-8.5, corresponding to low and high greenhouse-gas concentrations, respectively. From our analysis, we estimate that in 2050, 2100, and 2150, up to 50.5 km2, 118.7 km2 and 147.7 km2 of the coast could be submerged, respectively, while beaches could retreat at rates of up to 5.8 m/yr. In this area, sea-level rise will be accelerated by natural and anthropogenic land subsidence at rates of up to −7.5 ± 1.7 mm/yr. Local infrastructure and residential areas are thus highly exposed to an increasing risk of severe inundation by storm surges and sea-level rise in the next decades.",
                "authors": "G. Scardino, M. Anzidei, Paolo Petio, E. Serpelloni, V. Santis, A. Rizzo, Serena Isabella Liso, Marina Zingaro, D. Capolongo, A. Vecchio, A. Refice, G. Scicchitano",
                "citations": 23
            },
            {
                "title": "Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction",
                "abstract": "The Graphical User Interface (GUI) is pivotal for human interaction with the digital world, enabling efficient device control and the completion of complex tasks. Recent progress in Large Language Models (LLMs) and Vision Language Models (VLMs) offers the chance to create advanced GUI agents. To ensure their effectiveness, there's a pressing need for qualified benchmarks that provide trustworthy and reproducible evaluations -- a challenge current benchmarks often fail to address. To tackle this issue, we introduce Mobile-Env, a comprehensive toolkit tailored for creating GUI benchmarks in the Android mobile environment. Mobile-Env offers an isolated and controllable setting for reliable evaluations, and accommodates intermediate instructions and rewards to reflect real-world usage more naturally. Utilizing Mobile-Env, we collect an open-world task set across various real-world apps and a fixed world set, WikiHow, which captures a significant amount of dynamic online contents for fully controllable and reproducible evaluation. We conduct comprehensive evaluations of LLM agents using these benchmarks. Our findings reveal that even advanced models (e.g., GPT-4V and LLaMA-3) struggle with tasks that are relatively simple for humans. This highlights a crucial gap in current models and underscores the importance of developing more capable foundation models and more effective GUI agent frameworks.",
                "authors": "Danyang Zhang, Hongshen Xu, Zihan Zhao, Lu Chen, Ruisheng Cao, Kai Yu",
                "citations": 2
            },
            {
                "title": "A Retrospect to Multi-prompt Learning across Vision and Language",
                "abstract": "The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.",
                "authors": "Ziliang Chen, Xin Huang, Quanlong Guan, Liang Lin, Weiqi Luo",
                "citations": 2
            },
            {
                "title": "Holocene Sea Level Recorded by Beach Rocks at Ionian Coasts of Apulia (Italy)",
                "abstract": "Beach rocks are located along many coasts of the Mediterranean basin. The early diagenesis environment and the mean sea level along the shoreline make these landforms useful in the reconstruction of relative sea-level changes and, in particular, as SLIPs (sea-level index points). The beach rocks surveyed along the Ionian coast of Apulia were found to be well preserved at three specific depth ranges: 6–9 m, 3–4 m, and from the foreshore to about 1.20 m. Morpho-bathymetric and dive surveys were performed to assess both the geometries and the extension of the submerged beach rocks. Samples were collected at these different depths in the localities of Lido Torretta, Campomarino di Maruggio, San Pietro in Bevagna, and Porto Cesareo. Bivalve shells were identified and isolated from the beach rock samples collected at a depth of 7 m; AMS dating provided a calibrated age of about 7.8 ka BP. Their morphology and petrological features, along with the time constraints, enabled us to (i) reconstruct the local sea-level curve during the Holocene, (ii) corroborate acquired knowledge of the relative sea-level history, and (iii) identify possible local vertical land movement (VLM).",
                "authors": "G. Mastronuzzi, F. De Giosa, G. Quarta, M. Pallara, G. Scardino, G. Scicchitano, Cosmo Peluso, C. Antropoli, C. Caporale, M. Demarte",
                "citations": 2
            },
            {
                "title": "SmartTrim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models",
                "abstract": "Despite achieving remarkable performance on various vision-language tasks, Transformer-based Vision-Language Models (VLMs) suffer from redundancy in inputs and parameters, significantly hampering their efficiency in real-world applications. Moreover, the degree of redundancy in token representations and model parameters, such as attention heads, varies significantly for different inputs. In light of the challenges, we propose SmartTrim, an adaptive acceleration framework for VLMs, which adjusts the computational overhead per instance. Specifically, we integrate lightweight modules into the original backbone to identify and prune redundant token representations and attention heads within each layer. Furthermore, we devise a self-distillation strategy to enhance the consistency between the predictions of the pruned model and its fully-capacity counterpart. Experimental results across various vision-language tasks consistently demonstrate that SmartTrim accelerates the original model by 2-3 times with minimal performance degradation, highlighting the effectiveness and efficiency compared to previous approaches. Code will be available at https://github.com/kugwzk/SmartTrim.",
                "authors": "Zekun Wang, Jingchang Chen, Wangchunshu Zhou, Haichao Zhu, Jiafeng Liang, Liping Shan, Ming Liu, Dongliang Xu, Qing Yang, Bing Qin",
                "citations": 2
            },
            {
                "title": "Compound Text-Guided Prompt Tuning via Image-Adaptive Cues",
                "abstract": "Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T.",
                "authors": "Hao Tan, Jun Li, Yi Zhou, Jun Wan, Zhen Lei, Xiangyu Zhang",
                "citations": 2
            },
            {
                "title": "Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples",
                "abstract": "While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race&gender). We conduct extensive experiments using our generated dataset which reveal the intersectional social biases present in state-of-the-art VLMs.",
                "authors": "Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Vasudev Lal",
                "citations": 2
            },
            {
                "title": "GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction",
                "abstract": "Predicting pedestrian behavior is the key to ensure safety and reliability of autonomous vehicles. While deep learning methods have been promising by learning from annotated video frame sequences, they often fail to fully grasp the dynamic interactions between pedestrians and traffic, crucial for accurate predictions. These models also lack nuanced common sense reasoning. Moreover, the manual annotation of datasets for these models is expensive and challenging to adapt to new situations. The advent of Vision Language Models (VLMs) introduces promising alternatives to these issues, thanks to their advanced visual and causal reasoning skills. To our knowledge, this research is the first to conduct both quantitative and qualitative evaluations of VLMs in the context of pedestrian behavior prediction for autonomous driving. We evaluate GPT-4V(ision) on publicly available pedestrian datasets: JAAD and WiDEVIEW. Our quantitative analysis focuses on GPT-4V's ability to predict pedestrian behavior in current and future frames. The model achieves a 57% accuracy in a zero-shot manner, which, while impressive, is still behind the state-of-the-art domain-specific models (70%) in predicting pedestrian crossing actions. Qualitatively, GPT-4V shows an impressive ability to process and interpret complex traffic scenarios, differentiate between various pedestrian behaviors, and detect and analyze groups. However, it faces challenges, such as difficulty in detecting smaller pedestrians and assessing the relative motion between pedestrians and the ego vehicle.",
                "authors": "Jia Huang, Peng Jiang, Alvika Gautam, S. Saripalli",
                "citations": 2
            },
            {
                "title": "LT at SemEval-2023 Task 1: Effective Zero-Shot Visual Word Sense Disambiguation Approaches using External Knowledge Sources",
                "abstract": "The objective of the SemEval-2023 Task 1: Visual Word Sense Disambiguation (VWSD) is to identify the image illustrating the indented meaning of a target word and some minimal additional context. The omnipresence of textual and visual data in the task strongly suggests the utilization of the recent advances in multi-modal machine learning, i.e., pretrained visiolinguistic models (VLMs). Often referred to as foundation models due to their strong performance on many vision-language downstream tasks, these models further demonstrate powerful zero-shot capabilities. In this work, we utilize various pertained VLMs in a zero-shot fashion for multiple approaches using external knowledge sources to enrich the contextual information. Further, we evaluate our methods on the final test data and extensively analyze the suitability of different knowledge sources, the influence of training data, model sizes, multi-linguality, and different textual prompting strategies. Although we are not among the best-performing systems (rank 20 of 56), our experiments described in this work prove competitive results. Moreover, we aim to contribute meaningful insights and propel multi-modal machine learning tasks like VWSD.",
                "authors": "Florian Schneider, Chris Biemann",
                "citations": 2
            },
            {
                "title": "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models",
                "abstract": "Vision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical applications, VLMs are usually applied to specific scenarios, such as e-commerce and news fields, so the generalization of VLMs in specific domains should be given more attention. In this paper, we comprehensively investigate the capabilities of popular VLMs in a specific field, the food domain. To this end, we build a food caption dataset, Food-500 Cap, which contains 24,700 food images with 494 categories. Each image is accompanied by a detailed caption, including fine-grained attributes of food, such as the ingredient, shape, and color. We also provide a culinary culture taxonomy that classifies each food category based on its geographic origin in order to better analyze the performance differences of VLM in different regions. Experiments on our proposed datasets demonstrate that popular VLMs underperform in the food domain compared with their performance in the general domain. Furthermore, our research reveals severe bias in VLMs' ability to handle food items from different geographic regions. We adopt diverse probing methods and evaluate nine VLMs belonging to different architectures to verify the aforementioned observations. We hope that our study will bring researchers' attention to VLM's limitations when applying them to the domain of food or culinary cultures, and spur further investigations to address this issue.",
                "authors": "Zheng Ma, Mianzhi Pan, Wenhan Wu, Ka Leong Cheng, Jianbing Zhang, Shujian Huang, Jiajun Chen",
                "citations": 2
            },
            {
                "title": "Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models",
                "abstract": "Prompt learning has become one of the most efficient paradigms for adapting large pre-trained vision-language models to downstream tasks. Current state-of-the-art methods, like CoOp and ProDA, tend to adopt soft prompts to learn an appropriate prompt for each specific task. Recent CoCoOp further boosts the base-to-new generalization performance via an image-conditional prompt. However, it directly fuses identical image semantics to prompts of different labels and significantly weakens the discrimination among different classes as shown in our experiments. Motivated by this observation, we first propose a class-aware text prompt (CTP) to enrich generated prompts with label-related image information. Unlike CoCoOp, CTP can effectively involve image semantics and avoid introducing extra ambiguities into different prompts. On the other hand, instead of reserving the complete image representations, we propose text-guided feature tuning (TFT) to make the image branch attend to class-related representation. A contrastive loss is employed to align such augmented text and image representations on downstream tasks. In this way, the image-to-text CTP and text-to-image TFT can be mutually promoted to enhance the adaptation of VLMs for downstream tasks. Extensive experiments demonstrate that our method outperforms the existing methods by a significant margin. Especially, compared to CoCoOp, we achieve an average improvement of 4.03% on new classes and 3.19% on harmonic-mean over eleven classification benchmarks.",
                "authors": "Sifan Long, Zhen Zhao, Junkun Yuan, Zichang Tan, Jiangjiang Liu, Luping Zhou, Sheng-sheng Wang, Jingdong Wang",
                "citations": 2
            },
            {
                "title": "Enhancing HOI Detection with Contextual Cues from Large Vision-Language Models",
                "abstract": "Human-Object Interaction (HOI) detection aims at detecting human-object pairs and predicting their interactions. However, conventional HOI detection methods often struggle to fully capture the contextual information needed to accurately identify these interactions. While large Vision-Language Models (VLMs) show promise in tasks involving human interactions, they are not tailored for HOI detection. The complexity of human behavior and the diverse contexts in which these interactions occur make it further challenging. Contextual cues, such as the participants involved, body language, and the surrounding environment, play crucial roles in predicting these interactions, especially those that are unseen or ambiguous. Moreover, large VLMs are trained on vast image and text data, enabling them to generate contextual cues that help in understanding real-world contexts, object relationships, and typical interactions. Building on this, in this paper we introduce ConCue, a novel approach for improving visual feature extraction in HOI detection. Specifically, we first design specialized prompts to utilize large VLMs to generate contextual cues within an image. To fully leverage these cues, we develop a transformer-based feature extraction module with a multi-tower architecture that integrates contextual cues into both instance and interaction detectors. Extensive experiments and analyses demonstrate the effectiveness of using these contextual cues for HOI detection. The experimental results show that integrating ConCue with existing state-of-the-art methods significantly enhances their performance on two widely used datasets.",
                "authors": "Yu-Wei Zhan, Fan Liu, Xin Luo, Liqiang Nie, Xin-Shun Xu, Mohan S. Kankanhalli",
                "citations": 2
            },
            {
                "title": "Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment",
                "abstract": "Large-scale pre-trained Vision Language Models (VLMs) have proven effective for zero-shot classification. Despite the success, most traditional VLMs-based methods are restricted by the assumption of partial source supervision or ideal target vocabularies, which rarely satisfy the open-world scenario. In this paper, we aim at a more challenging setting, Realistic Zero-Shot Classification, which assumes no annotation but instead a broad vocabulary. To address the new problem, we propose the Self Structural Semantic Alignment (S3A) framework, which extracts the structural semantic information from unlabeled data while simultaneously self-learning. Our S3A framework adopts a unique Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR algorithm includes iterative clustering on images, voting within each cluster to identify initial class candidates from the vocabulary, generating discriminative prompts with large language models to discern confusing candidates, and realigning images and the vocabulary as structural semantic alignment. Finally, we propose to self-train the CLIP image encoder with both individual and structural semantic alignment through a teacher-student learning strategy. Our comprehensive experiments across various generic and fine-grained benchmarks demonstrate that the S3A method substantially improves over existing VLMs-based approaches, achieving a more than 15% accuracy improvement over CLIP on average. Our codes, models, and prompts are publicly released at https://github.com/sheng-eatamath/S3A.",
                "authors": "Shengxiang Zhang, Muzammal Naseer, Guangyi Chen, Zhiqiang Shen, Salman A. Khan, Kun Zhang, F. Khan",
                "citations": 2
            },
            {
                "title": "Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding",
                "abstract": "Vision-Language Models (VLMs), such as CLIP, exhibit strong image-text comprehension abilities, facilitating advances in several downstream tasks such as zero-shot image classification, image-text retrieval, and text-to-image generation. However, the compositional reasoning abilities of existing VLMs remains subpar. The root of this limitation lies in the inadequate alignment between the images and captions in the pretraining datasets. Additionally, the current contrastive learning objective fails to focus on fine-grained grounding components like relations, actions, and attributes, resulting in “bag-of-words” representations. We introduce a simple and effective method to improve compositional reasoning in VLMs. Our method better leverages available datasets by refining and expanding the standard image-text contrastive learning framework. Our approach does not require specific annotations and does not incur extra parameters. When integrated with CLIP, our technique yields notable improvement over state-of-the-art baselines across five vision-language compositional benchmarks.11We open-source our code at https://github.com/lezhang7/Enhance-FineGrained.",
                "authors": "Le Zhang, Rabiul Awal, Aishwarya Agrawal",
                "citations": 2
            },
            {
                "title": "SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering",
                "abstract": "Video question–answering is a fundamental task in the field of video understanding. Though current state-of-the-art video–language pretrained models have yielded appealing performance, they are at the cost of huge computational power and thus hard to deploy on many platforms with limited resources. An economical workaround simply samples a small portion of frames to tune an image–text model on these sampled frames. However, the sampling methods adopted by these VLMs are quite simple and straightforward— such methods are aimless and often inevitably omit the key frames from which the correct answer can be deduced, and the situation becomes worse as the sampling sparsity increases, which particularly requires when the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namely the most dominant frames (MDF) and most implied frames (MIF), to maximally preserve those frames that are most likely vital to the given questions. MDF passively minimizes the risk of key frame omission in a bootstrap manner, while MIF actively searches key frames customized for each video–question pair with the assistance of auxiliary models. The experimental results on three public datasets and three advanced VLMs (CLIP, GIT and All-in-one) demonstrate that our proposed strategies can boost the performance for image–text pretrained models. The source codes pertaining to the method proposed in this paper are publicly available at https://github.com/declare-lab/sas-vqa.",
                "authors": "Wei Han, Hui Chen, MingSung Kan, Soujanya Poria",
                "citations": 2
            },
            {
                "title": "Videoprompter: an ensemble of foundational models for zero-shot video understanding",
                "abstract": "Vision-language models (VLMs) classify the query video by calculating a similarity score between the visual features and text-based class label representations. Recently, large language models (LLMs) have been used to enrich the text-based class labels by enhancing the descriptiveness of the class names. However, these improvements are restricted to the text-based classifier only, and the query visual features are not considered. In this paper, we propose a framework which combines pre-trained discriminative VLMs with pre-trained generative video-to-text and text-to-text models. We introduce two key modifications to the standard zero-shot setting. First, we propose language-guided visual feature enhancement and employ a video-to-text model to convert the query video to its descriptive form. The resulting descriptions contain vital visual cues of the query video, such as what objects are present and their spatio-temporal interactions. These descriptive cues provide additional semantic knowledge to VLMs to enhance their zeroshot performance. Second, we propose video-specific prompts to LLMs to generate more meaningful descriptions to enrich class label representations. Specifically, we introduce prompt techniques to create a Tree Hierarchy of Categories for class names, offering a higher-level action context for additional visual cues, We demonstrate the effectiveness of our approach in video understanding across three different zero-shot settings: 1) video action recognition, 2) video-to-text and textto-video retrieval, and 3) time-sensitive video tasks. Consistent improvements across multiple benchmarks and with various VLMs demonstrate the effectiveness of our proposed framework. Our code will be made publicly available.",
                "authors": "Adeel Yousaf, Muzammal Naseer, Salman H. Khan, F. Khan, Mubarak Shah",
                "citations": 2
            },
            {
                "title": "Language-Based Augmentation to Address Shortcut Learning in Object-Goal Navigation",
                "abstract": "Deep Reinforcement Learning (DRL) has shown great potential in enabling robots to find certain objects (e.g., ‘find a fridge’) in environments like homes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL methods are predominantly trained and evaluated using environment simulators. Although DRL has shown impressive results, the simulators may be biased or limited. This creates a risk of shortcut learning, i.e., learning a policy tailored to specific visual details of training environments. We aim to deepen our understanding of shortcut learning in ObjectNav, its implications and propose a solution. We design an experiment for inserting a shortcut bias in the appearance of training environments. As a proof-of-concept, we associate room types to specific wall colors (e.g., bedrooms with green walls), and observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to environments where this is not the case (e.g., bedrooms with blue walls). We find that shortcut learning is the root cause: the agent learns to navigate to target objects, by simply searching for the associated wall color of the target object's room. To solve this, we propose Language-Based (L-B) augmentation. Our key insight is that we can leverage the multimodal feature space of a Vision-Language Model (VLM) to augment visual representations directly at the feature-level, requiring no changes to the simulator, and only an addition of one layer to the model. Where the SOTA ObjectNav method's success rate drops 69%, our proposal has only a drop of 23%. Code is available at https://github.com/Dennishoftijzer/L-B_Augmentation",
                "authors": "Dennis Hoftijzer, G. Burghouts, Luuk J. Spreeuwers",
                "citations": 1
            },
            {
                "title": "Assessing Language and Vision-Language Models on Event Plausibility",
                "abstract": "Transformer-based Language Models (LMs) excel in many tasks, but they appear to lack robustness in capturing crucial aspects of event knowledge due to their reliance on surface-level linguistic features and the mismatch between language descriptions and real-world occurrences. In this paper, we investigate the potential of Transformer-based Vision-Language Models (VLMs) in comprehending Generalized Event Knowledge (GEK) , aiming to determine whether the inclusion of a visual component affects the mastery of GEK. To do so, we compare multimodal Transformer models with unimodal ones on a task evaluating the plausibility of curated minimal sentence pairs. We show that current VLMs generally perform worse than their unimodal counterparts, suggesting that VL pre-training strategies are not yet as effective to model semantic understanding and resulting models are more akin to bag-of-words in this context.",
                "authors": "Maria Cassese, Alessandro Bondielli, Alessandro Lenci",
                "citations": 1
            },
            {
                "title": "Subsampling of Frequent Words in Text for Pre-training a Vision-Language Model",
                "abstract": "In this paper, we introduce Subsampling of frequent Words for Contrastive Language-Image Pre-training (SW-CLIP), a novel approach for the training Vision-Language Models (VLMs). SW-CLIP uses frequency-based subsampling of words that has been previously proposed to train skip-gram models in natural language processing and applies it to the textual training data of VLMs. We report on experiments that demonstrate the ability of frequency-based subsampling to speed up training and also to deliver a substantial improvement in accuracy in a number of downstream zero-shot (i.e., transfer) classification tasks. We notice that the classification test sets on which SW-CLIP seems to be particularly effective are those in which the labels of the classes occur infrequently as words in the training data, and thus have a high probability of being retained during frequency-based subsampling of the model training data. Overall, the advantages of SW-CLIP demonstrated in this paper serves to motivated further future work in text subsampling for the training of VLMs. Our code and pre-trained weights are available at https://github.com/Anastasiais-ml/sw_clip.git",
                "authors": "Mingliang Liang, Martha Larson",
                "citations": 1
            },
            {
                "title": "Unseen And Adverse Outdoor Scenes Recognition Through Event-based Captions",
                "abstract": "This paper presents EventCAP, i.e., event-based captions, for refined and enriched qualitative and quantitative captions by Deep Learning (DL) models and Vision Language Models (VLMs) with different tasks in a complementary manner. Indoor and outdoor images are used for object recognition and captioning. However, outdoor images in events change in wide ranges due to natural phenomena, i.e., weather changes. Such dynamical changes may degrade segmentation by illumination and object shape changes. This increases unseen objects and scenes under such adverse conditions. On the other hand, single state-of-art (SOTA) DLs and VLMs work with single or limited tasks, Therefore, this paper proposes EventCAP with captions with physical scales and objects’ surface properties. Moreover, an iterative VQA model is proposed to refine in-complete segmented images with the prompts. A higher se-mantic level in captions for real-world scene descriptions is experimentally shown compared to SOTA VLMs.",
                "authors": "Hidetomo Sakaino",
                "citations": 1
            },
            {
                "title": "Breaking Boundaries Between Linguistics and Artificial Intelligence",
                "abstract": "There is a wide connection between linguistics and artificial intelligence (AI), including the multimodal language matching. Multi-modal robots possess the capability to process various sensory modalities, including vision, auditory, language, and touch, offering extensive prospects for applications across various domains. Despite significant advancements in perception and interaction, the task of visual-language matching remains a challenging one for multi-modal robots. Existing methods often struggle to achieve accurate matching when dealing with complex multi-modal data, leading to potential misinterpretation or incomplete understanding of information. Additionally, the heterogeneity among different sensory modalities adds complexity to the matching process. To address these challenges, we propose an approach called vision-language matching with semantically aligned embeddings (VLMS), aimed at improving the visual-language matching performance of multi-modal robots.",
                "authors": "Jinhai Wang, Yi Tie, Xia Jiang, Yilin Xu",
                "citations": 1
            },
            {
                "title": "Large Deformation Prediction of Wing Model and Geometric Nonlinear Aeroelastic Analysis",
                "abstract": "Large flexible aircraft like High-altitude Long-endurance (HALE) airplane and large transport airplane possess large structural deformation with aerodynamic loads, which makes the dynamic characteristics changing obviously. Structural deformation predictions are vital to large flexible aircraft design and aeroelastic simulations. Deformation calculation considering geometric nonlinearities with full order models lack of practicality due to low simulation efficiency. Traditional reduced-order model (ROM) based on least square approach obtains high simulation efficiency but require large amounts of sample data participating model building. This study investigates support vector regression (SVR) algorithm to build prediction model to calculate the static deformation of flexible structure considering geometric nonlinearities. Performance of the prediction models are compared under evaluation with root mean square error (RMSE). Moreover, the prediction model can be coupled with aerodynamics models based on interpolation approach for static aeroelastic analysis. The non-planar vortex lattice method (VLM) is proposed here with appropriate accuracy and efficiency. Finally, a flexible wing model with single beam is adopted and the static aeroelastic response is compared between prediction model and wind tunnel test. The results demonstrate that the prediction model proposed has good performance and great practical application value.",
                "authors": "Chao An, LiPeng Zhu, Changchuan Xie, Chao Yang",
                "citations": 1
            },
            {
                "title": "LVDiffusor: Distilling Functional Rearrangement Priors From Large Models Into Diffusor",
                "abstract": "Object rearrangement, a fundamental challenge in robotics, demands versatile strategies to handle diverse objects, configurations, and functional needs. To achieve this, the AI robot needs to learn functional rearrangement priors to specify precise goals that meet the functional requirements. Previous methods typically learn such priors from either laborious human annotations or manually designed heuristics, which limits scalability and generalization. In this letter, we propose a novel approach that leverages large models to distill functional rearrangement priors. Specifically, our approach collects diverse arrangement examples using both LLMs and VLMs and then distills the examples into a diffusion model. During test time, the learned diffusion model is conditioned on the initial configuration and guides the positioning of objects to meet functional requirements. In this way, we balance zero-shot generalization with time efficiency. Extensive experiments in multiple domains, including real-world scenarios, demonstrate the effectiveness of our approach in generating compatible goals for object rearrangement tasks, significantly outperforming baseline methods.",
                "authors": "Yiming Zeng, Mingdong Wu, Long Yang, Jiyao Zhang, Hao Ding, Hui Cheng, Hao Dong",
                "citations": 1
            },
            {
                "title": "Towards Versatile and Efficient Visual Knowledge Injection into Pre-trained Language Models with Cross-Modal Adapters",
                "abstract": "Humans learn language via multi-modal knowledge. However, due to the text-only pre-training scheme, most existing pre-trained language models (PLMs) are hindered from the multi-modal information. To inject visual knowledge into PLMs, existing methods incorporate either the text or image encoder of vision-language models (VLMs) to encode the visual information and update all the original parameters of PLMs for knowledge fusion. In this paper, we propose a new plug-and-play module, X-adapter, to flexibly leverage the aligned visual and textual knowledge learned in pre-trained VLMs and efficiently inject them into PLMs. Specifically, we insert X-adapters into PLMs, and only the added parameters are updated during adaptation. To fully exploit the potential in VLMs, X-adapters consist of two sub-modules, V-expert and T-expert, to fuse VLMs’ image and text representations, respectively. We can opt for activating different sub-modules depending on the down-stream tasks. Experimental results show that our method can significantly improve the performance on object-color reasoning and natural language understanding (NLU) tasks compared with PLM baselines.",
                "authors": "Xinyun Zhang, Haochen Tan, Han Wu, Mingjie Zhan, Ding Liang, Bei Yu",
                "citations": 1
            },
            {
                "title": "Dynamic Texts From UAV Perspective Natural Images",
                "abstract": "Drone-based image processing offers valuable capabilities for surveillance, detection, and tracking in vast areas, aiding in disaster search and rescue, and monitoring artificial events like traffic jams and outdoor activities under adversarial weather conditions. Nonetheless, this technology encounters numerous challenges, including handling variations in scales and perspectives and coping with environmental factors like sky interference and the presence of far and small objects. Besides, ensuring high visibility distance in 3D depth is crucial for safe flights in various settings, including airports, cities, and fields. However, local weather conditions can change rapidly during flights, leading to visibility issues caused by fog and clouds. Due to the cost of visibility measurement sensors, lower-cost methods using portable apparatus are desired for flight routines. Therefore, this paper proposes a camera-based visibility and weather condition estimation approach using complementary multiple Deep Learning (DL) and Vision Language Models (VLM) under adversarial conditions. Experimental results show the superiority of enhanced 2D/3D captions with physical scales over SOTA VLMs.",
                "authors": "Hidetomo Sakaino",
                "citations": 1
            },
            {
                "title": "PILL: Plug Into LLM with Adapter Expert and Attention Gate",
                "abstract": "Due to the remarkable capabilities of powerful Large Language Models (LLMs) in effectively following instructions, there has been a growing number of assistants in the community to assist humans. Recently, significant progress has been made in the development of Vision Language Models (VLMs), expanding the capabilities of LLMs and enabling them to execute more diverse instructions. However, it is foreseeable that models will likely need to handle tasks involving additional modalities such as speech, video, and others. This poses a particularly prominent challenge of dealing with the complexity of mixed modalities. To address this, we introduce a novel architecture called PILL: Plug Into LLM with adapter expert and attention gate to better decouple these complex modalities and leverage efficient fine-tuning. We introduce two modules: Firstly, utilizing Mixture-of-Modality-Adapter-Expert to independently handle different modalities, enabling better adaptation to downstream tasks while preserving the expressive capability of the original model. Secondly, by introducing Modality-Attention-Gating, which enables adaptive control of the contribution of modality tokens to the overall representation. In addition, we have made improvements to the Adapter to enhance its learning and expressive capabilities. Experimental results demonstrate that our approach exhibits competitive performance compared to other mainstream methods for modality fusion. For researchers interested in our work, we provide free access to the code and models at https://github.com/DsaltYfish/PILL.",
                "authors": "Fangyuan Zhang, Tingting Liang, Zhengyuan Wu, Yuyu Yin",
                "citations": 1
            },
            {
                "title": "SelfEval: Leveraging the discriminative nature of generative models for evaluation",
                "abstract": "We present an automated way to evaluate the text alignment of text-to-image generative diffusion models using standard image-text recognition datasets. Our method, called SelfEval, uses the generative model to compute the likelihood of real images given text prompts, and the likelihood can be used to perform recognition tasks with the generative model. We evaluate generative models on standard datasets created for multimodal text-image discriminative learning and assess fine-grained aspects of their performance: attribute binding, color recognition, counting, shape recognition, spatial understanding. Existing automated metrics rely on an external pretrained model like CLIP (VLMs) or LLMs, and are sensitive to the exact pretrained model and its limitations. SelfEval sidesteps these issues, and to the best of our knowledge, is the first automated metric to show a high degree of agreement for measuring text-faithfulness with the gold-standard human evaluations across multiple generative models, benchmarks and evaluation metrics. SelfEval also reveals that generative models showcase competitive recognition performance on challenging tasks such as Winoground image-score compared to discriminative models. We hope SelfEval enables easy and reliable automated evaluation for diffusion models.",
                "authors": "Sai Saketh Rambhatla, Ishan Misra",
                "citations": 1
            },
            {
                "title": "Assessing Language and Vision-Language Models on Event Plausibility",
                "abstract": "Transformer-based Language Models (LMs) excel in many tasks, but they appear to lack robustness in capturing crucial aspects of event knowledge due to their reliance on surface-level linguistic features and the mismatch between language descriptions and real-world occurrences. In this paper, we investigate the potential of Transformer-based Vision-Language Models (VLMs) in comprehending Generalized Event Knowledge (GEK) , aiming to determine whether the inclusion of a visual component affects the mastery of GEK. To do so, we compare multimodal Transformer models with unimodal ones on a task evaluating the plausibility of curated minimal sentence pairs. We show that current VLMs generally perform worse than their unimodal counterparts, suggesting that VL pre-training strategies are not yet as effective to model semantic understanding and resulting models are more akin to bag-of-words in this context.",
                "authors": "Maria Cassese, Alessandro Bondielli, Alessandro Lenci",
                "citations": 1
            },
            {
                "title": "Towards Versatile and Efficient Visual Knowledge Integration into Pre-trained Language Models with Cross-Modal Adapters",
                "abstract": "Humans learn language via multi-modal knowledge. However, due to the text-only pre-training scheme, most existing pre-trained language models (PLMs) are hindered from the multi-modal information. To inject visual knowledge into PLMs, existing methods incorporate either the text or image encoder of vision-language models (VLMs) to encode the visual information and update all the original parameters of PLMs for knowledge fusion. In this paper, we propose a new plug-and-play module, X-adapter, to flexibly leverage the aligned visual and textual knowledge learned in pre-trained VLMs and efficiently inject them into PLMs. Specifically, we insert X-adapters into PLMs, and only the added parameters are updated during adaptation. To fully exploit the potential in VLMs, X-adapters consist of two sub-modules, V-expert and T-expert, to fuse VLMs' image and text representations, respectively. We can opt for activating different sub-modules depending on the downstream tasks. Experimental results show that our method can significantly improve the performance on object-color reasoning and natural language understanding (NLU) tasks compared with PLM baselines.",
                "authors": "Xinyun Zhang, Haochen Tan, Han Wu, Bei Yu",
                "citations": 1
            },
            {
                "title": "ProbVLM: Probabilistic Adapter for Frozen Vision-Language Models",
                "abstract": "Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model. Code is available at https://github.com/ExplainableML/ProbVLM.",
                "authors": "Uddeshya Upadhyay, Shyamgopal Karthik, Massimiliano Mancini, Zeynep Akata",
                "citations": 1
            },
            {
                "title": "Active Prompt Learning in Vision Language Models",
                "abstract": "Pre-trained Vision Language Models (VLMs) have demonstrated notable progress in various zero-shot tasks, such as classification and retrieval. Despite their performance, because improving performance on new tasks requires task-specific knowledge, their adaptation is essential. While labels are needed for the adaptation, acquiring them is typically expensive. To overcome this challenge, active learning, a method of achieving a high performance by obtaining labels for a small number of samples from experts, has been studied. Active learning primarily focuses on selecting unlabeled samples for labeling and leveraging them to train models. In this study, we pose the question, “how can the pre-trained VLMs be adapted under the active learning framework?” In response to this inquiry, we observe that (1) simply applying a conventional active learning framework to pre-trained VLMs even may degrade performance compared to random selection because of the class imbalance in labeling candidates, and (2) the knowledge of VLMs can provide hints for achieving the balance before labeling. Based on these observations, we devise a novel active learning framework for VLMs, denoted as PCB. To assess the effectiveness of our approach, we conduct experiments on seven different real-world datasets, and the results demonstrate that PCB surpasses conventional active learning and random sampling methods. Code is available at https://github.com/kaist-dmlab/pcb.",
                "authors": "Jihwan Bang, Sumyeong Ahn, Jae-Gil Lee",
                "citations": 1
            },
            {
                "title": "Bridging Low-level Geometry to High-level Concepts in Visual Servoing of Robot Manipulation Task Using Event Knowledge Graphs and Vision-Language Models",
                "abstract": "In this paper, we propose a framework of building knowledgeable robot control in the scope of smart human-robot interaction, by empowering a basic uncalibrated visual servoing controller with contextual knowledge through the joint usage of event knowledge graphs (EKGs) and large-scale pretrained vision-language models (VLMs). The framework is expanded in twofold: first, we interpret low-level image geometry as high-level concepts, allowing us to prompt VLMs and to select geometric features of points and lines for motor control skills; then, we create an event knowledge graph (EKG) to conceptualize a robot manipulation task of interest, where the main body of the EKG is characterized by an executable behavior tree, and the leaves by semantic concepts relevant to the manipulation context. We demonstrate, in an uncalibrated environment with real robot trials, that our method lowers the reliance of human annotation during task interfacing, allows the robot to perform activities of daily living more easily by treating low-level geometric-based motor control skills as high-level concepts, and is beneficial in building cognitive thinking for smart robot applications.",
                "authors": "Chen Jiang, Martin Jägersand",
                "citations": 1
            },
            {
                "title": "QR-CLIP: Introducing Explicit Knowledge for Location and Time Reasoning",
                "abstract": "\n This paper focuses on reasoning about the location and time behind images. Given that pre-trained vision-language models (VLMs) exhibit excellent image and text understanding capabilities, most existing methods leverage them to match visual cues with location and time-related descriptions. However, these methods cannot look beyond the actual content of an image, failing to produce satisfactory reasoning results, as such reasoning requires connecting visual details with rich external cues (e.g., relevant event contexts). To this end, we propose a novel reasoning method,\n QR-CLIP\n , aim at enhancing the model's ability to reason about location and time through interaction with external explicit knowledge such as Wikipedia. Specifically, QR-CLIP consists of two modules: 1) The\n Quantity\n module abstracts the image into multiple distinct representations and uses them to search and gather external knowledge from different perspectives that are beneficial to model reasoning. 2) The\n Relevance\n module filters the visual features and the searched explicit knowledge and dynamically integrates them to form a comprehensive reasoning result. Extensive experiments demonstrate the effectiveness and generalizability of QR-CLIP. On the WikiTiLo dataset, QR-CLIP boosts the accuracy of location (country) and time reasoning by 7.03% and 2.22%, respectively, over previous SOTA methods. On the more challenging TARA dataset, it improves the accuracy for location and time reasoning by 3.05% and 2.45%, respectively. The source code is at\n https://github.com/Shi-Wm/QR-CLIP\n .\n",
                "authors": "Weimin Shi, Mingchen Zhuge, Zhong Zhou, D. Gao, Deng-Ping Fan",
                "citations": 1
            },
            {
                "title": "Breaking Boundaries Between Linguistics and Artificial Intelligence",
                "abstract": "There is a wide connection between linguistics and artificial intelligence (AI), including the multimodal language matching. Multi-modal robots possess the capability to process various sensory modalities, including vision, auditory, language, and touch, offering extensive prospects for applications across various domains. Despite significant advancements in perception and interaction, the task of visual-language matching remains a challenging one for multi-modal robots. Existing methods often struggle to achieve accurate matching when dealing with complex multi-modal data, leading to potential misinterpretation or incomplete understanding of information. Additionally, the heterogeneity among different sensory modalities adds complexity to the matching process. To address these challenges, we propose an approach called vision-language matching with semantically aligned embeddings (VLMS), aimed at improving the visual-language matching performance of multi-modal robots.",
                "authors": "Jinhai Wang, Yi Tie, Xia Jiang, Yilin Xu",
                "citations": 1
            },
            {
                "title": "Vulvar Leiomyosarcomas: A Case Series with Clinical Comparison to Uterine Leiomyosarcomas and Review of the Literature",
                "abstract": "Case series Patients: — Final Diagnosis: Vulvar leiomyosarcoma Symptoms: Vulvar mass • vulvar pain • vulvar swelling Clinical Procedure: — Specialty: Obstetrics and Gynecology • Oncology Objective: Rare disease Background: Leiomyosarcomas of the vulva (VLMS) are very rare among gynecological malignancies, with a lack of knowledge on clinical presentation, prognosis, and therapeutic management. Case Reports: The database of the German Clinical Center of Competence for Genital Sarcomas and Mixed Tumors in Greifswald (DKSM) was reviewed between the years 2010 and 2020. A total of 8 cases of VLMS were retrieved and analyzed retrospectively. One exemplary case of VLMS was outlined in detail: A 45-year-old premenopausal woman presented with increasing vulvar swelling and discomfort. Given the suspicion of a Bartholin’s gland abscess, the mass was excised. Final pathology revealed a solid tumor consistent with a moderately differentiated leiomyosarcoma of the vulva. A wide local excision was subsequently performed followed by adjuvant external beam radiation. The clinical features of these 8 cases of VLMS were compared to 26 cases of VLMS found in a review of the literature and to a total of 276 cases of uterine leiomyosarcoma (ULMS) from the same database (DKSM). Conclusions: In addition to rapid growth, observed in both tumor entities, VLMS most commonly presented as Bartholin’s gland abscess or cyst and ULMS as leiomyoma. In this cohort, the prognosis of VLMS was much better than that of ULMS, most probably due to the significantly smaller tumor size of VLMS at diagnosis. Further data and larger studies on VLMS are needed to calculate recurrence and survival rates more accurately and define the role of adjuvant radiotherapy.",
                "authors": "S. Verta, Z. Alwafai, Nika Schleede, C. Brambs, C. Christmann, V. Reichert, M. Zygmunt, B. Plattner, G. Köhler",
                "citations": 1
            },
            {
                "title": "Compositional Semantics for Open Vocabulary Spatio-semantic Representations",
                "abstract": "General-purpose mobile robots need to complete tasks without exact human instructions. Large language models (LLMs) is a promising direction for realizing commonsense world knowledge and reasoning-based planning. Vision-language models (VLMs) transform environment percepts into vision-language semantics interpretable by LLMs. However, completing complex tasks often requires reasoning about information beyond what is currently perceived. We propose latent compositional semantic embeddings z* as a principled learning-based knowledge representation for queryable spatio-semantic memories. We mathematically prove that z* can always be found, and the optimal z* is the centroid for any set Z. We derive a probabilistic bound for estimating separability of related and unrelated semantics. We prove that z* is discoverable by iterative optimization by gradient descent from visual appearance and singular descriptions. We experimentally verify our findings on four embedding spaces incl. CLIP and SBERT. Our results show that z* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics for ideal uniformly distributed high-dimensional embeddings. We demonstrate that a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181 overlapping semantics by 42.23 mIoU, while improving conventional non-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared with a popular SOTA model.",
                "authors": "Robin Karlsson, Francisco Lepe-Salazar, K. Takeda",
                "citations": 1
            },
            {
                "title": "SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models",
                "abstract": "Despite achieving remarkable performance on various vision-language tasks, Transformer-based pretrained vision-language models (VLMs) still suffer from efficiency issues arising from long inputs and numerous parameters, limiting their real-world applications. However, the huge computation is redundant for most samples and the degree of redundancy and the respective components vary significantly depending on tasks and input instances. In this work, we propose an adaptive acceleration method S MART T RIM for VLMs, which adjusts the inference overhead based on the complexity of instances. Specifically, S MART T RIM incorporates lightweight trimming modules into the backbone to perform task-specific pruning on redundant inputs and parameters, without the need for additional pre-training or data augmentation. Since visual and textual representations complement each other in VLMs, we propose to leverage cross-modal interaction information to provide more critical semantic guidance for identifying redundant parts. Meanwhile, we introduce a self-distillation strategy that encourages the trimmed model to be consistent with the full-capacity model, which yields further performance gains. Experimental results demonstrate that S MART T RIM significantly reduces the computation overhead ( 2 - 3 times) of various VLMs with comparable performance (only a 1 - 2% degradation) on various vision-language tasks. Compared to previous acceleration methods, S MART T RIM attains a better efficiency-performance trade-off, demonstrating great potential for application in resource-constrained scenarios.",
                "authors": "Zekun Wang, Jingchang Chen, Wangchunshu Zhou, Ming Liu, Bing Qin",
                "citations": 1
            },
            {
                "title": "Self-Adaptive Sampling for Efficient Video Question-Answering on Image--Text Models",
                "abstract": "Video question-answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namely the most domain frames (MDF) and most implied frames (MIF), to maximally preserve those frames that are most likely vital to the given questions. MDF passively minimizes the risk of key frame omission in a bootstrap manner, while MIS actively searches key frames customized for each video--question pair with the assistance of auxiliary models. The experimental results on three public datasets from three advanced VLMs (CLIP, GIT and All-in-one) demonstrate that our proposed strategies can boost the performance for image-text pretrained models. The source codes pertaining to the method proposed in this paper are publicly available at https://github.com/declare-lab/sas-vqa.",
                "authors": "Wei Han, Hui Chen, MingSung Kan, Soujanya Poria",
                "citations": 1
            },
            {
                "title": "VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training",
                "abstract": "Vision-and-language pre-trained models (VLMs) have achieved tremendous success in the cross-modal area, but most of them require a large amount of parallel image-caption data for pre-training. Collating such data is expensive and labor-intensive. In this work, we focus on reducing such need for generative vision-and-language pre-training (G-VLP) by taking advantage of the visual pre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2) as decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which hinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we conduct extensive experiments to empirically investigate how to design and pre-train our model. Based on our experimental results, we propose a novel G-VLP framework, Visual Conditioned GPT (VC-GPT) , and pre-train it with a small-scale image-caption corpus (Visual Genome, only 110k distinct images). Evaluating on the image captioning downstream tasks (MSCOCO and Flickr30k Captioning), VC-GPT achieves either the best or the second-best performance across all evaluation metrics over the previous works which consume around 30 times more distinct images during cross-modal pre-training.",
                "authors": "Ziyang Luo, Yadong Xi, Rongsheng Zhang, Jing Ma",
                "citations": 17
            },
            {
                "title": "Task-Oriented Multi-Modal Mutual Learning for Vision-Language Models",
                "abstract": "Prompt learning has become one of the most efficient paradigms for adapting large pre-trained vision-language models to downstream tasks. Current state-of-the-art methods, like CoOp and ProDA, tend to adopt soft prompts to learn an appropriate prompt for each specific task. Recent CoCoOp further boosts the base-to-new generalization performance via an image-conditional prompt. However, it directly fuses identical image semantics to prompts of different labels and significantly weakens the discrimination among different classes as shown in our experiments. Motivated by this observation, we first propose a class-aware text prompt (CTP) to enrich generated prompts with label-related image information. Unlike CoCoOp, CTP can effectively involve image semantics and avoid introducing extra ambiguities into different prompts. On the other hand, instead of reserving the complete image representations, we propose text-guided feature tuning (TFT) to make the image branch attend to class-related representation. A contrastive loss is employed to align such augmented text and image representations on downstream tasks. In this way, the image-to-text CTP and text-to-image TFT can be mutually promoted to enhance the adaptation of VLMs for downstream tasks. Extensive experiments demonstrate that our method outperforms the existing methods by a significant margin. Especially, compared to CoCoOp, we achieve an average improvement of 4.03% on new classes and 3.19% on harmonic-mean over eleven classification benchmarks.",
                "authors": "Sifan Long, Zhen Zhao, Junkun Yuan, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang, Jingdong Wang",
                "citations": 0
            },
            {
                "title": "Compositional Reasoning in Vision-Language Models",
                "abstract": "Vision–language models (VLMs) have achieved impressive performance on long-standing visual question-answering (VQA) benchmarks by utilizing large-scale image–caption pre-training on web datasets. These models are able to perform a variety of downstream tasks out of the box, including zero-shot VQA, but have also been shown to struggle with understanding concepts such as object attributes, relations, and compositional reasoning. In this paper, we test whether such multi-modal models are able to reason compositionally. To do so, we decompose “com-positional” questions from the CLEVR dataset and confirm that state-of-the-art VLMs struggle with composition. We reveal that these models can accurately identify pieces of information but struggle to relate them together to answer compositional questions. Additionally, we find that in-context VQA examples do not improve VQA performance or compositional reasoning in these models.",
                "authors": "Jessica Li, Apoorv Khandelwal, Ellie Pavlick",
                "citations": 0
            },
            {
                "title": "p-Laplacian Adaptation for Generative Pre-trained Vision-Language Models",
                "abstract": "Vision-Language models (VLMs) pre-trained on large corpora have demonstrated notable success across a range of downstream tasks. In light of the rapidly increasing size of pre-trained VLMs, parameter-efficient transfer learning (PETL) has garnered attention as a viable alternative to full fine-tuning. One such approach is the adapter, which introduces a few trainable parameters into the pre-trained models while preserving the original parameters during adaptation.\nIn this paper, we present a novel modeling framework that recasts adapter tuning after attention as a graph message passing process on attention graphs, where the projected query and value features and attention matrix constitute the node features and the graph adjacency matrix, respectively. Within this framework, tuning adapters in VLMs necessitates handling heterophilic graphs, owing to the disparity between the projected query and value space.\nTo address this challenge, we propose a new adapter architecture, p-adapter, which employs p-Laplacian message passing in Graph Neural Networks (GNNs). Specifically, the attention weights are re-normalized based on the features, and the features are then aggregated using the calibrated attention matrix, enabling the dynamic exploitation of information with varying frequencies in the heterophilic attention graphs.\nWe conduct extensive experiments on different pre-trained VLMs and multi-modal tasks, including visual question answering, visual entailment, and image captioning. The experimental results validate our method's significant superiority over other PETL methods. Our code is available at https://github.com/wuhy68/p-Adapter/.",
                "authors": "Haoyuan Wu, Xinyun Zhang, Peng Xu, Peiyu Liao, Xufeng Yao, Bei Yu",
                "citations": 0
            },
            {
                "title": "Morphing technology for gust alleviation: an UAS application",
                "abstract": "Abstract. Atmospheric turbulence can significantly affect aircraft missions in terms of aerodynamic loads and vibration. These effects are particularly meaningful for MALE-HALE UAS because of their high aspect ratios and because of their low speed, sometimes comparable with that of the gust itself. Many studies have been conducted to reach the goal of efficient gust alleviation. A viable solution appears the application of morphing technology. However, the design of morphing aircraft is a strongly multidisciplinary effort involving different expertise from structures to aerodynamics and flight control. In this study, a multidisciplinary wing-and-tail morphing strategy is proposed for attaining gust attenuation in UAVs. The strategy is based on the combined use of: i) an automatic detection system that identifies gust direction and entity and ii) an aeroelastic model stemming from the coupling between a high-order structural model that is able to resolve the motion and the strain and stress distributions of wings with complex internal structures and a Vortex Lattice Method (VLM) model that accounts for the aerodynamics of the wing-tail system. The gust alleviation strategy employs the information from the detection system and the aeroelastic model to determine the modifications of the wing and the tail surfaces aimed at contrasting wind effects, reducing induced loads and flight path errors. Numerical results are presented to assess the capability of the framework.",
                "authors": "F. Montano",
                "citations": 0
            },
            {
                "title": "Relative and Absolute Sea Level Change Variability in The Palabuhanratu Bay Waters",
                "abstract": "SLR could affect the waters of Palabuhanratu Bay, which are exposed to the Indian Ocean. Sea levels are rising due to RSL and ASL changes. RSL has increased by 22.86 mm/year based on tidal data for 2013–2022. In contrast, ASL has increased by 4.48 mm/year based on satellite altimetry data for 1992–2022. The research uses the linear regression method to get the SLR value. According to earlier studies, dynamic change elements connected to alterations in the atmosphere and ocean circulation are typically considered when analyzing sea level variations. This work explores sea-level variations, including dynamic and static changes impacted by geological processes like deformation. Dynamic change factors, such as climate anomalies, ENSO, and IOD, affect these changes. Regarding RSL changes, ENSO has a more decisive influence than IOD, and vice versa for ASL changes. In the La Nina phenomenon and negative IOD phase, RSL and ASL trend changes are enhanced, whereas they are lowered in the El Nino and positive IOD phases. These waters’ sea-level variations are only slightly impacted by local processes, such as VLM. It has a more decisive influence on RSL changes than ASL.",
                "authors": "Eva Novita, Masita Dwi Mandini Manessa, M. Patria",
                "citations": 0
            },
            {
                "title": "Open-RoadAtlas: Leveraging VLMs for Road Condition Survey with Real-Time Mobile Auditing",
                "abstract": "Road surveying plays a vital role in effective road network management for local governments. However, current practices pose challenges due to their costly, time-consuming, and inaccurate nature. In this paper, we propose an automated survey platform that supports weed, defect and asset monitoring with instance segmentation models. Empowered by recent advancements in vision-language models (VLMs), our solution offers improved flexibility for novel tasks with a limited label set. For domain specific classes, such as pavement cracks and potholes, we train a detector to identify their location given our sparsely annotated images, and alleviate false-positives by rejecting predictions outside regions of interest identified by VLMs. The proposed system directly involves managers in the survey process through a mobile application. The application allows three core functions: 1) capture and cloud upload, 2) real-time survey trajectory monitoring, 3) open-vocabulary detection.",
                "authors": "Djamahl Etchegaray, Yandan Luo, Zachary FitzChance, Anthony Southon, Jinjiang Zhong",
                "citations": 0
            },
            {
                "title": "Development of a multi-field computational tool for high-fidelity static aeroelastic simulations",
                "abstract": "Abstract. A new method for high-fidelity aeroelastic static analysis of composite laminated wings is proposed. The structural analysis and the fluid-dynamic analysis are coupled in a heterogeneous staggered process. The Finite Element Method (FEM), the Carrera Unified Formulation (CUF) and Equivalent Plate Modelling (EPM) are combined to model complex three-dimensional geometries in a bi-dimensional framework; Computational Fluid Dynamics (CFD) is employed to solve the Navier-Stokes equations and different turbulence models (i.e. Spalart-Allmaras) through SU2, an open-source software that implements C++ routines for 3D fluid-dynamics analysis. The Moving Least Square patch technique is adopted to manage the fluid-structure interaction. The use of an equivalent plate model, as opposite to 1D models often employed in the literature, shows competitive performances in terms of number of degrees of freedom. High-fidelity aerodynamics allows studying non-linear phenomena associated to irregularities of the fluid-structure interaction, showing a level of accuracy that low-fidelity methods such as Vortex Lattice Method (VLM) and Doublet Lattice Method (DLM) are unable to provide. Such advantages are balanced by the need to elaborate a staggered iterative method for the resolution of static aeroelastic problems, which leads to higher computational costs.",
                "authors": "M. Grifò",
                "citations": 0
            },
            {
                "title": "Çocukluk Çağı Benign Baş Boyun Kitlelerinde Tanı ve Tedavi Yaklaşımı",
                "abstract": "Objective: Head and neck masses are frequently encountered in childhood. Radiological characterization of these lesions plays a crucial role in determining the appropriate treatment approach. At this stage, considering radiation exposure, radiological treatment and, if necessary, surgery should be directed. This study, it was aimed to discuss the diagnostic approach in childhood benign head and neck masses. \nMethods: A retrospective analysis was conducted on 64 pediatric patients who presented with head and neck masses and underwent radiological imaging between 2018 and 2022. Demographic data, radiological diagnostic methods, lesion localization, characterization, treatment approaches, and pathological findings of patients with congenital and acquired masses were obtained from the hospital system. \nResults: The distribution of age and gender among the 64 patients was similar across groups. Head and neck masses were predominantly located in the anterolateral cervical triangle. Thyroglossal duct cysts were the most common cystic masses, displaying mixed sonographic characteristics. Venolymphatic malformations (VLM) were the second most frequently observed masses, predominantly exhibiting cystic features. Hemangiomas were the most common solid masses encountered. In addition, while the cystic lesions were most frequently congenital, the solid ones were acquired. \nRegarding treatment planning, 32 patients (50%) underwent surgical intervention. Among the patients, 15.6% of the patients underwent sclerotherapy at least once. Medical treatment involved non-selective beta-blocker administration in 10 (15.6%) patients. \nConclusion: The etiology of childhood head and neck masses encompasses a wide spectrum. Therefore, accurate diagnosis plays a crucial role in guiding clinicians toward appropriate medical and/or surgical treatment options.",
                "authors": "Sabriye Gülçin Bozbeyoğlu, Neslihan Gülçi̇n",
                "citations": 0
            },
            {
                "title": "Context-Aware Prompt Tuning for Vision-Language Model with Dual-Alignment",
                "abstract": "Large-scale vision-language models (VLMs), e.g., CLIP, learn broad visual concepts from tedious training data, showing superb generalization ability. Amount of prompt learning methods have been proposed to efficiently adapt the VLMs to downstream tasks with only a few training samples. We introduce a novel method to improve the prompt learning of vision-language models by incorporating pre-trained large language models (LLMs), called Dual-Aligned Prompt Tuning (DuAl-PT). Learnable prompts, like CoOp, implicitly model the context through end-to-end training, which are difficult to control and interpret. While explicit context descriptions generated by LLMs, like GPT-3, can be directly used for zero-shot classification, such prompts are overly relying on LLMs and still underexplored in few-shot domains. With DuAl-PT, we propose to learn more context-aware prompts, benefiting from both explicit and implicit context modeling. To achieve this, we introduce a pre-trained LLM to generate context descriptions, and we encourage the prompts to learn from the LLM's knowledge by alignment, as well as the alignment between prompts and local image features. Empirically, DuAl-PT achieves superior performance on 11 downstream datasets on few-shot recognition and base-to-new generalization. Hopefully, DuAl-PT can serve as a strong baseline. Code will be available.",
                "authors": "Hongyu Hu, Tiancheng Lin, Jie Wang, Zhenbang Sun, Yi Xu",
                "citations": 0
            },
            {
                "title": "Vision-Language Models Learn Super Images for Efficient Partially Relevant Video Retrieval",
                "abstract": "\n In this paper, we propose an efficient and high-performance method for partially relevant video retrieval. The method aims to retrieve long videos that contain at least one moment relevant to the input text query. The challenge lies in encoding dense frames using visual backbones. This requires models to handle the increased frames, resulting in significant computation costs for long videos. To mitigate the costs, previous studies use lightweight visual backbones, yielding sub-optimal retrieval performance due to their limited capabilities. However, it is undesirable to simply replace the backbones with high-performance large vision-and-language models (VLMs) due to their low efficiency. To address this dilemma, instead of dense frames, we focus on super images, which are created by rearranging the video frames in an\n \n \\(N\\times N\\)\n \n grid layout. This reduces the number of visual encodings to\n \n \\(\\frac{1}{N^{2}}\\)\n \n and mitigates the low efficiency of large VLMs. Based on this idea, we make two contributions. First, we explore whether VLMs generalize to super images in a zero-shot setting. To this end, we propose a method called query-attentive super image retrieval (QASIR), which attends to partial moments relevant to the input query. The zero-shot QASIR yields two discoveries: (1) it enables VLMs to generalize to super images and (2) the grid size\n \n \\(N\\)\n \n , image resolution, and VLM size are key trade-off parameters between performance and computation costs. Second, we introduce fine-tuning and hybrid QASIR that combines high- and low-efficiency models to strike a balance between performance and computation costs. This reveals two findings: (1) the fine-tuning QASIR enhances VLMs to learn super images effectively, and (2) the hybrid QASIR minimizes the performance drop of large VLMs while reducing the computation costs.\n",
                "authors": "Taichi Nishimura, Shota Nakada, Masayoshi Kondo",
                "citations": 0
            },
            {
                "title": "Semantically Grounded QFormer for Efficient Vision Language Understanding",
                "abstract": "General purpose Vision Language Models (VLMs) have received tremendous interest in recent years, owing to their ability to learn rich vision-language correlations as well as their broad zero-shot competencies. One immensely popular line of work utilizes frozen unimodal models, by bridging vision representations to language using a trainable module called the QFormer. However, this method relies heavily on large-scale multimodal pretraining with huge computational overheads. To that end, we propose a more efficient framework for QFormer-based vision-language alignment. Our key idea relies on the observation that QFormer latents correspond more strongly to the frozen LLM's intermediate latent space. Consequently, instead of using QFormer latents as inputs to the LLM, we alter the framework by using the latents to directly condition the LLM latent space for image-to-text generation. We demonstrate the effectiveness of our approach against existing baselines in improving the efficiency of vision-language pretraining.",
                "authors": "Moulik Choraria, Nitesh Sekhar, Yue Wu, Xu Zhang, Prateek Singhal, L. Varshney",
                "citations": 0
            },
            {
                "title": "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models",
                "abstract": "Vision-language models (VLMs) like CLIP have demonstrated remarkable applicability across a variety of downstream tasks, including zero-shot image classification. Recently, the use of prompts or adapters for efficient transfer learning (ETL) has gained significant attention for effectively adapting to downstream tasks. However, previous studies have overlooked the challenge of varying transfer difficulty of downstream tasks. In this paper, we empirically analyze how each ETL method behaves with respect to transfer difficulty. Our observations indicate that utilizing vision prompts and text adapters is crucial for adaptability and generalizability in domains with high difficulty. Also, by applying an adaptive ensemble approach that integrates task-adapted VLMs with pre-trained VLMs and strategically leverages more general knowledge in low-difficulty and less in high-difficulty domains, we consistently enhance performance across both types of domains. Based on these observations, we propose an adaptive ensemble method that combines visual prompts and text adapters with pre-trained VLMs, tailored by transfer difficulty, to achieve optimal performance for any target domain. Upon experimenting with extensive benchmarks, our method consistently outperforms all baselines, particularly on unseen tasks, demonstrating its effectiveness.",
                "authors": "Yongjin Yang, Jongwoo Ko, SeYoung Yun",
                "citations": 0
            },
            {
                "title": "I NTRIGUING P ROPERTIES OF V ISUAL -L ANGUAGE",
                "abstract": "The growing popularity of large-scale visual-language models (VLMs) has led to their employment in various downstream applications as they provide a rich source of image and text representations. However, these representations are highly entangled and complex to interpret by machine learning developers and practitioners. Recent works have shown visualizations of image regions that VLMs focus on but fail to describe the change in explanations generated for visual-language classifiers in zero-shot (using image and text representations) vs. fine-tuned settings (using image representations). In this work, we perform the first empirical study to establish the trustworthy properties of explanations generated for VLMs used in zero-shot vs. fine-tune settings. We show that explanations for zero-shot visual-language classifiers are more faithful than their fine-tuned counterpart. Further, we demonstrate that VLMs tend to attribute high importance to gender, despite being non-indicative of the downstream task. Our experiments on multiple real-world datasets show interesting VLM behavior in zero-shot vs. fine-tuned settings, opening up new frontiers in understanding the trustworthiness of large-scale visual-language models.",
                "authors": "Chirag Agarwal",
                "citations": 0
            },
            {
                "title": "Interactive Learning and Control in the Era of Large Models",
                "abstract": "In this talk, I will discuss the problem of interactive learning by discussing how we can actively learn objective functions from human feedback capturing their preferences. I will then talk about how the value alignment and reward design problem can have solutions beyond active preference-based learning by tapping into the rich context available from large language models. In the second section of the talk, I will more generally talk about the role of large pretrained models in today�s robotics and control systems. Specifically, I will present two viewpoints: 1) pretraining large models for downstream robotics tasks, and 2) finding creative ways of tapping into the rich context of large models to enable more aligned embodied AI agents. For pretraining, I will introduce Voltron, a language-informed visual representation learning approach that leverages language to ground pretrained visual representations for robotics. For leveraging large models, I will talk about a few vignettes about how we can leverage LLMs and VLMs to learn human preferences, allow for grounded social reasoning, or enable teaching humans using corrective feedback. Finally, I will conclude the talk by discussing some preliminary results on how large models can be effective pattern machines that can identify patterns in a token invariant fashion and enable pattern transformation, extrapolation, and even show some evidence of pattern optimization for solving control problems.",
                "authors": "Dorsa Sadigh",
                "citations": 0
            },
            {
                "title": "Towards the Atlas of Living Flanders, a Challenging Path",
                "abstract": "In Belgium, a federal country in the heart of Europe, the competencies for nature conservation and nature policy lie within the regions. The Research Institute for Nature and Forest (INBO) is an independent research institute, funded by the Flemish regional government, which underpins and evaluates biodiversity policy and management by means of applied scientific research, and sharing of data and knowledge.\n One of the 12 strategic goals in the 2009-2015 INBO strategic planning was that: 'INBO manages data and makes them accessible. It looks into appropriate data gathering methods and means by which to disseminate data and make them readily available'. Since 2009, the INBO has steadily evolved into a research institute with a strong emphasis on open data and open science. In 2010 INBO became a data publisher for the Global Biodiversity Information Facility (GBIF), adopted an open data and open access policy and is known for being an open science institute in Flanders, Belgium. In 2021, a question arose from the council of ministers on the possibility and availability of a public portal for biodiversity data. The goal of this portal should be to ensure findability, availability, and optimal usability of biodiversity data, initially for policy makers, but also for the wider public. With the Living Atlas project already high on our radar, an analysis project, funded by the Flemish government, started in December 2021. All the entities in the department of 'Environment' contributed to a requirements and feasibility study, a proof of concept (POC) Living Atlas for Flanders was set up and the required budget was calculated.\n During the requirements and feasibility study we questioned the agency for nature and forest (ANB), the Flanders Environment Agency (VMM), Flemish land agency (VLM) and the Department of Environment with the help of a professional inquiry agency IPSOS on the possible relevance for policy of a Flemish biodiversity portal, the need of high resolution data (geographical and temporal scale) and the availability of biodiversity data in Flanders, focussed on key species, protected species and other Flemish priority species.\n During the technical proof of concept, we tested the Living Atlases (LA) software suite as the most mature candidate for a Flemish Living Atlas. We checked how we could set up a LA installation in our own Amazon Web Services (AWS) environment, evaluated all the used technologies, estimated the maintenance and infrastructure cost, the needed profiles and the number of full-time equivalent personnel we would need to run a performant Atlas of Living Flanders.\n The goal of this talk is to inform the audience on the steps we took, the hurdles we encountered and how we are trying to convince our policy makers of the benefits of an Atlas of Living Flanders.",
                "authors": "Dimitri Brosens, Sten Migerode, Aaike De Wever",
                "citations": 0
            },
            {
                "title": "Region-Attention Prompt Learning for CLIP",
                "abstract": " Pre-trained Visual Language Models (VLMs) like CLIP have shown great potential in the multimodal domain. Among this, using different modal contexts and interaction features to construct prompt can stimulate the model’s prior knowledge circuit more accurately, thus generating better outputs. However, in CLIP, the formal mismatch of textual descriptions between the pre-training and inference phases results in a suboptimal representation ability of prompt, which is detrimental to model alignment learning. Therefore, Region-Attention Prompt (RAP) is proposed, which introduces region features to enrich the semantic representation of prompt. RAP is acquired by the Cross-Attention mechanism between images and texts, and it is essentially a region-level prompt with category-sensitive properties. For each category, RAP adaptively assigns greater attention weight to image regions that are more semantically relevant to the category. Besides, CLIP is equipped with RAP (called RA-CLIP) to improve image classification performance in generalization scenarios. Extensive experiments demonstrate that RA-CLIP outperforms the current SOTA CoCoOp 0.4% - 4.16% on base classes and 0.25% - 11.34% on new classes, across 7 datasets. In addition, we show that focusing on category-related regions to construct prompt can further improve the model’s alignment ability.",
                "authors": "Yiming Pan, Hua Cheng, Yiquan Fang, Yufei Liu",
                "citations": 0
            },
            {
                "title": "Biologically-Motivated Learning Model for Instructed Visual Processing",
                "abstract": "As part of understanding how the brain learns, ongoing work seeks to combine biological knowledge and current artificial intelligence (AI) modeling in an attempt to find an efficient biologically plausible learning scheme. Current models of biologically plausible learning often use a cortical-like combination of bottom-up (BU) and top-down (TD) processing, where the TD part carries feedback signals used for learning. However, in the visual cortex, the TD pathway plays a second major role of visual attention, by guiding the visual process to locations and tasks of interest. A biological model should therefore combine the two tasks, and learn to guide the visual process. We introduce a model that uses a cortical-like combination of BU and TD processing that naturally integrates the two major functions of the TD stream. The integrated model is obtained by an appropriate connectivity pattern between the BU and TD streams, a novel processing cycle that uses the TD part twice, and the use of 'Counter-Hebb' learning that operates across the streams. We show that the 'Counter-Hebb' mechanism can provide an exact backpropagation synaptic modification. We further demonstrate the model's ability to guide the visual stream to perform a task of interest, achieving competitive performance compared with AI models on standard multi-task learning benchmarks. The successful combination of learning and visual guidance could provide a new view on combining BU and TD processing in human vision, and suggests possible directions for both biologically plausible models and artificial instructed models, such as vision-language models (VLMs).",
                "authors": "R. Abel, S. Ullman",
                "citations": 0
            },
            {
                "title": "Vision-Language Matching for Text-to-Image Synthesis via Generative Adversarial Networks",
                "abstract": "Text-to-image synthesis is an attractive but challenging task that aims to generate a photo-realistic and semantic consistent image from a specific text description. The images synthesized by off-the-shelf models usually contain limited components compared with the corresponding image and text description, which decreases the image quality and the textual-visual consistency. To address this issue, we propose a novel Vision-Language Matching strategy for text-to-image synthesis, named VLMGAN*, which introduces a dual vision-language matching mechanism to strengthen the image quality and semantic consistency. The dual vision-language matching mechanism considers textual-visual matching between the generated image and the corresponding text description, and visual-visual consistent constraints between the synthesized image and the real image. Given a specific text description, VLMGAN* firstly encodes it into textual features and then feeds them to a dual vision-language matching-based generative model to synthesize a photo-realistic and textual semantic consistent image. Besides, the popular evaluation metrics for text-to-image synthesis are borrowed from simple image generation, which mainly evaluate the reality and diversity of the synthesized images. Therefore, we introduce a metric named Vision-Language Matching Score (VLMS) to evaluate the performance of text-to-image synthesis which can consider both the image quality and the semantic consistency between the synthesized image and the description. The proposed dual multi-level vision-language matching strategy can be applied to other text-to-image synthesis methods. We implement this strategy on two popular baselines, which are marked with ${\\text{VLMGAN}_{+\\text{AttnGAN}}}$ and ${\\text{VLMGAN}_{+\\text{DFGAN}}}$. The experimental results on two widely-used datasets show that the model achieves significant improvements over other state-of-the-art methods.",
                "authors": "Qingrong Cheng, Keyu Wen, X. Gu",
                "citations": 14
            },
            {
                "title": "Relative Sea-Level Rise Scenario for 2100 along the Coast of South Eastern Sicily (Italy) by InSAR Data, Satellite Images and High-Resolution Topography",
                "abstract": "The global sea-level rise (SLR) projections for the next few decades are the basis for developing flooding maps that depict the expected hazard scenarios. However, the spatially variable land subsidence has generally not been considered in the current projections. In this study, we use geodetic data from global navigation satellite system (GNSS), synthetic aperture radar interferometric measurements (InSAR) and sea-level data from tidal stations to show the combined effects of land subsidence and SLR along the coast between Catania and Marzamemi, in south-eastern Sicily (southern Italy). This is one of the most active tectonic areas of the Mediterranean basin, which drives accelerated SLR, continuous coastal retreat and increasing effects of flooding and storms surges. We focus on six selected areas, which show valuable coastal infrastructures and natural reserves where the expected SLR in the next few years could be a potential cause of significant land flooding and morphological changes of the coastal strip. Through a multidisciplinary study, the multi-temporal flooding scenarios until 2100, have been estimated. Results are based on the spatially variable rates of vertical land movements (VLM), the topographic features of the area provided by airborne Light Detection And Ranging (LiDAR) data and the Intergovernmental Panel on Climate Change (IPCC) projections of SLR in the Representative Concentration Pathways RCP 2.6 and RCP 8.5 emission scenarios. In addition, from the analysis of the time series of optical satellite images, a coastal retreat up to 70 m has been observed at the Ciane river mouth (Siracusa) in the time span 2001–2019. Our results show a diffuse land subsidence locally exceeding 10 ± 2.5 mm/year in some areas, due to compacting artificial landfill, salt marshes and Holocene soft deposits. Given ongoing land subsidence, a high end of RSLR in the RCP 8.5 at 0.52 ± 0.05 m and 1.52 ± 0.13 m is expected for 2050 AD and 2100 AD, respectively, with an exposed area of about 9.7 km2 that will be vulnerable to inundation in the next 80 years.",
                "authors": "M. Anzidei, G. Scicchitano, G. Scardino, C. Bignami, C. Tolomei, A. Vecchio, E. Serpelloni, V. Santis, C. Monaco, M. Milella, A. Piscitelli, G. Mastronuzzi",
                "citations": 36
            },
            {
                "title": "Features of new vision-incorporated third-generation video laryngeal mask airways",
                "abstract": null,
                "authors": "A. Van Zundert, S. Gatt, T. V. Van Zundert, C. M. Kumar, Jaideep J. Pandit",
                "citations": 23
            },
            {
                "title": "Lignin Distribution on Cell Wall Micro-Morphological Regions of Fibre in Developmental Phyllostachys pubescens Culms",
                "abstract": "Bamboo is a natural fibre reinforced composite with excellent performance which is, to a certain extent, an alternative to the shortage of wood resources. The heterogeneous distribution and molecular structure of lignin is one of the factors that determines its performance, and it is the key and most difficult component in the basic research into the chemistry of bamboo and in bamboo processing and utilization. In this study, the distribution of lignin components and lignin content in micro-morphological regions were measured in semi-quantitative level by age and radial location by means of visible-light microspectrophotometry (VLMS) coupled with the Wiesner and Maule reaction. There as guaiacyl lignin and syringyl lignin in the cell wall of the fibre. Lignin content of the secondary cell wall and cell corner increased at about 10 days, reached a maximum at 1 year, and then decreased gradually. From 17 days to 4 years, the lignin content of the secondary cell wall in the outer part of bamboo is higher than that in the middle part (which is, in turn, higher than that in the inner part of the bamboo). VLSM results of the micro-morphological regions showed that bamboo lignification developed by aging. Guaiacyl and syringl lignin units can be found in the cell wall of the fibre, parenchyma, and vessel. There was a difference in lignin content among different ages, different radial location, and different micro-morphological regions of the cell wall. The fibre walls were rich in guaiacyl lignin in the early stage of lignification and rich in syringyl units in the later stage of lignification. The guaiacyl and syringyl lignin deposition of bamboo green was earlier than that of the middle part of bamboo culm, and that of the middle part of bamboo culm was earlier than that of bamboo yellow. The single molecule lignin content of the thin layer is higher than that of thick layers, while the primary wall is higher than the secondary cell wall, showing that lignin deposition is consistent with the rules of cell wall formation. The obtained cytological information is helpful to understand the origin of the anisotropic, physical, mechanical, chemical, and machining properties of bamboo.",
                "authors": "Bo Liu, Lina Tang, Qianqian Chen, Liming Zhu, Xianwu Zou, Botao Li, Q. Zhou, Yuejin Fu, Yun Lu",
                "citations": 11
            },
            {
                "title": "Reticular Formation and Pain: The Past and the Future",
                "abstract": "The involvement of the reticular formation (RF) in the transmission and modulation of nociceptive information has been extensively studied. The brainstem RF contains several areas which are targeted by spinal cord afferents conveying nociceptive input. The arrival of nociceptive input to the RF may trigger alert reactions which generate a protective/defense reaction to pain. RF neurons located at the medulla oblongata and targeted by ascending nociceptive information are also involved in the control of vital functions that can be affected by pain, namely cardiovascular control. The RF contains centers that belong to the pain modulatory system, namely areas involved in bidirectional balance (decrease or enhancement) of pain responses. It is currently accepted that the imbalance of pain modulation towards pain facilitation accounts for chronic pain. The medullary RF has the peculiarity of harboring areas involved in bidirectional pain control namely by the existence of specific neuronal populations involved in antinociceptive or pronociceptive behavioral responses, namely at the rostroventromedial medulla (RVM) and the caudal ventrolateral medulla (VLM). Furthermore the dorsal reticular nucleus (also known as subnucleus reticularis dorsalis; DRt) may enhance nociceptive responses, through a reverberative circuit established with spinal lamina I neurons and inhibit wide-dynamic range (WDR) neurons of the deep dorsal horn. The components of the triad RVM-VLM-DRt are reciprocally connected and represent a key gateway for top-down pain modulation. The RVM-VLM-DRt triad also represents the neurobiological substrate for the emotional and cognitive modulation of pain, through pathways that involve the periaqueductal gray (PAG)-RVM connection. Collectively, we propose that the RVM-VLM-DRt triad represents a key component of the “dynamic pain connectome” with special features to provide integrated and rapid responses in situations which are life-threatening and involve pain. The new available techniques in neurobiological studies both in animal and human studies are producing new and fascinating data which allow to understand the complex role of the RF in pain modulation and its integration with several body functions and also how the RF accounts for chronic pain.",
                "authors": "I. Martins, I. Tavares",
                "citations": 79
            },
            {
                "title": "Prompting Large Pre-trained Vision-Language Models For Compositional Concept Learning",
                "abstract": "This work explores the zero-shot compositional learning ability of large pre-trained vision-language models(VLMs) within the prompt-based learning framework and propose a model (\\textit{PromptCompVL}) to solve the compositonal zero-shot learning (CZSL) problem. \\textit{PromptCompVL} makes two design choices: first, it uses a soft-prompting instead of hard-prompting to inject learnable parameters to reprogram VLMs for compositional learning. Second, to address the compositional challenge, it uses the soft-embedding layer to learn primitive concepts in different combinations. By combining both soft-embedding and soft-prompting, \\textit{PromptCompVL} achieves state-of-the-art performance on the MIT-States dataset. Furthermore, our proposed model achieves consistent improvement compared to other CLIP-based methods which shows the effectiveness of the proposed prompting strategies for CZSL.",
                "authors": "Guangyue Xu, Parisa Kordjamshidi, J. Chai",
                "citations": 9
            },
            {
                "title": "Leveraging per Image-Token Consistency for Vision-Language Pre-training",
                "abstract": "Most existing vision-language pre-training (VLP) approaches adopt cross-modal masked language modeling (CMLM) to learn vision-language associations. However, we find that CMLM is insufficient for this purpose according to our observations: (1) Modality bias: a considerable amount of masked tokens in CMLM can be recovered with only the language information, ignoring the visual inputs. (2) Underutilization of the unmasked tokens: CMLM primarily focuses on the masked token but it cannot simultaneously leverage other tokens to learn vision-language associations. To handle those limitations, we propose EPIC (lEveraging Per Image-Token Consistency for vision-language pre-training). In EPIC, for each image-sentence pair, we mask tokens that are salient to the image (i.e., Saliency-based Masking Strategy) and replace them with alternatives sampled from a language model (i.e., Inconsistent Token Generation Procedure), and then the model is required to determine for each token in the sentence whether it is consistent with the image (i.e., Image-Token Consistency Task). The proposed EPIC method is easily combined with pre-training methods. Extensive experiments show that the combination of the EPIC method and state-of-the-art pre-training approaches, including ViLT, ALBEF, METER, and X-VLM, leads to significant improvements on downstream tasks. Our coude is released at https://github.com/gyhdog99/epic",
                "authors": "Yunhao Gou, Tom Ko, Hansi Yang, J. Kwok, Yu Zhang, Mingxuan Wang",
                "citations": 8
            },
            {
                "title": "MOMA-LRG: Language-Refined Graphs for Multi-Object Multi-Actor Activity Parsing",
                "abstract": "Video-language models (VLMs), large models pre-trained on numerous but noisy video-text pairs from the internet, have revolutionized activity recognition through their remarkable generalization and open-vocabulary capabilities. While complex human activities are often hierarchical and compositional, most existing tasks for evaluating VLMs focus only on high-level video understanding, making it difficult to accurately assess and interpret the ability of VLMs to understand complex and fine-grained human activities. Inspired by the recently proposed MOMA framework, we define activity graphs as a single universal representation of human activities that encompasses video understanding at the activity, subactivity, and atomic action level. We redefine activity parsing as the overarching task of activity graph generation, requiring understanding human activities across all three levels. To facilitate the evaluation of models on activity parsing, we introduce MOMA-LRG (Multi-Object Multi-Actor Language-Refined Graphs), a large dataset of complex human activities with activity graph annotations that can be readily transformed into natural language sentences. Lastly, we present a model-agnostic and lightweight approach to adapting and evaluating VLMs by incorporating structured knowledge from activity graphs into VLMs, addressing the individual limitations of language and graphical models. We demonstrate strong performance on activity parsing and few-shot video classification, and our framework is intended to foster future research in the joint modeling of videos, graphs, and language.",
                "authors": "Zelun Luo, Zane Durante, Linden Li, Wanze Xie, Ruochen Liu, Emily Jin, Zhuoyi Huang, Lun Yu Li, Jiajun Wu, Juan Carlos Niebles, E. Adeli, Li Fei-Fei",
                "citations": 8
            },
            {
                "title": "Correlation Information Bottleneck: Towards Adapting Pretrained Multimodal Models for Robust Visual Question Answering",
                "abstract": null,
                "authors": "Jingjing Jiang, Zi-yi Liu, Nanning Zheng",
                "citations": 7
            },
            {
                "title": "Pre-Trained Word Embedding and Language Model Improve Multimodal Machine Translation: A Case Study in Multi30K",
                "abstract": "Multimodal machine translation (MMT) is an attractive application of neural machine translation (NMT) that is commonly incorporated with image information. However, the MMT models proposed thus far have only comparable or slightly better performance than their text-only counterparts. One potential cause of this infeasibility is a lack of large-scale data. Most previous studies mitigate this limitation by employing large-scale textual parallel corpora, which are more accessible than multimodal parallel corpora, in various ways. However, these corpora are still available on only a limited scale in low-resource language pairs or domains. In this study, we leveraged monolingual (or multimodal monolingual) corpora, which are available at scale in most languages and domains, to improve MMT models. Our approach follows that of previous unimodal works that use monolingual corpora to train the word embedding or language model and incorporate them into NMT systems. While these methods demonstrated the advantage of using pre-trained representations, there is still room for MMT models to improve. To this end, our system employs debiasing procedures for the word embedding and multimodal extension of the language model (visual-language model, VLM) to make better use of the pre-trained knowledge in the MMT task. The results of evaluations conducted on the de facto MMT dataset for the English–German translation indicate that the improvement obtained using well-tailored word embedding and VLM is approximately +1.84 BLEU and +1.63 BLEU, respectively. The evaluation on multiple language pairs reveals their adoptability across the languages. Beyond the success of our system, we also conducted an extensive analysis on VLM manipulation and showed promising areas for developing better MMT models by exploiting VLM; some benefits brought by either modality are missing, and MMT with VLM generates less fluent translations. Our code is available at https://github.com/toshohirasawa/mmt-with-monolingual-data.",
                "authors": "Tosho Hirasawa, Masahiro Kaneko, Aizhan Imankulova, Mamoru Komachi",
                "citations": 6
            },
            {
                "title": "VIPHY: Probing \"Visible\" Physical Commonsense Knowledge",
                "abstract": "In recent years, vision-language models (VLMs) have shown remarkable performance on visual reasoning tasks (e.g. attributes, location). While such tasks measure the requisite knowledge to ground and reason over a given visual instance, they do not, however, measure the ability of VLMs to retain and generalize such knowledge. In this work, we evaluate their ability to acquire\"visible\"physical knowledge -- the information that is easily accessible from images of static scenes, particularly across the dimensions of object color, size and space. We build an automatic pipeline to derive a comprehensive knowledge resource for calibrating and probing these models. Our results indicate a severe gap between model and human performance across all three tasks. Furthermore, our caption pretrained baseline (CapBERT) significantly outperforms VLMs on both size and spatial tasks -- highlighting that despite sufficient access to ground language with visual modality, they struggle to retain such knowledge. The dataset and code are available at https://github.com/Axe--/ViPhy .",
                "authors": "Shikhar Singh, Ehsan Qasemi, Muhao Chen",
                "citations": 6
            },
            {
                "title": "First steps of planet formation around very low mass stars and brown dwarfs",
                "abstract": null,
                "authors": "P. Pinilla",
                "citations": 6
            },
            {
                "title": "Orthostatic hypotension with nondipping: phenotype of neurodegenerative disease",
                "abstract": null,
                "authors": "M. Nagai, Masaya Kato, K. Dote",
                "citations": 4
            },
            {
                "title": "FashionVQA: A Domain-Specific Visual Question Answering System",
                "abstract": "Humans apprehend the world through various sensory modalities, yet language is their predominant communication channel. Machine learning systems need to draw on the same multimodal richness to have informed discourses with humans in natural language; this is particularly true for systems specialized in visually-dense information, such as dialogue, recommendations, and search engines for clothing. To this end, we train a visual question-answering (VQA) system to answer complex natural language questions about apparel in fashion photoshoot images. The key to the successful training of our VQA model is the automatic creation of a visual question-answering dataset with 168 million samples from item attributes of 207 thousand images using diverse templates. The sample generation employs a strategy that considers the difficulty of the question-answer pairs to emphasize challenging concepts. We see that using the same transformer for encoding the question and decoding the answer, as in language models, achieves maximum accuracy, showing that visual language models (VLMs) make the optimal visual question-answering systems for our dataset. The accuracy of the best model surpasses the human expert level. Our approach for generating a large-scale multimodal domain-specific dataset provides a path for training specialized models capable of communicating in natural language. The training of such domain-expert models, e.g., our fashion VLM model, cannot rely solely on the large-scale general-purpose datasets collected from the web.",
                "authors": "Min Wang, A. Mahjoubfar, Anupama Joshi",
                "citations": 3
            },
            {
                "title": "Components of 21 years (1995–2015) of absolute sea level trends in the Arctic",
                "abstract": "Abstract. The Arctic Ocean is at the frontier of the fast-changing climate in the northern latitudes, and sea level trends are a bulk measure of ongoing\nprocesses related to climate change. Observations of sea level in the Arctic Ocean are nonetheless difficult to validate with independent measurements,\nand this is globally the region where the sea level trend (SLT) is most uncertain. The aim of this study is to create a satellite-independent\nreconstruction of Arctic SLT, as it is observed by altimetry and tide gauges (TGs). Previous studies use Gravity Recovery and Climate Experiment\n(GRACE) observations to estimate the manometric (mass component of) SLT. GRACE estimates, however, are challenged by large mass changes on land, which\nare difficult to separate from much smaller ocean mass changes. Furthermore, GRACE is not available before 2003, which significantly limits the period\nand makes the trend more vulnerable to short-term changes. As an alternative approach, this study estimates the climate-change-driven Arctic\nmanometric SLT from the Arctic sea level fingerprints of glaciers, Greenland, Antarctica and glacial isostatic adjustment (GIA) with the addition of the\nlong-term inverse barometer (IB) effect. The halosteric and thermosteric components complete the reconstructed Arctic SLT and are estimated by\ninterpolating 300 000 temperature (T) and salinity (S) in situ observations. The SLT from 1995–2015 is compared to the observed SLT from altimetry and 12 selected tide gauges (TGs) corrected for vertical land movement\n(VLM). The reconstructed estimate manifests the salinity-driven halosteric component as dominating the spatial SLT pattern with variations\nbetween −7 and 10 mm yr−1. The manometric SLT in comparison is estimated to be 1–2 mm yr−1 for most of the Arctic Ocean. The\nreconstructed SLT shows a larger sea level rise in the Beaufort Sea compared to altimetry, an issue that is also identified by previous studies. There is a\nTG-observed sea level rise in the Siberian Arctic in contrast to the sea level fall from the reconstructed and altimetric estimate. From 1995–2015 the reconstructed SLT agrees within the 68 % confidence interval with the SLT from observed altimetry in 87 % of the\nArctic between 65∘ N and 82∘ N (R=0.50) and with 5 of 12 TG-derived (VLM-corrected) SLT estimates. The residuals are seemingly\nsmaller than results from previous studies using GRACE estimates and modeled T–S data. The spatial correlation of the reconstructed SLT to\naltimetric SLT during the GRACE period (2003–2015) is R=0.38 and R=0.34/R=0.37 if GRACE estimates are used instead of the constructed\nmanometric component. Thus, the reconstructed manometric component is suggested as a legitimate alternative to GRACE that can be projected into the\npast and future.\n",
                "authors": "Carsten Bjerre Ludwigsen, O. Andersen, S. Rose",
                "citations": 3
            },
            {
                "title": "A software architecture perspective about Moodle flexibility for supporting empirical research of teaching theories",
                "abstract": null,
                "authors": "M. Campo, Analía Amandi, Julio Cesar Biset",
                "citations": 27
            },
            {
                "title": "Differential Ascending Projections From the Male Rat Caudal Nucleus of the Tractus Solitarius: An Interface Between Local Microcircuits and Global Macrocircuits",
                "abstract": "To integrate and broadcast neural information, local microcircuits and global macrocircuits interact within certain specific nuclei of the central nervous system. The structural and functional architecture of this interaction was determined for the caudal nucleus of the tractus solitarius (NTS) at the level of the area postrema (AP), a relay station of peripheral viscerosensory information that is processed and conveyed to brain regions concerned with autonomic-affective and other interoceptive reflexive functions. Axon collaterals of most small NTS cells (soma <150 μm2) establish excitatory or inhibitory local microcircuits likely to control the activity of nearby NTS cells and to transfer peripheral signals to efferent projection neurons. At least two types of cells that constitute efferent pathways from the caudal NTS (cNTS) were distinguished: (1) a greater numbers of small cells, seemingly forming local excitatory microcircuits via recurrent axon collaterals, that project specifically and unidirectionally to the lateral parabrachial nucleus; and (2) a much smaller numbers of cells likely to establish multiple global connections, mostly via the medial forebrain bundle (MFB) or the dorsal longitudinal fascicle (DLF), with a wide range of brain regions, including the ventrolateral medulla (VLM), hypothalamus, central nucleus of the amygdala (ACe), bed nucleus of the stria terminalis (BNST), spinal cord dorsal horn, brainstem reticular formation, locus coeruleus (LC), periaqueductal gray (PAG) and periventricular diencephalon (including the epithalamus). The evidence presented here suggests that distinct cNTS cell types distinguished by projection pattern and related structural and functional features participate differentially in the computation of viscerosensory information and coordination of global macro-networks in a highly organized manner.",
                "authors": "Y. Kawai",
                "citations": 42
            },
            {
                "title": "Size and structures of disks around very low mass stars in the Taurus star-forming region",
                "abstract": "We aim to estimate if structures, such as cavities, rings, and gaps, are common in disks around VLMS and to test models of structure formation in these disks. We also aim to compare the radial extent of the gas and dust emission in disks around VLMS, which can give us insight about radial drift. We studied six disks around VLMS in the Taurus star-forming region using ALMA Band 7 ($\\sim 340\\,$GHz) at a resolution of $\\sim0.1''$. The targets were selected because of their high disk dust content in their stellar mass regime. Our observations resolve the disk dust continuum in all disks. In addition, we detect the $^{12}$CO ($J=3-2$) emission line in all targets and $^{13}$CO ($J=3-2$) in five of the six sources. The angular resolution allows the detection of dust substructures in three out of the six disks, which we studied by using UV-modeling. Central cavities are observed in the disks around stars MHO\\,6 (M5.0) and CIDA\\,1 (M4.5), while we have a tentative detection of a multi-ringed disk around J0433. Single planets of masses $0.1\\sim0.4\\,M_{\\rm{Jup}}$ would be required. The other three disks with no observed structures are the most compact and faintest in our sample. The emission of $^{12}$CO and $^{13}$CO is more extended than the dust continuum emission in all disks of our sample. When using the $^{12}$CO emission to determine the gas disk extension $R_{\\rm{gas}}$, the ratio of $R_{\\rm{gas}}/R_{\\rm{dust}}$ in our sample varies from 2.3 to 6.0, which is consistent with models of radial drift being very efficient around VLMS in the absence of substructures. Our observations do not exclude giant planet formation on the substructures observed. A comparison of the size and luminosity of VLMS disks with their counterparts around higher mass stars shows that they follow a similar relation.",
                "authors": "N. Kurtovic, P. Pinilla, F. Long, M. Benisty, C. Manara, A. Natta, I. Pascucci, L. Ricci, A. Scholz, L. Testi",
                "citations": 25
            },
            {
                "title": "Harmonics and decaying DC estimation using Volterra LMS/F algorithm",
                "abstract": "Real time estimation of power system harmonics requires the proper choice of filter structure and algorithm for coefficient adjustment. Though LMS (Least Mean Square) adaptive filter has an elegant and simple structure; the performance degrades in low SNR and non-stationary condition. On the other hand LMF (Least Mean Fourth) adaptive filter provides better estimation but the performance is limited due to high computational complexity. Thus LMF is not a suitable choice for real time estimation of harmonics. In this paper; combined LMS/F adaptive filter is modified using Volterra expansion of input samples up to second order to form VLMS/F filter. This filter offers advantage in terms of estimation accuracy and computational complexity in a low SNR condition. Estimation results of VLMS/F are compared with other variants of LMS for harmonic parameters and decaying DC.",
                "authors": "Umamani Subudhi, H. K. Sahoo",
                "citations": 44
            },
            {
                "title": "Prefrontal‐medullary circuit activation attenuates stress reactivity",
                "abstract": "Organismal survival and adaptation to stress rely on brainstem catecholaminergic neurons. In particular, catecholaminergic neurons in the rostral ventrolateral medulla (RVLM) drive sympathetic activity and enable physiological adaptations, including vasoconstriction, corticosterone release, and glycemic mobilization. However, it is unclear how brain regions involved in the cognitive appraisal of stress regulate the activity of RVLM neurons. Our previous studies found that the rodent infralimbic prefrontal cortex (IL) integrates behavioral and physiological responses to stress. Thus, a potential IL‐to‐RVLM connection would represent a crucial link between stress appraisal and sympathetic reactivity. In the current study, we investigated a direct IL‐to‐RVLM circuit by targeting a genetically‐encoded anterograde tracer under the control of the CaMKIIα promoter to the IL of adult male and female rats. Analysis revealed that IL terminals apposed RVLM neurons expressing the catecholamine‐synthesizing enzyme dopamine beta hydroxylase. Further, IL input to catecholamine cells was widespread throughout the rostral to caudal extent of the VLM in both male and female rodents. Quantification of innervation density revealed that males had a larger proportion of VLM catecholamine neurons receiving IL inputs relative to female rats. Additionally, IL appositions were identified on GABAergic and glycinergic neurons in both sexes. Accordingly, we hypothesized that IL projections may activate local RVLM inhibitory cells to limit sympathetic output. To test this hypothesis, we injected a viral vector coding for channelrhodopsin‐2 (ChR2) in the IL of males and females. Next, a fiber optic cannula was implanted dorsal to the RVLM to evoke IL synaptic glutamate release. Animals then received photostimulation during restraint stress with blood sampled to determine stress reactivity. Compared to controls, male rats expressing ChR2 on IL terminals had suppressed glycemic stress responses (p < 0.05). In contrast, stimulation of the IL‐RVLM circuit in females did not affect glucose mobilization (p < 0.05). However, ChR2 decreased corticosterone responses to stress relative to control rats in both sexes (males, p < 0.01; females, p < 0.05). Thus, both male and female rats have a direct circuit from the IL portion of the mPFC to catecholamine‐synthesizing cells of the RVLM that limits glucocorticoid stress responses, likely through the activation of local inhibitory neurons. However, the density of this circuit is greater in males, potentially accounting for reduced glycemic responses. Ultimately, excitatory/inhibitory balance at IL synapses in the RVLM may be critical for the health consequences of stress.",
                "authors": "Sebastian A. Pace, Ema Lukinic, Tyler Wallace, Derek Schaeuble, Jacob Moore, Brent Myers",
                "citations": 1
            },
            {
                "title": "Non-Classical Crystal Morphology and Secondary Phase Directed Growth of Tetragonal SnO Microcrystals",
                "abstract": "Sn(II) oxide (SnO) is a metastable two-dimensional layered oxide, composed of SnO pyramidal units with an inter-layer spacing 4.84 Å. In literature, another metastable phase was also found during oxidation of epitaxial SnO films having an orthorhombic structure [1], equivalent to the high-pressure phase of SnO 2 , stabilized at ambient conditions through strain [2]. On the other hand, Moreno et al. have shown that SnO is an intrinsically cation deficient non-stoichiometric phase, accommodated by static displacement waves which give rise to a tweed contrast in electron micrographs. This strain coupling caused by metal vacancies is predicted to stabilize the highly disordered nonstoichiometric phase, but open questions regarding charge localization and oxidation states remain [3]. Here, a wet-chemical approach to synthesize SnO single crystalline sheets with an average thickness of ~50 nm having lateral dimension in the micron scale is reported. The microstructural features of the sheets are studied using various electron microscopy techniques. The observed features agree with that in literature and paves path for further studies. SEM shows the sheets to be textured along the [001] direction agreeing with the reported surface energies for the layered crystal. VLM of the sheets shows a 4-fold symmetric fringe and AFM of the sheets reveals an inverse-pyramid morphology. Co-relating the VLM and AFM data allows us to conclude that the fringes seen is due to interference inside the crystal and the synthesis is morphologically uniform. Further, SEM and TEM imaging reveals that the growth occurs via dendrites. Electron diffraction",
                "authors": "J. Koushik, R. Rai, N. Ravishankar",
                "citations": 1
            },
            {
                "title": "A One Man Army- TrueNat Testing for the Identification of COVID-19 in Firozabad, Uttar Pradesh, India",
                "abstract": "Introduction: The Coronavirus Disease 2019 (COVID-19) pandemic has affected the entire world. The need of timely detection of the virus has been of prime importance and the efforts to develop sensitive, specific, rapid, portable and cost effective diagnostic methods promoted the indigenous development of TrueNat testing for viral load in COVID-19 detection which had been previously designed for detection of Tuberculosis and other infectious organisms. Aim: To see the importance of TrueNat testing among symptomatic and asymptomatic cases in different age groups and gender. Materials and Methods: In this retrospective study conducted in the Department of Microbiology, Autonomous State Medical College and SNM Hospital, Firozabad, Uttar Pradesh, India, from June 2020 - May 2021, a total of 4,659 samples were collected from patients (Influenza Like Illness (ILI), Severe Acute Respiratory Illness (SARI), symptomatic, asymptomatic, those seeking hospitalisation, emergency), contacts and travellers and were subjected to testing by TrueNat (Molbio Quattro). The cases were divided into group A of patients who presented with symptoms ≤7 days; group B of patients who presented with signs and symptoms >7 days and group C comprised of asymptomatic patients. The symptoms of patients were associated with the Cycle threshold (Ct) values of the Envelope (E) gene and the RNAdependent RNA polymerase gene (RdRp) gene. The Chi-square test was done to test the statistical significance of association of symptomatic patients with the outcome of the test. Results: The maximum number of positive cases was found in the age group 20-39 years (p-value <0.05). The least positivity was found in the higher (80 years) and lower (below 9 years) age groups. The positivity rates had no significant impact on the gender. The percentage positivity as detected by Truenat testing was 3.3% and maximum positive patients were found in the group having symptoms <7 days (p<0.05). On association of the Ct values of E gene and RdRp gene with the symptoms it was found that 28.1% and 27.2% of the patients were in the high Ct value group. Conclusion: TrueNat was found to be a portable and easy to perform test which did not require special laboratory set up. The use of Viral Lysis Medium (VLM) reduced the time of RNA extraction which not only rendered it safer to perform but expedited the results.",
                "authors": "Lekha Tuli, Rohit Patawa",
                "citations": 1
            },
            {
                "title": "VLMS based Channel Estimator for OTFS VLC System",
                "abstract": "Visible light communication (VLC) is an eco-friendly and low-cost emerging technology for beyond 5G communication systems. However, it has been found that the performance of visible light based communication system deteriorates significantly due to the non-linearity of light-emitting-diode (LED) and Doppler spread due to relative mobility between the transmitter and the receiver. In contrast to the conventional orthogonal frequency division multiplexing (OFDM) scheme, recently proposed orthogonal time frequency space (OTFS) modulation scheme addresses the problem of distortion due to mobility. However, LED non-linearity causes degradation in the channel estimation and overall bit error rate (BER) performance of the VLC system. These non-linear distortions significantly deteriorate the signal reception, thereby resulting in poor estimation of channel state information (CSI). Traditional linear channel estimation schemes such as zero-forcing (ZF) and minimum mean square error (MMSE) have been proposed for OTFS, however, they perform poorly in the presence of non-linearity. In this paper, we have analyzed the performance of OTFS employing Volterra least mean square (VLMS) based channel estimator for non-linear VLC system. Simulations performed over the mobile VLC channel model modelled by random way-point channel model indicate that OTFS with VLMS can be employed for channel estimation for VLC system impaired due to user mobility and non-linear LED characteristic.",
                "authors": "Anupma Sharma, V. Bhatia",
                "citations": 1
            },
            {
                "title": "Plasma Nitriding and Its Effect on the Corrosion Resistance of Stainless Steel 1.4006",
                "abstract": ": The plasma nitriding (PN) technology was applied on the martensitic stainless steel 1.4006. The influence of PN on the corrosion resistance of selected material was investigated. The chemical composition of selected steel was verified using the Q4 TASMAN device. The PN process was performed using two stage nitriding procedure. After plasma cleaning procedure at 515 °C for 45 min in a nitriding atmosphere ratio 20H2:2N2 (l/h) was the first stage nitriding procedure performed at 520°C for 16 hours in a nitriding atmosphere ratio 25H2:5N2 (l/h) and followed by the second stage of nitriding procedure performed at 525°C for 4 hours in a nitriding atmosphere ratio 28H2:4N2 (l/h). The microstructure and mechanical properties of the nitride layers were studied using OES spectrometry, optical microscopy, and hardness testing. The depths of plasma nitride layers were also estimated using a cross-sectional microhardness profiles measurement. The corrosion resistance testing of PN stainless steel 1.4006 samples were carried out in a 5 % neutral sodium chloride solution (NSS) in accordance with ISO 9227 standard in the VLM GmbH SAL 400-FL corrosion chamber and visually evaluated. Microhardness and surface hardness of experimental samples were significantly increased, but the corrosion resistance remarkably decreased.",
                "authors": "M. Krbaťa, Robert Ciger",
                "citations": 0
            },
            {
                "title": "Long-term sea-level variability along the coast of Japan during the 20th century revealed by a 1/10\n \n \n \n $$^{\\circ }$$\n \n \n \n ∘\n \n \n OGCM",
                "abstract": null,
                "authors": "H. Nakano, Shogo L. Urakawa, K. Sakamoto, T. Toyoda, Yuma Kawakami, G. Yamanaka",
                "citations": 0
            },
            {
                "title": "KITE FOIL MAST VENTILATION STUDY",
                "abstract": "Considering the evolution of the racing sailing yacht in the last decade, we have seen the increasingly extensive use of hydrofoil systems able to support and ﬂy boats over the free surface. The great advantage of these systems is to increase comfort in navigation and to reduce drag. Unfortunately, these systems, in addition to the great advantages in terms of eﬃciency, bring with them problems linked above all to their functioning between two ﬂuids, air and water. In fact, the hydrofoils systems are subjected to natural ventilation and cavitation. In particular, the phenomenon of ventilation is typically present when there is a surface piercing strut that includes air and water in particular conditions of use; the geometry and physical conditions allow the creation of a region with a lower pressure than the atmospheric one, which then causes a cavity connected to the external environment. Ventilation is therefore an important phenomenon to be taken into consideration when designing hydrofoil appendages for racing boats and understanding the phenomena is fundamental for the success of the project. Using the numerical simulation, in this case CFD, it is possible to investigate the favorable conditions of formation of the ventilated cavity for the conditions of use of a foil appendage. In order to use CFD as a forecasting and design tool, it was necessary to carry out a validation campaign using a reference benchmark; the results of the investigation made it possible to ﬁne-tune the CFD tool to be able to predict the phenomenon of ventilation in a robust manner. By applying the method developed on a kite foil surface piercing strut case, it was possible to estimate the performance diﬀerences of 2D sections and planform shapes to understand the ventilation tolerance of new candidate designs for construction. Furthermore, it was possible to visualize the ventilation trend by means of numerical indices able to visually show the behavior of one design compared to another. These methods could be used together with low ﬁdelity methods (VLM, panel code, lifting line) to build response surfaces or surrogate models to be used in performances prediction..",
                "authors": "S. Bartesaghi, Giorgio Provinciali, Franco Lovato",
                "citations": 0
            },
            {
                "title": "Circumbinary and circumstellar discs around the eccentric binary IRAS 04158+2805 — a testbed for binary–disc interaction",
                "abstract": "IRAS 04158+2805 has long been thought to be a very low mass T-Tauri star (VLMS) surrounded by a nearly edge-on, extremely large disc. Recent observations revealed that this source hosts a binary surrounded by an extended circumbinary disc with a central dust cavity. In this paper, we combine ALMA multi-wavelength observations of continuum and 12CO line emission, with Hα imaging and Keck astrometric measures of the binary to develop a coherent dynamical model of this system. The system features an azimuthal asymmetry detected at the western edge of the cavity in Band 7 observations and a wiggling outflow. Dust emission in ALMA Band 4 from the proximity of the individual stars suggests the presence of marginally resolved circumstellar discs. We estimate the binary orbital parameters from the measured arc of the orbit from Keck and ALMA astrometry. We further constrain these estimates using considerations from binary-disc interaction theory. We finally perform three SPH gas + dust simulations based on the theoretical constraints; we post-process the hydrodynamic output using radiative transferMonte Carlomethods and directly compare themodels with observations. Our results suggest that a highly eccentric e ∼ 0.5–0.7 equal mass binary, with a semi-major axis of ∼ 55 au, and small/moderate orbital plane vs. circumbinary disc inclination θ . 30◦ provides a good match with observations. A dust mass of ∼ 1.5× 10−4M best reproduces the flux in Band 7 continuum observations. Synthetic CO line emission maps qualitatively capture both the emission from the central region and the non-Keplerian nature of the gas motion in the binary proximity.",
                "authors": "E. Ragusa, D. Fasano, C. Toci, G. Duchêne, N. Cuello, M. Villenave, G. V. D. Plas, G. Lodato, F. M'enard, Daniel J. Price, C. Pinte, K. Stapelfeldt, S. Wolff",
                "citations": 7
            },
            {
                "title": "Development of Unified High-Fidelity Flight Dynamic Modeling Technique for Unmanned Compound Aircraft",
                "abstract": "This study presents the unified high-fidelity flight dynamic modeling technique for compound aircraft. The existing flight dynamic modeling technique is absolutely depended on the experimental data measured by wind tunnel. It means that the existing flight dynamic model cannot be used for analyzing a new configuration aircraft. The flight dynamic modeling has to be implemented when a performance analysis has to be performed for new type aircraft. This technique is not effective for analyzing the performance of the new configuration aircraft because the shapes of compound aircraft are very various. The unified high-fidelity flight dynamic modeling technique is developed in this study to overcome the limitation of the existing modeling technique. First, the unified rotor and wing models are developed to calculate the aerodynamic forces generated by rotors and wings. The revolutions per minute (RPM) and pitch change with rotation direction are addressed by rotor models. The unified wing model calculates the induced velocity by using the vortex lattice method (VLM) and the Biot–Savart law. The aerodynamic forces and moments for wings and rotors are computed by strip theory in each model. Second, the performance analysis such as propeller performance and trim for compound aircraft is implemented to check the accuracy between the proposed modeling technique and the helicopter trim, linearization, and simulation (HETLAS) program which is validated. It is judged that this study raises the efficiency of aircraft performance analysis and the airworthiness evaluation.",
                "authors": "Do hyeon Lee, Chang-joo Kim, Seong Han Lee",
                "citations": 7
            },
            {
                "title": "Revisiting Vertical Land Motion and Sea Level Trends in the Northeastern Adriatic Sea Using Satellite Altimetry and Tide Gauge Data",
                "abstract": "We propose a revisited approach to estimating sea level change trends based on the integration of two measuring systems: satellite altimetry and tide gauge (TG) time series of absolute and relative sea level height. Quantitative information on vertical crustal motion trends at six TG stations of the Adriatic Sea are derived by solving a constrained linear inverse problem. The results are verified against Global Positioning System (GPS) estimates at some locations. Constraints on the linear problem are represented by estimates of relative vertical land motion between TG couples. The solution of the linear inverse problem is valid as long as the same rates of absolute sea level rise are observed at the TG stations used to constrain the system. This requirement limits the applicability of the method with variable absolute sea level trends. The novelty of this study is that we tried to overcome such limitations, subtracting the absolute sea level change estimates observed by the altimeter from all relevant time series, but retaining the original short-term variability and associated errors. The vertical land motion (VLM) solution is compared to GPS estimates at three of the six TGs. The results show that there is reasonable agreement between the VLM rates derived from altimetry and TGs, and from GPS, considering the different periods used for the processing of VLM estimates from GPS. The solution found for the VLM rates is optimal in the least square sense, and no longer depends on the altimetric absolute sea level trend at the TGs. Values for the six TGs’ location in the Adriatic Sea during the period 1993–2018 vary from −1.41 ± 0.47 mm y−1 (National Research Council offshore oceanographic tower in Venice) to 0.93 ± 0.37 mm y−1 (Rovinj), while GPS solutions range from −1.59 ± 0.65 (Venice) to 0.10 ± 0.64 (Split) mm y−1. The absolute sea level rise, calculated as the sum of relative sea level change rate at the TGs and the VLM values estimated in this study, has a mean of 2.43 mm y−1 in the period 1974–2018 across the six TGs, a mean standard error of 0.80 mm y−1, and a sample dispersion of 0.18 mm y−1.",
                "authors": "F. De Biasio, G. Baldin, S. Vignudelli",
                "citations": 16
            },
            {
                "title": "Devising Gamification for Vocabulary Development and Motivation: An Experimental, Mixed-Model Study 1",
                "abstract": "This study aims to scrutinize the effects of gamified Turkish vocabulary teaching on vocabulary development, and motivation on learning words. A nested mixed-method experimental design (gamification-based vocabulary learning for the experimental group instructor-led vocabulary learning for the control group) and was conducted with 34 fourth-grade students. Data were collected using the Vocabulary Achievement Test (VAT) and the Vocabulary Learning Motivation Scale (VLMS), interview forms, diaries and activity notebooks, video recordings, and researchers’ diaries. Significant differences were found between the pretest and posttest scores of the gamification-based and instructor-led vocabulary learning groups measuring vocabulary achievements and motivation. There were considerable differences in favor of the experimental group between mean pretest and posttest scores of the experimental and control groups for vocabulary achievement, whereas no significant differences were found in terms of motivation. It was found that experimental group’s vocabulary achievement, awareness and competencies were improved; as well as their willingness and motivation to engage in vocabulary learning, use of previously unknown words, vocabulary development, and embracing of the gamification application and its components, and also showed indicators of intrinsic and extrinsic motivation. On the other hand, quantitative results of the control group were at acceptable levels, although their qualitative results were poorer in terms of generating themes and being able to evaluate learning progress. The study was concluded with recommendations for practitioners and future research.",
                "authors": "Berrin Genç Ersoy, Ş. Dilek, Belet Boyacı",
                "citations": 5
            },
            {
                "title": "SPECIAL VINBERG CONES",
                "abstract": null,
                "authors": "D. V. Alekseevsky, V. Cortes",
                "citations": 5
            },
            {
                "title": "The nature of the giant exomoon candidate Kepler-1625 b-i",
                "abstract": "The recent announcement of a Neptune-sized exomoon candidate around the transiting Jupiter-sized object Kepler-1625 b could indicate the presence of a hitherto unknown kind of gas giant moons, if confirmed. Three transits have been observed, allowing radius estimates of both objects. Here we investigate possible mass regimes of the transiting system that could produce the observed signatures and study them in the context of moon formation in the solar system, i.e. via impacts, capture, or in-situ accretion. The radius of Kepler-1625 b suggests it could be anything from a gas giant planet somewhat more massive than Saturn (0.4 M_Jup) to a brown dwarf (BD) (up to 75 M_Jup) or even a very-low-mass star (VLMS) (112 M_Jup ~ 0.11 M_sun). The proposed companion would certainly have a planetary mass. Possible extreme scenarios range from a highly inflated Earth-mass gas satellite to an atmosphere-free water-rock companion of about 180 M_Ear. Furthermore, the planet-moon dynamics during the transits suggest a total system mass of 17.6_{-12.6}^{+19.2} M_Jup. A Neptune-mass exomoon around a giant planet or low-mass BD would not be compatible with the common mass scaling relation of the solar system moons about gas giants. The case of a mini-Neptune around a high-mass BD or a VLMS, however, would be located in a similar region of the satellite-to-host mass ratio diagram as Proxima b, the TRAPPIST-1 system, and LHS 1140 b. The capture of a Neptune-mass object around a 10 M_Jup planet during a close binary encounter is possible in principle. The ejected object, however, would have had to be a super-Earth object, raising further questions of how such a system could have formed. In summary, this exomoon candidate is barely compatible with established moon formation theories. If it can be validated as orbiting a super-Jovian planet, then it would pose an exquisite riddle for formation theorists to solve.",
                "authors": "R. Heller",
                "citations": 27
            },
            {
                "title": "Cancer Risk in Klippel-Trenaunay Syndrome.",
                "abstract": "Background: Klippel-Trenaunay syndrome (KTS) is an overgrowth syndrome defined by capillary/venous/lymphatic malformations (CVLM) with soft tissue and/or bone hypertrophy. Whether KTS predisposes to cancer is not clear. Methods and Results: We surveyed members of the K-T Support Group (KTSG) and reviewed PubMed for \"Klippel Trenaunay Syndrome\" or \"CVLM\" and \"cancer.\" Individuals with cancer were reviewed for confirmation of KTS, tumor type, location, and age at presentation. Of 223 KTSG respondents, 24 (10.8%) reported 26 malignancies or benign brain tumors (diagnosed from 6 months to 68 years of age, median 41 years), including 3 who were younger than 18 years (2 with Wilms tumor). Nine of twenty-six cancers were basal cell carcinomas (4% of respondents). From 475 articles, we identified 11 cancers or brain tumors in 10 individuals with KTS. Four of these were in children (Wilms tumor n = 2; rhabdomyosarcoma n = 1; serous borderline tumor n = 1). Tumors in adults included basal cell carcinoma (n = 1), squamous cell carcinoma of skin (n = 2), and angiosarcoma, Hodgkin disease, glioblastoma, malignant hemangiopericytoma in one patient each. Ulceration or lymphedema associated with VLM or capillary malformations were associated with some basal cell or squamous cell carcinomas and angiosarcomas. Conclusions: The risk of embryonal cancer other than Wilms tumor in children with KTS does not appear to be higher than in the general population. Wilms tumor incidence is under 5%, and routine surveillance is not indicated. In adults, particular attention should be paid to skin in the area of malformations. These conclusions may not apply to all overgrowth syndromes with vascular malformations.",
                "authors": "J. Blatt, M. Finger, V. Price, S. Crary, A. Pandya, D. Adams",
                "citations": 21
            },
            {
                "title": "Sea Level Rise Scenario for 2100 A.D. in the Heritage Site of Pyrgi (Santa Severa, Italy)",
                "abstract": "Sea level rise is one of the main risk factors for the preservation of cultural heritage sites located along the coasts of the Mediterranean basin. Coastal retreat, erosion, and storm surges are posing serious threats to archaeological and historical structures built along the coastal zones of this region. In order to assess the coastal changes by the end of 2100 under the expected sea level rise of about 1 m, we need a detailed determination of the current coastline position based on high resolution Digital Surface Models (DSM). This paper focuses on the use of very high-resolution Unmanned Aerial Vehicles (UAV) imagery for the generation of ultra-high-resolution mapping of the coastal archaeological area of Pyrgi, Italy, which is located near Rome. The processing of the UAV imagery resulted in the generation of a DSM and an orthophoto with an accuracy of 1.94 cm/pixel. The integration of topographic data with two sea level rise projections in the Intergovernmental Panel on Climate Change (IPCC) AR5 2.6 and 8.5 climatic scenarios for this area of the Mediterranean are used to map sea level rise scenarios for 2050 and 2100. The effects of the Vertical Land Motion (VLM) as estimated from two nearby continuous Global Navigation Satellite System (GNSS) stations located as close as possible to the coastline are included in the analysis. Relative sea level rise projections provide values at 0.30 ± 0.15 cm by 2050 and 0.56 ± 0.22 cm by 2100 for the IPCC AR5 8.5 scenarios and at 0.13 ± 0.05 cm by 2050 and 0.17 ± 0.22 cm by 2100, for the IPCC Fifth Assessment Report (AR5) 2.6 scenario. These values of rise correspond to a potential beach loss between 12.6% and 23.5% in 2100 for Representative Concentration Pathway (RCP) 2.6 and 8.5 scenarios, respectively, while, during the highest tides, the beach will be provisionally reduced by up to 46.4%. In higher sea level positions and storm surge conditions, the expected maximum wave run up for return time of 1 and 100 years is at 3.37 m and 5.76 m, respectively, which is capable to exceed the local dune system. With these sea level rise scenarios, Pyrgi with its nearby Etruscan temples and the medieval castle of Santa Severa will be exposed to high risk of marine flooding, especially during storm surges. Our scenarios show that suitable adaptation and protection strategies are required.",
                "authors": "M. Anzidei, F. Doumaz, A. Vecchio, E. Serpelloni, L. Pizzimenti, R. Civico, M. Greco, G. Martino, F. Enei",
                "citations": 21
            },
            {
                "title": "Implementation of a Multiobjective Control for Islanded Hybrid Microgrid",
                "abstract": "This article presents an implementation of a fast and accurate control strategy for a synchronous generator driven by a diesel engine and a two-stage solar photovoltaic (PV) with a battery storage-based microgrid (MG) system. This MG feeds active power from a solar PV array to the distribution network by operating it at its maximum power point (MPP). The MPP of the solar PV array is estimated by using an incremental conductance control. An adaptive control of volterra least mean square/fourth (VLMS/F) is applied to an islanded hybrid MG to control the power converter that is interconnecting dc sources to the ac system, to improve various power quality indices such as harmonics suppression, compensation of reactive power, balancing of output currents from the diesel generator set, and neutral current compensation in a three-phase four-wire system. The simulation and experimental investigations of the developed MG show the performance superiority of the VLMS/F control strategy over the conventional controls.",
                "authors": "V. Narayanan, S. Kewat, Bhim Singh",
                "citations": 4
            },
            {
                "title": "An adaptive sequential sampling strategy-based multi-objective optimization of aerodynamic configuration for a tandem-wing UAV via a surrogate model",
                "abstract": "Multi-objective optimization of aerodynamic configuration for a tandem-wing unmanned aerial vehicle (UAV) via a surrogate model is appropriate in the primary stages of aircraft design. This study presents an adaptive sequential sampling strategy, which takes into account the principle of entropy rank and selection pooling based on a sigmoid function (ESP), in order to save time and construct a surrogate model database with considerable approximation accuracy. The entire procedure of optimization is divided into four parts, involving problem formulation for design variables and objectives, database construction for the surrogate model, multi-objective optimization with the surrogate models, and ESP adaptive sequential sampling to update the database. Firstly, a comparative study of the different surrogate models is carried out to assess their approximation performance. This verifies that the radial basis function (RBF) surrogate model outperforms the other models across the board. Then, we conduct two tests with typical mathematical problems to validate the effectiveness and applicability of the proposed method. We also develop a multi-objective optimization of the aerodynamic configuration for a tandem-wing UAV, aiming to maximize the lifting coefficient at the ascent (CLascent) and the lift-drag ratio (Kcruise) during the cruise. In this case, the RBF surrogate model is proven more suitable than the other common methods to replace the real values calculated by the non-planar vortex-lattice method (VLM) during the process of optimization. Furthermore, a comparison with large minimal distance (LMD) sequential sampling and disposable Latin hypercube sampling (LHS) is carried out alongside the optimization. These results show that the approximation precision achieved using ESP strategy is greater, highlighting the superiority of the ESP adaptive sequential sampling strategy in reducing the number of samples and raising the approximation accuracy. Finally, after the refinement of the database, an optimal Pareto front set is obtained to guide the primary design of the aerodynamic configuration for the tandem-wing UAV. Then, it is verified that the selected trade-off optimal design point has a better aerodynamic performance than the initial reference point, improving CLascent and Kcruise by 6.44% and 10.85%, respectively.",
                "authors": "Q. Shi, Hua Wang, Hao Cheng, Feng Cheng, Menglong Wang",
                "citations": 3
            },
            {
                "title": "Relative sea-level rise scenario for 2100 along the coasts of south eastern Sicily by GNSS and InSAR data, satellite images and high-resolution topography",
                "abstract": "<p>The global sea-level rise (SLR) projections for the next decades are the basis for developing flooding maps that depict the expected hazard scenarios. However, the spatially variable land subsidence has generally not been considered in the current projections. In this study, we use geodetic data from global navigation satellite system (GNSS), synthetic aperture radar interferometric measurements (InSAR) and sea-level data from tidal stations to show subsidence rates and SLR along the coast between Catania and Marzamemi, in south-eastern Sicily (southern Italy). This is one of the most active tectonic areas of the Mediterranean basin, which is affected to accelerated SLR, continuous coastal retreat and increasing effects of flooding and storms surges. We focus on six selected areas, which show valuable coastal infrastructures and natural reserves where the expected SLR in the next years could be a potential cause of significant land flooding and morphological changes of the coastal strip. Through a multidisciplinary study, the multi-temporal flooding scenarios until 2100, have been estimated. Results are based on the spatially variable rates of vertical land movements (VLM), the topographic features of the area provided by airborne Light Detection And Ranging (LiDAR) data and the Intergovernmental Panel on Climate Change (IPCC) projections of SLR in the Representative Concentration Pathways RCP2.6 and RCP8.5 emission scenarios. In addition, from the analysis of the time series of optical satellite images, a coastal retreat up to 70 m has been observed at the Ciane river mouth (Siracusa) in the time span 2001-2019. Our results show a diffuse land subsidence locally exceeding 10 &#177; 2.0 mm/yr<sup>-1</sup> in some areas, due to compacting artificial landfill, salt marshes and Holocene soft deposits. Given ongoing land subsidence a high end of RSLR in the RCP8.5 at 0.52&#177; 0.05 m and 1.52&#177;0.13 m is expected for 2050 AD and 2100 AD, respectively, with a projected area of about 9.7 km<sup>2</sup> that will be vulnerable to inundation in the next 80 years.</p>",
                "authors": "M. Anzidei, G. Scicchitano, G. Scardino, C. Bignami, C. Tolomei, A. Vecchio, E. Serpelloni, Vincenzo De Santis, C. Monaco, M. Milella, A. Piscitelli, G. Mastronuzzi, L. Pizzimenti",
                "citations": 1
            },
            {
                "title": "Composite Metallic Nano Emitters Coated with a Layer of Insulator Covered by Au Layer",
                "abstract": "Abstract: In this work, the differences in the behavior and properties of the emitted electron beam from tungsten (W) tips were studied before and after coating these tips with a thin layer of dielectric material followed by a thin layer of gold, to improve the emission current density, stability and emission current pattern concentration. The core of the composite cathode is made of high-purity tungsten (W). Measurements have been made with clean W emitters before and after coating these tips with two types of epoxy resins (epoxy 478 resins or epoxy UPR- 4 resins) followed by a thin layer of gold. For critical comparison, several tungsten tips with various apex-radii have been prepared using electrochemical etching techniques. The emitters have been coated by dielectric thin films of various thicknesses and the layer of Au used for coating the Epoxy layer has the same thicknesses. Their behavior has been recorded before and after the process of coating. These measurements include the current-voltage (I-V) characteristics and Fowler-Nordheim (F-N) plots. Imaging has also been done using a visible light microscope (VLM), along with a scanning electron microscope (SEM) to help in characterizing the epoxy layer thickness on the tip surface after coating. Besides, the emission patterns have been recorded from the phosphorescent screen of a field electron emission microscope (FEM). Having two types of composite systems tested under similar conditions provided several advantages. These measurements helped in producing a new type of emitters that have more suitable features with each of the two resins.\nKeywords: Cold field emission, Nano emitter, Dielectric coating, Au layer.",
                "authors": "",
                "citations": 4
            },
            {
                "title": "Design of in-Depth Conformance Gel Treatment to De-Risk ASP Flooding in a Major Carbonate Reservoir",
                "abstract": "\n Mature carbonate reservoirs under waterflood in Kuwait suffer from relatively low oil recovery due to poor sweep efficiency, both areal and microscopic. An Alkaline-Surfactant-Polymer (ASP) pilot is in progress targeting the Sabriyah Mauddud (SAMA) reservoir in pursuit of reserves growth and production sustainability. SAMA suffers from reservoir heterogeneities mainly associated with permeability contrast which may be improved with a conformance treatment to de-risk pre-mature breakthrough of water and chemical EOR agents in preparation for subsequent ASP injection and to improve reservoir contact by the injected fluids. Design of the gel conformance treatment was multi-faceted. Rapid breakthrough of tracers at the pilot producer from each of the individual injectors, less than 3 days, implied a direct connection from the injectors to the producer and poses significant risk to the success of the pilot. A dynamic model of the SAMA pilot was used to estimate in the potential injection of either a high viscous polymer solution (~200 cp) or a gel conformance treatment to improve contact efficiency, diverting injected fluid into oil saturated reservoir matrix. High viscosity polymer injection scenarios were simulated in the extracted subsector model and showed little to no effect on diverting fluids from the high permeability streak into the matrix. Gel conformance treatment, however, provides benefit to the SAMA pilot with important limitations. Gel treatment diverts injected fluid from the high permeability zone into lower permeability, higher oil saturated reservoir. After a gel treatment, the ASP increases the oil cut from 3% to 75% while increasing the cumulative oil recovery by more than 50 MSTB oil over ASP following a high viscosity polymer slug alone. Laboratory design of the gel conformance system for the SAMA ASP pilot involved blending of two polymer types (AN 125SH, an ATBS type polymer, and P320 VLM and P330, synthetic copolymers) and two crosslinkers (chromium acetate and X1050, an organic crosslinker). Bulk testing with the polymer-crosslinker combinations indicated that SAMA reservoir brine resulted in not gel system that would work in the SAMA reservoir, resulting in the recommendation of using 2% KCl in treated water for gel formulation. AN 125 SH with S1050 produce good gels but with short gelation times and AS 125 SH with chromium acetate developed low gels consistency in both waters. P330 and P320 VLM gave good gels with slow gelation times with X1050 crosslinker in 2% KCl. Corefloods with the P330-X 1050 showed good injectivity and ultimately a reduction of permeability of about 200-fold. A P330-X 1050 was recommended for numerical simulation studies. Numerical simulator was calibrated by matching bulk gel viscosity increases and coreflood permeability changes. Numerical simulation indicated two of the four injection wells (SA-0557 and SA-0559) injection profile will change compared to water. Overall injection rate was reduced by the conformance treatment and was the corresponding oil rate. Total oil production from the center pilot production well (SA-0560) decreased with gel treatment but ultimately increased to greater rates",
                "authors": "M. Al-Murayri, A. Hassan, D. Alrukaibi, A. Al-Qenae, Jimmy Nesbit, Philippe Al Khoury, B. Thery, A. Zaitoun, G. Omonte, N. Salehi, M. Pitts, K. Wyatt, E. Dean",
                "citations": 1
            },
            {
                "title": "Coastal Sea Level Trends from a Joint Use of Satellite Radar Altimetry, GPS and Tide Gauges: Case Study of the Northern Adriatic Sea",
                "abstract": "For the last century, tide gauges have been used to measure sea level change along the world’s coastline. However, tide gauges are heterogeneously distributed and sparse in coverage. The measured sea level changes are also affected by solid-Earth geophysics. Since 1992, satellite radar altimetry technique made possible to measure heights at sea independent of land changes. Recently various efforts started to improve the sea level record reprocessing past altimetry missions to create an almost 30 year-long combined record for sea level research studies. Moreover, coastal altimetry, i.e. the extension of altimetry into the oceanic coastal zone and its exploitation for looking at climate-scale variations of sea level, has had a steady progress in recent years and has become a recognized mission target for present and future satellite altimeters. Global sea level rise is today well acknowledged. On the opposite, the regional and local patterns are much more complicated to observe and explain. Sea level falls in some places and rises in others, as a consequence of natural cycles and anthropogenic causes. As relative sea level height continues to increase, many coastal cities can have the local elevation closer to the flooding line. It is evident that at land-sea interface a single technique is not enough to de-couple land and sea level changes. Satellite radar altimetry and tide gauges would coincide at coast if land had no vertical motion. By noting this fact, the difference of the two independent measurements is a proxy of land motion. In this chapter, we review recent advances in open ocean and coastal altimetry to measure sea level changes close to the coasts over the satellite radar altimetry era. The various methods to measure sea level trends are discussed, with focus on a more robust inverse method that has been tested in the Northern Adriatic Sea, where Global Positioning System (GPS) data are available to conduct a realistic assessment of uncertainties. The results show that the classical approach of estimating Vertical Land Motion (VLM) provides values that are almost half of those provided by the new Linear Inverse Problem With Constraints (LIPWC) method, in a new formulation which makes use of a change of variable (LIPWCCOV). Moreover, the accuracy of the new VLM estimates is lower when compared to the VLM estimated from GPS measurements. The experimental Sea Level Climate Change Initiative (SLCCI) data set (high resolution along track) coastal sea level product (developed within Climate Change Initiative (CCI project) that has been also assessed in the Gulf of Trieste show that the trends calculated with the gridded and along track datasets exhibit some differences, probably due to the different methodologies used in the generation of the products.",
                "authors": "S. Vignudelli, F. D. Biasio",
                "citations": 1
            },
            {
                "title": "Expressive and Efficient Model Transformation with an Internal DSL of Xtend",
                "abstract": "Model transformation (MT) of very large models (VLMs), with millions of elements, is a challenging cornerstone for applying Model-Driven Engineering (MDE) technology in industry. Recent research efforts that tackle this problem have been directed at distributing MT on the Cloud, either directly, by managing clusters explicitly, or indirectly, via external NoSQL data stores. In this paper, we draw attention back to improving efficiency of model transformations that use EMF natively and that run on non-distributed environments, showing that substantial performance gains can still be reaped on that ground. We present Yet Another Model Transformation Language (YAMTL), a new internal domain-specific language (DSL) of Xtend for defining declarative MT, and its execution engine. The part of the DSL for defining MT is similar to ATL in terms of expressiveness, including support for advanced modelling contructs, such as multiple rule inheritance and module composition. In addition, YAMTL provides support for specifying execution control strategies. We experimentally demonstrate that the presented transformation engine outperforms other representative MT engines by using the batch transformation component of the VIATRA CPS benchmark. The improvement is, at least, one order of magnitude over the up-to-now fastest solution in all of the assessed scenarios. The software artefacts accompanying this work have been approved by the artefact evaluation committee and are available at http://remodd.org/node/585.",
                "authors": "A. Boronat",
                "citations": 20
            },
            {
                "title": "The mechanisms of neuroplasticity during acclimatization to‐ and deacclimatization from chronic hypercapnia are fundamentally different",
                "abstract": "A hallmark of respiratory‐related diseases, such as chronic obstructive pulmonary disease (COPD), is chronic hypercapnia (CH). Common therapeutic intervention for patients experiencing CH is mechanical ventilation to restore blood‐gas homeostasis. While many neurophysiological consequences of CH have been characterized, little is known about the effects reversing CH. The primary goal of the present study was to test the hypothesis that abrupt deacclimatization from CH would significantly alter markers of neuroplasticity and tryptophan metabolism, but not levels of excitatory neuromodulators. Utilizing our goat model of increased inspired CO2 (InCO2)‐induced CH, adult female goats were exposed to an InCO2 of 6% for 30 days (d) followed by a return to room air for 24 hours (h). Following 24h of recovery, goats were euthanized, and brainstems were rapidly extracted. Tissue punches of key nuclei throughout the brainstem respiratory network (hypoglossal motor nucleus (XII), nucleus tractus solitarius (NTS)/dorsal motor nucleus of the vagus (DMV), ventral respiratory column (VRC), medullary raphe (MR), ventrolateral medulla (VLM), retrotrapezoid nucleus (RTN), and cuneate nucleus (CN)) were obtained and used for either western blot or HPLC analysis. Changes in glutamatergic signaling, neuroinflammation, tryptophan metabolism, and neuromodulators concentration, following recovery from CH, were assessed. Following 24h of recovery, there were no significant differences in AMPA or NMDA receptor expression or phosphorylation, compared to room air control goats, across all nuclei investigated. Similarly, there were no significant differences in the inflammatory cytokine IL1B following 24h recovery across all nuclei investigated, compared to room air control goats. However, there was significantly (P<0.05) lower expression of key enzymes of tryptophan metabolism (Indolamine 2,3‐dioxygenase (IDO); Tryptophan Hydroxylase (TPH) and neuronal markers (NeuN) compared to control within the rostral VLM and MR. Within the solitary complex (NTS & DMV), there was significantly (P>0.05) greater concentrations in norepinephrine (+132%±49), serotonin (+348%±142), and dopamine (+86%±53) at 24h of recovery compared to 30d of CH. We conclude that although specific markers of neuroplasticity and neuroinflammation were unchanged from control during deacclimatization, there were brainstem‐site dependent changes in excitatory neuromodulators during deacclimatization from CH. These data suggest that the mechanisms of neuroplasticity during acclimatization to‐ and deacclimatization from‐ CH are fundamentally different.",
                "authors": "Kirstyn J. Buchholz, Nicholas J. Burgraff, S. Neumueller, M. Hodges, L. Pan, H. Forster",
                "citations": 0
            },
            {
                "title": "Intravenous Leiomyosarcoma of the Lower Extremity: As Peripheral as It Gets",
                "abstract": "Leiomyosarcomas of the vascular system (vLMSs) are rare tumors that commonly originate from large proximal and central veins. Pancreatic metastasis is rare for sarcomas, and surgical excision with large margins is the treatment of choice. We present a case of a 32-year-old female with primary vLMS originating from the distal crural veins and local invasion of the fibula. A prior open biopsy site was suboptimal. The patient was treated with neoadjuvant chemotherapy and radiotherapy, followed by surgery. The follow-up radiological imaging showed pancreatic head metastasis, which is also an extremely rare site for vLMS.",
                "authors": "Levent Umur, S. Çakmak, M. Isyar, Hamdi Tokoz",
                "citations": 0
            },
            {
                "title": "Validation of sea surface heights from satellite altimetry along the Indian coast",
                "abstract": "<p>Satellite altimetry provides measurements of sea surface height of centimeter-level accuracy over open oceans. However, its accuracy reduces when approaching the coastal areas and over land regions. Despite this downside, altimetric measurements are still applied successfully in these areas through altimeter retracking processes. This study aims to calibrate and validate retracted sea level data of Envisat, ERS-2, Topex/Poseidon, Jason-1, 2, SARAL/AltiKa, Cryosat-2 altimetric missions near the Indian coastline. We assessed the reliability, quality, and performance of these missions by comparing eight tide gauge (TG) stations along the Indian coast. These are Okha, Mumbai, Karwar, and Cochin stations in the Arabian Sea, and Nagapattinam, Chennai, Visakhapatnam, and Paradip in the Bay of Bengal. To compare the satellite altimetry and TG sea level time series, both datasets are transformed to the same reference datum. Before the calculation of the bias between the altimetry and TG sea level time series, TG data are corrected for Inverted Barometer (IB) and Dynamic Atmospheric Correction (DAC). Since there are no prior VLM measurements in our study area, VLM is calculated from TG records using the same procedure as in the Technical Report NOS organization CO-OPS 065.&#160;</p><p>Keywords&#8212; Tide gauge, Sea level, North Indian ocean, satellite altimetry, Vertical land motion</p>",
                "authors": "M. Murshan, B. Devaraju, N. Balasubramanian, O. Dikshit",
                "citations": 0
            },
            {
                "title": "1 Cost-benefit analysis of coastal flood defence measures in the North 1 Adriatic Sea 2",
                "abstract": "8 The combined effect of global sea level rise and local subsidence phenomena poses a major threat to coastal 9 settlements. Flooding events are expected to grow in frequency and magnitude, increasing the potential 10 economic losses and costs of adaptation. In Italy, a large share of the population and economic activities are 11 located along the coast of the peninsula, although risk of inundation is not uniformly distributed. The low12 lying coastal plain of Northeast Italy is the most sensitive to relative sea level changes. Over the last half a 13 century, the entire north-eastern Italian coast has experienced a significant rise in relative sea level, the main 14 component of which was land subsidence. In the forthcoming decades, sea level rise is expected to become the 15 first driver of coastal inundation hazard. We propose an assessment of flood hazard and risk linked with 16 extreme sea level scenarios, both under historical conditions and sea level rise projections at 2050 and 2100. 17 We run a hydrodynamic inundation model on two pilot sites located in the North Adriatic Sea along the 18 Emilia-Romagna coast: Rimini and Cesenatico. Here, we compare alternative risk scenarios accounting for the 19 effect of planned and hypothetical seaside renovation projects against the historical baseline. We apply a flood 20 damage model developed for Italy to estimate the potential economic damage linked to flood scenarios and 21 we calculate the change in expected annual damage according to changes in the relative sea level. Finally, 22 damage reduction benefits are evaluated by means of cost-benefit analysis. Results suggest an overall 23 profitability of the investigated projects over time, with increasing benefits due to increased probability of 24 intense flooding in the next future. 25 Key-words: coastal inundation Italy extreme sea level rise 26 Abbreviations: MSL (Mean Sea Level); TWL (Total Water Level); ESL (Extreme Sea Level); SLR (Sea Level 27 Rise); VLM (Vertical Land Movements); DTM (Digital Terrain Model); EAD (Expected Annual Damage) 28",
                "authors": "M. Amadio, A. Essenfelder, S. Bagli, Sepehr Marzi, P. Mazzoli, J. Mysiak, S. Roberts",
                "citations": 0
            },
            {
                "title": "Influence of non-tidal atmospheric and oceanic loading deformation on the stochastic properties of over 10,000 GNSS vertical land motion time series",
                "abstract": "<p>Over the past two decades, numerous studies demonstrated that the stochastic variability in GNSS position time series &#8211; often referred to as noise &#8211; is both temporally and spatially correlated. The time correlation of this stochastic variability can be well approximated by a linear combination of white noise and power-law stochastic processes with different amplitudes. Although acknowledged in many geodetic studies, the presence of such power-law processes in GNSS position time series remains largely unexplained. Considering that these power-law processes are the primary source of uncertainty for velocity estimates, it is crucial to identify their origin(s) and to try to reduce their influence on position time series.</p><p>&#160;</p><p>Using the Least-Squares Variance Component Estimation method, we analysed the influence of removing surface mass loading deformation on the stochastic properties of vertical land motion time series (VLMs). We used the position time series of over 10,000 globally distributed GNSS stations processed by the Nevada Geodetic Laboratory at the University of Nevada, Reno, and loading deformation time series computed by the Earth System Modelling (ESM) team at GFZ-Potsdam. Our results show that the values of stochastic parameters, namely, white noise amplitude, spectral index, and power-law noise amplitude, but also the spatial correlation, are systematically influenced by non-tidal atmospheric and oceanic loading deformation. The observed change in stochastic parameters often translates into a reduction of trend uncertainties, reaching up to -75% when non-tidal atmospheric and oceanic loading deformation is highest.</p>",
                "authors": "K. Gobron, P. Rebischung, O. de Viron, M. Van Camp, A. Demoulin",
                "citations": 0
            },
            {
                "title": "About the modelling of the SED for the inner boundary of protoplanetary discs at the lower stellar mass regime",
                "abstract": "\n In order to improve the physical interpretation about innermost dusty regions in protoplanetary discs around brown dwarf (BD), and even very low mass star (VLMS), we present a grid of models taking into account two different sets: (i) The set called standard model, that simulates an axisymmetric dusty disc with an inner curved wall. (ii) The perturbed one called non-standard where the axisymmetry of the inner edge has been broken. We have achieved a fitting for the disc structure able to explain the spectral energy distribution (SED). As the main condition, we assume that the changes of the inner wall geometry in the tongue-like shape depend on the Rayleigh–Taylor instability (R-TIns) generated in the inner disc edge. For each object, we parametrize the shape of the inner wall to find a time-dependent model that enables us to explain the photometric near-Infrared variability and connect the changes on the inner disc structure with the amplitude of such variability. We re-analysed photometric measurements from near to mid-infrared wavelengths of a sample of 6 BDs and one VLMS in different cloud associations which were previously studied by other authors. We also show that the flux change calculated between the non-standard and the standard configurations models the observed variability in LRLL 1679. The magnitude changes due to these fluctuations slightly depend on the wavelength and they can present differences of up to 0.9 mag. We suggest that if the R-TIns persist enough time, the features in the protoplanetary inner disc, e.g. inner holes or gaps evolve.",
                "authors": "Sebastián Morales-Gutiérrez, E. Nagel, O. Barragán",
                "citations": 0
            },
            {
                "title": "A review and technical perspective towards the role of UAV as a mode of transport in healthcare sector in India",
                "abstract": "This paper describes the technical and legal sides of Unmanned Aerial Vehicle and its operations for emergency medical situations like accidents, emergency hospitalization etc. in India. It highlights the details about mode of operations like flight aerodynamics, types of payloads, aircraft launching and landing system, payload dropping mechanisms, powerplant selection, etc. In 2018, The Director General of Civil Aviation of Government of India announces new policies for Remotely Piloted Unmanned Systems. According to these policies, designing and operating the Unmanned Aerial Vehicle in Indian air space becomes somewhat easier than before. Although, these regulations cannot give complete freedom of what designer and operator wants in their Unmanned Aerial Systems but, it helps to design the Unmanned Aerial System with systematic approach from the foundation stage. India is the 2nd most populated country in the world with poor health sector. Because of this high densely population, it is difficult to treat every needful person. Many accident cases are registered daily in India and maximum of them lost their lives just because of lack of treatment at early stages. Even ambulance takes 20 to 40 minutes of time to reach at accidental place by road (In rural area that too is not possible). If we can be able to provide the necessary treatments like first aid treatment, medicines, living organs, blood etc. by Aerial route (within 5-7 minutes) then it is possible to save the life of many peoples across the country. This project is trying to mention the details about unmanned aerial system which can solve this problem and help provide the treatment in medical emergencies. keywords Unmanned Aerial Vehicle, Unmanned Aerial System, Medical emergency, DGCA, RPAS. _____________________________________________________________________________________________________ I.INTRODUCTION The term Unmanned Aerial Vehicle (UAV) was first introduced in the 1980s which defines, an autonomous or remotely piloted, multirole aircraft which can carry a payload or performing any task like surveillance without direct interference of humans. This definition makes UAV different from other aerial vehicles such as airplanes, helicopters, missiles, gliders, balloons etc. The UAS, refers to the system in which one or more unmanned aerial vehicles in combination with payloads, a navigations and control system, sensors, instruments and data linkages working together to operate the vehicle without pilot. Other term used to refer UAS is remotely piloted aircraft system (RPAS). From recent years, implementation of UAS was seen widely in land survey, urban planning and development, mapping, mining industry, archaeology, environment protection, disaster management, aerial photography, construction etc. Because of affordable in cost, great performance and time saving ability, UAVs are highly demanded in market. UAVs was utilized for military purposes at their initial period. The first successful radio control aircraft took-off in 1924. Through the development of software and electronics and the mass production of processors, sensors, and batteries produced for electronic devices, UAV designs have become easier, more affordable and more accessible and hence this began to utilize UAVs for commercial world. Currently, plenty of nations like USA, European union, Israel, India, China and so on are utilizing UAVs in their military and civil operations. This review article aims to highlight the importance and applications regarding the use of small UAVs to transport blood and medicines from blood banks/medical stores to critical access hospitals, accidental place in times of critical and emergency situation. This technology has the potential to benefit peoples in distant areas. Just like other sectors, health sector also has the best usage of UAVs to improve their services. Transportation is huge barrier in delivery of medicines and vaccines in distant or high densely populated areas in countries like India. UAVs can be the best alternative in case of any emergencies for the transportation of medicines in such areas. It may be a quick means of transportation of medicines and other facilities. This system will be affordable cost of transportation for medical facilities. The contribution of UAVs for caring of patients, accident victims in the manner to provide immediate medicines, blood in less time for assess can save someone’s life. Some hospitals in Switzerland started using rotor wing UAVs (Figure-1) to deliver the blood samples and other medicines. This step helps many peoples in Switzerland to save their lives. This UAVs are capable of carrying payloads up to 2 kg and move at velocity up to 57.6 km/h at an altitude of 110 m and the range up to 20 km. This UAVs are equipped with parachute so that in case of any damage occurs in air, the UAVs are able to land safely without harming others as well as cargo inside it. The medicine delivery start-up Zipline from African country – Rwanda, uses fixed wing UAVs (Figure-2) to deliver the blood and medicines. Its UAVs has operating range of 80 km and move at speed of 100 km/h making it one of the innovative and Publication Since 2012 | ISSN: 2321-9939 | ©IJEDR 2021 Year 2021, Volume 9, Issue 1 IJEDR2101021 International Journal of Engineering Development and Research (www.ijedr.org) 171 successful Unmanned Aerial System. The medicines are dropped at delivery location with the help of parachutes. The UAV can carry 1.75 kg of payload. It was in service from the year 2016. Fig-1 Rotor wing UAV by Swiss post [1] Fig-2 Fixed wing UAV by Zipline [2] At present, medical supplies in the United States of America (USA) are delivered by conventional road transport as well as airplanes and helicopters. The availability of blood and other medicines is limited during emergencies at critical access hospitals, due to this, conventional mode of transportation may become disrupted. II. METHODOLOGY A research study was conducted on available scientific papers and journals related to this topic. Some information was collected from official website of drone companies which are working in this field with successful results. Since research into the implementation of UAVs in health sector around the globe is at initial stage, literature survey was conducting on limited available sources. Design Tushar et.al (2020) discussed about Aerodynamic characterization of Albatross Inspired wing for UAVs [3]. In the paper author elaborated detailed design analysis on the aerodynamics of wing which can bear high lift to weight ratio. UAVs having High payload fraction are eligible to carry more payloads. Wing is most important part of fixed wing aircraft for their contribution of generating the lift. According to requirements and applications of UAV, its wing shape and other characteristics differs. Similarly, for the stability, empennage plays an important role. The software which was used for design and simulations was XFLR5. It uses 3D panel and Vortex Lattice Method (VLM) for analysis and results. By considering all aerodynamic geometries of designed UAV like wing span, aspect ratio and wing area for main wing, horizontal tail and vertical tail at constant flight speed, the aerodynamic analysis was performed. The design objective is accomplished by obtaining the graph of lift coefficient versus angle of attack, lift coefficient versus drag coefficient and ratio of Lift to Drag coefficient versus angle of attack which defines the lift characteristics of wing in order to obtain the maximum lift enhancing design requirement carrying highest payload. Fuselage which satisfies the condition to hold and release the cargo by launch and drop mechanism. Also, fuselage should not make big disturbance on wing to act against lift. Some more bio-inspired wing designs were discussed by Dwivedi et",
                "authors": "Tushar Ms Waykar, K. Vaishnavi, Y. D. Dwivedi",
                "citations": 0
            },
            {
                "title": "Human NLRP1 Is a Sensor of 3CL Proteases from Pathogenic Coronaviruses in Lung Epithelial Cells",
                "abstract": "Inflammation observed in SARS-CoV-2-infected patients suggests that inflammasomes, proinflammatory intracellular complexes, regulate various steps of infection. Lung epithelial cells express inflammasome-forming sensors and constitute the primary entry door of SARS-CoV-2. Here, we describe that the NLRP1 inflammasome detects SARS-CoV-2 infection in human lung epithelial cells. Specifically, human NLRP1 is cleaved at the Q333 site by multiple coronavirus 3CL proteases, which triggers inflammasome assembly, cell death and limits the production of infectious viral particles. Analysis of NLRP1-associated pathways unveils that 3CL proteases also cleave and inactivate the pyroptosis executioner Gasdermin (GSDM)-D. Consequently, Caspase-3 and GSDM-E promote alternative cell pyroptosis, a process exacerbated in cells exhibiting imparied type I interferon production. Finally, analysis of pyroptosis markers in plasma from COVID-19 patients with characterized severe pneumonia due to Interferon alterations identify GSDM-E/Caspase-3 as biological markers of disease severity. Overall, our findings identify NLRP1 as a key sensor of SARS-CoV-2 infection in lung epithelia.<br><br>Funding Information: This project has been funded on lab own funds from unrelated grants from the Fondation pour la Recherche Médicale (FRM) and ERC StG (INFLAME) to EM, from ERC StG (ANTIViR) to CG, by the French Ministry of Health with the participation of the Groupement Interrégional de Recherche Clinique et d’Innovation Sud-Ouest Outre-Mer (PHRCI 2020 IMMUNOMARK-COV) to G-M.B. The ASB3 structure is supported by LABEX, Investissement d’Avenir and foundation Bettencourt grants to ON. MP and RP were respectively funded by a CIFRE PhD fellowship and a research grant from Invivogen. SB is supported by a PhD fellowship from Mali ministry of education and from the FRM (FDT 12794). SALC is supported by a Vaincre La Mucoviscidose (VLM) PhD fellowship.<br><br>Declaration of Interests: Authors declare no conflict of interest. <br><br>Ethics Approval Statement: All donors had given written informed consent and the study was approved by the ethical review board “Comité de Protection des Personnes Est-III” (ID-RCB 2020-A01292-37).<br>",
                "authors": "R. Planès, Miriam Pinilla, Karin Santoni, Audrey Hessel, Kenneth Lay, P. Paillette, A. Valadão, K. Robinson, P. Bastard, Ida Rossi, D. Péricat, Salimata Bagayoko, S. Leon-Icaza, Y. Rombouts, E. Perouzel, M. Tiraby, Covid Human Genetic Effort, Qian Zhang, E. Jouanguy, O. Neyrolles, C. Goujon, F. Zhong, G. Martin-Blondel, Stein Silva, J. Casanova, C. Cougoule, B. Reversade, J. Marcoux, E. Ravet, E. Meunier",
                "citations": 0
            },
            {
                "title": "Computational analysis of low mass moment of inertia flying wing",
                "abstract": "The primary goal in development of MAVs is to develop a flying body that weigh as less as 90 grams, with a span of 15cm [1]. Since it is challenging to meet these design characteristics of a MAVs with current technology, there has been a lot of research going on in this direction. This paper is also a continuation of the same quest. In this research a delta wing micro air vehicle was analyzed at low sub sonic speed at different angles of attack for determining lift, drag force, stall angle and L/D ratio. CFD and XFLR have been used as computational tools. For CFD analysis, a far field technique was used to calculate the lift and drag coefficients. Rectangular domain was created around the body with a size 10 times greater than body so that flow behavior can be captured accurately. Inflation layers were created around the body to capture boundary layer. Fine mesh was created at the regions of higher gradients. In fluent flow was analyzed using two turbulent models (SA and k-ε). A potential flow solver (XFLR) was also used to compare results from RANS methodology with vortex lattice method (VLM). Validation of computational results was carried out by comparing with earlier research concerning wind tunnel testing of MAV [2] at 100,000 Re. This research will help the aero dynamists to understand the complete procedure of computational and XFLR analysis of MAVs. It was found that XFLR predicts higher values of lift coefficient and lower values of drag coefficient at all angles of attack. Consequently, lift to drag ratio as predicted by XFLR was found greater than RANS models. Further no appreciable difference was noticed between lift and drag characteristics predicted by SA and k-ε models.",
                "authors": "Sara Khushbash, A. Javed, T. Shams",
                "citations": 0
            },
            {
                "title": "Modified Volterra LMS algorithm to fractional order for identification of Hammerstein non-linear system",
                "abstract": "In this study, a new non-linear recursive mechanism for Volterra least mean square (VLMS) algorithm is proposed in the domain of non-linear adaptive signal processing and control. The proposed adaptive scheme is developed by applying concepts and theories of fractional calculus in weight adaptation structure of standard VLMS approach. The design scheme based on fractional VLMS (F-VLMS) algorithm is applied to parameter estimation problem of non-linear Hammerstein Box-Jenkins system for different noise and step size variations. The adaptive variables of F-VLMS are compared from actual parameters of the system as well as with the results of conventional VLMS for each case to verify its correctness. Comprehensive statistical analyses are conducted based on sufficient large number of independent runs and performance indices in terms of mean square error, variance account for and Nash–Sutcliffe efficiency establish the worth and effectiveness of the scheme.",
                "authors": "Naveed Ishtiaq Chaudhary, Muhammad Saeed Aslam, R. Zahoor",
                "citations": 22
            },
            {
                "title": "Primary vaginal leiomyosarcoma: case report of a rare gynaecological malignancy and diagnostic challenge in a resource-constraint setting",
                "abstract": "ABSTRACT Primary vaginal leiomyosarcoma (VLMS) is an extremely rare variant of primary vaginal cancers with very poor prognosis irrespective of the stage at presentation and the type of treatment received. It is easily recurrent and has a high propensity for haematogenous spread especially to the lungs. We present the case of a 34-year-old Para 1 + 1 (1 alive) woman with recurrent vaginal mass of 8 years duration after two surgical excisions without histological evaluation. She had examination under anaesthesia and a wide local excision of the vaginal mass. Histological examination of the mass revealed poorly differentiated VLMS with positive surgical margins and she was commenced on adjuvant chemo-radiation. Histological evaluation remains the hallmark for diagnosing rare malignancies like VLMS, which unfortunately is not a standard practice in some resource-constraint settings.",
                "authors": "Adeyemi A Okunowo, A. Ugwu, E. Owie, Habibat F Kolawole, L. Adebayo, O. A. Kusamotu, J. O. Kuku, A. Soibi-Harry, E. Ohazurike, A. Banjo",
                "citations": 5
            },
            {
                "title": "Warehouse Location Design Using AS/RS Technologies: An Interval Valued Intuitionistic Fuzzy AHP Approach",
                "abstract": null,
                "authors": "C. Kahraman, B. Oztaysi, Sezi Cevik Onar",
                "citations": 5
            },
            {
                "title": "Gesture errors in left and right hemisphere damaged patients: A behavioural and anatomical study",
                "abstract": "OBJECTIVE\nErroneous gesture execution is at the core of motor cognition difficulties in apraxia. While a taxonomy of errors may provide important information about the nature of the disorder, classifications are currently often inconsistent. This study aims to identify the error categories which distinguish apraxic from non-apraxic patients.\n\n\nMETHOD\nTwo groups of mixed (bucco-facial and limb) and bucco-facial apraxic patients suffering from stroke were compared to non-apraxic, left and right hemisphere damaged patients in tasks tapping the ability to perform limb and bucco-facial actions. The errors were analysed and classified into 6 categories relating to content, configuration or movement, spatial or temporal parameters and unrecognisable actions. Furthermore, an anatomical investigation (VLMS) was conducted in the whole group of left hemisphere damaged patients to investigate potential correlates of the various error categories.\n\n\nRESULTS\nAlthough all the above error typologies may be observed, the most indicative of mixed apraxia is the content-related one relate to content in all the typologies of actions (transitive and intransitive), and configuration errors in transitive ones. Configuration and content errors in mouth actions seem to be typical of bucco-facial apraxia. Spatial errors are similar in both apraxic and right brain damaged, non-apraxic patients. A lesion mapping analysis of left-brain damaged patients demonstrates that all but the spatial error category are associated with the fronto-parietal network. Moreover, content errors are also associated with fronto-insular lesions and movement errors with damage to the paracentral territory (precentral and postcentral gyri). Spatial errors are often associated to ventral frontal lesions.\n\n\nCONCLUSIONS\nBucco-facial and mixed apraxic patients make different types of errors in different types of actions. Not all errors are equally indicative of apraxia. In addition, the various error categories are associated with at least partially different neural correlates.",
                "authors": "M. Scandola, V. Gobbetto, S. Bertagnoli, C. Bulgarelli, Loredana Canzano, S. Aglioti, V. Moro",
                "citations": 4
            },
            {
                "title": "Sea Level Trends and Variability in the Adriatic Sea and Around Venice",
                "abstract": null,
                "authors": "S. Vignudelli, F. D. Biasio, A. Scozzari, S. Zecchetto, A. Papa",
                "citations": 8
            },
            {
                "title": "Molecular Scaffold Growth of Two-Dimensional, Strong Interlayer-Bonding-Layered Materials",
                "abstract": "Currently, most two-dimensional (2D) materials that are of interest to emergent applications have focused on van der Waals–layered materials (VLMs) because of the ease with which the layers can be separated (e.g., graphene). Strong interlayer-bonding-layered materials (SLMs) in general have not been thoroughly explored, and one of the most critical present issues is the huge challenge of their preparation, although their physicochemical property transformation should be richer than VLMs and deserves greater attention. MAX phases are a classical kind of SLM. However, limited to the strong interlayer bonding, their corresponding 2D counterparts have never been obtained, nor has there been investigation of their fundamental properties in the 2D limitation. Here, the authors develop a controllable bottom-up synthesis strategy for obtaining 2D SLMs single crystal through the design of a molecular scaffold with Mo 2GaC, which is a typical kind of MAX phase, as an example. The superconducting transitions of Mo 2GaC at the 2D limit are clearly inherited from the bulk, which is consistent with Berezinskii–Kosterlitz–Thouless behavior. The authors believe that their molecular scaffold strategy will allow the fabrication of other high-quality 2D SLMs single crystals, which will further expand the family of 2D materials and promote their future application.",
                "authors": "Mengqi Zeng, Yunxu Chen, E. Zhang, Jiaxu Li, R. Mendes, X. Sang, Shulin Luo, Wenmei Ming, Yuhao Fu, M. Du, Lijun Zhang, D. Parker, R. Unocic, Kai Xiao, Chenglai Wang, Tao Zhang, Yao Xiao, M. Rümmeli, F. Xiu, L. Fu",
                "citations": 8
            },
            {
                "title": "Venolymphatic Malformation of the Tongue and Response to Sclerotherapy Using Sodium Tetradecyl Sulfate",
                "abstract": "A 28-years-old healthy woman presented with a bluish swelling on the dorsum of tongue, which was present for almost two decades. Her parents noticed bluish discoloration of tongue when she was approximately 8 years old, which enlarged at puberty to a pea-sized swelling. The lesion increased progressively to the present size in the last 2–3 years. Examination revealed a bilobed swelling, 3 × 4 cm in size [Figure 1A], present over the dorsum of middle one-third of the left half of the tongue. The swelling was soft, partially compressible with no thrill or bruit. The patient was majorly asymptomatic with no pain or difficulty in mastication but gave a history of occasional bleeding while chewing hard food. Rest of the mucocutaneous examination and physical examination was normal. Her hematological investigations and blood biochemistry revealed no abnormality. High-resolution ultrasonography (USG) showed an anechoic lesion on the dorsum of the tongue on the left side. The superficial, irregular, anechoic, cystic component reached up to the tongue surface. Lesion had a vaguely echogenic deeper component, which showed mild vascularity on color Doppler imaging [Figure 2A]. Contrast-enhanced computed tomography (CECT) of the face showed a non-enhancing, low attenuation mass of water density (Houndsfield Unit was 10-12 HU) with a deeper well-defined, irregular, homogenously enhancing component [Figure 2B] with enhancement persisting in the delayed phases as well. These findings were consistent with combined venolymphatic malformation (VLM) of tongue. The patient was treated by sclerotherapy using intralesional injections of 3% sodium tetradecyl sulfate (STS). The lesion was injected with insulin syringe at multiple sites, using approximately 0.3–0.4 mL of solution. Postinjection, manual compression using gauze piece was given for 3 min. She reported transient pain and burning sensation immediately after the injection that resolved spontaneously in the next 15 min. No delayed adverse effects were noted. Two such injections at 3-week interval led to complete resolution of the lesion [Figure 1B] with no recurrence after 1 year.",
                "authors": "A. Singal, S. Bhatt",
                "citations": 3
            },
            {
                "title": "Fast photometric variability of very low mass stars in IC 348: detection of superflare in an M dwarf",
                "abstract": "We present here optical I-band photometric variability study down to $\\simeq$ 19 mag of a young ($\\sim$2-3 Myr) star-forming region IC 348 in the Perseus molecular cloud. We aim to explore the fast rotation (in the time-scales of hours) in Very Low Mass stars (VLMs) including Brown Dwarfs (BDs). From a sample of 177 light-curves using our new I-band observations, we detect new photometric variability in 22 young M-dwarfs including 6 BDs, which are bonafide members in IC 348 and well-characterized in the spectral type of M-dwarfs. Out of 22 variables, 11 M dwarfs including one BD show hour-scale periodic variability in the period range 3.5 - 11 hours and rest are aperiodic in nature. Interestingly, an optical flare is detected in a young M2.75 dwarf in one night data on 20 December 2016. From the flare light curve, we estimate the emitted flared energy of 1.48 $\\times$ 10$^{35}$ ergs. The observed flared energy with an uncertainty of tens of per cent is close to the super-flare range ($\\sim$ 10$^{34}$ ergs), which is rarely observed in active M dwarfs.",
                "authors": "Samrat Ghosh, S. Mondal, S. Dutta, R. Das, S. Joshi, S. Lata, Dhrimadri Khata, A. Panja",
                "citations": 3
            },
            {
                "title": "Glutamate receptor plasticity in brainstem respiratory nuclei following chronic hypercapnia in goats",
                "abstract": "Patients that retain CO2 in respiratory diseases such as chronic obstructive pulmonary disease (COPD) have worse prognoses and higher mortality rates than those with equal impairment of lung function without hypercapnia. We recently characterized the time‐dependent physiologic effects of chronic hypercapnia in goats, which suggested potential neuroplastic shifts in ventilatory control mechanisms. However, little is known about how chronic hypercapnia affects brainstem respiratory nuclei (BRN) that control multiple physiologic functions including breathing. Since many CNS neuroplastic mechanisms include changes in glutamate (AMPA (GluR) and NMDA (GluN)) receptor expression and/or phosphorylation state to modulate synaptic strength and network excitability, herein we tested the hypothesis that changes occur in glutamatergic signaling within BRN during chronically elevated inspired CO2 (InCO2)‐hypercapnia. Healthy goats were euthanized after either 24 h or 30 days of chronic exposure to 6% InCO2 or room air, and brainstems were rapidly extracted for western blot analyses to assess GluR and GluN receptor expression within BRN. Following 24‐hr exposure to 6% InCO2, GluR or GluN receptor expression were changed from control (P < 0.05) in the solitary complex (NTS & DMV),ventrolateral medulla (VLM), medullary raphe (MR), ventral respiratory column (VRC), hypoglossal motor nucleus (HMN), and retrotrapezoid nucleus (RTN). These neuroplastic changes were not found following 30 days of chronic hypercapnia. However, at 30 days of chronic hypercapnia, there was overall increased (P < 0.05) expression of glutamate receptors in the VRC and RTN. We conclude that time‐ and site‐specific glutamate receptor neuroplasticity may contribute to the concomitant physiologic changes that occur during chronic hypercapnia.",
                "authors": "Nicholas J. Burgraff, S. Neumueller, Kirstyn J. Buchholz, M. Hodges, L. Pan, H. Forster",
                "citations": 7
            },
            {
                "title": "Formation of multiple low-mass stars, brown dwarfs, and planemos via gravitational collapse",
                "abstract": "The origin of very low-mass stars (VLMS) and brown dwarfs (BDs) is still an unresolved topic of star formation. We here present numerical simulations of the formation of VLMS, BDs, and planet mass objects (planemos) resulting from the gravitational collapse and fragmentation of solar mass molecular cores with varying rotation rates and initial density perturbations. Our simulations yield various types of binary systems including the combinations VLMS-VLMS, BD-BD, planemo-planemo, VLMS-BD, VLMS-planemos, BD-planemo. Our scheme successfully addresses the formation of wide VLMS and BD binaries with semi-major axis up to 441 AU and produces a spectrum of mass ratios closer to the observed mass ratio distribution (q > 0.5). Molecular cores with moderate values of the ratio of kinetic to gravitational potential energy (0.16 <= beta <= 0.21) produce planemos. Solar mass cores with rotational parameters beta outside of this range yield either VLMS/BDs or a combination of both. With regard to the mass ratios we find that for both types of binary systems the mass ratio distribution varies in the range 0.31 <= q <= 0.74. We note that in the presence of radiative feedback, the length scale of fragmentation would increase by approximately two orders of magnitude, implying that the formation of binaries may be efficient for wide orbits, while being suppressed for short-orbit systems.",
                "authors": "Rafeel Riaz, S. Vanaverbeke, D. Schleicher",
                "citations": 8
            },
            {
                "title": ": Influence of pick time distribution on expected throughput of dual-tray VLMs",
                "abstract": null,
                "authors": "Goran Đukić, Tihomir Opetuk, H. Cajner, B. Gajšek",
                "citations": 1
            },
            {
                "title": "A Comprehensive Study of Adaptive LNA Nonlinearity Compensation Methods in Direct RF Sampling Receivers",
                "abstract": "This paper studies the effects of nonlinear distortion of Low Noise Amplifier (LNA) in the multichannel direct-RF sampling receiver (DRF). The main focus of our work is to study and compare the effectiveness of the different adaptive compensation algorithms, including the inverse-based and subtract-based Least Mean Square (LMS) algorithm with a fixed and variable step size. The models for the compensation circuits have been analytically derived. As the major improvements, the effectiveness of the compensation circuits under the ADC quantization noise effect is evaluated. The bit-error-rates (BER) in dynamic signal-to-noise ratio (SNR) scenarios are calculated. We have proposed the use of variable step-size LMS (VLMS) to shorten the convergence time and to improve the compensation effect in general. To evaluate and compare different compensation methods, a complex Matlab model of the Ultra high frequency (UHF) DRF with 4-QPSK channels was implemented. The simulation results show that all compensation methods significantly improve the receiver performance, with the convergence time of the VLMS algorithm does not exceed 5.10 samples, the adjacent channel power ratios (ACPR) are reduced more than 30 dBc, and the BERs decrease by 2–3 orders of magnitude, compared with the noncompensated results. The simulation results also indicate that the subtraction method in general has better performance than the inversion method.",
                "authors": "V. N. Anh, Hai-Nam Le, Tran Thi Hong Tham, Trinh Quang Kien",
                "citations": 1
            },
            {
                "title": "Analysis of hand segmentation on challenging hand over face scenario",
                "abstract": "One of the challenging problems in computer vision is hand segmentation, especially when the hands overlap with the face. There are many applications that require this type of segmentation, such as sign language recognition, action recognition and recognition of objects that hands interact with. Hand over face is a challenging scenario where faces are occluded by hands, that can be used to test the performance of hand segmentation methods. Not much work has been done on this topic. After analysis of related datasets for hand segmentation that include hands in front of or near to the face, we introduce our challenging public dataset for the hand-over-face segmentation problem. The new dataset contains 4384 annotated frames and includes color, depth, infrared streams recorded by Kinect. Additionally, hand(s) locations and shapes data using Leap Motion sensor, which is an infrared hand shape sensor, are included. We compare two leading semantic segmentation methods: SegNet [1] and RefineNet [12], to analyze the new dataset. Two experiments were executed: the first one for hand-background segmentation and the other one for right hand- left hand- background segmentation. RefineNet shows significantly better accuracy, 14% better than that of SegNet, on our new dataset. Nonetheless, the highest accuracy archived was 62.2%, demonstrating that VLM-HandOverFace1 is a challenging dataset for the current state of the art.",
                "authors": "Sakher Ghanem, Ashiq Imran, V. Athitsos",
                "citations": 6
            },
            {
                "title": "Quantum Calculus-based Volterra LMS for Nonlinear Channel Estimation",
                "abstract": "A novel adaptive filtering method called q-Volterra least mean square (q-VLMS) is presented in this paper. The q-VLMS is a nonlinear extension of conventional LMS and it is based on Jackson's derivative also known as q-calculus. In Volterra LMS, due to large variance of input signal, the convergence speed is very low. With proper manipulation, we successfully improved the convergence performance of the Volterra LMS. The proposed algorithm is analyzed for the step-size bounds and results of analysis are verified through computer simulations for nonlinear channel estimation problem.",
                "authors": "Muhammad Usman, M. Ibrahim, Jawwad Ahmad, Syed Saiq Hussain, M. Moinuddin",
                "citations": 5
            },
            {
                "title": "Advances in Document Layout Analysis",
                "abstract": "Handwritten Text Segmentation (HTS) is a task within the Document Layout Analysis field that aims to detect and extract the different page regions of interest found in handwritten documents. HTS remains an active topic, that has gained importance with the years, due to the increasing demand to provide textual access to the myriads of handwritten document collections held by archives and libraries. This thesis considers HTS as a task that must be tackled in two specialized phases: detection and extraction. We see the detection phase fundamentally as a recognition problem that yields the vertical positions of each region of interest as a by-product. The extraction phase consists in calculating the best contour coordinates of the region using the position information provided by the detection phase. Our proposed detection approach allows us to attack both higher level regions: paragraphs, diagrams, etc., and lower level regions like text lines. In the case of text line detection we model the problem to ensure that the system's yielded vertical position approximates the fictitious line that connects the lower part of the grapheme bodies in a text line, commonly known as the baseline. One of the main contributions of this thesis, is that the proposed modelling approach allows us to include prior information regarding the layout of the documents being processed. This is performed via a Vertical Layout Model (VLM). We develop a Hidden Markov Model (HMM) based framework to tackle both region detection and classification as an integrated task and study the performance and ease of use of the proposed approach in many corpora. We review the modelling simplicity of our approach to process regions at different levels of information: text lines, paragraphs, titles, etc. We study the impact of adding deterministic and/or probabilistic prior information and restrictions via the VLM that our approach provides. Having a separate phase that accurately yields the detection position (base- lines in the case of text lines) of each region greatly simplifies the problem that must be tackled during the extraction phase. In this thesis we propose to use a distance map that takes into consideration the grey-scale information in the image. This allows us to yield extraction frontiers which are equidistant to the adjacent text regions. We study how our approach escalates its accuracy proportionally to the quality of the provided detection vertical position. Our extraction approach gives near perfect results when human reviewed baselines are provided.",
                "authors": "Vicente Bosch Campos",
                "citations": 0
            },
            {
                "title": "DIFERENTIAL DIAGNOSES OF ANGIOKERATOMAS",
                "abstract": "Angiokeratomas (AK) are probably the vascular lesions that induce more confusion in the literature. The most accepted classification of AK was performed by Imperial and Helwig[1], and they divide these lesions into five classical types: Mibelli’s AK, Fordyce’s AK, corporis diffusum AK, circumscribed naeviforme AK and solitary or multiple adquired angiokeratomas. Nevertheless, in the clinical practice, is not unusual to see AK type lesions associated to different types of vascular anomalies, and these lesions have difficult handling. In ISSVA classification AKs have been included as vascular anomalies provisionally unclassified. We perform the differential diagnoses of AKs and according to clinical picture, histopathological aspect, immunohistochemical markers and radiological findings proposed to divide AKs mainly in two groups. Primary AKs are the classical types. The new inmunohistochemical findings suggest that these lesions might be included as mixed capillary-lymphatic malformations. Secondary AKs are related with different vascular anomalies, secondary to other process with lymphatic obstruction, related to drugs, or associated with no vascular lesions as lymphoid lesions. Different underlying vascular anomelies might be related with AKs including deep capillary-lymphatic malformations (CLM), venous-lymphatic malformations (VLM), capillary-lymphatic-venous malformations (CLVM) (Klippel-Trenaunauy Sd), deep lymphatic malformations (LM), venous malformations (VM) as hyperkeratotic venous malformation, cavenomatous cerebral malformations, traumatic arteriovenous fistula and eccrine angiokeratomatous hamartoma. Clinical aspect, radiological studies and histopathological examination might help to do a correct diagnosis of this heterogenous entity. Jesús del Pozo, Felipe Sacristán, Manuel Gómez Tellado, Servicios de Dermatología, Anatomía Patológicay, Cirugía Pediátrica Unidad de Anomalías Vasculares Pediátricas, Instituto de Investigación Biomédica de A Coruña (INIBIC). Complexo Hospitalario Universitario de A Coruña (CHUAC). Sergas. Universidade da Coruña (UDC). As Xubias, 84. 15006, A Coruña, España. ABSTRACT",
                "authors": "J. Pozo, F. Sacristán, M. Tellado, S. Dermatología, Anatomía Patológicay, Cirugía Pediátrica",
                "citations": 0
            },
            {
                "title": "A new look at some old shortleaf pine progeny tests: lessons for silvicultural opportunities through partnerships.",
                "abstract": "—Starting in the 1980s, 155 shortleaf pine progeny tests were established by the USDA Forest Service on national forests across the range the species. Originally intended to support the agency’s timber management program (post-clearcutting and subsequent reforestation with planting), these progeny tests were largely abandoned as the Forest Service’s forest management policies changed. Over the years, some of these shortleaf pine progeny tests were lost to natural disturbances or harvested, but many still remain as more-or-less intact outplantings. Recently, large-scale planting needs to support shortleaf pine restoration on public lands has reignited interest in these established progeny tests, spurring the Southern Region (with the assistance of the Southern Research Station) to take another look at them. Recently, the Southern Region’s Forest Management Unit and the Southern Research Station (SRS) have partnered to reevaluate some shortleaf pine (Pinus echinata) progeny tests established in the 1980s and early 1990s in Arkansas and Oklahoma (Hossain et al., in press). Although not established as a research trial, the progeny tests that remain in good condition still retain useful information that can help managers address concerns with this declining species across its natural range. This may be particularly true if new DNA marker technology can shed further light on the genetic nature of these shortleaf pine families, or if sufficient numbers remain to make statistical comparisons of growth and yield performance. If successful, the partnership between the Southern Region and SRS may be expanded regionally to include more progeny tests on other national forests, with emphasis placed on the subregions (e.g., southern Appalachians, Piedmont) that have experienced the most dramatic declines in shortleaf abundance. To date, staff of the SRS and Ouachita and Ozark–St. Francis National Forests have investigated dozens of these progeny tests and have formally resampled 14 of these tests. Field sampling began in 2018 and will continue into at least the fall of 2019, with limited assessments conducted to date (for example, Hossain et al. 2020; Hossain et al., in press). From this analysis, we can make a few preliminary assessments on the potential of this effort for this paper. First, although many of these progeny tests were not logged or destroyed by natural disturbances, some had received enough damage from past harvests, fires, ice storms, beetles, and other forest health problems that we chose not to sample them because too few families remained sufficiently intact. For instance, many—if not most—of the pines in these progeny tests had been impacted by multiple glaze events that have struck Arkansas and 1 Project Leader and Research Forester (DCB) and ORISE Fellow (SMH), USDA Forest Service, Southern Research Station, P.O. Box 3516 UAM, Monticello, AR 71656; Regional Geneticist (BSC), USDA Forest Service, Southern Regional Office, Atlanta, GA; USDA Forest Service, Southern Research Station, Monticello, AR; Forestry Technician (VLM), USDA Forest Service, Southern Research Station, Hot Springs, AR; and Project Leader and Research Geneticist (CDN), USDA Forest Service, Southern Research Station, Lexington, KY. DCB is corresponding author: to contact, call 870-367-3465 or email at don.c.bragg@usda.gov.",
                "authors": "D. Bragg, B. Crane, S. Hossain, V. McDaniel, C. Nelson",
                "citations": 0
            },
            {
                "title": "BRM Targeting Compounds for the Potential Treatment of Cancer.",
                "abstract": "Title. BRM Targeting Compounds and Associated Methods of Use. Patent Application Number. WO 2019/195201 A1 Publication Date. October 10, 2019 Priority Application. US 62/651,186 and US 62/797,754 Priority Date. April 01, 2018, and January 28, 2019 Inventors. Crew, A. P.; Wang, J.; Berlin, M.; Dragovich, P.; Chen, H.; Staben, L. Assignee Company. Arvinas Operations, Inc., Five Science Park, New Haven, Connecticut 06511 (US), and Genentech, Inc., 1 DNA Way, South San Francisco, California 94080-4990 (US). Disease Area. Cancer Biological Target. SMARCA Proteins Summary. Reversible protein phosphorylation plays a critical role in the regulation of various cellular processes, including intracellular signaling and protein−protein interaction. Major causes of tumorigenesis and cancer malignancy are consequences of aberrant phosphorylation and dephosphorylation of proteins in humans. Eukaryotic cells compact their DNA, the full genetic code for life in very small volume in the cell nucleus. Furthermore, complexing chromosomal DNA connects with core histone proteins to form chromatin, which allows the double-stranded DNA to be wrapped around an octameric core of histone proteins. Chromosomal DNA can pack tightly against one another and may impose a steric barrier that prevents transcription, replication, recombination, and DNA repair machineries from binding to the DNA. Accessibility to the DNA is facilitated by two classes of enzymes, ATP-dependent nucleosome remodelers and histone-modifying enzymes: the mammalian switch/sucrose nonfermentable (SWI/SNF) complexes and polycomb repressor complex 2 (PRC2). The mammalian SWI/SNF complexes (also called BAF complex for Brg-/Brama-associated factor complex) regulate chromatin remodeling and are powered by two closely related ATPase subunits, SMARCA4 and SMARCA2 (BRAHMA or BRM). SMARCA4 and SMARCA2 (SWI/SNF related, matrixassociated, actin-dependent regulator of chromatin, subfamily A, members 4 and 2, respectively) are coexpressed in most tissue types with the exception of embryonic stem cells in which SMARCA4 is the only ATPase expressed unit. SWI/SNF complexes utilize the energy derived from ATP hydrolysis to achieve the coordinate mobilization of nucleosomes and the modulation of chromatin accessibility. Chromatin remodeling complexes (CRCs) utilize the energy of ATP hydrolysis to disrupt DNA−nucleosome contacts, also move nucleosomes along the DNA, and eject or exchange nucleosomes. SWI/SNF complexes are required for several aspects of development and more recently have been strongly implicated in human disease. Mutations in the genes encoding the canonical SWI/SNF subunits are observed in nearly 20% of all cancers, including lung, ovarian, uterine, gastric, cervical, and esophageal. SMARCA4 is frequently mutated in primary tumors, while SMARCA2 inactivation is infrequent in tumor development. SMARCA2 has been shown to be an essential gene in SMARCA4-related cancer cell lines. Thus, SMARCA2 may be targeted in SMARCA4-related or -deficient cancers. However, lack of specificity and the inability to target and modulate SMARCA2 remains an obstacle to the development of effective treatments. Consequently, small-molecules that target SMARCA2 and potentiate the VHL’s substrate specifically may help provide promising therapeutic agents. The present Patent Highlight describes bifunctional (proteolysis targeting chimeric, PROTAC) compounds which function to recruit endogenous proteins to an E3 ubiquitin ligase for degradation. The PROTAC compounds have a broad range of pharmacological activities, and an effective amount is used for the treatment or amelioration of disease conditions such as cancer, including SMARCA4-related or -deficient cancer. Furthermore, the PROTAC compounds comprise of an E3 ubiquitin ligase binding moiety (ligand for an E3 ubiquitin ligase or “ULM” group) and a moiety that binds a target protein (protein/polypeptide targeting ligand or “PTM” group) such as the target protein/polypeptide that is placed in close proximity to the ubiquitin ligase to effect degradation of that protein. The ubiquitination ligase modulator (ULM) can be Von Hippel− Lindau E3 ubiquitin ligase (VHL) binding moiety (VLM).",
                "authors": "Robert B. Kargbo",
                "citations": 0
            },
            {
                "title": "Contamination of chickens by Salmonella spp., in Brazil: an important public health problem",
                "abstract": "Introduction: Bacteria of the genus Salmonella are important pathogens involved in the contamination of various foods, such as chickens, and may cause food poisoning. Aim: The present study aimed to review the literature on the prevalence of chickens contaminated with Salmonella spp., which are commercializated in different Brazilian states. Material and methods: This was a literary review. The absolute frequency and the total percentage of contaminated samples was calculated and the Qui-square statistical test was applied, considering statistically significant p <0.05. Results: 616 publications were retrieved, but only 10 articles were included to compose the results. The cataloged studies were carried out in 14 different brazilian states, and it was observed that of 5,030 chicken samples analyzed, the mean prevalence of samples contaminated with Salmonellawas 7,3% (n= 365). In addition, the prevalence of samples in the different studies ranged from 2.5% to 44.6%. The most prevalent serotype was S. Enteritidis (28,7%) and a statistically significant association between the type of raw material for commercialization and the result of the chicken samples microbiological analysis was observed (p<0.001), where the carcasses represented 90.1% of the contaminated samples. Conclusion: Thus, the data presented in this study can serve as subsidy for the development of necessary, political or legislative, measures that allow a better control of commercialized chickens in Brazil.Descriptors: Salmonella; Foodborne Diseases; Epidemiology.ReferencesSouza GC, Gonsalves HRO, Gonsalves HEO, Coelho JLS. Caracteristica microbiologica da carne de frango. ACSA. 2014;10(1):12-17.Pinto LAM, Pinto MM, Bovo J, Mateus GAP, Tavres FO, Baptista ATA, et al. Aspectos ambientais do abate de aves: uma revisao. Rev Uninga. 2018;22(3):44-50.Oliveira ME, Oliveira RLZ, Souza MFLZ, Harada ES, Tech ARB. Desenvolvimento de sensores para monitoramento de ambiente aviario com enfase em controle termico. Int J Agric & Biol Eng. 2018;12(3):234-40.Cintra APR, Andrade MCG, Lazarini MM, Assis DCS, Silva GR, Menezes LDM, et al. Influence of cutting room temperature on the microbiological quality of chicken breast meat. Arq Bras Med Vet Zootc. 2016;68(3):814-20.Ruckert DAS, Pinto PSA, Santos BM, Moreira MAS, Rodrigues ACA. Pontos criticos de controle de Salmonella spp. no abate de frangos. Arq Bras Med Vet Zootec. 2009;61(2):326-30.Oliveira AP, Sola MC, Feistel JC, Moreira NM, Oliveira JJ. 2013. Salmonella enterica: genes de virulencia e ilhas de patogenicidade. Enciclopedia Biosfera. 2013;9(16):1947-72.Brasil. Ministerio da Saude [homepage na internet]. Surtos de doencas transmitidas por alimentos no Brasil [acesso em 15 jul 2018]. Disponivel em: http://portalarquivos2.saude.gov.br/images/pdf/2018/julho/02/Apresentacao-Surtos-DTA-Junho-2018.pdf.Borsoi A, Moraes HLS, Salle CTP, Nascimento VP. Numero mais provavel de Salmonella isoladas de carcacas de frango resfriadas. Cienc Rural. 2010;40(11):2338-42.Cardoso KF, Rall VLM, Mendes AA, Paz ICLA, Komiyama CM. Pesquisa de salmonella e coliformes termotolerantes em cortes de frango obtidos no comercio de Botucatu/SP. Hig Aliment. 2009;23(176/179):165-68.Cunha-Neto AD, Carvalho LA, Carvalho RCT, Dos Prazeres Rodrigues D, Mano SB, Figueiredo EES, Conte-Junior CA. Salmonella isolated from chicken carcasses from a slaughterhouse in the state of Mato Grosso, Brazil: antibiotic resistance profile, serotyping, and characterization by repetitive sequence-based PCR system. Poult Sci. 2018;97(4):1373-81. Duarte DAM, Ribeiro AR, Vasconcelos AMM, Santos SB, Silva JVD, Andrade PLA, et al. Occurrence of Salmonella spp. in broiler chicken carcasses and their susceptibility to antimicrobial agents. Braz J Microbiol. 2009;40(3):569-73.Medeiros MA, Oliveira DC, Rodrigues DP, Freitas DR. Prevalence and antimicrobial resistance of Salmonella in chicken carcasses at retail in 15 Brazilian cities. Rev Panam Salud Publica. 2011;30(6):555-60.Menezes LDM, Lima AL, Pena EC, Silva GR, Klein RWT, Silva CA, et al. Caracterizacao microbiologica de carcacas de frangos de corte produzidas no estado de Minas Gerais. Arq Bras Med Vet Zootec. 2018;70(2):623-27.Moreira GN, Rezende CSM, Carvalho RN, Mesquita SQP, Oliveira AN, Arruda MLT. Ocorrencia de Salmonella sp. em carcacas de frangos abatidose comercializados em municipios do estado de Goias. Rev Inst Adolfo Lutz. 2008;67(2):126-30.Possebon FS, Costa LFZP, Yamatogi RS, Rodrigues MV, Sudano MJ, Pinto JPAN. A refrigeracao no diagnostico de Salmonella spp. utilizando o metodo microbiologico tradicional e reacao em cadeia da polimerase em carcacas de frango. Cienc Rural. 2012;42(1):131-35.Tessari ENC, Cardoso ALSP, Kanashiro AMI, Stoppa GFZ, Luciano RL, Castro AGM. Ocorrencia de Salmonella spp. em carcacas de frangos industrialmente processadas procedentes de exploracoes industriais do Estado de Sao Paulo, Brasil. Cienc Rural, 2008; 38(9):2557-60.Yamatogi RS, Galvao JA, Baldini ED, Souza Junior LCT, Rodrigues MV, Pinto JPAN. Avaliacao da unidade analitica na deteccao de Salmonella spp. em frangos a varejo. Rev Inst Adolfo Lutz. 2011;70(4):637-40.Sharma J, Kumar D, Hussain S, Pathak A, Shukla M, Kumar VP, et al. Prevalence, antimicrobial resistence and virulence genes characterization of montyphoidal Salmonella isolated from retail chicken meat shops in Northern India. Food Control. 2019;102:104-11.Harb A, Babib I, Mezal EH, Kareem HS, Laird T, O’dea M, et al. Ocurrence, antimicrobial resistence and whole-genome sequencing analysis of Salmonella isolates from chicken carcasses imported into Iraq from four different countries. Int J Food Microbiol. 2018;284:84-90.Zwe YH, Yentang VC, Aung KT, Gutierrez RA, Ng LC, Yuk HG. Prevalence, sequence types, antibiotic resistance and, gyrA mutations of Salmonella isolated from retail fresh chicken meat in Singapore. Food Control. 2018;90:233-40.Goni AM, Effarizah ME, Rusul G. Prevalence, antimicrobial resistance, resistance genes and class 1 integrons of Salmonella serovars in leafy vegetables, chicken carcasses and related processing environments in Malaysian fresh food markets. Food Control. 2018;91:170-80.Zhu J, Wang Y, Song X, Cui S, Xu H, Yang B, et al. Prevalence and quantification of Salmonella contamination in raw chicken carcasses at the retail in China. Food Control. 2014;44:198-202.Kramarenko T, Nurmoja I, Karssin A, Meremae K., Horman A, Roasto M. The prevalence and serovar diversity of Salmonella in various food products in Estonia. Food Control. 2014;42:43-7.Smadi H, Sargeant JM, Shannon HS, Raina P. Growth and inactivation of Salmonella at low  refrigerated storage temperatures and thermal inactivation on raw chicken meat and laboratory media: Mixed effect meta-analysis. Journal of Epidemiology and global Health. 2012;2(4):165-79.Cardoso ALSP, Tessari ENC. Salmonella enteritidis em aves e na saude publica: revisao da literatura. R cient eletr Med Vet. 2013;11(21).Realpe-Delgado ME, Munoz-Delgado AB, Donado-Godoy P, Rey-Ramirez LM, Diaz-Guevara PL, Arevalo-Mayorga SA. Epidemiologia de Salmonella spp., Listeria monocytogenes y Campylobacter spp., en la cadena productiva avicula. Iatreia. 2016;22(4):397-406.Shinohara NKS, Barros VB, Jimenez SMC, Machado ECL, Dutra RAF, Lima Filho JL. Salmonella spp., importante agente patogenico veiculado em alimentos. Cienc. saude coletiva. 2008;13(5):1675-83.Lv S, Si W, Yu S, Li Z, Wang X, Chen L, Zhang W, Liu S. Characteristics of invasion-reduced hilA gene mutant of Salmonella Enteritidis in vitro and in vivo. Res Vet Sci. 2015;101:63-8.Feasey NA, Hadfield J, Keddy KH, Dallman TJ, Jacobs J, Deng X, et al. Distinct Salmonella Enteritidis lineages associated with enterocolitis in high-income settings and invasive disease in low-income settings. Nat Genet. 2014;48(10):1211-17.",
                "authors": "Francisco Patricio de Andrade Júnior, Brenda Tamires de Medeiros Lima, Brencarla de Medeiros Lima, L. Cordeiro, V. S. A. Barbosa, E. Lima",
                "citations": 0
            },
            {
                "title": "Interactive comment on “The Zone of Influence: Matching sea level variability from coastal altimetry and tide gauges for vertical land motion estimation” by Julius Oelsmann et al",
                "abstract": "ZOI should be defined in the abstract, if space allows it. L24&59: Many thanks for citing my 2017 paper, but there is no need to add it twice to the reference list. 2017a/b should be 2017. L62: change accuracy by precision L271-273: “To confine the ZOI, we select subsets of the data containing the bestperforming statistics (i.e. highest correlation, lowest RMS SAT-TG or residual annual cycle) above the Xth-percentile according to the distribution of the statistic S in a 300 km radius around the tide gauges.” » It is not clear whether the TG and SAT series were detrended and deseasoned before comparing them. If the TG and SAT series were not deseasoned and the seasonal variation is prominent in the series, then there is the risk that the three metrics are telling us almost the same thing, that is, the impact of the amplitude and phase differences between the seasonal signals in both series. The correlation and the RMS of the differences may be more representative from deseasoned series, as done in past studies. In addition the authors fit the seasonal variation together with the linear trend in the SAT-TG series, i.e., the residual seasonal variation may play a minor role in the estimated VLM and its uncertainty. I’m not sure if this is",
                "authors": "A. Santamaría‐Gómez",
                "citations": 0
            },
            {
                "title": "Expression of Tacit Knowledge by Actors of Smart Technologies",
                "abstract": ". Industry 4.0 is an ever-expanding set of complementary smart technologies and at the same time a system of social interactions of people through technologies. The bottleneck of Industry 4.0 is a discrepancy of, on the one hand, informal dealing with knowledge performed by people meaningfully by means of natural language. On the other hand, the representations and operations of knowledge those being assumed and executed by computer without reference to human's reasoning. As a promising way to overcome this inconsistency, the article analyzes possibility of smart technologies development based on the concept of dual knowledge of Polanyi. There has been done an analysis of researches which touch the concept of dual knowledge and which propose methods to apply the concept in practice. The paper emphasizes the importance of taking into account the features of the use of natural language as a pragmatic way of expressing tacit knowledge by actors of smart technologies. To assist actors expressing their latent ideas the paper proposes a visual linking mechanism (VLM) for natural language utterances. VLM allows creating a technology that assists a person in a “spiral” process of expressing tacit knowledge and coordinates his own outcomes with the views of other people. The article explains the functioning of the VLM by the example of the expression of ordinary human knowledge and reports VLM instrumental features.",
                "authors": "G. Kanygin, O. Kononova",
                "citations": 0
            },
            {
                "title": "Advances in Document Layout Analysis",
                "abstract": "Handwritten Text Segmentation (HTS) is a task within the Document Layout Analysis field that aims to detect and extract the different page regions of interest found in handwritten documents. HTS remains an active topic, that has gained importance with the years, due to the increasing demand to provide textual access to the myriads of handwritten document collections held by archives and libraries. This thesis considers HTS as a task that must be tackled in two specialized phases: detection and extraction. We see the detection phase fundamentally as a recognition problem that yields the vertical positions of each region of interest as a by-product. The extraction phase consists in calculating the best contour coordinates of the region using the position information provided by the detection phase. Our proposed detection approach allows us to attack both higher level regions: paragraphs, diagrams, etc., and lower level regions like text lines. In the case of text line detection we model the problem to ensure that the system's yielded vertical position approximates the fictitious line that connects the lower part of the grapheme bodies in a text line, commonly known as the baseline. One of the main contributions of this thesis, is that the proposed modelling approach allows us to include prior information regarding the layout of the documents being processed. This is performed via a Vertical Layout Model (VLM). We develop a Hidden Markov Model (HMM) based framework to tackle both region detection and classification as an integrated task and study the performance and ease of use of the proposed approach in many corpora. We review the modelling simplicity of our approach to process regions at different levels of information: text lines, paragraphs, titles, etc. We study the impact of adding deterministic and/or probabilistic prior information and restrictions via the VLM that our approach provides. Having a separate phase that accurately yields the detection position (base- lines in the case of text lines) of each region greatly simplifies the problem that must be tackled during the extraction phase. In this thesis we propose to use a distance map that takes into consideration the grey-scale information in the image. This allows us to yield extraction frontiers which are equidistant to the adjacent text regions. We study how our approach escalates its accuracy proportionally to the quality of the provided detection vertical position. Our extraction approach gives near perfect results when human reviewed baselines are provided.",
                "authors": "Vicente Bosch Campos",
                "citations": 0
            },
            {
                "title": "Conceptual Design & Validation of a Multi-Planetary Transit Vehicle",
                "abstract": "This is a space era for unmanned aerial vehicles (UAVs) with complex autonomous systems to have an objective research on interplanetary studies. The UAV design for the space application is a complicated approach due to its involvement with initial conditions such as the atmospheric data of that particular planet and in later stages, where the experimental testing setup requires a separate infrastructure to match that atmospheric conditions. So, where the planetary data required for design are density, temperature, viscosity, pressure and gravity. These are initial conditions which affects the design and performance parameters in the conceptual stage of the planetary UAV. This paper describes the UAV conceptual design methodology for the Mars application which includes the configuration selection, sizing, aerodynamic analysis and performance evaluation using low fidelity tools such as XFLR5 and OCTAVE (Low order code). The design performance is compared with Earth atmosphere and Martian atmosphere to estimate the impact created by the initial conditions. However, the performance validation for the design on Earth atmosphere is justified in correlation with XFLR5 and wind tunnel data of Sky walker X8. But the design on Mars atmosphere is correlated with the literature available from recent research studies and observed to be fair, which would an acceptable correlation during the conceptual stage. Keywords—Conceptual design, Aerodynamics, Peformance, Space application, UAVs, XFLR5, OCTAVE, Martian atmosphere,VLM, Low fidelity tools, space drones, Sky Walker X8, Wind tunnel Data.",
                "authors": "Sai Vinay Sandapeta, S. Parre, C. Srinath, Vaddempudi Sai Naveen, B. Nikhil, R. Bhagat",
                "citations": 0
            },
            {
                "title": "Perspectiva do uso de punica granatum e plantago major no tratamento de úlcera traumática",
                "abstract": "Introducao: A populacao demonstra um interesse cada vez maior por terapias menos agressivas, e os medicamentos fitoterapicos surgem como uma alternativa, levando-se em conta que as pessoas acreditam nos beneficios do tratamento natural e veem na fitoterapia um metodo de cura e prevencao mais barato e acessivel. Porem, a maior parte de informacoes sobre o uso de fitoterapicos na area da Odontologia provem da sabedoria popular, sendo ainda muito escassas pesquisas a respeito de indicacoes, efeitos toxicos e formas de uso das plantas. Objetivo: Buscou-se verificar a possibilidade de Punica granatum e Plantago major poderem interferir de forma positiva no tratamento da ulcera traumatica. Metodologia: Diante disso, foi realizada uma revisao de literatura em um periodo que compreende os anos de 2012 a 2018 nas plataformas de pesquisa Google Academico, Scielo (Scientific Eletronic Library Online) e PubMed (National Center for Biotechnology Information). Conclusao: A literatura relata que tanto a Plantago Major, mais conhecida como tanchagem, como a Punica Granatum(roma), possuem varias atividades farmacologicas, dentre as relevantes ao tratamento dessa patologia estao os efeitos anti-inflamatorios, cicatrizante, antimicrobiana e antioxidante, que podem atuar de maneira sinergica. De tal modo, urge que cada vez mais se fomente o interesse pela Fitoterapia aplicada a Odontologia.Descritores: Fitoterapia; Odontologia; Plantago Major; Roma (Fruta).ReferenciasAleluia CM, Procopio VC, Oliveira MTG, Furtado PGS, Giovannini JFG, Mendonca SMS. Fitoterapicos na odontologia. Rev Odontol Univ Cid Sao Paulo. 2017;27(2):126-34.Nascimento Junior BJ, Tinel LO, Silva ES, Rodrigues LA, Freitas TON, Nunes XP et al. Avaliacao do conhecimento e percepcao dos profissionais da estrategia de saude da familia sobre o uso de plantas medicinais e fitoterapia em Petrolina-PE, Brasil. Rev bras plantas med. 2016;18(1):57-66.Castro RD, Oliveira JA, Vasconcelos LC, Maciel PP, Brasil VLM. Brazilian scientific production on herbal medicines used in dentistry. Rev bras plantas med. 2014;16(3):618-27.Cruz MT. Fitoterapicos: Estudos com plantas para fins terapeutico e medicinal. Acervo da Iniciacao Cientifica, 2013.Souza GFM, Silva MRA, Mota ET, Torre AM, Gomes JP. Plantas medicinais x raizeiros: Uso na odontologia. Rev cir traumatol buco-maxilo-fac. 2016;16(3):21-9.Evangelista SS, Sampaio FC, Parente RC, Bandeira MFCL. Phytotherapics in Odontology: ethnobotanical study in Manaus. Rev bras plantas med. 2013;15(4):513-19.Barbosa VLSA, Nobrega DRM, Cavalcanti AL. Estudo bibliometrico de pesquisas realizadas com fitoterapicos na Odontologia. RBCS. 2012;16(2):123-30.Souza LRG. Prescricao de fitoterapicos por estudantes dos cursos de odontologia das universidades publicas do Rio Grande do Norte [monografia]. Natal: Universidade Federal do Rio Grande do Norte; 2014.Moreira W. Revisao de literatura e desenvolvimento cientifico: conceitos e estrategias para confeccao. Janus. 2004;1(1):19-31.Neville BW, Damm DD, Allen CM, Bouquot JE. In: Lesoes fisicas e quimicas. Rio de Janeiro: Elsevier; 2009.Nobre IBB, Athias RB. Lesoes bucais causadas pelo uso de proteses dentarias removiveis [monografia]. Porto Velho: Centro Universitario Sao Lucas; 2017.Peixoto APT, Peixoto GC, Alessandrettic R. Relacao entre o uso de protese removivel e ulcera traumatica: revisao de literatura. J Oral Invest. 2015;4(1):26-32.Teixeira DS. Efeito da aplicacao topica de clorexidina, iodopovidona e eritromicina no reparo de ulceras traumaticas em ventre lingual de ratos: Analise clinica, histologica e microbiologica [dissertacao]. Porto Alegre: Faculdade de Odontologia da Pontificia Universidade Catolica do Rio Grande do Sul; 2017.Ladehoff LV. Protese parcial removivel e suas possiveis falhas biologicas contribuicoes da literatura [monografia]. Florianopolis: Universidade Federal de Santa Catarina; 2015.Rossato AE, Pierini MM, Amaral PA, Santos RR, Citadini-Zanette V. Plantago major L.: Tansagem. In: Rossato AE. Fitoterapia racional: aspectos taxonomicos, agroecologicos, etnobotânicos e terapeuticos. Florianopolis: DIOESC; 2012.Ventura PAO, Jesus JPO, Nogueira JRS, Galdos-Riveros Alvaro Carlos. Analise fitoquimica e avaliacao da susceptibilidade antimicrobiana de diferentes tipos de extratos de Plantago major L. (Plantaginaceae). Infarma. 2016;28:33-9.Lacerda SRL. Estudo microbiologico da acao de extratos vegetais hidroalcoolicos sobre microorganismos bucais [monografia]. Campina Grande: Universidade Estadual da Paraiba; 2012.Monteiro MHDA. Fitoterapia na odontologia: Levantamento dos principais produtos de origem vegetal para saude bucal [monografia]. Rio de Janeiro: Instituto de Tecnologia de Farmacos – Farmanguinhos / FIOCRUZ; 2014.Najafian Y, Hamedi SS, Farshchi MK, Feyzabadi Z. Plantago major in traditional persian medicine and modern phytotherapy: a narrative review. Electron Physician. 2018;10(2):6390-99.Trevino S, Aguila-Rosas J, Gonzalez-Coronel MA, Carmona-Gutierrez G, Rubio-Rosas E, Lopez-Lopez G et al. Estudios preliminares de caracterizacion y accion cicatrizante de nanomatrices de ZnO con extracto de Plantago major en la piel de rata. Rev mex cienc farm. 2014;45(4):1-7. Baptista LBM. Avaliacao in vitro da atividade antimicrobiana e antioxidante de extratos fitoterapicos produzidos na pastoral da saude de venda nova do Imigrante-ES [dissertacao]. Vitoria: Universidade Federal do Espirito Santo - UFES; 2012.Santos KB. Teores de compostos bioativos e capacidade antioxidante das folhas da tansagem (Plantago major) [monografia]. Apucarana: Universidade Tecnologica Federal do Parana - UTFPR; 2017.Bezerra AS, Stankievicz AS, Kaufmann AI, Machado AAR, Uczay J. Composicao nutricional e atividade antioxidante de plantas alimenticias nao convencionais da regiao Sul do Brasil. Arq Bras Alim. 2017,2(3):182-88.Bhandary SK, Kumari S, Bhat VS, KP S, Bekal MP, Sherly S. Preliminary phytochemical screening of various extracts of punica granatum peel, whole fruit and seeds. J Health Sci. 2012;2(4):34-8.Santos MGC, Nobrega DRM, Arnaud RR, Santos RC, Gomes DQC, Pereira JV. Punica granatum Linn. prevention of oral candidiasis in patients undergoing anticancer treatment. Rev odontol UNESP. 2017;46(1):33-8.  Jasuja ND, Saxena R, Chandra S, Sharma R. Pharmacological characterization and beneficial uses of Punica granatum. Asian J Plant Sci. 2012;11(3):251-67.Tozetto JT, Tozetto AT, Hoshino BT, Andrighetti CR, Ribeiro EB, Cavalheiro L et al. Extract of Punica granatum L.: An alternative to BHT as an antioxidant in semissolid emulsified systems. Quim Nova. 2017;40(1):97-104.Rahimi HR, Arastoo M, Ostad SN. A comprehensive review of punica granatum (pomegranate) properties in toxicological, pharmacological, cellular and molecular biology researches. Iran J Pharm Res. 2012;11(2):385-400.Nascimento Junior BJ, Santos AMT, Souza AT, Santos EO, Xavier MR, Mendes RL et al. Estudo da acao da roma (Punica granatum L.) na cicatrizacao de ulceras induzidas por queimadura em dorso de lingua de ratos Wistar (Rattus norvegicus). Rev bras plantas med. 2016;18(2):423-32.Ghalayani P, Zolfaghary B, Farhad AR, Tavangar A, Soleymani B. The efficacy of Punica granatum extract in the management of recurrent aphthous stomatitis. J Res Pharm Pract. 2013(2):88-92.",
                "authors": "Alessandro Marques de Souza Júnior, Jordanya Feitosa Soares, Sérvulo da Costa Rodrigues Neto, André Paulo Gomes Simões, A. Filho",
                "citations": 0
            },
            {
                "title": "Benchmarking New CEASIOM with CPACS adoption for aerodynamic analysis and flight simulation",
                "abstract": "Purpose \n \n \n \n \nThe purpose of this paper is to present the status of the on-going development of the new computerized environment for aircraft synthesis and integrated optimization methods (CEASIOM) and to compare results of different aerodynamic tools. The concurrent design of aircraft is an extremely interdisciplinary activity incorporating simultaneous consideration of complex, tightly coupled systems, functions and requirements. The design task is to achieve an optimal integration of all components into an efficient, robust and reliable aircraft with high performance that can be manufactured with low technical and financial risks, and has an affordable life-cycle cost. \n \n \n \n \nDesign/methodology/approach \n \n \n \n \nCEASIOM (www.ceasiom.com) is a framework that integrates discipline-specific tools like computer-aided design, mesh generation, computational fluid dynamics (CFD), stability and control analysis and structural analysis, all for the purpose of aircraft conceptual design. \n \n \n \n \nFindings \n \n \n \n \nA new CEASIOM version is under development within EU Project AGILE (www.agile-project.eu), by adopting the CPACS XML data-format for representation of all design data pertaining to the aircraft under development. \n \n \n \n \nResearch limitations/implications \n \n \n \n \nResults obtained from different methods have been compared and analyzed. Some differences have been observed; however, they are mainly due to the different physical modelizations that are used by each of these methods. \n \n \n \n \nOriginality/value \n \n \n \n \nThis paper summarizes the current status of the development of the new CEASIOM software, in particular for the following modules: CPACS file visualizer and editor CPACSupdater (Matlab) Automatic unstructured (Euler) & hybrid (RANS) mesh generation by sumo Multi-fidelity CFD solvers: Digital Datcom (Empirical), Tornado (VLM), Edge-Euler & SU2-Euler, Edge-RANS & SU2-RANS Data fusion tool: aerodynamic coefficients fusion from variable fidelity CFD tools above to compile complete aero-table for flight analysis and simulation.",
                "authors": "A. Jungo, Mengmeng Zhang, J. Vos, A. Rizzi",
                "citations": 7
            },
            {
                "title": "Using binary statistics in Taurus-Auriga to distinguish between brown dwarf formation processes",
                "abstract": "Whether BDs form as stars through gravitational collapse (\"star-like\") or BDs and some very low-mass stars constitute a separate population which form alongside stars comparable to the population of planets, e.g. through circumstellar disk (\"peripheral\") fragmentation, is one of the key questions of the star-formation problem. For young stars in Taurus-Auriga the binary fraction is large with little dependence on primary mass above ~0.2Msun, while for BDs it is <10%. We investigate a case in which BDs in Taurus formed dominantly through peripheral fragmentation. The decline of the binary frequency in the transition region between star-like and peripheral formation is modelled. A dynamical population synthesis model is employed in which stellar binary formation is universal. Peripheral objects form separately in circumstellar disks with a distinctive initial mass function (IMF), own orbital parameter distributions for binaries and a low binary fraction. A small amount of dynamical processing of the stellar component is accounted for as appropriate for the low-density Taurus-Auriga embedded clusters. The binary fraction declines strongly between the mass-limits for star-like and peripheral formation. The location of characteristic features and the steepness depend on these mass-limits. Such a trend might be unique to low density regions hosting dynamically unprocessed binary populations. The existence of a strong decline in the binary fraction -- primary mass diagram will become verifiable in future surveys on BD and VLMS binarity in the Taurus-Auriga star forming region. It is a test of the (non-)continuity of star formation along the mass-scale, the separateness of the stellar and BD populations and the dominant formation channel for BDs and BD binaries in regions of low stellar density hosting dynamically unprocessed populations.",
                "authors": "M. Marks, E. Mart'in, V. B'ejar, N. Lodieu, P. Kroupa, M. E. Manjavacas, I. Thies, R. L'opez, S. Velasco",
                "citations": 7
            },
            {
                "title": "Development of biodegradable scaffolds loaded with vancomycin micropartricles for the treatment of osteomyelitis",
                "abstract": "In this present research work, the development of biodegradable scaffolds loaded with Vancomycin micropartricles was carried out for the treatment of Osteomyelitis. Characterization Vancomycin Loaded microparticles and evaluation of the microparticles loaded scaffolds and also to carry out In-vitro release studies. Vancomycin hydrochloride microparticles were prepared with the help of double emulsion method. HPMC and Polaxomer 407 has been taken as the main polymers for the preparation of the microparticles. Chitosan was taken as the major polymer for the preparation of scaffolds for its greater biocompatibility and biodegradability. The preparation was done with the help of the solvent casting method. The formulation was taken for further characterization and evaluation studies. Fourier-Transform Infrared Spectroscopy and Differential scanning calorimetry were carried out for the pure vancomycin drug, and the chitosan polymer X-ray diffraction was carried out to check the crystallinity of the prepared scaffolds. The particle size, zeta potential and polydispersity index for vancomycin loaded microparticles were found to be 577.0±102.5 nm, 1624 mv and 0.254. The maximum and sustained release rate of the drug was found to be 95.6±0.478, at 16th Hr. By taking all the reports, a conclusion can be drawn that, the formulated VLM biodegradable scaffolds will show burst release at the initial time of administration, which is essential for the wound healing activity and will be sustained throughout the process of treatment of osteomyelitis.",
                "authors": "Souvik Chakraborty, D. V. Gowda, N. VishalGupta.",
                "citations": 3
            },
            {
                "title": "Offline Delta-Driven Model Transformation with Dependency Injection",
                "abstract": null,
                "authors": "A. Boronat",
                "citations": 3
            },
            {
                "title": "Comparison of numerical predictions of the supersonic expansion inside micronozzles of micro–resistojets",
                "abstract": "The present work provides a numerical investigation of the supersonic flow inside a planar micronozzle configuration under different gas rarefaction conditions. Two different propellants have been considered, namely water vapor and nitrogen, which relate to their use in VLMs (the former) and cold gas microthrusters (the latter), respectively. Furthermore, two different numerical approaches have been used due to the different gas rarefaction regime, i.e. the typical continuum Navier–Stokes with partial slip assumption at walls and the particle–based Direct Simulation Monte Carlo (DSMC) technique. As a result, under high–pressure operating conditions, both water and nitrogen flows supersonically expanded into the micronozzle without chocking in combination with a linear growth of the boundary layer on walls. However, when low–pressure operating condition are imposed and a molecular regime is established inside the micronozzle, a very rapid expansion occurred close to the nozzle exit in combination with a strong chocking of the flow and a micronozzle quality reduction of about 40%. Furthermore, water exhibited specific higher specific impulse than nitrogen above 60%.",
                "authors": "M. D. Giorgi, D. Fontanarosa, A. Ficarella",
                "citations": 2
            },
            {
                "title": "Coastal 3D mapping using very high resolution satellite images and UAV imagery: new insights from the SAVEMEDCOASTS project",
                "abstract": "Global climate changes are a main factor of risk for infrastructures and people living along the coasts around the world. In this context, sea level rise, coastal retreat and storm surges pose serious threats to coastal zones. In order to assess the expected coastal changes for the next decades, a detailed knowledge of the site’s topography (coastline position, DTM, bathymetry) is needed. This paper focuses on the use of very high resolution satellite data and UAV imagery for the generation of accurate very-high and ultra-high mapping of coastal areas. In addition, the use of very high resolution multi-spectral satellite data is investigated for the generation of coastal bathymetry maps. The paper presents a study for the island of Lipari and the coasts of Cinque Terre (Italy) and the island of Lefkas (Greece). For Lefkas, two areas of the island were mapped (the city of Lefkas and its adjoining lagoon in the north side of the island, and the Bay of Vasiliki at the south part of the island) using World View 1, and Wolrd View 3 satellite images, and UAV imagery. The satellite processing provided results that demonstrated an accuracy of approximately 0.25 m plannimetrically and 0.70 m vertically. The processing of the UAV imagery resulted in the generation of DTMs and orthophotos with an accuracy of approximately 0.03-0.04 meters. In addition, for the Vasiliki bay in the south of the island the World View 3 imagery was used for the estimation of a bathymetry map of the bay. The achieved results yielded an accuracy of 0.4 m. For the sites of Lipari and Cinque Terre (both in Italy), UAV surveys allowed to extract a DTM at about 2 cm of pixel resolution. The integration of topographic data with high resolution multibeam bathymetry and expected sea level rise from IPCC AR5 2.6 and 8.5 climatic scenarios, will be used to map sea level rise scenarios for 2050 and 2100, taking into account the Vertical Land Motion (VLM) as estimated from CGPS data. The above-mentioned study was realized during the implementation of the SAVEMEDCOASTS project (Sea level rise scenarios along the Mediterranean coasts, funded by the European Commission ECHO A.5, GA ECHO/SUB/2016/742473/PREV16, www.savemedcoasts.eu).",
                "authors": "P. Patias, C. Georgiadis, M. Anzidei, D. Kaimaris, C. Pikridas, G. Mallinis, F. Doumaz, A. Bosman, V. Sepe, Antonio Vecchie",
                "citations": 3
            },
            {
                "title": "YAMTL Solution to the TTC 2018 Social Media Case",
                "abstract": "Software models raise the level of abstraction of software artefacts involved in the design, implementation and testing phases of software systems. Such models may be used to automate many of the tasks involved in them, where queries play an important role. Moreover, some of those models may be inferred automatically from existing software artefacts, e.g., by means of reverse engineering, yielding potentially very large models (VLMs). Technology to analyse VLMs eﬃciently enables the application of model-driven software development in industry and is the subject of study in the TTC 2018 Social Media Case. YAMTL is both a model transformation (MT) language that is available as an internal DSL of Xtend and a companion MT engine that can be used from any JVM application and that supports incremental execution of MT. In this paper, we present the YAMTL solution to the social media case and discuss its performance, scalability and memory usage w.r.t. the reference solution. The YAMTL solution was deemed to be the most scalable solution at the TTC 2018.",
                "authors": "A. Boronat",
                "citations": 3
            },
            {
                "title": "Study on Propulsion Performance according to Variation of Rake Distribution at Propeller Tip",
                "abstract": "Interest in fossil energy depletion and global warming has increased in recent years. The International Maritime Organization (IMO) has been gradually applying indicators related to energy efficiency for newly constructed ships from 2013. In particular, the Energy Efficiency Design Index (EEDI) represents the amount of carbon dioxide emissions generated during the transportation of 1tonne of cargo per mile, and it should be reduced by 30% by 2025, beginning with a 10% reduction in January 2013. As a result, research is continued to improve hull form and propulsion systems to reduce EEDI worldwide. For the propulsion system, research has been conducted for a long time to improve the propulsion performance by establishing a compound propulsion system. The compound propulsion system can be classified as pre device, main device, and post device. (Carton, 2015) Preswirl stator and Mewis duct, etc. are known to be somewhat more effective in pre device, and highperformance special propeller, contra rotating propeller and duct propeller are known to be effective as main device. Although the performance varies depending on the ship types, the approximate energy reduction effect is known to be about 3 to 4%. In addition, twisted rudder, rudder bulb and fin are known to be effective as post device. The performance of rudder has been improved by modifying the shape of the rudder or by installing an additive such as a fin or bulb. This study deals with the Kappel Propeller (Forward type) and Tip Rake Propeller (Backward type) which are known as newly developed propeller concept. In the Kappel propeller the tip is located on the suction side of the propeller, the transition between the blade and the tip is smooth, the tip is hydro-dynamically loaded and it is designed so as to contribute to the generation of lift and thrust. It should be noted that in the CLT propeller, contrary to Kappel propellers, the end plates are unloaded and operate as barriers, avoiding the communication of water between the pressure and the suction side of the blades, allowing to establish finite load at the tip of the blade. (G. Gennaro, et al., 2012) Both propellers are from Tip Fin Propeller concept that originally came from winglet which is widely used in aircraft. Winglets reduce the induced drag by weakening the vortex at the wing accordingly lead to a higher efficiency (Ha, et al., 2014). In other words, the two propellers have the principle of preventing the three-dimensional effect of vortex by reducing the fluctuating pressure changes at the wing tips. However, the comparison of the performance and the design techniques of the above two types propellers is less known (Yamasaki, et al., 2013). The parametric study for the optimum rake shape has been conducted in the present study. The performance of the finally optimized propellers was verified by CFD and model tests. Figure 1 Kappel Propeller (Left), CLT Propeller (Right) THE DESIGN OF TIP RAKE PROPELLER BASED ON POTENTIAL ANALYSIS KP505 which has been designed for KRISO container ship was selected as the reference propeller. This propeller is widely used for academic and comparative purposes as its performance and geometry are opened called as KCS. This propeller might be proper for the present comparative study because no radial rake was applied. Rake was applied to the reference propeller by adopting the parameters as the starting radii, the maximum rake size, and the rake application type (backward and forward). The starting points of rake distribution were selected as 0.4R, 0.5R and 0.6R and along to the blade tip. If the starting point is earlier than 0.4R or after 0.7R, it is very difficult to induce the rake effect actually. The maximum rake variation is from 1% to 10% of the propeller diameter. If it is larger than 10%, the profile of the propeller has excessive curvature and very difficult to be continuous along radii. Figure 2 represents the radial distribution of the rake applied to the propeller design of the forward type and backward type respectively. The rake shape along radii was designed as a sine curve. Figure 2 Rake distributions depending on radius Numerical analysis of the propeller open water performance is performed using KPA4 that is a potentialbased propeller analysis program. Potential-based numerical analysis has been used commonly in the study of the open water performance of the propeller (Kim, et al., 1993; Kim & Lee, 2005). The KPA4 has been developed based on the VLM (Vortex Lattice Method). The diameter of the model propeller for POW analysis is 250mm and the propeller rotation speed is 16rps. The analysis was performed with J=0.05 to J=1.00 at an interval of 0.05. The computed POW efficiency of the propellers were compared at designed KT/J 2 =0.4725 as shown in Figure 3. Based on these results, the optimal shape was selected for the two types of forward and backward propellers. The optimal was selected for the backward type which is the case of STP=0.4R and xG/D=0.02 was higher than the reference propeller as 2.15% and for the forward type, STP=0.5R and xG/D=0.03 is 3.35% higher. After the decided optimum rake has been applied to the propeller there was some non-smooth surface in the 3-D modeling. The discontinuity was observed as shown in Figure 4. Therefore the rake distribution was smoothly modified as shown in Figure 5. After the finally faired rake was applied the improvement of 1.9% by forward and 2.2% by backward was obtained respectively compared to the reference propeller which is shown in Table 3.",
                "authors": "J. Kang, Moon-Chan Kim, H. Kim, I. Shin",
                "citations": 1
            },
            {
                "title": "Design, fabrication and characterization of mems based micro heater for vaporizing liquid microthruster",
                "abstract": "Demand and development in the space industry are conditioning more costeffective satellites without a reduction in functionality. One of the means of satisfying thus requirements is miniaturization. Constructing, smarter, lighter and cheaper satellites to reduce the cost of delivering it to the orbit as the main chunk of mission cost falls on delivering the satellite to space. Lighter versions of satellites called nano-satellites (CubeSats) which could be fabricated in mass scale in a reproducible manner might be a suitable solution. In simple words nanosatellite is a state of the art device packed with advanced logic electronics, sensor and actuator arrays as well as power management systems, etc. to perform numerous tasks. Considering the already high cost for large scale integration of various components it is very important to extend the lifetime of the devices as much as possible. Beside radiation which is one of the main reason for the failure of the components, satellites are useful only if they remain in the orbit and not deviate from the specific path it was set due to drag. Therefore the propulsion system is required to control the altitude to increase the operational lifetime of the satellite. The project is a continuation of the work in realizing a new generation of noble green Vaporizing Liquid Micro Thruster (VLM) functioning in environmentally friendly propellant(water) fabricated in Else Kooi Laboratory (EKL) with the cooperation of Aerospace Engineering (AE) faculty with an intention of contributing to the DELFFI mission. Thesis includes fabrication and testing of a thruster with a heavy emphasis on improving the performance of the heating chamber. Fabrication was accomplished by designing modules separately and assembling them through part by part exposure in the lithography. Such an approach was taken due to the need for systematic analysis of numerous factors affecting performance by altering them and observe the effect on performance. Several heating chambers were designed with varying performance attributes and tested on its ability to deliver heat efficiently as well as affect to thruster performance like pressure drop. In addition to it, several fabrication techniques not used previously for the fabrication of thrusters were experimented and implemented on making various channel structures giving freedom of more 3D design. As a result novel type of Microelectromechanical Systems (MEMS) VLM heating chambers were fabricated by modular design and tested. Developed process is robust and flexible which allows to manufacture different types of VLM thrusters depending on operation demand and suited for integration. The novelty introduced in the project is step-wise heating as well as the application of a new type of localized heating chamber with improved geometry for enhanced heating efficiency and for wall temperature measurements.",
                "authors": "Ali Kurmanbay",
                "citations": 1
            },
            {
                "title": "Transmission Microscopy: Beginning Automation",
                "abstract": "The TEM [1] has been used for studying changes that occur in materials since it was first developed. Heating and straining holders were quickly developed and radiation damage (sometimes a misnomer) could be studied using any holder, of course! Today’s TEMs allow us to study these processes with atomic resolution; we can actually determine locations with sub-Å lateral resolution. These advances are aided by new cameras that allow us to record processes in milliseconds (or innovative ‘illumination’ systems that allow us to record changes that may occur in nanoseconds—the DTEM). In an ETEM, or using an ‘environmental’ holder in a CTEM of STEM, we can now modify and control the environment around the specimen so that it is not necessarily UHV but could be an oxidizing gas or even a liquid [2]. Not only has resolution improved due to the achievement of Cs correction but CEOS and Nion have joined JEOL and the late FEI with acronyms for company names! Future TEMs may have much more space around the specimen when Cs correction becomes the norm, as it has long been for VLM.",
                "authors": "C. B. Carter, David B. Williams",
                "citations": 1
            },
            {
                "title": "ECONOMIC AND PERFORMANCE ANALYSIS OF DUAL-BAY VERTICAL LIFT MODULES (VLMS)",
                "abstract": "Warehouse picking is one of the most time and cost consuming activities in a warehouse, often requiring the presence of human operators, who travel within the aisles to retrieve the items needed by the customers. Several studies demonstrate that the travelling activity can represent even the 50% of the total picking time, with a subsequent creation of a separate storage and picking area for small objects. In the last years, new solutions for order picking systems have been developed, especially for small items. One of these solutions requires Vertical Lift Modules (VLMs), storage columns with extractable trays. In this paper, the employ of dualbay VLMs, compared to a traditional bin-shelving warehouse, has been analysed from an economic point of view. Some mathematical formulations have been developed, to estimate the total annual cost and the respective convenience limits of both systems, according to their productivity. Moreover, some useful guidelines for practitioners are derived.",
                "authors": "F. Sgarbossa, Martina Calzavara, A. Persona",
                "citations": 2
            },
            {
                "title": "Design Optimization of a Regional Transport Aircraft with Hybrid Electric Distributed Propulsion Systems",
                "abstract": "In recent years, there has been a growing shift in the world towards sustainability. For civil aviation, this is reflected in the goals of several organizations including NASA and ACARE as significantly increased fuel efficiency along with reduced harmful emissions in the atmosphere. Achieving the goals necessitates the advent of novel and radical aircraft technologies, NASA’s X-57, is one such concept using distributed electric propulsion (DEP) technology. Although practical implementation of DEP is achievable due to the scale invariance of highly efficient electric motors, the current battery technology restricts its adoption for commercial transport aircraft. A Hybrid Electric Distributed Propulsion (HEDiP) system offers a promising alternative to the all-electric system. It leverages the benefits of DEP when coupled with a hybrid electric system. One of the areas needing improvement in HEDiP aircraft design is the fast and accurate estimation of wing aerodynamic characteristics in the presence of multiple propellers. A VLM based estimation technique was developed to address this requirement. This research is primarily motivated by the need to have mature conceptual design methods for HEDiP aircraft. Therefore, the overall research objective is to develop an effective conceptual design capability based on a proven multidisciplinary design optimization (MDO) framework, and to demonstrate the resulting capability by applying it to the conceptual design of a regional transport aircraft (RTA) with HEDiP systems. Design Optimization of a Regional Transport Aircraft with Hybrid Electric Distributed Propulsion Systems Rajkumar Vishnu Ganesh GENERAL AUDIENCE ABSTRACT Recent years have seen a growing movement to steer the world towards sustainability. For civil aviation, this is reflected in the goals of key organizations, such as NASA and ACARE, to significantly improve fuel efficiency, reduce harmful emissions, and decrease direct heat release in the atmosphere. Achieving such goals requires novel technologies along with radical aircraft concepts driven by efficiency maximization as well as using energy sources other than fossil fuel. NASA’s all-electric X-57 is one such concept using the Distributed Electric Propulsion (DEP) technology with multiple electric motors and propellers placed on the wing. However, today’s all-electric aircraft suffer from the heavy weight penalty associated with batteries to power electric motors. In the near term, a Hybrid Electric Distributed Propulsion (HEDiP) system offers a promising alternative. HEDiP combines distributed propulsion (DiP) technology powered by a mix of two energy sources, battery and fossil fuel. The overall goal of the present study is to investigate potential benefits of HEDiP systems for the design of optimal regional transport aircraft (RTA). To perform this study, the aerodynamics module of the Pacelab Aircraft Preliminary Design (APD) software system was modified to account for changes in wing aerodynamics due to the interaction with multiple propellers. This required the development of the Wing Aerodynamic Simulation with Propeller Effects (WASPE) code. In addition, a Wing Propeller Configuration Optimization (WIPCO) code was developed to optimize the placement of propellers based on location, number, and direction of rotation. The updated APD was applied to develop the HERMiT 2E series of RTA. The results demonstrated the anticipated benefits of HEDiP technologies over conventional aircraft, and provided a better understanding of the sensitivity of RTA designs to battery technology and level of hybridization, i.e., power split between batteries and fossil fuels. The HERMiT 6E/I was then designed to quantify the benefits of HEDiP systems over a baseline Twin Otter aircraft. The results showed that a comparable performance could be obtained with more than 50% saving in mission energy costs for a small weight penalty. The HERMiT 6E/I also requires only about 38% of the mission fuel borne by the baseline. This means a correspondingly lower direct atmospheric heat release, reduction in carbon dioxide and NOx emissions along with reduced energy consumptions. This Thesis is dedicated to my wonderful family",
                "authors": "V. Rajkumar",
                "citations": 2
            },
            {
                "title": "Effective Identification and Prediction of Breast Cancer Gene Using Volterra Based LMS/F Adaptive Filter",
                "abstract": null,
                "authors": "Lopamudra Das, J. Das, S. Nanda",
                "citations": 0
            },
            {
                "title": "Comparative Study Between MEMS and Conventional Thrusters for Small Spacecraft Micropropulsion",
                "abstract": "Miniaturization of spacecraft has been gaining wide interest in the space industry, given its potential for reducing space missions' costs and providing a novel approach to enhancing and facilitating spaceflight. Recently, a lot of research has been successfully put into this field along with the advancements that make it more feasible, though a major obstacle to achieving the new generation of spacecraft is the technical challenge of fitting a suitable propulsion system. Useful chemical propellants are usually corrosive, flammable, and/or toxic, so alternatives need to be found. The aerospace industry is shifting towards green and nontoxic propulsion systems, so water could be used as an effective propellant, considering its relatively high mass density and low molecular mass. New microelectromechanical systems (MEMS) technologies show promising opportunities for the integration of miniaturized propulsion systems, due to their versatility and robustness. In this thesis, a comparative study of nozzle flow, heat transfer, and thermodynamics in two different thrusters is conducted. One thruster is based on MEMS, with a typically quasi-2D geometry, while the second thruster is based on more conventional technologies and manufacturing techniques, with an axially symmetrical 3D shape. After briefly introducing micropropulsion and discussing the propellant selection and nozzle fabrication along with the background theory related to micropropulsion as well as the analytical and OpenFOAM numerical (DSMC, continuum, and a hybrid approach containing both to accommodate to the variation in Knudsen number throughout the computational domain) modeling methods, the used methodology is based on using OpenFOAM's DSMC solver (dsmcFoam+) following the mesh creation using blockMesh and snappyHexMesh and developed analytical model (using MATLAB and CoolProp) along with an additional VLM ANSYS Fluent CFD model prepared in advance at TU Delft, where their (steady state as well as transient for DSMC) results (including the same conventional and MEMS nozzles) are processed and discussed. The nozzles are simulated for inlet pressures of 5 and 7 bar at inlet temperatures of 550 and 773 K for a total of four cases for each nozzle. To note, many of dsmcFoam+'s functionalities (mass flow rate measurements, inlet pressure boundary condition, axisymmetric capabilities, statistical error measurements, and dynamic load balancing) are implemented and described along with the full methodology, as Blender (with add-ons) and ParaView with a Python script to extract averaged data (along the nozzle and plume region) along with sampleDict are also used in pre and post-processing respectively and the simulations are carried out on a computer cluster. Furthermore, a quite interesting theoretical project on the side has been independently worked on in parallel. It started as a noticed idea that was decided to be explored using equations, which led to extended continuum/kinetic dimensionless numbers for diffusivity and rarefaction intensity relative to the studied object’s timescale. Ultimately, there are tradeoffs to choosing either thruster, where it is impractical to fault one nozzle for not performing better than the other, as it comes back to the desired features and nature of the mission each is undertaking, where compromises have to be made.",
                "authors": "K. Khamis",
                "citations": 0
            },
            {
                "title": "A search for fast photometric variability in very low mass stars and brown dwarfs",
                "abstract": "We present here I-band time-series photometric studies of a few very low-mass stars (VLMs) and brown dwarfs (BDs) in the young (age of 2-3 Myr) IC348 star-forming region in the Perseus Molecular Cloud. Our main aim is to explore the fast rotating (of time scale of a few hours) BDs and VLMs in the I-band. From our preliminary results, we find significant variability in the young M5 dwarf 2MASS J03443896+3203196 with a rotation period of 16.037 hours.",
                "authors": "Samrat Ghosh, S. Mondal, R. Das, S. Joshi, S. Lata, Siddhartha Biswas",
                "citations": 0
            },
            {
                "title": "A Practical Tool for Determination and Observation of 3D Wing Aerodynamic Characteristics at High Angles of Attack",
                "abstract": "In this study, a practical aerodynamic tool, which can calculate and show non-linear aerodynamic characteristics of three-dimensional wing, is developed to observe flow phenomena at high angles of attack. In particular, considering the success of the developed mathematical model in predicting preand post-stall aerodynamic behavior, some additions have been made which may be important in wing design and analysis stages. In this way, it is possible to calculate and visualize data such as surface pressure and velocity distribution, flow separation points, lift distribution, as well as aerodynamic performance coefficients of a wing in milliseconds on personal computer. Specifically, the program provides information on the effectiveness of the control surfaces at high attack angles by determining the flow separation points of the various wing geometries. The developed program stands out as a quick and practical tool in the early design stages. Thanks to the visualizations, this program will be very advantageous for both engineering education and R&D projects. INTRODUCTION Visualization has great bene ts in understanding physical events. For this reason, in many scienti c elds, it has been preferred to visualize an event to be investigated in an experimental setting. The biggest challenge for the test environment is its di cult and expensive installation and operation. However, with the rapid development of computers since the 1960s, it has become easier to solve physical problems at various levels. Especially after the 1990s, with the development of personal computers and programming languages, it has become possible to solve the mathematical model of many problems in any environment, convert numerical results into graphics and similar visual tools, create visualizations and animations by using graphical interfaces of computers. In aerodynamics education, it can be di cult for the students to understand the relationship between two-dimensional and three-dimensional ow. In addition, in some design studies, the behavior and ow characteristics of the wing in high angles of attack, especially in the stall region, is a matter of curiosity. It is possible to calculate the performance coe cients and visualize the ow characteristics ∗Res. Assist. in Aeronautical Engineering Department, Email: karalih@itu.edu.tr †Prof. in Aeronautical Engineering Department, Email: yukselen@itu.edu.tr AIAC-2019-147 Karali and Yukselen of the wing in these regions by using computational uid dynamics programs such as Fluent and OpenFOAM. However, these tools are not practical in terms of the computational power and time requirement [Cummings et al., 2015]. Low-order potential methods can partially meet these needs in the linear zone [Katz and Plotkin, 2001]. Programs such as XFLR5, Tornado VLM and OpenVSP, developed by various institutions or individuals, stand out as a fast, practical and inexpensive tool with the potential methods they use (Figure 1). However, these type programs can usually only perform linear region calculations. Y ukselen has developed various codes to simulate aerodynamics problems for use in aerodynamic education, and presented some of them in academic publishings [Yukselen, 2012, 2014 and 2015]. In an unpublished study, he developed also a simulation code applying lifting line method to visualize the pressure distribution on a wing and its variation with geometry and angle of attack. But this code is, again, works at low and moderate angles of attack.",
                "authors": "Hasan Karali, M. A. Yükselen",
                "citations": 0
            },
            {
                "title": "A GENETIC ALGORITHM BASED DESIGN OPTIMIZATION METHOD FOR WING-TAIL COMBINATIONS OF UAVs",
                "abstract": "The aim of this study is to develop a design optimization program considering aerodynamic, performance and weight criteria which can be used in the conceptual and preliminary design stages of aircraft design which is a multidisciplinary field. For this reason, low order analysis methods were preferred in order to make the program foldable in terms of cost and time. Therefore, design analysis program was developed by using Vortex Ring Method (VRM) for aerodynamic analysis, general performance formulas for performance calculation and statistical weight estimation formulas for weight estimation. For the optimization study, genetic algorithm, which is frequently preferred in design optimization studies, was preferred. In this direction, the design analysis program has been integrated with the genetic algorithm and the design optimization program has been developed. After the programs were tested by verification studies, they were used to optimize the wing of an UAV and its system consisting of wing and tail for various purposes. The results obtained from the application were evaluated and their usability for the complex design optimization problem was examined. Nomenclature AR Aspect Ratio W0 Aircraft Gross Weight Arf Airfoil WHT Horizontal Tail Weight b Wingspan of Lifting Surface WVT Vertical Tail Weight c Specific Fuel Consumption WVtail V Tail Weight larm Tail Moment Arm αdih Dihedral Angle nz Ultimate Load Factor αinc Incidence Angle q Dynamic Pressure αswp Sweep Angle R Location Vector αtws Twist Angle S Lifting Surfaces Area λ Taper Ratio t/c Airfoil Thickness Ratio η Propeller Efficiency Vcruise Cruise Speed 1 M. Sc. in Aeronautical and Astronautical Engineering, Email: yildizsih@itu.edu.tr 2 Prof. in Aeronautical Engineering Department, Email: yukselen@itu.edu.tr AIAC-2019-149 Yildiz & Yukselen 2 Ankara International Aerospace Conference INTRODUCTION Air vehicle design process is a complex and multidisciplinary field of study. The reason why this process is complex is not only that the works of the different disciplines affect each other, but also each work area has too many parameters that affect the air vehicle in itself. It is inevitable that a precise calculation of the effects of combinations of these parameters on the aircraft will bring this design process to an unbearable dimension in terms of cost and time. Therefore, there is a need for fast design tools that provide approximate results for use in the conceptual and preliminary design process of air vehicles. However, because of having too many parameters, having only fast design tools is not enough to make a successful and reliable design in a short time. Therefore, optimization programs are needed together with rapid analysis methods. Recently, the importance given to the defense industry and investments have increased in our country. Therefore, many air vehicle projects are being carried out. In the process of these air vehicle projects, knowledge of aircraft design is needed especially for the UAV. Therefore, the optimization and design analysis tools to be developed will contribute to the design processes of these air vehicles. In the conceptual and preliminary design stages of air vehicles, panel methods have been used widely due to their quick and acceptable approximate results. As the design process is multidisciplinary and depends on many parameters, optimization and different discipline analysis tools are needed in addition to the aerodynamic analysis programs to produce a successful aircraft design. In this context, several design optimization studies using panel methods are encountered in the literature. One of these studies is about the optimization of a single-element wing by using Vortex Lattice Method (VLM) with 2D viscous airfoil experimental data and an optimization technique including wing area, wingspan, taper ratio, leading edge sweep and geometric twist as optimization variables [Pinzon, 2008]. 2D data is taken from look-up tables according to the local lift coefficients of the wing sections obtained from VLM calculations. In another study, Hajec uses a non-linear lifting line method with 2D viscous airfoil solver XFOIL for single-wing aerodynamic analysis and an evolutionary optimization algorithm [Hajec 2009]. For the multielement lifting surface systems Demircan uses the Vortex Lattice Method for potential flow solutions and a genetic algorithm method for the optimization of wing-tail combinations [Demircan, 2018]. In this last study viscosity effects are ignored. The aim of this study is to develop a computer program which can optimize the wing-tail systems consisting of two or more lifting surface, to be used for various purposes, especially in unmanned aerial vehicle design works. METHOD In the present study, Vortex Ring Method (VRM) is applied for the solution of potential flow around lifting-surface systems (wing-tail combinations). However, in order to take into account also the viscosity effects, 2D experimental/numerical airfoil data is used. In addition, weight estimation and performance calculations were added to the program and statistical weight formulas were used for weight estimation. General performance formulas are used for performance calculations. These formulas are explained in the applications section. For the optimizations, one of the evolutionary algorithms, the genetic algorithm is preferred. Vortex Ring Method for Multi Element Lifting Surface Systems The Vortex Ring Method (VRM) used in this study is a version of Vortex Lattice Method (VLM). The details of its formulations for a single element wing can be found in several aerodynamics text books (see for example [Katz and Plotkin, 2001]). In the present paper a formulation of the method for multi-element lifting surfaces developed by Yükselen [Yükselen, 2015, 2017] is followed. AIAC-2019-149 Yildiz & Yukselen 3 Ankara International Aerospace Conference Both, in VLM and VRM the thickness effects of a wing are neglected and the wing is considered as a thin surface. This surface is divided into spanwise and chordwise panels as shown in Fig. 1. In VRM, vortex rings are placed on each panel and horse-shoe vortices at the trailing edge of the wing, while horse-shoe vortices placed on all of the panels in VLM. Figure 1: Vortex ring distribution and placement The front arm of the vortex ring is located on the quarter line of a panel, while the aft arm is located on the quarter line of the next panel in the direction of chord. The trailing arms of the vortex ring are located on the left and right sides of the panel. The trailing arms of the horseshoe vortices placed on the wing trailing edge go to infinity in the direction of free flow. The strengths of vortex rings are unknowns of the potential flow problem. In order to obtain vortex strengths, tangent flow boundary condition is applied at the control points on each panel, which results with a linear equation system. Control points are at the three quarter central point of each panel (centroid of vortex ring). With the application of the surface boundary condition at each control points of a multi-element lifting surface system the following equations system is obtained:",
                "authors": "S. Yildiz, Adil Yukselen",
                "citations": 0
            },
            {
                "title": "VAPORIZING LIQUID MICROTHRUSTER TESTING UNDER PULSED MODE OPERATION",
                "abstract": "Microthrusters are special category of propulsion device used to propel micro sized satellites. It is designed as per the mission requirements. There are various kinds of propulsion requirements such as continuous mode operation for orbit transfer (from one planet to another), orbit shift or adjustment for asteroid mining, pulsed mode operation for attitude control of satellites, and gravitation or solar drag compensation in orbit. Continuous mode operation is a high propellant consuming operation and designed cautiously to reach the destination with onboard available propellant. While pulsed mode operation is widely used for LEO (Low earth orbit) applications, where gravitation drag, atmospheric drag and solar drags are dominating. This paper focuses on the pulsed mode operation of vaporization liquid microthruster in vacuum operating condition. The pulsed mode operation involves timely thrust generation for the fine tuning of the positioning of the microsatellites. The operational timing in this mode of operation ranges from milliseconds to a few seconds at maximum. The operating time is decided based on the adjustment requirement for the positioning of the microsatellites. Vaporizing liquid microthrusters use green propellant to produce thrust. Tests are conducted under vacuum condition to simulate the actual space conditions and corresponding results are plotted. Results has shown a maximum thrust value of 290 μN at 1 sec of valve operating time, 335 μN at 2 seconds, 413 μN at 3 seconds, 524 μN at 4 seconds and 590 μN at 5 seconds of valve operating time for 200°C of a constant VLM temperature respectively. The effect of the dibble volume has also been discussed for the vaporization liquid microthruster using di-ionized water as liquid propellant.",
                "authors": "R. Ranjan, F. Riaz, P. Lee, S. Chou",
                "citations": 0
            },
            {
                "title": "Ablation of PHOX2B‐Derived Astrocytes Results in Neuronal Dystrophy‐Like Neuropathology in the RTN",
                "abstract": "The transcription factor Phox2b is a regulator of the autonomic nervous system development and is especially expressed by respiratory circuits involved in chemosensory integration, as neurons of solitary tract nucleus (NTS) that relay information from peripheral chemoreceptors to the ventral respiratory column, and by central chemoreceptors located in the retrotrapezoid nucleus (RTN). In addition to neurons, recent studies show that astrocytes from ventral medullary surface also detect PCO2 increase, and contribute to respiratory drive. Since PHOX2B biased progenitor cells generate astrocytes that have a developmental connection with RTN neurons, the aim of this study is determine the extent to which selective ablation of these astrocytes affect synapse integrity in the retrotrapezoid nucleus. We utilized the astrocyte ablation technique described by Tsai et al., 2012. In this model, the astrocyte‐specific ALDH1L1 promoter induces GFP expression in the absence of cre recombinase, and in the presence of cre recombinase expresses diphtheria toxin A (DTA). We interbred ALDH1L1loxp‐GFP‐STOP‐loxp‐DTA mice to PHOX2Bcre mice. Transmission electron photomicrographs were captured from the RTN/VLM area and morphometric analysis was performed using FIJI. We noted that axon terminals in the mutants (PHOX2Bcre, ALDH1L1loxp‐GFP‐STOP‐loxp‐DTA) were on average larger in area and perimeter. Furthermore, we noted an increase in the percentage of axon terminals showing autophagic vacuoles/phagosomes. The proportion of axon terminals with other organelles, including mitochondria and dense core neurosecretory vesicles, was not different between groups. In summary, PHOX2B‐derived, astrocyte ablated mice show synaptic pathology characterized by dysmorphic neuroaxonal morphology, suggesting an underlying defect in the afferent input into the RTN. These data suggest that PHOX2B‐derived astrocytes may play a role in regulating the synaptic integrity of neuronal inputs into RTN neurons. Other experiments are being performed in order to elucidate if the cell body of these neurons is located in the commissural NTS, and also, if PHOX2b derived astrocytes depletion can imply local regulation of presynaptic autophagy, which can lead to neurodegeneration and breathing impair.",
                "authors": "Talita Melo e Silva, C. Czeisler, Summer R. Fair, D. Alzate-Correa, Ana Takakura, T. Moreira, J. Otero",
                "citations": 0
            },
            {
                "title": "Glutamate Receptor Plasticity in Brainstem Respiratory Nuclei Following Chronic Hypercapnia in Goats",
                "abstract": "Patients that retain CO2 with respiratory diseases such as chronic obstructive pulmonary disease (COPD) have worse prognoses and higher mortality rates than those with equal impairment of lung function without hypercapnia. However, little is known about the effects of chronic hypercapnia on the neurochemical mechanisms controlling brainstem respiratory nuclei (BRN). Common mechanisms of neuroplasticity involve changes in glutamate receptor expression/phosphorylation to modulate synaptic strength and network excitability, driven by changes in both AMPA (GluR) and NMDA (GluN) glutamate receptors. Accordingly, herein we tested the hypothesis that changes occur in glutamatergic signaling within the BRN during exposure to chronically elevated inspired CO2 (InCO2) induced hypercapnia. Healthy goats were euthanized after either 24 hours or 30 days of chronic exposure to 6% InCO2 or room air. The brainstems were extracted for western blot analyses to assess GluR and GluN receptor expression and phosphorylation state within the BRN. Compared to room air control goats, the following significant (P<0.05) changes were found after 24 hours of exposure to chronic hypercapnia: 1) AMPA receptor expression was greater, and NMDA receptor expression lower within the rostral solitary complex. 2) NMDA receptor expression/phosphorylation and APMA receptor phosphorylation were lower within the rostral ventrolateral medulla (VLM), while AMPA receptor expression/phosphorylation were greater in the caudal VLM. 3) AMPA receptor expression/phosphorylation were lower within the rostral retrotrapezoid nucleus. And, 4) NMDA receptor expression was greater within the caudal Raphe nucleus. The data indicate that within 24 hours of chronic hypercapnia changes have occurred in four BRN in GluR/GluN signaling which may contribute to an attenuated CO2‐induced hyperpnea and attenuated CO2 chemoreflex between 1 and 24 hours of hypercapnia (J. Physiol. 2018). GluR/GluN changes found at 24 hours of hypercapnia had returned to control levels after 30 days of hypercapnia. However, compared to room air control goats, the following significant (P<0.05) changes were found following 30 days of hypercapnia: 1) NMDA receptor phosphorylation was greater in the caudal ventral respiratory column, and 2) AMPA receptor expression was greater in the caudal RTN. The changes in GluR/GluN signaling between 24 hours and 30 days of chronic InCO2 may contribute to a sustained hyperpnea and normalization of the CO2 chemoreflex at 30 days.",
                "authors": "Nicholas J. Burgraff, Kirstyn J. Buchholz, S. Neumueller, L. Pan, M. Hodges, H. Forster",
                "citations": 0
            },
            {
                "title": "Coupling a Numerical Optimization Technique with a Panel Method or a Vortex Lattice Method to Design Cavitating Propellers in Non-Uniform Inflows",
                "abstract": "In this study, a nonlinear optimization method, which is coupled with either a panel method or a vortex lattice method, is used to design open propellers in uniform, circumferentially averaged or non-uniform inflow. A B-spline geometry with 4× 4 control points is used to ensure that the propeller blade is accurately defined with fewer parameters. The optimization objective is to maximize the efficiency of the propeller while satisfying the given propeller thrust, and different cavity area or pressure constraints are applied. The influence of those constraints are studied, and propeller geometries are designed in different cases. The optimal efficiency as a function of the thrust coefficients are compared with those from other references, and the optimal circulations from this method are compared with those predicted from the lifting line optimization theory. It is shown that this method satisfies the optimization objectives and can be used in the practice of designing cavitating propellers. Keyword: optimization; panel method; vortex lattice method; cavitating propeller design Introduction The preliminary design of marine propellers has been gradually developing in the past. Generally, the methods used can be categorized into two types [Kerwin and Hadler, 2010]. The first one involves using systematic series of propellers, whose open water characteristics are already known, either from experimental tests or from computational fluid dynamic simulations [Yeo et al., 2014]. A widely seen example is by using the Wageningen B-screw series [van Lammeren et al., 1969]. The second type of methods do not require and are not limited to the known propeller series, but are based on the optimal radial distribution of circulation. These methods need the circumferentially averaged inflow, and are more adapted to the nominal wake filed downstream of the ship hull. An alternative approach can be seen as an improvement of the second type of methods. Mishima and Kinnas [1997] developed a numerical optimization method which is coupled with a Vortex Lattice Method (VLM) to design the cavitating propellers in the non-uniform inflow. A major advantage of this method is that it considers the circumferential variance of the inflow, which is important in predicting the periodic pressure fluctuations and the vibrations on the ship hull caused by the propeller in unsteady non-axisymmetric inflows. This method uses a B-spline to represent the blade geometry, and the optimization objective and constraints (thrust, torque, cavitation area, etc.) are approximated via second-order Taylor expansions. This method allows a constrained amount of cavitation in the design and thus can be applied to design propellers working under a moderate or even high loading. A numerical code called CAVOPT-3D (3-Dimensional CAVitating propeller blade OPTimizaiton) was developed and later improved by Griffin and Kinnas [1998] to include the minimum pressure constraints and a one-variable quadratic skew optimization. In the Ocean Engineering Group of University of Texas at Austin, the prediction of flows around propellers has been studied intensively in the past decades. The two major codes are MPUF-3A, which uses the VLM, and PROPCAV (PROPeller CAVitation), which uses the Boundary Element Method (BEM, more commonly known as the panel method). Some major improvements in MPUF-3A include: the thickness loading coupling scheme by Kinnas [1992], the unsteady wake alignment scheme and the non-linear terms in pressure evaluation by He [2010], etc. In PROPCAV, Kinnas et al. [2007] coupled the panel method with a boundary layer solver XFOIL [Drela, 1989] and developed the viscous/inviscid interaction method; Tian and Kinnas [2012] introduced a pseudo-unsteady wake alignment scheme ∗Corresponding Author, Weikang Du: allendu1988@utexas.edu (full wake alignment scheme, or FWA) to improve the accuracy of the predicted propeller performances, especially at low advance ratios. In this paper, the nonlinear optimization method is coupled with the newest version of MPUF-3A and PROPCAV to design cavitating propellers. Different constraints will be applied, and various inflows, including uniform, circumferentially averaged and non-uniform inflow will be used. The efficiency and optimal circulation from the current method will be compared with those from other references [Kerwin and Hadler, 2010] or the lifting line optimization method [Menéndez Arán and Kinnas, 2014].",
                "authors": "Weikang Du, S. Kinnas",
                "citations": 1
            },
            {
                "title": "High fidelity aerodynamics models for blended wing body design",
                "abstract": "The Blended Wing Body (BWB) configuration is seen by many as one of the possible future protagonists of Civil Aviation, both for its potential benefits in terms of Aerodynamics performances and fuel consumption and for the possibility of incorporating Aero-Propulsive integration technology. However, the design of such an innovative aircraft does not come without any difficulties, which are mainly related to its unconventional shape and the lack of knowledge. In the framework of the development of a preliminary design tool (FAST), this work aimed at creating a High Fidelity Aerodynamics Model of the ISAE-OENRA BWB geometry, whose main purpose was the validation of the Lower Fidelity tools which are implemented at early stage phases of BWB design. The development of such a method allowed also to verify important properties of the flow around the geometry like efficiency, compressibility and trim, and paved the way to a further study of the stability and the control of the BWB configuration. The limits of this approach are to be found in the characteristics of the method (Euler equations) and its incapacity to picture important effects like friction Drag and separation. The following step of the research will be the implementation of a RANS model of the geometry. Citation: CERQUETANI L, SGUEGLIA A, BENARD E, SCHMOLLGRUBER P (2018) High Fidelity Aerodynamics Models for Blended Wing Body Design. Int Jr Rob and Auto Engg: IJARE-103. International Journal of Robotics and Automation Engineering, Issue 2018, Vol. 01 2 classical BWB geometry, draw conclusions on the choice of some standard design parameters and propose optimization strategies. The majority of these papers highlights the importance of reliability and cost efficiency in aerodynamics computations to fasten optimization routines and get accurate and consistent results. However, so far no systematic study capable of merging Drag and consumption minimization, trim and stability constraints, structure constraints and propulsion integration has been carried out [3]. FAST [5] is a software that has been developed by ONERA and ISAE-SUPAERO to perform preliminary design and optimization of the classical \"wing-tube\" configuration aircraft, and which is now being updated to be applied on Blended Wing Body unconventional configuration. The research behind the development of such a powerful tool needs the collaboration and the involvement of many disciplines, whose contribution may take different forms: results validation, models comparison and analysis, methodology study and information collection. In this context, the purpose of this work was to develop a High Fidelity Aerodynamics model of the Blended Wing Body geometry to test the methodology, collect information, verify Aerodynamics and Gas Dynamics properties and compare with results obtained with other methods. In particular, the philosophical approach which constitutes the basis of this work consists in validating Low Fidelity preliminary design methods (FAST, VLM) using a High Fidelity model. The project therefore aims at running Computational Fluid Dynamics (CFD) simulations, structured on one of the main sets of equations derived from NavierStokes equations and which constitute the basis of the numerical methods in Aerodynamics: Euler equations. The method has been fully developed and simulation cases have been studied. Compressibility, stability and efficiency are just some of the topics that have been studied thanks to the results of these simulations. This paper contains a description of all the steps which contributed to shape the methodology: familiarization with ISAE-ONERA Blended Wing Body geometry, creation of the mesh, choice of the solver and of the setting of the simulation parameters, data collection and results analysis.",
                "authors": "L. Cerquetani",
                "citations": 1
            },
            {
                "title": "High Fidelity Modelling for High Altitude Long Endurance Solar Powered Aircraft",
                "abstract": "High Altitude Long Endurance (HALE) platforms are the aerial platforms capable of flying in the stratosphere for long periods of time. This master thesis presents aircraft system identification procedures geared towards such fixed wing platforms where aerodynamic forces and moments are parametrically modelled with so-called stability and control derivatives. The first part of the thesis addresses local System identification procedures intended for controller synthesis at low altitude flights whereas the second part of the thesis deals with a preliminary study on a new global system identification method. \nThe local system identification procedure is based on the two step method, which offers flexibility regarding the aerodynamic structure. Therefore, it is suitable for the development of a system identification tool chain for various fixed wing platforms. Various system identification experiments have been conducted to collect flight test data. The parameters for the estimation of aerodynamic forces and moments are then found through an optimization procedure. Such parameters have been validated using a validation set from flight test data and their applicability for controller synthesis has been demonstrated. \nGlobal system identification typically requires the collection of flight test data at multiple points in the flight envelope and often, is combined with extensive Computational \nFluid Dynamics (CFD) solutions as well as wind-tunnel experiments. Such an approach is time consuming and costly. This thesis presents a new method to overcome the limitations of the current methodology by applying a Parameter search on VLM-based (Vortex Lattice Method) dynamic simulations of aircraft System identification manoeuvres and correcting the estimated models with available flight test data. The current study shows improvements in fidelity with decrease in Root Mean Squared Error (RMSE) by factor 0.2 and 0.5 for x-axis and z-axis forces in body frame respectively, while reducing the effort for obtaining a model with similar fidelity.",
                "authors": "Jongseo Lee",
                "citations": 2
            },
            {
                "title": "Lithic residue analysis at Star Carr",
                "abstract": "Ancient trace residues left on stone artefacts by people represent a source of potentially fruitful data about diet, technology, and behaviour, but their investigation is not problem-free. Rather, correct identification of degraded residues and determination of their natural or anthropogenic origin remains at the heart of current methodological development in lithic residue analysis. This thesis addresses these issues by examining: 1) 13 modern reference residues on flint flakes, 2) modern residues on 78 experimentally buried flint flakes at Star Carr and off-site, and 3) residue traces on 138 archaeological stone artefacts from Star Carr. The study of modern reference residues showed that only residue types bearing diagnostic structures can be confidently identified by visual analysis alone. The study of experimentally buried flakes showed that tree resin, softwood tissue, and red ochre preserved after both one month and 11 months burial periods and across three burial environments, and were the most likely candidates to be encountered archaeologically. When the archaeological material was examined using reflected visible light microscopy (VLM), hypotheses of residue origin based on visual observations were tested against chemical information collected from the residues. Importantly, the microscopic hypotheses of residue identity based on comparison with reference residues and published literature were, in nearly all cases, falsified by confocal Raman microspectroscopy (micro-Raman) and gas chromatography-mass spectrometry (GC-MS). Key identifications were: iron (III) oxide, gypsum, quartz, pyrite, and organics. Some residue samples also contained compounds consistent with pine tree resin, but this finding is considered preliminary. These results from stone artefacts highlight the need in lithic residue analysis for: 1) more careful consideration of chemical processes in the burial environment, and 2) further incorporation of appropriate scientific techniques to verify microscopic residue identifications.",
                "authors": "S. Croft",
                "citations": 2
            },
            {
                "title": "Benchmarking New CEASIOMwith CPACS adoption for aerodynamic analysis and fl ight simulation",
                "abstract": "Purpose – The purpose of this paper is to present the status of the on-going development of the new computerized environment for aircraft synthesis and integrated optimization methods (CEASIOM) and to compare results of different aerodynamic tools. The concurrent design of aircraft is an extremely interdisciplinary activity incorporating simultaneous consideration of complex, tightly coupled systems, functions and requirements. The design task is to achieve an optimal integration of all components into an efficient, robust and reliable aircraft with high performance that can be manufactured with low technical and financial risks, and has an affordable life-cycle cost. Design/methodology/approach – CEASIOM (www.ceasiom.com) is a framework that integrates discipline-specific tools like computer-aided design, mesh generation, computational fluid dynamics (CFD), stability and control analysis and structural analysis, all for the purpose of aircraft conceptual design. Findings – A new CEASIOM version is under development within EU Project AGILE (www.agile-project.eu), by adopting the CPACS XML dataformat for representation of all design data pertaining to the aircraft under development. Research limitations/implications – Results obtained from different methods have been compared and analyzed. Some differences have been observed; however, they are mainly due to the different physical modelizations that are used by each of these methods. Originality/value – This paper summarizes the current status of the development of the new CEASIOM software, in particular for the following modules: CPACS file visualizer and editor CPACSupdater (Matlab) Automatic unstructured (Euler) & hybrid (RANS) mesh generation by sumo Multi-fidelity CFD solvers: Digital Datcom (Empirical), Tornado (VLM), Edge-Euler & SU2-Euler, Edge-RANS & SU2-RANS Data fusion tool: aerodynamic coefficients fusion from variable fidelity CFD tools above to compile complete aero-table for flight analysis and simulation.",
                "authors": "A. Jungo, Mengmeng Zhang, A. Rizzi",
                "citations": 0
            },
            {
                "title": "Toxocariasis, una enfermedad que aún merece nuestra atención",
                "abstract": "Sr Editor.Dentro de las zoonosis parasitarias, que son de importancia en salud publica, se encuentra la Toxocariasis, esta es una enfermedad causada por estadios larvarios del nematodo de Toxocara canis y Toxocara cati que tiene como hospedero definitivo el intestino de perros y gatos (1), siendo otros de sus hospederos paratenicos el hombre, y algunos otros mamiferos (2). Esta enfermedad parasitaria se da en todo el mundo y a pesar de su alta prevalencia en los paises tropicales en desarrollo, la Toxocariasis se considera una enfermedad desatendida (3). La Toxocariasis humana se adquiere con la ingesta de huevos embrionados del parasito desde el suelo, larvas presentes en vegetales o en carne cruda de sus hospedadores (3,4). En el humano no se completa el ciclo vital del nematodo, despues de la ingesta, las larvas infecciosas eclosionan del huevo, el estadio inmaduro de la larva permanece en el humano por prolongados periodos de tiempo en el que se disemina causando danos en los diferentes tejidos que pueda localizarse (5). Esta zoonosis tiene muchas implicancias clinicas desde patologias oculares, viscerales, respiratorias y epidemiologicas (3,6), los metodos diagnosticos requieren de personal profesional entrenado (6). Clinicamente se pueden distinguir tres formas de presentacion de la enfermedad: el sindrome de Larva Migrans Visceral (SLMV), definido por un compromiso de organos como el higado, pulmon, piel, sistema nervioso, musculoesqueletico, rinon y corazon; el sindrome de Larva Migrans Ocular (OLM), caracterizado por una afeccion al ojo y a los nervios opticos y la Toxocariasis inaparente o encubierta, forma mas leve, que comprende un espectro clinico que va desde una infeccion casi asintomatica hasta la migracion de larvas a organos blanco especificos( 6,7). El SLMV es la manifestacion mas grave de Toxocariasis y se presenta particularmente en ninos menores de 5 anos (7). Es importante destacar que los articulos publicados sobre esta enfermedad en America Latina son limitados. En un estudio realizado en Lima – Peru se encontro 20,46% de seropositividad con alta proporcion en ninos de 1 a 10 anos (3). En otro estudio que se llevo a cabo al norte de Lima se encontro 31,1% (201/646) de los pacientes estudiados presentaron anticuerpos anti-toxocara(8). En una revision, se encontro que la seroprevalencia en America Latina oscila entre 1,8 y 66 % y varia en los paises, en Peru se aproxima el 32% de prevalencia, mientras que en Venezuela llega al 66% (9). La necesidad en el futuro de programas de control debe enfocarse en la biologia molecular de las proteinas que son usadas en inmunodiagnostico de VLM y OLM, asi como en el desarrollo de vacunas basadas en ADN que ofrezcan una proteccion de por vida. Anteriormente la identificacion de especies de Toxocara se determinaba por las caracteristicas morfologicas del parasito, sin embargo, actualmente se han desarrollado tecnicas moleculares mas precisas para el diagnostico, que implican el uso de ADN (10), pruebas serologicas para la deteccion de anticuerpos especificos mediante ensayos inmunoabsorbentes ligados a enzimas (ELISA) aumentando la sensibilidad y especificidad diagnostica, teniendo tambien menos reacciones cruzadas, asi como proteinas recombinantes para medir anticuerpos recombinantes especificos de Toxocara(2,11). El control adecuado de esta zoonosis parasitaria constituiria una gran disminucion de riesgos a la salud publica en paises en desarrollo, mayormente en el riesgo de infeccion en ninos menores de 10 anos que son mas susceptibles al contagio, y mas susceptibles a adquirir formas mas patogenas de Toxocariasis. Es necesario impulsar la colaboracion entre investigadores relacionada a la Toxocariasis, para lograr una investigacion superior a nivel mundial y sobre todo en los paises en desarrollo donde tiene mayor importancia por su alta incidencia, ademas tambien desde el punto de vista epidemiologico, clinico, la ecologico, molecular y tratamiento asociado con la Toxocariasis.",
                "authors": "S. Osores, Kelly Olivos Caicedo, V. E. F. Rojas",
                "citations": 0
            },
            {
                "title": "TOXOCARIASIS, UNA ENFERMEDAD QUE AÚN MERECE NUESTRA ATENCIÓN:",
                "abstract": "Sr Editor.Dentro de las zoonosis parasitarias, que son de importancia en salud publica, se encuentra la Toxocariasis, esta es una enfermedad causada por estadios larvarios del nematodo de Toxocara canis y Toxocara cati que tiene como hospedero definitivo el intestino de perros y gatos (1), siendo otros de sus hospederos paratenicos el hombre, y algunos otros mamiferos (2). Esta enfermedad parasitaria se da en todo el mundo y a pesar de su alta prevalencia en los paises tropicales en desarrollo, la Toxocariasis se considera una enfermedad desatendida (3). La Toxocariasis humana se adquiere con la ingesta de huevos embrionados del parasito desde el suelo, larvas presentes en vegetales o en carne cruda de sus hospedadores (3,4). En el humano no se completa el ciclo vital del nematodo, despues de la ingesta, las larvas infecciosas eclosionan del huevo, el estadio inmaduro de la larva permanece en el humano por prolongados periodos de tiempo en el que se disemina causando danos en los diferentes tejidos que pueda localizarse (5). Esta zoonosis tiene muchas implicancias clinicas desde patologias oculares, viscerales, respiratorias y epidemiologicas (3,6), los metodos diagnosticos requieren de personal profesional entrenado (6). Clinicamente se pueden distinguir tres formas de presentacion de la enfermedad: el sindrome de Larva Migrans Visceral (SLMV), definido por un compromiso de organos como el higado, pulmon, piel, sistema nervioso, musculoesqueletico, rinon y corazon; el sindrome de Larva Migrans Ocular (OLM), caracterizado por una afeccion al ojo y a los nervios opticos y la Toxocariasis inaparente o encubierta, forma mas leve, que comprende un espectro clinico que va desde una infeccion casi asintomatica hasta la migracion de larvas a organos blanco especificos( 6,7). El SLMV es la manifestacion mas grave de Toxocariasis y se presenta particularmente en ninos menores de 5 anos (7). Es importante destacar que los articulos publicados sobre esta enfermedad en America Latina son limitados. En un estudio realizado en Lima – Peru se encontro 20,46% de seropositividad con alta proporcion en ninos de 1 a 10 anos (3). En otro estudio que se llevo a cabo al norte de Lima se encontro 31,1% (201/646) de los pacientes estudiados presentaron anticuerpos anti-toxocara(8). En una revision, se encontro que la seroprevalencia en America Latina oscila entre 1,8 y 66 % y varia en los paises, en Peru se aproxima el 32% de prevalencia, mientras que en Venezuela llega al 66% (9). La necesidad en el futuro de programas de control debe enfocarse en la biologia molecular de las proteinas que son usadas en inmunodiagnostico de VLM y OLM, asi como en el desarrollo de vacunas basadas en ADN que ofrezcan una proteccion de por vida. Anteriormente la identificacion de especies de Toxocara se determinaba por las caracteristicas morfologicas del parasito, sin embargo, actualmente se han desarrollado tecnicas moleculares mas precisas para el diagnostico, que implican el uso de ADN (10), pruebas serologicas para la deteccion de anticuerpos especificos mediante ensayos inmunoabsorbentes ligados a enzimas (ELISA) aumentando la sensibilidad y especificidad diagnostica, teniendo tambien menos reacciones cruzadas, asi como proteinas recombinantes para medir anticuerpos recombinantes especificos de Toxocara(2,11). El control adecuado de esta zoonosis parasitaria constituiria una gran disminucion de riesgos a la salud publica en paises en desarrollo, mayormente en el riesgo de infeccion en ninos menores de 10 anos que son mas susceptibles al contagio, y mas susceptibles a adquirir formas mas patogenas de Toxocariasis. Es necesario impulsar la colaboracion entre investigadores relacionada a la Toxocariasis, para lograr una investigacion superior a nivel mundial y sobre todo en los paises en desarrollo donde tiene mayor importancia por su alta incidencia, ademas tambien desde el punto de vista epidemiologico, clinico, la ecologico, molecular y tratamiento asociado con la Toxocariasis.",
                "authors": "S. Iglesias-Osores, K. Olivos-Caicedo, Virgilio E. Failoc-Rojas",
                "citations": 0
            },
            {
                "title": "Consistent estimates of sea level and vertical land motion based on satellite radar altimetry",
                "abstract": "Satellite radar altimetry is often considered to be the most succesful spaceborne remote sensing technique ever. Satellite radar altimeters were designed for static geodetic and ocean dynamics applications. The goal of the geodetic mission phases, which have a dense ground-track spacing, is primarily to acquire information about the marine gravity field. This enables the estimation of mean dynamic topography (geographical sea surface height patterns due to ocean currents) and deep-ocean bathymetry. The primary goal of the oceanographic mission phases is to gain information about time-varying currents and ocean dynamics. TOPEX/Poseidon is the first altimetry mission to reveal sea surface height variations related to ocean dynamics as the El Nino Southern Oscillation (ENSO). During the mission it became clear that secular changes in sea level could also be monitored. Already in 1995, Nerem (1995) computed a Global Mean Sea Level (GMSL) time series from the TOPEX/Poseidon data. Currently, the GMSL record spans 26 years, in which TOPEX/Poseidon time series is extended with the Jason-1a2a3 observations. The estimated secular trend of GMSL over the altimetry era is approximately 3 mm yr−1. The succes of the TOPEX/Poseidon mission spawned the Argo project with the deployment of the first floats in the year 2000. One argued that Argo would support the future Jason missions in separating changes into the two components (density and mass) of sea level. The Argo project aims to estimate temperature and salinity over a depth of 2000 meter using floats, which enable the estimation of density or steric sea level changes. By subtracting the steric signal from the absolute sea level measured by Jason (steric-corrected altimetry), the second component of sea level changes, mass, is estimated. The launch of the Gravity Recovery And Climate Experiment (GRACE) satellites in 2002 made it possible to independently validate oceanic mass variations. If the sum of the mass and steric components equals total sea level within the uncertainties, the sea level is said to be closed. Besides these two oceanic components, ocean bottom deformation or Vertical Land Motion (VLM) also affects the sea level observed by altimeters. Over the open ocean VLM signals are generally small after a correction for Glacial Isostatic Adjustment (GIA), but near large mass variations they might become significant. Additionally, tide-gauge records are affected by VLM changes, because they are connected to land. Therefore they measure sea level relative to the sea floor, while the satellite altimeters observe the absolute variations. To bring tide gauges in the same reference frame as the altimeters, corrections for VLM have to be applied, which is usually done with nearby Global Navigation Satellite System (GNSS) data...",
                "authors": "Marcel Kleinherenbrink",
                "citations": 0
            },
            {
                "title": "3 Soil-Transmitted Helminthic Zoonoses in Humans and Associated Risk Factors",
                "abstract": "The soil is an important route for transmission of numerous human pathogens, including the five major soil-transmitted helminths (STHs), also known as geohelminths, namely: roundworm (Ascaris lumbricoides), whipworm (Trichuris trichiura), hookworms (Ancylostoma duodenale and Necator americanus), and threadworm Strongyloides stercoralis (Brooker et al., 2006). An estimated one billion people are currently infected with STHs worldwide, particularly in resource-poor settings (WHO, 2011). Although overall mortality due to STH infections is low, morbidity may be significant given the pronounced impact on nutrition, growth, physical fitness, cognitive functions among infected infants, schoolchildren and adults from developing countries (Bethony et al., 2006). In Africa, an estimated 89.9 million children harbor STHs, many of whom are co-infected with two or more STH species (WHO, 2011). Zoonotic agents, comprising a wide variety of bacteria, viruses, and parasites, account for almost two thirds of all known human infections. Some helminthoses that commonly infect canids and felids are typically soil-transmitted. This chapter focuses on two major groups of STHs that cause disease in humans: (a) the ascarids Toxocara canis and T. cati, associated with visceral and ocular larva migrans, and (b) the hookworms Ancylostoma braziliense and Anc. caninum, associated with cutaneous larva migrans. We review the current geographic distribution, laboratory diagnosis and clinical spectrum of these infections, examine the relative contribution of some risk factors for infection and disease, and discuss potential control measures for reducing the burden of disease in companion animals and humans. A third soil-transmitted ascarid species that can cause human disease is Baylisascaris procyonis, commonly found in raccoons in North America. Human infections are characterized by severe neurological disease, leading to death or long-lasting sequelae (Watts et al., 2006). Another nematode species, Gnathostoma spinigerum, has occasionally been found in biopsy sample from patients with suspected VLM. More recently species of Toxocara including T. malayensis, a parasite of the domestic cat, and T. lyncus, which infects the caracal, have been identified, but their role in human disease remains unknown (Despommier, 2003). Soil-transmitted larval infection with other common canine and feline hookworms, such as Anc. ceylanicum, Anc. tubaeforme and Uncinaria stenocephala, can also cause occasional dermatological lesions in humans, and Anc. ceylanicum can readily develop in adults causing severe enteritis (Bowman",
                "authors": "V. Santarém, G. Rubinsky-Elefant, M. U. Ferreira",
                "citations": 0
            },
            {
                "title": "Near-IR trigonometric parallaxes of nearby stars in the Galactic plane using the VVV survey",
                "abstract": "We use the multi-epoch KS band observations, covering a ∼5 years baseline to obtain milli and sub-milli arcsec precision astrometry for a sample of eighteen previously known high proper motion sources, including precise parallaxes for these sources for the first time. In this pioneer study we show the capability of the VVV project to measure high precision trigonometric parallaxes for very low mass stars (VLMS) up to distances of ∼400 pc reaching farther than most other ground based surveys or space missions for these types of stars. Two stars in our sample are low mass companions to sources in the TGAS catalog, the VVV astrometry of the fainter source is consistent within 1-σ with the astrometry for the primary source in TGAS catalog, confirming the excellent astrometric quality of the VVV data even nearby of saturated sources, as in these cases. Additionally, we used spectral energy distribution to search for evidence of unresolved binary systems and cool subdwarfs. We detected five systems that are most likely VLMS belonging to the Galactic halo based on their tangential velocities, and four objects within 60 pc that are likely members of the thick disk. A more comprehensive study of high proper motion sources and parallaxes of VLMS and brown dwarfs with the VVV is ongoing , including thousands of newly discovered objects (Kurtev et al. 2016).",
                "authors": "J. C. Beamín, R. Méndez, R. L. Smart, R. Jara, R. Kurtev, Mariusz Gromadzki, V. Villanueva, D. Minniti, L. Smith, P. W. Lucas",
                "citations": 0
            },
            {
                "title": "High Frequency Labor",
                "abstract": "Continued technological progress has placed IT at the epicenter of new markets (Archak & Sundararajan 2009),spawning a significant reduction in traditional market search and market coordination costs (Malone, Yates & Benjamin 1987, Bakos 1997).In this work, premised on the observation that virtual labor markets (VLMs) can generally be engaged through two different modes of IT (i.e.Dashboard vs.API), this exploratory work begins to unpack the ramifications of these material differences for Crowdsourcing market function.",
                "authors": "John Prpić PhD",
                "citations": 0
            },
            {
                "title": "Glucoprivic sensitivity of hindbrain catecholamine neurons is astrocyte‐dependent",
                "abstract": "Hypoglycemia poses a threat to survival. Endocrine, metabolic, and behavioral counter‐regulatory responses (CRRs) act to correct blood glucose deficit. These responses require sensor, integrator, and effector elements to restore glucose homeostasis. Groups of hindbrain catecholamine (CA) neurons are essential for implementation of different CRRs which include activation of descending sympathetic pathways controlling epinephrine and glucagon release and ascending pathways controlling feeding behavior and glucocorticoids (3, 4). Hindbrain astrocytes respond to low glucose (1, 2, 5). Therefore, we postulated that CA neurons are activated by astrocytes during glucose deficit. To directly test the proposition that CA neuron responses to glucoprivic challenges are astrocyte dependent, we utilized male and female transgenic mice in which the calcium reporter construct (GCaMP5) was expressed selectively in tyrosine hydroxylase neurons (TH‐GCaMP5; TH+). We conducted live‐cell calcium imaging studies on tissue slices containing the nucleus of the solitary tract (NST) and the ventrolateral medulla (VLM); loci of glucoregulatory circuitry (4). Results show that hindbrain TH+ neurons are robustly activated by a glucoprivic challenge and that this response is dependent on functional astrocytes. Pretreatment of hindbrain slices with fluorocitrate (an astrocytic metabolic suppressor) abolished TH+ neuronal responses to glucoprivation, but not to glutamate stimulation. Pharmacologic results suggest that the astrocytic connection with TH+ neurons is purinergic via P2 receptors. Parallel imaging studies on hindbrain slices of NST from wild type C57BL/6J mice (astrocytes and neurons were prelabeled with a calcium reporter dye and an astrocytic vital dye) show that both cell types are activated by glucoprivation but astrocytes responded nearly a minute sooner than neurons. Pretreatment of these hindbrain slices with fluorocitrate eliminated both astrocytic and neuronal responses to glucoprivation; pretreatment with P2 antagonists only abolished neuronal responses to glucoprivation without interruption of astrocyte responses. These results support earlier work suggesting that the primary detection of glucoprivic signals by the hindbrain is mediated by astrocytes which utilize purinergic gliotransmission to activate critical effector neurons.",
                "authors": "R. Rogers, D. McDougal, S. Ritter, Emily Qualls‐Creekmore, G. Hermann",
                "citations": 0
            },
            {
                "title": "NIMG-89. DEEP LEARNING APPROACHES TO IDENTIFY INTRA-TUMORAL HETEROGENEITY OF LOW AND HIGH GRADE GLIOMAS",
                "abstract": "whereas it was not appreciated in 3 cases due to the tumor size. Bilateral involvement of the thalamus was noted in 7 patients on their initial scan, 1 patient had tumor confined to the cavum velum interpositum, which caused the abnormal signal in the thalamus bilaterally and 1 patient had left sided unilateral glioma which later progressed to the right side. CONCLUSIONS: Bithalamic tumors share a poor prognosis. Massa intermedia seems to be a possible route of spread of tumor from one thalamus to the other and subependymal region of the third ventricle may be contributing. These views open a newer channel for prophylactic radiation covering MI and third ven- tricle to prevent the intact thalamus and hence improve the prognosis of the patients. Genetic alterations found in WHO grade 2 and 3 gliomas IDH1/2 and TERT promoter mutations 1p19q co-deletion, alterations are great impact on the prognosis of the patient. In this research, the authors attempted to test the hypothesis that genetic alterations could contribute to the locations and heterogeneity of the tumor by analyzing 191 WHO grade 2 and 3 gliomas via MR radionomics. METHODS: 191 WHO grade 2 and 3 gliomas were retrospectively col- lected and the genomic DNA of the tumor was sequenced for IDH1/2 and TERT promoter mutations. Treatment naive MR images were also collected. T2 weighted, T1 weighted, FLAIR and Gd-enhanced T1 weighted images were collected for analysis. Voxel-based lesion mapping (VLM), and redio- nomics analysis was performed for all images and were further challenged to construct a model that predicts genetic alterations within the tumor. 126 patients were allocated as training set and 65 for validation set. Patient cohort was divided into the following 3 groups; IDH mt/TERTp wt, IDH mt/TERTp mt and IDH wt. A multi-regression model was constructed using the training set to predict the 3 genetic subgroups and the validation set was used for model validation. RESULTS: VLM revealed that IDH mt/TERTp wt gliomas dominated temporal lobe involvement while IDH mt/TERTp mt occupied the frontal lobe. IDH wt tumors, on the other hand, located at much posterior lobes and centered at the deep white matter. 15 radio- nomic features were identified for model construction to predict 3 genetic subgroups of WHO grade 2 and 3 gliomas. When these 15 texture elements were used to construct a prediction model of the 3 genetic subgroups, AUCs calculated from the training set ranged from 0.7 to 0.75. Accuracy for pre- diction was 63% for the training set and 60% for the validation set. BACKGROUND: conventional benchmarks on various computer vision tasks. The present study evaluates a novel deep learning architecture in automatically predicting IDH mutation from conventional MRI and iden-tifying tumor sub-regions that are most characteristic of mutation status. METHODS: MR imaging data from The Cancer Imaging Archives and cor-responding genomic data were downloaded for patients with low (LGG) and high-grade gliomas (HGG). Only patients with full preoperative MR including T2, FLAIR, precontrast-T1 and postcontrast-T1 were analyzed. A novel 3D fully connected deep learning architecture was trained to predict the likelihood of IDH mutation at any given voxel. Final prediction for a given tumor was based on the mean prediction for the tumor volume. RESULTS: A total of 5,259 axial slices of tumor from 457 glioma patients (200 LGG, 257 HGG) were included for analysis. Overall the algorithm correctly predicted IDH mutation with 94% accuracy on five-fold cross- validation. The resulting heat map for voxel-wise prediction identifies tumor sub-regions containing features most characteristic of IDH mutation. Visu- ally these features include faint or absent enhancement as well as central cystic regions with FLAIR suppression. CONCLUSIONS: A deep learning algorithm can predict IDH mutation with high accuracy from conventional MRI. In addition the algorithm is objective (requires no human interaction) and fast (several seconds from raw imaging data to prediction). Further investigation is ongoing to identify the best way to synthesize this data into clinical treatment paradigms. BACKGROUND: Magnetic resonance fingerprinting (MRF) is a non- invasive quantitative imaging technique, which allows rapid, simultaneous quantification of T1 and T2 relaxometry. We have demonstrated the ability of 2D single-slice MRF-based relaxometry to differentiate between various intraaxial brain tumors using first order statistics. In this study, we assess the utility of texture analysis on MRF quantitative maps to differentiate com-mon intra-axial adult brain tumors. MATERIALS AND METHODS: Sin- gle slice 2D-MRF acquisition was performed in 31 untreated brain tumors including 17 glioblastomas (GBM), 6 lower grade gliomas (LGG) and 8 metastases (METs). Regions of interest (ROI) for the solid tumor (ST) and peri-tumoral white matter (PW) (range: 0.32-12 and 0.25-2.5 cm 2 respec- tively) were drawn on quantitative T1 and T2 maps. Second-order texture features were calculated from gray level co-occurrence matrices (GLCM). Pearson correlation coefficients were applied for removal of redundant features. Five features including correlation, homogeneity, cluster-shade, information measure of correlation-1 and sum-average were compared across tumor types using the unpaired Student’s t-test and receiver operat- ing characteristic (ROC) analysis. RESULTS: In ST analysis, T1 correlation and T2 homogeneity of LGGs were higher compared to GBMs (p=0.009 and p=0.002 respectively). In PW analysis, T1 cluster-shade values of METs and LGGs were different (p= 0.004). There was difference in T2 correlation values between GBMs and METs (p=0.034). The ROC analysis revealed that T2 homogeneity of ST regions offers best separation between GBMs and LGGs with AUC of 0.91 (p=0.003) and between METs and LGGs with AUC of 0.94 (p=0.007). CONCLUSION: Texture analysis of MRF data improves characterization of tumoral and peri-tumoral regions and captures tissue heterogeneity above and beyond the first order features. MRF based texture analysis may offer a unique method for distinguishing various types of intraaxial adult brain tumors. BACKGROUND: Treatment-related imaging changes are often difficult to distinguish from true tumor progression. Treatment-related changes or pseudoprogression (PsP) subsequently subside or stabilize without any further treatment, whereas progressive tumor requires a more aggressive approach in patient management. Pseudoprogression can mimic true progression radi- ographically and may potentially alter the physician’s judgment about the recurrent disease. Hence, it can predispose a patient to overtreatment or be categorized as a non-responder and exclude him from the clinical trials. This study aims at assessing the potential of radiomics to discriminate PsP from progressive disease (PD) in glioblastoma (GBM) patients. METHODS: We retrospectively evaluated 304 GBM patients with new or increased enhance- ment on conventional MRI after treatment, of which it was uncertain for PsP versus PD. 149 patients had the histopathological evidence of PD and 27 of PsP. Remaining 128 patients were categorized into PD and PsP based on RANO criteria performed by a board-certified radiologist. Volumetrics using 3D slicer 4.3.1 and radiomics texture analysis were performed of the enhancing lesion(s) in question. RESULTS: Using the MRMR feature selec- tion method, we identified 100 significant features that were used to build a SVM model. Five texture features",
                "authors": "P. Chang, D. Bota, C. Filippi, D. Chow",
                "citations": 1
            },
            {
                "title": "Near-IR trigonometric parallaxes of nearby stars in the Galactic plane using the VVV",
                "abstract": "We use the multi-epoch K$\\rm _S$ band observations, covering a $\\sim$5 years baseline to obtain milli and sub-milli arcsec precision astrometry for a sample of eighteen previously known high proper motion sources, including precise parallaxes for these sources for the first time. In this study we show the capability of the VVV project to measure high precision trigonometric parallaxes for very low mass stars (VLMS) up to distances of $\\sim$250\\,pc reaching farther than most other ground based surveys or space missions for these types of stars. Additionally, we used spectral energy distribution to search for evidence of unresolved binary systems and cool sub-dwarfs. We detected five systems that are most likely VLMS belonging to the Galactic halo based on their tangential velocities, and four objects within 60 pc that are likely members of the thick disk. A more comprehensive study of high proper motion sources and parallaxes of VLMS and brown dwarfs with the VVV is ongoing , including thousands of newly discovered objects.",
                "authors": "J. C. Beamín, R. Mendez, R. Smart, R. Jara, R. Kurtev, M. Gromadzki, V. Villanueva, D. Minniti, L. Smith, P. Lucas",
                "citations": 1
            },
            {
                "title": "CloudTL: A New Transformation Language based on Big Data Tools and the Cloud",
                "abstract": "Model Driven Engineering (MDE) faces new challenges as models increase in size. These so called Very Large Models (VLMs) introduce new challenges, as their size and complexity cause transformation languages to have long execution times or even not being able to handle them due to memory issues. A new approach should be proposed to solve these challenges, such as automatic parallelization or making use of big data technologies, all of which should be transparent to the transformation developer. In this paper we present CloudTL, a new transformation language whose engine is based on big data tools to deal with VLMs in an efficient and scalable way, benchmarking it against the de facto standard, ATL.",
                "authors": "J. Aracil, Diego Sevilla Ruiz",
                "citations": 0
            },
            {
                "title": "Economic importance of air transport andairport activities in Belgium – Report 2015. National Bank of Belgium Working Paper No. 324",
                "abstract": "This study assesses the economic importance of air transport and airport activities in Belgium in \nterms of value added, employment and investment over the 2013-2015 period 1 . The sector \nconsidered embraces not only the activities directly connected with air transport, but also all those \nthat take place on site at the six Belgian airports (Antwerp, Brussels, Charleroi, Kortrijk, Liege and \nOstend). The study reviews the direct and indirect effects of the sector on the basis of \nmicroeconomic data (mainly obtained from the Central Balance Sheet Office) and mesoeconomic \ndata (from the National Accounts Institute). It also includes a social balance sheet analysis and an \nindication of credit risk using statistical models from the NBB’s In-house Credit Assessment System \n(ICAS). \nIn 2015, air transport and airport activities generated € 6 billion in direct and indirect value added \n(i.e. 1.5 % of Belgian GDP) and employed around 62 500 people in full-time equivalents (FTEs) \neither directly or indirectly (1.5 % of domestic employment including the self-employed). \nBrussels and Liege Airport remain the country’s biggest airports, respectively in terms of passenger \nand cargo traffic. In the aftermath of the terrorist attacks in March 2016, the regional airports \nreceived part of Brussels’ passenger traffic. All in all, Brussels recovered fairly quickly, especially \nfreight traffic, but also passenger traffic resumed gradually to tie in with growth again since \nNovember 2016. Brussels and Liege are the fastest growing airports during the 2013-2015 period, \nrespectively in terms of value added and employment. At Ostend Airport, these economic variables \nslumped in line with the evolution of freight traffic volumes. Antwerp’s growth rates went into the red \nas well, mainly under the influence of the difficulties faced by VLM Airlines. At Charleroi and Liege, \nthe trend of value added is downward, while that is not the case for employment. The smallest \nchanges are recorded in Kortrijk.",
                "authors": "S. Vennix",
                "citations": 0
            },
            {
                "title": "PaliGemma: A versatile 3B VLM for transfer",
                "abstract": "PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.",
                "authors": "Lucas Beyer, A. Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel M. Salz, Maxim Neumann, Ibrahim M. Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, A. Gritsenko, N. Houlsby, Manoj Kumar, Keran Rong, Julian Martin Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bovsnjak, Xi Chen, Matthias Minderer, P. Voigtlaender, Ioana Bica, Ivana Balazevic, J. Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, Xiao-Qi Zhai",
                "citations": 74
            },
            {
                "title": "An Image Grid Can Be Worth a Video: Zero-Shot Video Question Answering Using a VLM",
                "abstract": "Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging multiple frames in our proposed grid layout. The resulting single image is termed as an image grid. This format, while maintaining the appearance of a solitary image, effectively retains temporal information within the grid structure at the pixel level. Therefore, the image grid approach enables direct application of a single high-performance VLM without necessitating any video-data training. Our extensive experimental analysis across ten zero-shot VQA benchmarks, including five open-ended and five multiple-choice benchmarks, reveals that the proposed Image Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out of ten benchmarks. We also discuss how IG-VLM can be extended for long videos and provide an extension method that consistently and reliably improves the performance. Our code is are available at: https://github.com/imagegridworth/IG-VLM",
                "authors": "Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee",
                "citations": 35
            },
            {
                "title": "NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation",
                "abstract": "Vision-and-language navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavor to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision making and instruction following. We train NaVid with 510k navigation samples collected from continuous environments, including action-planning and instruction-reasoning samples, along with 763k large-scale web data. Extensive experiments show that NaVid achieves state-of-the-art performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.",
                "authors": "Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, Wang He",
                "citations": 27
            },
            {
                "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
                "abstract": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/",
                "authors": "Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David Held, Zackory Erickson",
                "citations": 24
            },
            {
                "title": "LLavaGuard: VLM-based Safeguards for Vision Dataset Curation and Safety Assessment",
                "abstract": "We introduce LlavaGuard, a family of VLM-based safeguard models, offering a versatile framework for evaluating the safety compliance of visual content. Specifically, we designed LlavaGuard for dataset annotation and generative model safeguarding. To this end, we collected and annotated a high-quality visual dataset incorporating a broad safety taxonomy, which we use to tune VLMs on context-aware safety risks. As a key innovation, LlavaGuard's new responses contain comprehensive information, including a safety rating, the violated safety categories, and an in-depth rationale. Further, our introduced customizable taxonomy categories enable the context-specific alignment of LlavaGuard to various scenarios. Our experiments highlight the capabilities of LlavaGuard in complex and real-world applications. We provide checkpoints ranging from 7B to 34B parameters demonstrating state-of-the-art performance, with even the smallest models outperforming baselines like GPT-4. We make our dataset and model weights publicly available and invite further research to address the diverse needs of communities and contexts.",
                "authors": "Lukas Helff, Felix Friedrich, Manuel Brack, K. Kersting, P. Schramowski",
                "citations": 5
            },
            {
                "title": "Slot-VLM: SlowFast Slots for Video-Language Modeling",
                "abstract": "Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.",
                "authors": "Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu",
                "citations": 5
            },
            {
                "title": "VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought",
                "abstract": "Large-scale LLMs and VLMs excel at few-shot learning but require high-quality examples. We introduce In-Context Abstraction Learning (ICAL), which iteratively refines suboptimal trajectories into high-quality data with optimized actions and detailed reasoning. Given an inefficient demonstration, a VLM corrects actions and annotates causal relationships, object states, subgoals, and task-relevant visuals, forming\"programs of thought.\"With human feedback, these programs are improved as the agent executes them in a similar environment. The resulting examples, used as prompt context or fine-tuning data, significantly boost decision-making while reducing human feedback needs. ICAL surpasses state-of-the-art in TEACh (dialogue-based instruction following), VisualWebArena (multimodal web agents), and Ego4D (egocentric video action anticipation). In TEACh, combining fine-tuning and retrieval on ICAL examples outperforms raw human demonstrations and expert examples, achieving a 17.5% increase in goal-condition success. In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success rate 1.6x over GPT-4V, while fine-tuning Qwen2-VL achieves a 2.8x improvement. In Ego4D, ICAL outperforms few-shot GPT-4V and remains competitive with supervised models. Overall, ICAL scales 2x better than raw human demonstrations and reduces manual prompt engineering.",
                "authors": "Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, Katerina Fragkiadaki",
                "citations": 4
            },
            {
                "title": "Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach",
                "abstract": "The integration of large-scale Vision-Language Models (VLMs) with embodied AI can greatly enhance the generalizability and the capacity to follow open instructions for robots. However, existing studies on object manipulation are not up to full consideration of the 6-DoF requirements, let alone establishing a comprehensive benchmark. In this paper, we propel the pioneer construction of the benchmark and approach for Open-instruction 6-DoF Object Rearrangement (Open6DOR). Specifically, we collect a synthetic dataset of 200+ objects and carefully design 5400+ Open6DOR tasks. These tasks are divided into the Position-track, Rotation-track, and 6-DoF-track for evaluating different embodied agents in predicting the positions and rotations of target objects.Besides, we also propose a VLM-based approach for Open6DOR, named Open6DOR-GPT, which empowers GPT-4V with 3D-awareness and simulation-assistance while exploiting its strengths in generalizability and instruction-following. We compare the existing embodied agents with our Open6DOR-GPT on the proposed Open6DOR benchmark and find that Open6DOR-GPT achieves the state-of-the-art performance. We further show the impressive performance of Open6DOR-GPT in diverse real-world experiments.",
                "authors": "Yufei Ding, Haoran Geng, Chaoyi Xu, Xiaomeng Fang, Jiazhao Zhang, Songlin Wei, Qiyu Dai, Zhizheng Zhang, He Wang",
                "citations": 5
            },
            {
                "title": "VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model",
                "abstract": "Vision Language Models (VLMs) have recently been adopted in robotics for their capability in common sense reasoning and generalizability. Existing work has applied VLMs to generate task and motion planning from natural language instructions and simulate training data for robot learning. In this work, we explore using VLM to interpret human demonstration videos and generate robot task planning. Our method integrates keyframe selection, visual perception, and VLM reasoning into a pipeline. We named it SeeDo because it enables the VLM to ''see'' human demonstrations and explain the corresponding plans to the robot for it to ''do''. To validate our approach, we collected a set of long-horizon human videos demonstrating pick-and-place tasks in three diverse categories and designed a set of metrics to comprehensively benchmark SeeDo against several baselines, including state-of-the-art video-input VLMs. The experiments demonstrate SeeDo's superior performance. We further deployed the generated task plans in both a simulation environment and on a real robot arm.",
                "authors": "Beichen Wang, Juexiao Zhang, Shuwen Dong, Irving Fang, Chen Feng",
                "citations": 4
            },
            {
                "title": "VLM-PL: Advanced Pseudo Labeling approach for Class Incremental Object Detection via Vision-Language Model",
                "abstract": "In the field of Class Incremental Object Detection (CIOD), creating models that can continuously learn like humans is a major challenge. Pseudo-labeling methods, although initially powerful, struggle with multi-scenario incremental learning due to their tendency to forget past knowledge. To overcome this, we introduce a new approach called Vision-Language Model assisted Pseudo-Labeling (VLM-PL). This technique uses Vision-Language Model (VLM) to verify the correctness of pseudo ground-truths (GTs) without requiring additional model training. VLM-PL starts by deriving pseudo GTs from a pre-trained detector. Then, we generate custom queries for each pseudo GT using carefully designed prompt templates that combine image and text features. This allows the VLM to classify the correctness through its responses. Furthermore, VLM-PL integrates refined pseudo and real GTs from upcoming training, effectively combining new and old knowledge. Extensive experiments conducted on the Pascal VOC and MS COCO datasets not only highlight VLM-PL’s exceptional performance in multi-scenario but also illuminate its effectiveness in dual-scenario by achieving state-of-the-art results in both.",
                "authors": "Junsu Kim, Yunhoe Ku, Jihyeon Kim, Junuk Cha, Seungryul Baek",
                "citations": 4
            },
            {
                "title": "MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention",
                "abstract": "In the digital world, memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved, proactive solutions such as intervention are still limited, with current research focusing mostly on text-based content, neglecting the widespread influence of multimodal content like memes. Addressing this gap, we present \\textit{MemeGuard}, a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention. \\textit{MemeGuard} harnesses a specially fine-tuned VLM, \\textit{VLMeme}, for meme interpretation, and a multimodal knowledge selection and ranking mechanism (\\textit{MKS}) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the \\textit{\\textbf{I}ntervening} \\textit{\\textbf{C}yberbullying in \\textbf{M}ultimodal \\textbf{M}emes (ICMM)} dataset, a high-quality, labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage \\textit{ICMM} to test \\textit{MemeGuard}, demonstrating its proficiency in generating relevant and effective responses to toxic memes.",
                "authors": "Prince Jha, Raghav Jain, Konika Mandal, Aman Chadha, Sriparna Saha, P. Bhattacharyya",
                "citations": 3
            },
            {
                "title": "V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models",
                "abstract": "Advancements in autonomous driving have increasingly focused on end-to-end (E2E) systems that manage the full spectrum of driving tasks, from environmental perception to vehicle navigation and control. This paper introduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative autonomous driving (VICAD) framework with Vehicle-to-Everything (V2X) systems and large vision-language models (VLMs). V2X-VLM is designed to enhance situational awareness, decision-making, and ultimate trajectory planning by integrating multimodel data from vehicle-mounted cameras, infrastructure sensors, and textual information. The contrastive learning method is further employed to complement VLM by refining feature discrimination, assisting the model to learn robust representations of the driving environment. Evaluations on the DAIR-V2X dataset show that V2X-VLM outperforms state-of-the-art cooperative autonomous driving methods, while additional tests on corner cases validate its robustness in real-world driving conditions.",
                "authors": "Junwei You, Haotian Shi, Zhuoyu Jiang, Zilin Huang, Rui Gan, Keshu Wu, Xi Cheng, Xiaopeng Li, Bin Ran",
                "citations": 3
            },
            {
                "title": "VLM-Social-Nav: Socially Aware Robot Navigation Through Scoring Using Vision-Language Models",
                "abstract": "We propose VLM-Social-Nav, a novel Vision-Language Model (VLM) based navigation approach to compute a robot's motion in human-centered environments. Our goal is to make real-time decisions on robot actions that are socially compliant with human expectations. We utilize a perception model to detect important social entities and prompt a VLM to generate guidance for socially compliant robot behavior. VLM-Social-Nav uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. Our overall approach reduces reliance on large training datasets and enhances adaptability in decision-making. In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot. We observe at least 27.38% improvement in the average success rate and 19.05% improvement in the average collision rate in the four social navigation scenarios. Our user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior.",
                "authors": "Daeun Song, Jing Liang, Amirreza Payandeh, Amir Hossain Raj, Xuesu Xiao, Dinesh Manocha",
                "citations": 3
            },
            {
                "title": "VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for Human Annotation-Free Pathological Image Classification",
                "abstract": "Despite that deep learning methods have achieved remarkable performance in pathology image classification, they heavily rely on labeled data, demanding extensive human annotation efforts. In this study, we present a novel human annotation-free method for pathology image classification by leveraging pre-trained Vision-Language Models (VLMs). Without human annotation, pseudo labels of the training set are obtained by utilizing the zero-shot inference capabilities of VLM, which may contain a lot of noise due to the domain shift between the pre-training data and the target dataset. To address this issue, we introduce VLM-CPL, a novel approach based on consensus pseudo labels that integrates two noisy label filtering techniques with a semi-supervised learning strategy. Specifically, we first obtain prompt-based pseudo labels with uncertainty estimation by zero-shot inference with the VLM using multiple augmented views of an input. Then, by leveraging the feature representation ability of VLM, we obtain feature-based pseudo labels via sample clustering in the feature space. Prompt-feature consensus is introduced to select reliable samples based on the consensus between the two types of pseudo labels. By rejecting low-quality pseudo labels, we further propose High-confidence Cross Supervision (HCS) to learn from samples with reliable pseudo labels and the remaining unlabeled samples. Experimental results showed that our method obtained an accuracy of 87.1% and 95.1% on the HPH and LC25K datasets, respectively, and it largely outperformed existing zero-shot classification and noisy label learning methods. The code is available at https://github.com/lanfz2000/VLM-CPL.",
                "authors": "Lanfeng Zhong, Xin Liao, Shaoting Zhang, Xiaofan Zhang, Guotai Wang",
                "citations": 3
            },
            {
                "title": "SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models",
                "abstract": "Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large Language Models (MLLMs) can automate the creation of accurate and coherent radiological reports. Existing methods often hallucinate details in text-based reports that don’t accurately reflect the image content. To mitigate this, we introduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework. We employ a unique self-supervised loss that leverages similarity between pooled image representations and the contextual representations of the generated radiological text, alongside the standard Causal Language Modeling objective, to refine image-text representations. This allows the model to scrutinize and align the generated text through dynamic interaction between a given image and the generated text, therefore reducing hallucination and continuously enhancing nuanced report generation. SERPENT-VLM outperforms existing baselines such as LlaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and Radiology Objects in COntext (ROCO) datasets, and also proves to be robust against noisy images. A qualitative case study emphasizes the significant advancements towards more sophisticated MLLM frameworks for R2Gen, opening paths for further research into self-supervised refinement in the medical imaging domain.",
                "authors": "M. Kapadnis, Sohan Patnaik, Abhilash Nandy, Sourjyadip Ray, Pawan Goyal, Debdoot Sheet",
                "citations": 2
            },
            {
                "title": "VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding",
                "abstract": "3D visual grounding is crucial for robots, requiring integration of natural language and 3D scene understanding. Traditional methods depending on supervised learning with 3D point clouds are limited by scarce datasets. Recently zero-shot methods leveraging LLMs have been proposed to address the data issue. While effective, these methods only use object-centric information, limiting their ability to handle complex queries. In this work, we present VLM-Grounder, a novel framework using vision-language models (VLMs) for zero-shot 3D visual grounding based solely on 2D images. VLM-Grounder dynamically stitches image sequences, employs a grounding and feedback scheme to find the target object, and uses a multi-view ensemble projection to accurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3D datasets show VLM-Grounder outperforms previous zero-shot methods, achieving 51.6% Acc@0.25 on ScanRefer and 48.0% Acc on Nr3D, without relying on 3D geometry or object priors. Codes are available at https://github.com/OpenRobotLab/VLM-Grounder .",
                "authors": "Runsen Xu, Zhiwei Huang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin",
                "citations": 2
            },
            {
                "title": "VLM-MPC: Vision Language Foundation Model (VLM)-Guided Model Predictive Controller (MPC) for Autonomous Driving",
                "abstract": "Motivated by the emergent reasoning capabilities of Vision Language Models (VLMs) and their potential to improve the comprehensibility of autonomous driving systems, this paper introduces a closed-loop autonomous driving controller called VLM-MPC, which combines the Model Predictive Controller (MPC) with VLM to evaluate how model-based control could enhance VLM decision-making. The proposed VLM-MPC is structured into two asynchronous components: The upper layer VLM generates driving parameters (e.g., desired speed, desired headway) for lower-level control based on front camera images, ego vehicle state, traffic environment conditions, and reference memory; The lower-level MPC controls the vehicle in real-time using these parameters, considering engine lag and providing state feedback to the entire system. Experiments based on the nuScenes dataset validated the effectiveness of the proposed VLM-MPC across various environments (e.g., night, rain, and intersections). The results demonstrate that the VLM-MPC consistently maintains Post Encroachment Time (PET) above safe thresholds, in contrast to some scenarios where the VLM-based control posed collision risks. Additionally, the VLM-MPC enhances smoothness compared to the real-world trajectories and VLM-based control. By comparing behaviors under different environmental settings, we highlight the VLM-MPC's capability to understand the environment and make reasoned inferences. Moreover, we validate the contributions of two key components, the reference memory and the environment encoder, to the stability of responses through ablation tests.",
                "authors": "Keke Long, Haotian Shi, Jiaxi Liu, Xiaopeng Li",
                "citations": 2
            },
            {
                "title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model",
                "abstract": "The rapid advancement of Large Vision-Language models (LVLMs) has demonstrated a spectrum of emergent capabilities. Nevertheless, current models only focus on the visual content of a single scenario, while their ability to associate instances across different scenes has not yet been explored, which is essential for understanding complex visual content, such as movies with multiple characters and intricate plots. Towards movie understanding, a critical initial step for LVLMs is to unleash the potential of character identities memory and recognition across multiple visual scenarios. To achieve the goal, we propose visual instruction tuning with ID reference and develop an ID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our research introduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory and recognition across four dimensions: matching, location, question-answering, and captioning. Our findings highlight the limitations of existing LVLMs in recognizing and associating instance identities with ID reference. This paper paves the way for future artificial intelligence systems to possess multi-identity visual inputs, thereby facilitating the comprehension of complex visual narratives like movies.",
                "authors": "Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, Ping Luo",
                "citations": 2
            },
            {
                "title": "AlignBot: Aligning VLM-powered Customized Task Planning with User Reminders Through Fine-Tuning for Household Robots",
                "abstract": "This paper presents AlignBot, a novel framework designed to optimize VLM-powered customized task planning for household robots by effectively aligning with user reminders. In domestic settings, aligning task planning with user reminders poses significant challenges due to the limited quantity, diversity, and multimodal nature of the reminders. To address these challenges, AlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for GPT-4o. This adapter model internalizes diverse forms of user reminders-such as personalized preferences, corrective guidance, and contextual assistance-into structured instruction-formatted cues that prompt GPT-4o in generating customized task plans. Additionally, AlignBot integrates a dynamic retrieval mechanism that selects task-relevant historical successes as prompts for GPT-4o, further enhancing task planning accuracy. To validate the effectiveness of AlignBot, experiments are conducted in real-world household environments, which are constructed within the laboratory to replicate typical household settings. A multimodal dataset with over 1,500 entries derived from volunteer reminders is used for training and evaluation. The results demonstrate that AlignBot significantly improves customized task planning, outperforming existing LLM- and VLM-powered planners by interpreting and aligning with user reminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline at 21.6%, reflecting a 65% improvement and over four times greater effectiveness. Supplementary materials are available at: https://yding25.com/AlignBot/",
                "authors": "Zhaxizhuoma, Pengan Chen, Ziniu Wu, Jiawei Sun, Dong Wang, Peng Zhou, Nieqing Cao, Yan Ding, Bin Zhao, Xuelong Li",
                "citations": 1
            },
            {
                "title": "Enhancing Video Transformers for Action Understanding with VLM-aided Training",
                "abstract": "Owing to their ability to extract relevant spatio-temporal video embeddings, Vision Transformers (ViTs) are currently the best performing models in video action understanding. However, their generalization over domains or datasets is somewhat limited. In contrast, Visual Language Models (VLMs) have demonstrated exceptional generalization performance, but are currently unable to process videos. Consequently, they cannot extract spatio-temporal patterns that are crucial for action understanding. In this paper, we propose the Four-tiered Prompts (FTP) framework that takes advantage of the complementary strengths of ViTs and VLMs. We retain ViTs' strong spatio-temporal representation ability but improve the visual encodings to be more comprehensive and general by aligning them with VLM outputs. The FTP framework adds four feature processors that focus on specific aspects of human action in videos: action category, action components, action description, and context information. The VLMs are only employed during training, and inference incurs a minimal computation cost. Our approach consistently yields state-of-the-art performance. For instance, we achieve remarkable top-1 accuracy of 93.8% on Kinetics-400 and 83.4% on Something-Something V2, surpassing VideoMAEv2 by 2.8% and 2.6%, respectively.",
                "authors": "Hui Lu, Hu Jian, Ronald Poppe, A. A. Salah",
                "citations": 1
            },
            {
                "title": "REO-VLM: Transforming VLM to Meet Regression Challenges in Earth Observation",
                "abstract": "The rapid evolution of Vision Language Models (VLMs) has catalyzed significant advancements in artificial intelligence, expanding research across various disciplines, including Earth Observation (EO). While VLMs have enhanced image understanding and data processing within EO, their applications have predominantly focused on image content description. This limited focus overlooks their potential in geographic and scientific regression tasks, which are essential for diverse EO applications. To bridge this gap, this paper introduces a novel benchmark dataset, called \\textbf{REO-Instruct} to unify regression and generation tasks specifically for the EO domain. Comprising 1.6 million multimodal EO imagery and language pairs, this dataset is designed to support both biomass regression and image content interpretation tasks. Leveraging this dataset, we develop \\textbf{REO-VLM}, a groundbreaking model that seamlessly integrates regression capabilities with traditional generative functions. By utilizing language-driven reasoning to incorporate scientific domain knowledge, REO-VLM goes beyond solely relying on EO imagery, enabling comprehensive interpretation of complex scientific attributes from EO data. This approach establishes new performance benchmarks and significantly enhances the capabilities of environmental monitoring and resource management.",
                "authors": "Xizhe Xue, Guoting Wei, Hao Chen, Haokui Zhang, Feng Lin, Chunhua Shen, Xiao Xiang Zhu",
                "citations": 1
            },
            {
                "title": "Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research",
                "abstract": "Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs. However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia. To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs' robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources. By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings. Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations. Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach. Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm. Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry.",
                "authors": "Qinglong Cao, Yuntian Chen, Lu Lu, Hao Sun, Zhenzhong Zeng, Xiaokang Yang, Dong-juan Zhang",
                "citations": 1
            },
            {
                "title": "IKIM at MEDIQA-M3G 2024: Multilingual Visual Question-Answering for Dermatology through VLM Fine-tuning and LLM Translations",
                "abstract": "This paper presents our solution to the MEDIQA-M3G Challenge at NAACL-ClinicalNLP 2024. We participated in all three languages, ranking first in Chinese and Spanish and third in English. Our approach utilizes LLaVA-med, an open-source, medical vision-language model (VLM) for visual question-answering in Chinese, and Mixtral-8x7B-instruct, a Large Language Model (LLM) for a subsequent translation into English and Spanish. In addition to our final method, we experiment with alternative approaches: Training three different models for each language instead of translating the results from one model, using different combinations and numbers of input images, and additional training on publicly available data that was not part of the original challenge training set.",
                "authors": "Marie Bauer, Constantin Seibold, J. Kleesiek, Amin Dada",
                "citations": 1
            },
            {
                "title": "VLM-KD: Knowledge Distillation from VLM for Long-Tail Visual Recognition",
                "abstract": "For visual recognition, knowledge distillation typically involves transferring knowledge from a large, well-trained teacher model to a smaller student model. In this paper, we introduce an effective method to distill knowledge from an off-the-shelf vision-language model (VLM), demonstrating that it provides novel supervision in addition to those from a conventional vision-only teacher model. Our key technical contribution is the development of a framework that generates novel text supervision and distills free-form text into a vision encoder. We showcase the effectiveness of our approach, termed VLM-KD, across various benchmark datasets, showing that it surpasses several state-of-the-art long-tail visual classifiers. To our knowledge, this work is the first to utilize knowledge distillation with text supervision generated by an off-the-shelf VLM and apply it to vanilla randomly initialized vision encoders.",
                "authors": "Zaiwei Zhang, Gregory P. Meyer, Zhichao Lu, Ashish Shrivastava, Avinash Ravichandran, Eric M. Wolff",
                "citations": 1
            },
            {
                "title": "Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration",
                "abstract": "To create culturally inclusive vision-language models (VLMs), developing a benchmark that tests their ability to address culturally relevant questions is essential. Existing approaches typically rely on human annotators, making the process labor-intensive and creating a cognitive burden in generating diverse questions. To address this, we propose a semi-automated framework for constructing cultural VLM benchmarks, specifically targeting multiple-choice QA. This framework combines human-VLM collaboration, where VLMs generate questions based on guidelines, a small set of annotated examples, and relevant knowledge, followed by a verification process by native speakers. We demonstrate the effectiveness of this framework through the creation of K-Viscuit, a dataset focused on Korean culture. Our experiments on this dataset reveal that open-source models lag behind proprietary ones in understanding Korean culture, highlighting key areas for improvement. We also present a series of further analyses, including human evaluation, augmenting VLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our dataset is available at https://huggingface.co/datasets/ddehun/k-viscuit.",
                "authors": "Yujin Baek, chaeHun Park, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, Jaegul Choo",
                "citations": 1
            },
            {
                "title": "Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes",
                "abstract": "—Recent research about Large Language Model based autonomous driving solutions shows a promising picture in planning and control fields. However, heavy computational resources and hallucinations of Large Language Models continue to hinder the tasks of predicting precise trajectories and instructing control signals. To address this problem, we propose Co-driver, a novel autonomous driving assistant system to empower autonomous vehicles with adjustable driving behaviors based on the understanding of road scenes. A pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness of our system is presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity of textual output of the Visual Language Model. Besides, we also contribute a dataset containing an image set and a corresponding prompt set for fine-tuning the Visual Language Model module of our system. In the real-world driving dataset, our system achieved 96 . 16% success rate in night scenes and 89 . 7% in gloomy scenes regarding reasonable predictions. Our Co-driver dataset will be released at https://github.com/ZionGo6/Co-driver .",
                "authors": "Ziang Guo, Artem Lykov, Zakhar Yagudin, Mikhail Konenkov, D. Tsetserukou",
                "citations": 5
            },
            {
                "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
                "abstract": "Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.",
                "authors": "Di Zhang, Jingdi Lei, Junxian Li, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou",
                "citations": 3
            },
            {
                "title": "VLM-guided Explicit-Implicit Complementary novel class semantic learning for few-shot object detection",
                "abstract": null,
                "authors": "Taijin Zhao, Heqian Qiu, Yu Dai, Lanxiao Wang, Hefei Mei, Fanman Meng, Qingbo Wu, Hongliang Li",
                "citations": 3
            },
            {
                "title": "VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving",
                "abstract": "In recent years, reinforcement learning (RL)-based methods for learning driving policies have gained increasing attention in the autonomous driving community and have achieved remarkable progress in various driving scenarios. However, traditional RL approaches rely on manually engineered rewards, which require extensive human effort and often lack generalizability. To address these limitations, we propose \\textbf{VLM-RL}, a unified framework that integrates pre-trained Vision-Language Models (VLMs) with RL to generate reward signals using image observation and natural language goals. The core of VLM-RL is the contrasting language goal (CLG)-as-reward paradigm, which uses positive and negative language goals to generate semantic rewards. We further introduce a hierarchical reward synthesis approach that combines CLG-based semantic rewards with vehicle state information, improving reward stability and offering a more comprehensive reward signal. Additionally, a batch-processing technique is employed to optimize computational efficiency during training. Extensive experiments in the CARLA simulator demonstrate that VLM-RL outperforms state-of-the-art baselines, achieving a 10.5\\% reduction in collision rate, a 104.6\\% increase in route completion rate, and robust generalization to unseen driving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any standard RL algorithms, potentially revolutionizing the existing RL paradigm that relies on manual reward engineering and enabling continuous performance improvements. The demo video and code can be accessed at: https://zilin-huang.github.io/VLM-RL-website.",
                "authors": "Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, Sikai Chen",
                "citations": 0
            },
            {
                "title": "VisionArena: 230K Real World User-VLM Conversations with Preference Labels",
                "abstract": "With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source platform where users interact with VLMs and submit preference votes - VisionArena spans 73K unique users, 45 VLMs, and 138 languages. Our dataset contains three subsets: VisionArena-Chat, 200k single and multi-turn conversations between a user and a VLM; VisionArena-Battle, 30K conversations comparing two anonymous VLMs with user preference votes; and VisionArena-Bench, an automatic benchmark of 500 diverse user prompts that efficiently approximate the live Chatbot Arena model rankings. Additionally, we highlight the types of question asked by users, the influence of response style on preference, and areas where models often fail. We find open-ended tasks like captioning and humor are highly style-dependent, and current VLMs struggle with spatial reasoning and planning tasks. Lastly, we show finetuning the same base model on VisionArena-Chat outperforms Llava-Instruct-158K, with a 17-point gain on MMMU and a 46-point gain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai",
                "authors": "Christopher Chou, Lisa Dunlap, Koki Mashita, Krishna Mandal, Trevor Darrell, I. Stoica, Joseph Gonzalez, Wei-Lin Chiang",
                "citations": 0
            },
            {
                "title": "ROOT: VLM based System for Indoor Scene Understanding and Beyond",
                "abstract": "Recently, Vision Language Models (VLMs) have experienced significant advancements, yet these models still face challenges in spatial hierarchical reasoning within indoor scenes. In this study, we introduce ROOT, a VLM-based system designed to enhance the analysis of indoor scenes. Specifically, we first develop an iterative object perception algorithm using GPT-4V to detect object entities within indoor scenes. This is followed by employing vision foundation models to acquire additional meta-information about the scene, such as bounding boxes. Building on this foundational data, we propose a specialized VLM, SceneVLM, which is capable of generating spatial hierarchical scene graphs and providing distance information for objects within indoor environments. This information enhances our understanding of the spatial arrangement of indoor scenes. To train our SceneVLM, we collect over 610,000 images from various public indoor datasets and implement a scene data generation pipeline with a semi-automated technique to establish relationships and estimate distances among indoor objects. By utilizing this enriched data, we conduct various training recipes and finish SceneVLM. Our experiments demonstrate that \\rootname facilitates indoor scene understanding and proves effective in diverse downstream applications, such as 3D scene generation and embodied AI. The code will be released at \\url{https://github.com/harrytea/ROOT}.",
                "authors": "Yonghui Wang, Shi-Yong Chen, Zhenxing Zhou, Siyi Li, Haoran Li, Weng Zhou, Houqiang Li",
                "citations": 0
            },
            {
                "title": "Dynamic Multimodal Prompt Tuning: Boost Few-Shot Learning with VLM-Guided Point Cloud Models",
                "abstract": ". Few-shot learning is crucial for downstream tasks involving point clouds, given the challenge of obtaining sufﬁcient datasets due to extensive collecting and labeling efforts. Pre-trained VLM-Guided point cloud models, containing abundant knowledge, can compensate for the scarcity of training data, potentially leading to very good performance. However, adapting these pre-trained point cloud models to speciﬁc few-shot learning tasks is challenging due to their huge number of parameters and high computational cost. To this end, we propose a novel Dynamic Multimodal Prompt Tuning method, named DMMPT, for boosting few-shot learning with pre-trained VLM-Guided point cloud models. Speciﬁcally, we build a dynamic knowledge collector capable of gathering task-and data-related information from various modalities. Then, a multi-modal prompt generator is constructed to integrate collected dynamic knowledge and generate multimodal prompts, which efﬁciently direct pre-trained VLM-guided point cloud models toward few-shot learning tasks and address the issue of limited training data. Our method is evaluated on benchmark datasets not only in a standard N-way K-shot few-shot learning setting, but also in a more challenging setting with all classes and K-shot few-shot learning.",
                "authors": "Xiang Gu, Shuchao Pang, Anan Du, Yifei Wang, Jixiang Miao, Jorge Díez",
                "citations": 0
            },
            {
                "title": "Navigation with VLM framework: Go to Any Language",
                "abstract": "Navigating towards fully open language goals and exploring open scenes in a manner akin to human exploration have always posed significant challenges. Recently, Vision Large Language Models (VLMs) have demonstrated remarkable capabilities in reasoning with both language and visual data. While many works have focused on leveraging VLMs for navigation in open scenes and with open vocabularies, these efforts often fall short of fully utilizing the potential of VLMs or require substantial computational resources. We introduce Navigation with VLM (NavVLM), a framework that harnesses equipment-level VLMs to enable agents to navigate towards any language goal specific or non-specific in open scenes, emulating human exploration behaviors without any prior training. The agent leverages the VLM as its cognitive core to perceive environmental information based on any language goal and constantly provides exploration guidance during navigation until it reaches the target location or area. Our framework not only achieves state-of-the-art performance in Success Rate (SR) and Success weighted by Path Length (SPL) in traditional specific goal settings but also extends the navigation capabilities to any open-set language goal. We evaluate NavVLM in richly detailed environments from the Matterport 3D (MP3D), Habitat Matterport 3D (HM3D), and Gibson datasets within the Habitat simulator. With the power of VLMs, navigation has entered a new era.",
                "authors": "Zecheng Yin, †. ChonghaoCheng, Yinghong Liao, Zhihao Yuan, Shuguang Cui, Zhen Li",
                "citations": 0
            },
            {
                "title": "DocVLM: Make Your VLM an Efficient Reader",
                "abstract": "Vision-Language Models (VLMs) excel in diverse visual tasks but face challenges in document understanding, which requires fine-grained text processing. While typical visual tasks perform well with low-resolution inputs, reading-intensive applications demand high-resolution, resulting in significant computational overhead. Using OCR-extracted text in VLM prompts partially addresses this issue but underperforms compared to full-resolution counterpart, as it lacks the complete visual context needed for optimal performance. We introduce DocVLM, a method that integrates an OCR-based modality into VLMs to enhance document processing while preserving original weights. Our approach employs an OCR encoder to capture textual content and layout, compressing these into a compact set of learned queries incorporated into the VLM. Comprehensive evaluations across leading VLMs show that DocVLM significantly reduces reliance on high-resolution images for document understanding. In limited-token regimes (448$\\times$448), DocVLM with 64 learned queries improves DocVQA results from 56.0% to 86.6% when integrated with InternVL2 and from 84.4% to 91.2% with Qwen2-VL. In LLaVA-OneVision, DocVLM achieves improved results while using 80% less image tokens. The reduced token usage allows processing multiple pages effectively, showing impressive zero-shot results on DUDE and state-of-the-art performance on MP-DocVQA, highlighting DocVLM's potential for applications requiring high-performance and efficiency.",
                "authors": "M. S. Nacson, Aviad Aberdam, Roy Ganz, Elad Ben Avraham, Alona Golts, Yair Kittenplon, Shai Mazor, Ron Litman",
                "citations": 0
            },
            {
                "title": "Make VLM Recognize Visual Hallucination on Cartoon Character Image with Pose Information",
                "abstract": "Leveraging large-scale Text-to-Image (TTI) models have become a common technique for generating exemplar or training dataset in the fields of image synthesis, video editing, 3D reconstruction. However, semantic structural visual hallucinations involving perceptually severe defects remain a concern, especially in the domain of non-photorealistic rendering (NPR) such as cartoons and pixelization-style character. To detect these hallucinations in NPR, We propose a novel semantic structural hallucination detection system using Vision-Language Model (VLM). Our approach is to leverage the emerging capability of large language model, in-context learning which denotes that VLM has seen some examples by user for specific downstream task, here hallucination detection. Based on in-context learning, we introduce pose-aware in-context visual learning (PA-ICVL) which improve the overall performance of VLM by further inputting visual data beyond prompts, RGB images and pose information. By incorporating pose guidance, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. Within selected two VLMs, GPT-4v, Gemini pro vision, our proposed PA-ICVL improves the hallucination detection with 50% to 78%, 57% to 80%, respectively. This research advances a capability of TTI models toward real-world applications by mitigating visual hallucinations via in-context visual learning, expanding their potential in non-photorealistic domains. In addition, it showcase how users can boost the downstream-specialized capability of open VLM by harnessing additional conditions. We collect synthetic cartoon-hallucination dataset with TTI models, this dataset and final tuned VLM will be publicly available.",
                "authors": "Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo",
                "citations": 0
            },
            {
                "title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models",
                "abstract": "Vision language models (VLMs) have shown promising reasoning capabilities across various benchmarks; however, our understanding of their visual perception remains limited. In this work, we propose an eye examination process to investigate how a VLM perceives images, specifically focusing on key elements of visual recognition, from primitive color and shape to semantic levels. To this end, we introduce a dataset named LENS to guide a VLM to follow the examination and check its readiness. Once the model is ready, we conduct the examination. Through this examination, we quantify and visualize VLMs' sensitivities to color and shape, and semantic matching. Our findings reveal that VLMs have varying sensitivity to different colors while consistently showing insensitivity to green across different VLMs. Also, we found different shape sensitivity and semantic recognition depending on LLM's capacity despite using the same fixed visual encoder. Our analyses and findings have potential to inspire the design of VLMs and the pre-processing of visual input to VLMs for improving application performance.",
                "authors": "Nam Hyeon-Woo, Moon Ye-Bin, Wonseok Choi, Lee Hyun, Tae-Hyun Oh",
                "citations": 0
            },
            {
                "title": "VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision",
                "abstract": "Human drivers rely on commonsense reasoning to navigate diverse and dynamic real-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models are typically optimized to mimic driving patterns observed in data, without capturing the underlying reasoning processes. This limitation constrains their ability to handle challenging driving scenarios. To close this gap, we propose VLM-AD, a method that leverages vision-language models (VLMs) as teachers to enhance training by providing additional supervision that incorporates unstructured reasoning information and structured action labels. Such supervision enhances the model's ability to learn richer feature representations that capture the rationale behind driving patterns. Importantly, our method does not require a VLM during inference, making it practical for real-time deployment. When integrated with state-of-the-art methods, VLM-AD achieves significant improvements in planning accuracy and reduced collision rates on the nuScenes dataset.",
                "authors": "Yi Xu, Yuxin Hu, Zaiwei Zhang, Gregory P. Meyer, Siva Karthik Mustikovela, Siddhartha Srinivasa, Eric M. Wolff, Xin Huang",
                "citations": 0
            },
            {
                "title": "Combining VLM and LLM for Enhanced Semantic Object Perception in Robotic Handover Tasks",
                "abstract": "We are utilizing a combination of Large Language Model (LLM) and Vision Language Model (VLM) to perform a robot-to-human handover task with semantic object knowledge. Current object perception systems for this task often work with a fixed set of objects and primarily consider geometric properties, neglecting semantic knowledge about where or where not to grasp an object. By applying LLM and VLM in a zero-shot fashion, we demonstrate that our approach can identify optimal and semantically correct handover parts for both the robot and the human in this handover task. We validate our approach quantitatively across several object categories.",
                "authors": "Jiayang Huang, Christian Limberg, Syed Muhammad Nashit Arshad, Qifeng Zhang, Qiang Li",
                "citations": 0
            },
            {
                "title": "ViTA: An Efficient Video-to-Text Algorithm using VLM for RAG-based Video Analysis System",
                "abstract": "Retrieval-augmented generation (RAG) is used in natural language processing (NLP) to provide query-relevant information in enterprise documents to large language models (LLMs). Such enterprise context enables the LLMs to generate more informed and accurate responses. When enterprise data is primarily videos, AI models like vision language models (VLMs) are necessary to convert information in videos into text. While essential, this conversion is a bottleneck, especially for large corpus of videos. It delays the timely use of enterprise videos to generate useful responses.We propose ViTA, a novel method that leverages two unique characteristics of VLMs to expedite the conversion process. As VLMs output more text tokens, they incur higher latency. In addition, large (heavyweight) VLMs can extract intricate details from images and videos, but they incur much higher latency per output token when compared to smaller (lightweight) VLMs that may miss details. To expedite conversion, ViTA first employs a lightweight VLM to quickly understand the gist or overview of an image or a video clip, and directs a heavyweight VLM (through prompt engineering) to extract additional details by using only a few (preset number of) output tokens. Our experimental results show that ViTA expedites the conversion time by as much as 43%, without compromising the accuracy of responses when compared to a baseline system that only uses a heavyweight VLM.",
                "authors": "Md. Adnan Arefeen, Biplob K. Debnath, M. Y. S. Uddin, S. Chakradhar",
                "citations": 0
            },
            {
                "title": "Barking Up The Syntactic Tree: Enhancing VLM Training with Syntactic Losses",
                "abstract": "Vision-Language Models (VLMs) achieved strong performance on a variety of tasks (e.g., image-text retrieval, visual question answering). However, most VLMs rely on coarse-grained image-caption pairs for alignment, relying on data volume to resolve ambiguities and ground linguistic concepts in images. The richer semantic and syntactic structure within text is largely overlooked. To address this, we propose HIerarchically STructured Learning (HIST) that enhances VLM training without any additional supervision, by hierarchically decomposing captions into the constituent Subject, Noun Phrases, and Composite Phrases. Entailment between these constituent components allows us to formulate additional regularization constraints on the VLM attention maps. Specifically, we introduce two novel loss functions: (1) Subject Loss, which aligns image content with the subject of corresponding phrase, acting as an entailment of standard contrastive/matching losses at the Phrase level; (2) Addition Loss, to balance attention across multiple objects. HIST is general, and can be applied to any VLM for which attention between vision and language can be computed; we illustrate its efficacy on BLIP and ALBEF. HIST outperforms baseline VLMs, achieving up to +9.8% improvement in visual grounding, +6.3% in multi-object referring segmentation, +1.1% in image-text retrieval, and +0.2% in visual question answering, underscoring the value of structuring learning in VLMs.",
                "authors": "Jiayun Luo, Mir Rayat Imtiaz Hossain, Boyang Li, Leonid Sigal",
                "citations": 0
            },
            {
                "title": "Trust but Verify: Programmatic VLM Evaluation in the Wild",
                "abstract": "Vision-Language Models (VLMs) often generate plausible but incorrect responses to visual queries. However, reliably quantifying the effect of such hallucinations in free-form responses to open-ended queries is challenging as it requires visually verifying each claim within the response. We propose Programmatic VLM Evaluation (PROVE), a new benchmarking paradigm for evaluating VLM responses to open-ended queries. To construct PROVE, we provide a large language model (LLM) with a high-fidelity scene-graph representation constructed from a hyper-detailed image caption, and prompt it to generate diverse question-answer (QA) pairs, as well as programs that can be executed over the scene graph object to verify each QA pair. We thus construct a benchmark of 10.5k challenging but visually grounded QA pairs. Next, to evaluate free-form model responses to queries in PROVE, we propose a programmatic evaluation strategy that measures both the helpfulness and truthfulness of a response within a unified scene graph-based framework. We benchmark the helpfulness-truthfulness trade-offs of a range of VLMs on PROVE, finding that very few are in-fact able to achieve a good balance between the two. Project page: \\url{https://prove-explorer.netlify.app/}.",
                "authors": "Viraj Prabhu, Senthil Purushwalkam, An Yan, Caiming Xiong, Ran Xu",
                "citations": 0
            },
            {
                "title": "BDNF Val66Met influences the mediation of sex differences in VLM scores by plasma BDNF in a cohort enriched with risk for Alzheimer‘s disease",
                "abstract": "Abstract Background Brain‐derived neurotrophic factor (BDNF)—a key neurotrophin involved in synaptic plasticity, neurogenesis, and neuroprotection—has been shown to mediate sex differences in verbal learning and memory (VLM) ability, but it remains unclear whether this relationship is conditionally dependent upon carriage of the Val66Met polymorphism in the BDNF gene. This study investigates how BDNFVal66Met carriage influences the mediation of sex differences in VLM scores by plasma BDNF levels in a cohort enriched for AD risk. Method Cognitively unimpaired participants in the Wisconsin Registry for Alzheimer’s Prevention (WRAP; n=198, age 63.8±6y, 66% women, 66% family history of AD, 38% apolipoprotein E4 (APOE‐ε4) carriers, 31% BDNFVal66Met carriers) underwent the Rey Auditory Verbal Learning Test (RAVLT). Scores from learning trials 3‐5 and the delayed recall test were aggregated as a VLM performance index. Plasma BDNF levels were measured using a Human BDNF Quantikine Immunoassay (R&D Systems). Striatified mediation analysis and bootstrapping were performed to test the conditional dependence of BDNF mediation on BDNFVal66Met carriage, and model covariates included age, APOE‐ε4 carriage, parental history of AD, education, hippocampal volume, and date difference between VLM and plasma data acquisition. Result Stratified mediation models showed a significant association between sex and VLM scores in BDNFVal66Met carriers [β=‐0.61; p=0.04], but no significant association between sex and BDNF levels [β=0.06; p=0.85]. By comparison, in BDNFVal66Val homozygotes, women had significantly higher BDNF levels [β=‐0.62; p<0.01] and VLM scores [β=‐0.77; p<0.01], and bootstrapping showed BDNF to be a significant partial mediator of the effect of sex on VLM [β=‐0.14; 95% CI: ‐.292, ‐.021]. Conclusion This study indicates that BDNFVal66Met carriage may attenuate the mediating role of plasma BDNF expression on the relationship between sex and VLM scores.",
                "authors": "Kyle J. Edmunds, Gabriella M. Mamlouk, Sarah R. Lose, Sanjay Asthana, M. Sager, Matthew Stremlau, Sterling C Johnson, Henriette van Praag, O. Okonkwo",
                "citations": 0
            },
            {
                "title": "Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model",
                "abstract": "We introduce Xmodel-VLM, a cutting-edge multimodal vision language model. It is designed for efficient deployment on consumer GPU servers. Our work directly confronts a pivotal industry issue by grappling with the prohibitive service costs that hinder the broad adoption of large-scale multimodal systems. Through rigorous training, we have developed a 1B-scale language model from the ground up, employing the LLaVA paradigm for modal alignment. The result, which we call Xmodel-VLM, is a lightweight yet powerful multimodal vision language model. Extensive testing across numerous classic multimodal benchmarks has revealed that despite its smaller size and faster execution, Xmodel-VLM delivers performance comparable to that of larger models. Our model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelVLM.",
                "authors": "Wanting Xu, Yang Liu, Langping He, Xucheng Huang, Ling Jiang",
                "citations": 0
            },
            {
                "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
                "abstract": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they fall short in addressing the unique demands of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities of geospatial data, which is critical for applications such as environmental monitoring, urban planning, and disaster management. Some of the unique challenges in geospatial domain include temporal analysis for changes, counting objects in large quantities, detecting tiny objects, and understanding relationships between entities occurring in Remote Sensing imagery. To address this gap in the geospatial domain, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and covers a diverse set of variations in visual conditions, object type, and scale. We evaluate several state-of-the-art VLMs to assess their accuracy within the geospatial context. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific examples, highlighting the room for further improvements. Specifically, the best-performing GPT4o achieves only 40\\% accuracy on MCQs, which is only double the random guess performance. Our benchmark is publicly available at https://github.com/The-AI-Alliance/GEO-Bench-VLM .",
                "authors": "M. S. Danish, Muhammad Akhtar Munir, Syed Roshaan Ali Shah, Kartik Kuckreja, F. Khan, Paolo Fraccaro, Alexandre Lacoste, Salman Khan",
                "citations": 0
            },
            {
                "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis",
                "abstract": "The Large Vision Language Model (VLM) has recently addressed remarkable progress in bridging two fundamental modalities. VLM, trained by a sufficiently large dataset, exhibits a comprehensive understanding of both visual and linguistic to perform diverse tasks. To distill this knowledge accurately, in this paper, we introduce a novel approach that explicitly utilizes VLM as an objective function form for the Human-Object Interaction (HOI) detection task (\\textbf{VLM-HOI}). Specifically, we propose a method that quantifies the similarity of the predicted HOI triplet using the Image-Text matching technique. We represent HOI triplets linguistically to fully utilize the language comprehension of VLMs, which are more suitable than CLIP models due to their localization and object-centric nature. This matching score is used as an objective for contrastive optimization. To our knowledge, this is the first utilization of VLM language abilities for HOI detection. Experiments demonstrate the effectiveness of our method, achieving state-of-the-art HOI detection accuracy on benchmarks. We believe integrating VLMs into HOI detection represents important progress towards more advanced and interpretable analysis of human-object interactions.",
                "authors": "Donggoo Kang, Dasol Jeong, Hyunmin Lee, Sangwoo Park, Hasil Park, Sunkyu Kwon, Yeongjoon Kim, Joonki Paik",
                "citations": 0
            },
            {
                "title": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation",
                "abstract": "The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through a vision perspective (objective), i.e., solely natural language supervision is sub-optimal for the MLLM's visual understanding ability. To that end, we propose OLA-VLM, the first approach distilling knowledge into the LLM's hidden representations from a set of target visual representations. Firstly, we formulate the objective during the pretraining stage in MLLMs as a coupled optimization of predictive visual embedding and next text-token prediction. Secondly, we investigate MLLMs trained solely with natural language supervision and identify a positive correlation between the quality of visual representations within these models and their downstream performance. Moreover, upon probing our OLA-VLM, we observe improved representation quality owing to the embedding optimization. Thirdly, we demonstrate that our OLA-VLM outperforms the single and multi-encoder baselines, proving our approach's superiority over explicitly feeding the corresponding features to the LLM. Particularly, OLA-VLM boosts performance by an average margin of up to 2.5% on various benchmarks, with a notable improvement of 8.7% on the Depth task in CV-Bench. Our code is open-sourced at https://github.com/SHI-Labs/OLA-VLM .",
                "authors": "Jitesh Jain, Zhengyuan Yang, Humphrey Shi, Jianfeng Gao, Jianwei Yang",
                "citations": 0
            },
            {
                "title": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection",
                "abstract": "Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.",
                "authors": "Qinqian Lei, Bo Wang, Robby T. Tan",
                "citations": 0
            },
            {
                "title": "TrojanRobot: Physical-World Backdoor Attacks Against VLM-based Robotic Manipulation",
                "abstract": "Robotic manipulation in the physical world is increasingly empowered by \\textit{large language models} (LLMs) and \\textit{vision-language models} (VLMs), leveraging their understanding and perception capabilities. Recently, various attacks against such robotic policies have been proposed, with backdoor attacks drawing considerable attention for their high stealth and strong persistence capabilities. However, existing backdoor efforts are limited to simulators and suffer from physical-world realization. To address this, we propose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic backdoor attack in the physical world. Specifically, we introduce a module-poisoning approach by embedding a backdoor module into the modular robotic policy, enabling backdoor control over the policy's visual perception module thereby backdooring the entire robotic policy. Our vanilla implementation leverages a backdoor-finetuned VLM to serve as the backdoor module. To enhance its generalization in physical environments, we propose a prime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing three types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation}, and \\textit{intentional} attacks, thus achieving finer-grained backdoors. Extensive experiments on the UR3e manipulator with 18 task instructions using robotic policies based on four VLMs demonstrate the broad effectiveness and physical-world stealth of TrojanRobot. Our attack's video demonstrations are available via a github link \\url{https://trojanrobot.github.io}.",
                "authors": "Xianlong Wang, Hewen Pan, Hangtao Zhang, Minghui Li, Shengshan Hu, Ziqi Zhou, Lulu Xue, Peijin Guo, Yichen Wang, Wei Wan, Aishan Liu, Leo Yu Zhang",
                "citations": 0
            },
            {
                "title": "Does VLM Classification Benefit from LLM Description Semantics?",
                "abstract": "Accurately describing images with text is a foundation of explainable AI. Vision-Language Models (VLMs) like CLIP have recently addressed this by aligning images and texts in a shared embedding space, expressing semantic similarities between vision and language embeddings. VLM classification can be improved with descriptions generated by Large Language Models (LLMs). However, it is difficult to determine the contribution of actual description semantics, as the performance gain may also stem from a semantic-agnostic ensembling effect, where multiple modified text prompts act as a noisy test-time augmentation for the original one. We propose an alternative evaluation scenario to decide if a performance boost of LLM-generated descriptions is caused by such a noise augmentation effect or rather by genuine description semantics. The proposed scenario avoids noisy test-time augmentation and ensures that genuine, distinctive descriptions cause the performance boost. Furthermore, we propose a training-free method for selecting discriminative descriptions that work independently of classname-ensembling effects. Our approach identifies descriptions that effectively differentiate classes within a local CLIP label neighborhood, improving classification accuracy across seven datasets. Additionally, we provide insights into the explainability of description-based image classification with VLMs.",
                "authors": "Pingchuan Ma, Lennart Rietdorf, Dmytro Kotovenko, Vincent Tao Hu, Bjorn Ommer",
                "citations": 0
            },
            {
                "title": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for Accelerating Large VLMs",
                "abstract": "Vision-language models (VLMs) have shown remarkable success across various multi-modal tasks, yet large VLMs encounter significant efficiency challenges due to processing numerous visual tokens. A promising approach to accelerating large VLM inference is using partial information, such as attention maps from specific layers, to assess token importance and prune less essential tokens. However, our study reveals three key insights: (i) Partial attention information is insufficient for accurately identifying critical visual tokens, resulting in suboptimal performance, especially at low token retention ratios; (ii) Global attention information, such as the attention map aggregated across all layers, more effectively preserves essential tokens and maintains comparable performance under aggressive pruning. However, the attention maps from all layers requires a full inference pass, which increases computational load and is therefore impractical in existing methods; and (iii) The global attention map aggregated from a small VLM closely resembles that of a large VLM, suggesting an efficient alternative. Based on these findings, we introduce a \\textbf{training-free} method, \\underline{\\textbf{S}}mall VLM \\underline{\\textbf{G}}uidance for accelerating \\underline{\\textbf{L}}arge VLMs (\\textbf{SGL}). Specifically, we employ the attention map aggregated from a small VLM to guide visual token pruning in a large VLM. Additionally, an early exiting mechanism is developed to fully use the small VLM's predictions, dynamically invoking the larger VLM only when necessary, yielding a superior trade-off between accuracy and computation. Extensive evaluations across 11 benchmarks demonstrate the effectiveness and generalizability of SGL, achieving up to 91\\% pruning ratio for visual tokens while retaining competitive performance.",
                "authors": "Wangbo Zhao, Yizeng Han, Jiasheng Tang, Zhikai Li, Yibing Song, Kai Wang, Zhangyang Wang, Yang You",
                "citations": 0
            },
            {
                "title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage",
                "abstract": "The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o mini model to generate queries, files, and trajectories, followed by query-file and trajectory verifiers. Based on the data synthesis pipeline, we collect the MM-Traj dataset that contains 20K tasks with trajectories of tool usage. Then, we develop the T3-Agent via \\underline{T}rajectory \\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently achieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B}, which outperforms untrained VLMs by $20\\%$, showing the effectiveness of the proposed data synthesis pipeline, leading to high-quality data for tool-usage capabilities.",
                "authors": "Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li",
                "citations": 0
            },
            {
                "title": "[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster",
                "abstract": "Large vision-language models (VLMs) often rely on a substantial number of visual tokens when interacting with large language models (LLMs), which has proven to be inefficient. Recent efforts have aimed to accelerate VLM inference by pruning visual tokens. Most existing methods assess the importance of visual tokens based on the text-visual cross-attentions in LLMs. In this study, we find that the cross-attentions between text and visual tokens in LLMs are inaccurate. Pruning tokens based on these inaccurate attentions leads to significant performance degradation, especially at high reduction ratios. To this end, we introduce FasterVLM, a simple yet effective training-free visual token pruning method that evaluates the importance of visual tokens more accurately by utilizing attentions between the [CLS] token and image tokens from the visual encoder. Since FasterVLM eliminates redundant visual tokens immediately after the visual encoder, ensuring they do not interact with LLMs and resulting in faster VLM inference. It is worth noting that, benefiting from the accuracy of [CLS] cross-attentions, FasterVLM can prune 95\\% of visual tokens while maintaining 90\\% of the performance of LLaVA-1.5-7B. We apply FasterVLM to various VLMs, including LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA, to demonstrate its effectiveness. Experimental results show that our FasterVLM maintains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing text-visual attention-based methods. Our code is available at https://github.com/Theia-4869/FasterVLM.",
                "authors": "Qizhe Zhang, Aosong Cheng, Ming Lu, Zhiyong Zhuo, Minqi Wang, Jiajun Cao, Shaobo Guo, Qi She, Shanghang Zhang",
                "citations": 0
            },
            {
                "title": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment",
                "abstract": "Benefiting from the powerful capabilities of Large Language Models (LLMs), pre-trained visual encoder models connected to LLMs form Vision Language Models (VLMs). However, recent research shows that the visual modality in VLMs is highly vulnerable, allowing attackers to bypass safety alignment in LLMs through visually transmitted content, launching harmful attacks. To address this challenge, we propose a progressive concept-based alignment strategy, PSA-VLM, which incorporates safety modules as concept bottlenecks to enhance visual modality safety alignment. By aligning model predictions with specific safety concepts, we improve defenses against risky images, enhancing explainability and controllability while minimally impacting general performance. Our method is obtained through two-stage training. The low computational cost of the first stage brings very effective performance improvement, and the fine-tuning of the language model in the second stage further improves the safety performance. Our method achieves state-of-the-art results on popular VLM safety benchmark.",
                "authors": "Zhendong Liu, Yuanbi Nie, Yingshui Tan, Jiaheng Liu, Xiangyu Yue, Qiushi Cui, Chong-Jun Wang, Xiaoyong Zhu, Bo Zheng",
                "citations": 0
            },
            {
                "title": "RoRA-VLM: Robust Retrieval-Augmented Vision Language Models",
                "abstract": "Current vision-language models (VLMs) still exhibit inferior performance on knowledge-intensive tasks, primarily due to the challenge of accurately encoding all the associations between visual objects and scenes to their corresponding entities and background knowledge. While retrieval augmentation methods offer an efficient way to integrate external knowledge, extending them to vision-language domain presents unique challenges in (1) precisely retrieving relevant information from external sources due to the inherent discrepancy within the multimodal queries, and (2) being resilient to the irrelevant, extraneous and noisy information contained in the retrieved multimodal knowledge snippets. In this work, we introduce RORA-VLM, a novel and robust retrieval augmentation framework specifically tailored for VLMs, with two key innovations: (1) a 2-stage retrieval process with image-anchored textual-query expansion to synergistically combine the visual and textual information in the query and retrieve the most relevant multimodal knowledge snippets; and (2) a robust retrieval augmentation method that strengthens the resilience of VLMs against irrelevant information in the retrieved multimodal knowledge by injecting adversarial noises into the retrieval-augmented training process, and filters out extraneous visual information, such as unrelated entities presented in images, via a query-oriented visual token refinement strategy. We conduct extensive experiments to validate the effectiveness and robustness of our proposed methods on three widely adopted benchmark datasets. Our results demonstrate that with a minimal amount of training instance, RORA-VLM enables the base model to achieve significant performance improvement and constantly outperform state-of-the-art retrieval-augmented VLMs on all benchmarks while also exhibiting a novel zero-shot domain transfer capability.",
                "authors": "Jingyuan Qi, Zhiyang Xu, Rulin Shao, Yang Chen, dingnan jin, Yu Cheng, Qifan Wang, Lifu Huang",
                "citations": 0
            },
            {
                "title": "VLM-Vac: Enhancing Smart Vacuums through VLM Knowledge Distillation and Language-Guided Experience Replay",
                "abstract": "In this paper, we propose VLM-Vac, a novel framework designed to enhance the autonomy of smart robot vacuum cleaners. Our approach integrates the zero-shot object detection capabilities of a Vision-Language Model (VLM) with a Knowledge Distillation (KD) strategy. By leveraging the VLM, the robot can categorize objects into actionable classes -- either to avoid or to suck -- across diverse backgrounds. However, frequently querying the VLM is computationally expensive and impractical for real-world deployment. To address this issue, we implement a KD process that gradually transfers the essential knowledge of the VLM to a smaller, more efficient model. Our real-world experiments demonstrate that this smaller model progressively learns from the VLM and requires significantly fewer queries over time. Additionally, we tackle the challenge of continual learning in dynamic home environments by exploiting a novel experience replay method based on language-guided sampling. Our results show that this approach is not only energy-efficient but also surpasses conventional vision-based clustering methods, particularly in detecting small objects across diverse backgrounds.",
                "authors": "Reihaneh Mirjalili, Michael Krawez, Florian Walter, Wolfram Burgard",
                "citations": 0
            },
            {
                "title": "Why context matters in VQA and Reasoning: Semantic interventions for VLM input modalities",
                "abstract": "The various limitations of Generative AI, such as hallucinations and model failures, have made it crucial to understand the role of different modalities in Visual Language Model (VLM) predictions. Our work investigates how the integration of information from image and text modalities influences the performance and behavior of VLMs in visual question answering (VQA) and reasoning tasks. We measure this effect through answer accuracy, reasoning quality, model uncertainty, and modality relevance. We study the interplay between text and image modalities in different configurations where visual content is essential for solving the VQA task. Our contributions include (1) the Semantic Interventions (SI)-VQA dataset, (2) a benchmark study of various VLM architectures under different modality configurations, and (3) the Interactive Semantic Interventions (ISI) tool. The SI-VQA dataset serves as the foundation for the benchmark, while the ISI tool provides an interface to test and apply semantic interventions in image and text inputs, enabling more fine-grained analysis. Our results show that complementary information between modalities improves answer and reasoning quality, while contradictory information harms model performance and confidence. Image text annotations have minimal impact on accuracy and uncertainty, slightly increasing image relevance. Attention analysis confirms the dominant role of image inputs over text in VQA tasks. In this study, we evaluate state-of-the-art VLMs that allow us to extract attention coefficients for each modality. A key finding is PaliGemma's harmful overconfidence, which poses a higher risk of silent failures compared to the LLaVA models. This work sets the foundation for rigorous analysis of modality integration, supported by datasets specifically designed for this purpose.",
                "authors": "Kenza Amara, Lukas Klein, Carsten T. Lüth, Paul F. Jäger, Hendrik Strobelt, Mennatallah El-Assady",
                "citations": 0
            },
            {
                "title": "VLM-Auto: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes",
                "abstract": "Recent research on Large Language Models for autonomous driving shows promise in planning and control. However, high computational demands and hallucinations still challenge accurate trajectory prediction and control signal generation. Deterministic algorithms offer reliability but lack adaptability to complex driving scenarios and struggle with context and uncertainty. To address this problem, we propose VLM-Auto, a novel autonomous driving assistant system to empower the autonomous vehicles with adjustable driving behaviors based on the understanding of road scenes. A pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness of our system is presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity of textual output of the Visual Language Model (VLM). Besides, we also contribute a dataset containing an image set and a corresponding prompt set for fine-tuning the VLM module of our system. In CARLA experiments, our system achieved $97.82\\%$ average precision on 5 types of labels in our dataset. In the real-world driving dataset, our system achieved $96.97\\%$ prediction accuracy in night scenes and gloomy scenes. Our VLM-Auto dataset will be released at https://github.com/ZionGo6/VLM-Auto.",
                "authors": "Ziang Guo, Zakhar Yagudin, Artem Lykov, Mikhail Konenkov, D. Tsetserukou",
                "citations": 0
            },
            {
                "title": "VLM-EMO: Context-Aware Emotion Classification with CLIP",
                "abstract": "Emotion recognition is a pivotal component in various sectors, including medical care, education, service industries, and public safety, due to its potential to enhance interaction and understanding within these contexts. Historically, traditional methods of emotion recognition have primarily concentrated on analyzing facial expressions. While effective to a degree, this approach offers restricted capacity for encoding context, and unable to capture the breadth and depth of emotional responses. In this paper, we proposed the VLM-EMO Model, a novel approach that enhances the capabilities of VLM (Visual Language Model) in the domain of emotion recognition. While VLM itself is a powerful tool for understanding the correlation between images and text, the VLM-EMO Model fine-tunes this ability to focus more intricately on emotional aspects within visual data. This is achieved by integrating emotional intelligence into the model's architecture, allowing it to discern subtle emotional cues and contexts within images.",
                "authors": "Yanyin Yao, Xue Mei, Jianping Xu, Zhichuang Sun, Cheng Zeng, Yuming Chen",
                "citations": 0
            },
            {
                "title": "EEG-VLM Toolbox: Extending voxel-based lesion mapping to multi-dimensional EEG data",
                "abstract": "Focal brain lesions (such as with stroke) cause functional changes in local and distributed neural systems. While there is a long history of post-stroke neurophysiological assessment using electroencephalography (EEG), the observed neurophysiological changes have rarely been related to specific lesion locations. Therefore, the relationships between anatomical injury and physiological changes after stroke remain unclear. Voxel-based lesion symptom mapping (VLSM) is a tool for statistically relating stroke lesion locations to “symptoms”, but current VLSM methods are restricted to symptoms that can be defined by a single value. Therefore, current VLSM techniques are unable to map the relationships between anatomical injury and multidimensional neurophysiological data such as EEG, which contains rich spatio-temporal information across different channels and frequency bands. Here we present a novel algorithm, EEG Voxel-based Lesion Mapping (EEG-VLM), that produces the set of significant relationships between precise neuroanatomical injury locations and neurophysiology (defined by a cluster of adjacent EEG channels and frequency bands). Further, the algorithm provides statistical analyses to define the overall significance of each neural structure-function relationship by correcting for multiple comparisons using a permutation test. Applying EEG-VLM to a dataset of recordings from chronic stroke patients performing a cued upper extremity movement task, we found that subjects with lesions in frontal subcortical white matter have reduced ipsilesional parietal cue-evoked EEG responses. These results are consistent with damage to a frontal-parietal network that has been associated with impairments in attention. EEG-VLM is a novel and unbiased method for relating neurophysiologic changes after stroke with neuroanatomic lesions. In the context of focal brain lesions associated with neurological impairments, we propose that this method will enable improved mechanistic understanding, facilitate biomarker development, and guide neurorehabilitation strategies.",
                "authors": "R. Hardstone, Lauren Ostrowski, A. N. Dusang, E. López-Larraz, Jessica Jesser, S. Cash, Steven C. Cramer, Leigh R. Hochberg, A. Ramos-Murguialday, David J Lin",
                "citations": 0
            },
            {
                "title": "Progressive Alignment with VLM-LLM Feature to Augment Defect Classification for the ASE Dataset",
                "abstract": "Traditional defect classification approaches are facing with two barriers. (1) Insufficient training data and unstable data quality. Collecting sufficient defective sample is expensive and time-costing, consequently leading to dataset variance. It introduces the difficulty on recognition and learning. (2) Over-dependence on visual modality. When the image pattern and texture is monotonic for all defect classes in a given dataset, the performance of conventional AOI system cannot be guaranteed. In scenarios where image quality is compromised due to mechanical failures or when defect information is inherently difficult to discern, the performance of deep models cannot be guaranteed. A main question is,\"how to solve those two problems when they occur at the same time?\"The feasible strategy is to explore another feature within dataset and combine an eminent vision-language model (VLM) and Large-Language model (LLM) with their astonishing zero-shot capability. In this work, we propose the special ASE dataset, including rich data description recorded on image, for defect classification, but the defect feature is uneasy to learn directly. Secondly, We present the prompting for VLM-LLM against defect classification with the proposed ASE dataset to activate extra-modality feature from images to enhance performance. Then, We design the novel progressive feature alignment (PFA) block to refine image-text feature to alleviate the difficulty of alignment under few-shot scenario. Finally, the proposed Cross-modality attention fusion (CMAF) module can effectively fuse different modality feature. Experiment results have demonstrated our method's effectiveness over several defect classification methods for the ASE dataset.",
                "authors": "Chih-Chung Hsu, Chia-Ming Lee, Chun-Hung Sun, Kuang-Ming Wu",
                "citations": 0
            },
            {
                "title": "AP-VLM: Active Perception Enabled by Vision-Language Models",
                "abstract": "Active perception enables robots to dynamically gather information by adjusting their viewpoints, a crucial capability for interacting with complex, partially observable environments. In this paper, we present AP-VLM, a novel framework that combines active perception with a Vision-Language Model (VLM) to guide robotic exploration and answer semantic queries. Using a 3D virtual grid overlaid on the scene and orientation adjustments, AP-VLM allows a robotic manipulator to intelligently select optimal viewpoints and orientations to resolve challenging tasks, such as identifying objects in occluded or inclined positions. We evaluate our system on two robotic platforms: a 7-DOF Franka Panda and a 6-DOF UR5, across various scenes with differing object configurations. Our results demonstrate that AP-VLM significantly outperforms passive perception methods and baseline models, including Toward Grounded Common Sense Reasoning (TGCSR), particularly in scenarios where fixed camera views are inadequate. The adaptability of AP-VLM in real-world settings shows promise for enhancing robotic systems' understanding of complex environments, bridging the gap between high-level semantic reasoning and low-level control.",
                "authors": "Venkatesh Sripada, Samuel Carter, Frank Guerin, Amir Ghalamzan",
                "citations": 0
            },
            {
                "title": "Disaster Damage Visualization by VLM-Based Interactive Image Retrieval and Cross-View Image Geo-Localization",
                "abstract": "We propose a framework for quickly selecting images that show the disaster situation from many images, estimating their locations with high accuracy, and displaying them on a map. The proposed framework introduces interactive image retrieval based on the Vision and Language Model (VLM), which can retrieve images from many images that show the disaster situation according to the user’s intention. Using the correlation between language and images based on VLM and the similarity between images selected interactively enables more accurate retrieval. Next, for selected images for which the location of the affected area is unknown, the location of the image is estimated with street address-level accuracy by matching it with an overhead image covering a large area of the city and map data and then displayed on a map. We confirmed the effectiveness of the proposed method on publicly available datasets such as CrisisNLP.",
                "authors": "Naoya Sogi, Takashi Shibata, Makoto Terao, Kenta Senzaki, Masahiro Tani, Royston Rodrigues",
                "citations": 0
            },
            {
                "title": "Img2CAD: Reverse Engineering 3D CAD Models from Images through VLM-Assisted Conditional Factorization",
                "abstract": "Reverse engineering 3D computer-aided design (CAD) models from images is an important task for many downstream applications including interactive editing, manufacturing, architecture, robotics, etc. The difficulty of the task lies in vast representational disparities between the CAD output and the image input. CAD models are precise, programmatic constructs that involves sequential operations combining discrete command structure with continuous attributes -- making it challenging to learn and optimize in an end-to-end fashion. Concurrently, input images introduce inherent challenges such as photo-metric variability and sensor noise, complicating the reverse engineering process. In this work, we introduce a novel approach that conditionally factorizes the task into two sub-problems. First, we leverage large foundation models, particularly GPT-4V, to predict the global discrete base structure with semantic information. Second, we propose TrAssembler that conditioned on the discrete structure with semantics predicts the continuous attribute values. To support the training of our TrAssembler, we further constructed an annotated CAD dataset of common objects from ShapeNet. Putting all together, our approach and data demonstrate significant first steps towards CAD-ifying images in the wild. Our project page: https://anonymous123342.github.io/",
                "authors": "Yang You, M. Uy, Jiaqi Han, R. Thomas, Haotong Zhang, Suya You, Leonidas J. Guibas",
                "citations": 2
            },
            {
                "title": "Can VLM Understand Children's Handwriting? An Analysis on Handwritten Mathematical Equation Recognition",
                "abstract": null,
                "authors": "Cleon Pereira Júnior, Luiz Rodrigues, N. Costa, Valmir Macário Filho, R. Mello",
                "citations": 1
            },
            {
                "title": "AltChart: Enhancing VLM-based Chart Summarization Through Multi-Pretext Tasks",
                "abstract": "Chart summarization is a crucial task for blind and visually impaired individuals as it is their primary means of accessing and interpreting graphical data. Crafting high-quality descriptions is challenging because it requires precise communication of essential details within the chart without vision perception. Many chart analysis methods, however, produce brief, unstructured responses that may contain significant hallucinations, affecting their reliability for blind people. To address these challenges, this work presents three key contributions: (1) We introduce the AltChart dataset, comprising 10,000 real chart images, each paired with a comprehensive summary that features long-context, and semantically rich annotations. (2) We propose a new method for pretraining Vision-Language Models (VLMs) to learn fine-grained chart representations through training with multiple pretext tasks, yielding a performance gain with ${\\sim}2.5\\%$. (3) We conduct extensive evaluations of four leading chart summarization models, analyzing how accessible their descriptions are. Our dataset and codes are publicly available on our project page: https://github.com/moured/AltChart.",
                "authors": "Omar Moured, Jiaming Zhang, M. Sarfraz, Rainer Stiefelhagen",
                "citations": 1
            },
            {
                "title": "VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use",
                "abstract": "While vision-language models (VLMs) have demonstrated remarkable performance across various tasks combining textual and visual information, they continue to struggle with fine-grained visual perception tasks that require detailed pixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs on such intricate visual elements remains an open challenge. In this paper, we present VipAct, an agent framework that enhances VLMs by integrating multi-agent collaboration and vision expert models, enabling more precise visual understanding and comprehensive reasoning. VipAct consists of an orchestrator agent, which manages task requirement analysis, planning, and coordination, along with specialized agents that handle specific tasks such as image captioning and vision expert models that provide high-precision perceptual information. This multi-agent approach allows VLMs to better perform fine-grained visual perception tasks by synergizing planning, reasoning, and tool use. We evaluate VipAct on benchmarks featuring a diverse set of visual perception tasks, with experimental results demonstrating significant performance improvements over state-of-the-art baselines across all tasks. Furthermore, comprehensive ablation studies reveal the critical role of multi-agent collaboration in eliciting more detailed System-2 reasoning and highlight the importance of image input for task planning. Additionally, our error analysis identifies patterns of VLMs' inherent limitations in visual perception, providing insights into potential future improvements. VipAct offers a flexible and extensible framework, paving the way for more advanced visual perception systems across various real-world applications.",
                "authors": "Zhehao Zhang, Ryan A. Rossi, Tong Yu, Franck Dernoncourt, Ruiyi Zhang, Jiuxiang Gu, Sungchul Kim, Xiang Chen, Zichao Wang, Nedim Lipka",
                "citations": 1
            },
            {
                "title": "Q-VLM: Post-training Quantization for Large Vision-Language Models",
                "abstract": "In this paper, we propose a post-training quantization framework of large vision-language models (LVLMs) for efficient multi-modal inference. Conventional quantization methods sequentially search the layer-wise rounding functions by minimizing activation discretization errors, which fails to acquire optimal quantization strategy without considering cross-layer dependency. On the contrary, we mine the cross-layer dependency that significantly influences discretization errors of the entire vision-language model, and embed this dependency into optimal quantization strategy searching with low search cost. Specifically, we observe the strong correlation between the activation entropy and the cross-layer dependency concerning output discretization errors. Therefore, we employ the entropy as the proxy to partition blocks optimally, which aims to achieve satisfying trade-offs between discretization errors and the search cost. Moreover, we optimize the visual encoder to disentangle the cross-layer dependency for fine-grained decomposition of search space, so that the search cost is further reduced without harming the quantization accuracy. Experimental results demonstrate that our method compresses the memory by 2.78x and increase generate speed by 1.44x about 13B LLaVA model without performance degradation on diverse multi-modal reasoning tasks. Code is available at https://github.com/ChangyuanWang17/QVLM.",
                "authors": "Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu",
                "citations": 1
            },
            {
                "title": "Towards vaccine lifecycle management (VLM): A systematic literature review of the issues and challenges",
                "abstract": null,
                "authors": "AL Sanae, Aicha Sekhari Seklouli, A. E. B. E. Idrissi, El Kinani Noredine",
                "citations": 1
            },
            {
                "title": "BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations",
                "abstract": "This paper presents Bag-of-Concept Graph (BACON) to gift models with limited linguistic abilities to taste the privilege of Vision Language Models (VLMs) and boost downstream tasks such as detection, visual question answering (VQA), and image generation. Since the visual scenes in physical worlds are structured with complex relations between objects, BACON breaks down annotations into basic minimum elements and presents them in a graph structure. Element-wise style enables easy understanding, and structural composition liberates difficult locating. Careful prompt design births the BACON captions with the help of public-available VLMs and segmentation methods. In this way, we gather a dataset with 100K annotated images, which endow VLMs with remarkable capabilities, such as accurately generating BACON, transforming prompts into BACON format, envisioning scenarios in the style of BACONr, and dynamically modifying elements within BACON through interactive dialogue and more. Wide representative experiments, including detection, VQA, and image generation tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel in their current cutting-edge solutions.",
                "authors": "Zhantao Yang, Ruili Feng, Keyu Yan, Huangji Wang, Zhicai Wang, Shangwen Zhu, Han Zhang, Jie Xiao, Ping Wu, Kai Zhu, Jixuan Chen, Chenwei Xie, Chaojie Mao, Yue Yang, Hongyang Zhang, Yu Liu, Fan Cheng",
                "citations": 1
            },
            {
                "title": "AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents",
                "abstract": "Vision Language Models (VLMs) have revolutionized the creation of generalist web agents, empowering them to autonomously complete diverse tasks on real-world websites, thereby boosting human efficiency and productivity. However, despite their remarkable capabilities, the safety and security of these agents against malicious attacks remain critically underexplored, raising significant concerns about their safe deployment. To uncover and exploit such vulnerabilities in web agents, we provide AdvWeb, a novel black-box attack framework designed against web agents. AdvWeb trains an adversarial prompter model that generates and injects adversarial prompts into web pages, misleading web agents into executing targeted adversarial actions such as inappropriate stock purchases or incorrect bank transactions, actions that could lead to severe real-world consequences. With only black-box access to the web agent, we train and optimize the adversarial prompter model using DPO, leveraging both successful and failed attack strings against the target agent. Unlike prior approaches, our adversarial string injection maintains stealth and control: (1) the appearance of the website remains unchanged before and after the attack, making it nearly impossible for users to detect tampering, and (2) attackers can modify specific substrings within the generated adversarial string to seamlessly change the attack objective (e.g., purchasing stocks from a different company), enhancing attack flexibility and efficiency. We conduct extensive evaluations, demonstrating that AdvWeb achieves high success rates in attacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings expose critical vulnerabilities in current LLM/VLM-based agents, emphasizing the urgent need for developing more reliable web agents and effective defenses. Our code and data are available at https://ai-secure.github.io/AdvWeb/ .",
                "authors": "Chejian Xu, Mintong Kang, Jiawei Zhang, Zeyi Liao, Lingbo Mo, Mengqi Yuan, Huan Sun, Bo Li",
                "citations": 1
            },
            {
                "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
                "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community.",
                "authors": "Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Lukasz Kuci'nski, Lerrel Pinto, Rob Fergus, Jakob N. Foerster, Jack Parker-Holder, Tim Rocktaschel",
                "citations": 1
            },
            {
                "title": "Beyond Captioning: Task-Specific Prompting for Improved VLM Performance in Mathematical Reasoning",
                "abstract": "Vision-Language Models (VLMs) have transformed tasks requiring visual and reasoning abilities, such as image retrieval and Visual Question Answering (VQA). Despite their success, VLMs face significant challenges with tasks involving geometric reasoning, algebraic problem-solving, and counting. These limitations stem from difficulties effectively integrating multiple modalities and accurately interpreting geometry-related tasks. Various works claim that introducing a captioning pipeline before VQA tasks enhances performance. We incorporated this pipeline for tasks involving geometry, algebra, and counting. We found that captioning results are not generalizable, specifically with larger VLMs primarily trained on downstream QnA tasks showing random performance on math-related challenges. However, we present a promising alternative: task-based prompting, enriching the prompt with task-specific guidance. This approach shows promise and proves more effective than direct captioning methods for math-heavy problems.",
                "authors": "Ayush Singh, Mansi Gupta, Shivank Garg, Abhinav Kumar, Vansh Agrawal",
                "citations": 0
            },
            {
                "title": "Integrating LLM, VLM, and Text-to-Image Models for Enhanced Information Graphics: A Methodology for Accurate and Visually Engaging Visualizations",
                "abstract": "This study presents an innovative approach to the creation of information graphics, where the accuracy of content and aesthetic appeal are of paramount importance. Traditional methods often struggle to balance these two aspects, particularly in complex visualizations like phylogenetic trees. Our methodology integrates the strengths of Large Language Models (LLMs), Vision Language Models (VLMs), and advanced text-to-image models to address this challenge. Initially, an LLM plans the layout and structure, employing Mermaid—a JavaScript-based tool that uses Markdown-like scripts for diagramming—to establish a precise and structured foundation. This structured script is crucial for ensuring data accuracy in the graphical representation. Following this, text-to-image models are employed to enhance the vector graphic generated by Mermaid, adding rich visual elements and enhancing overall aesthetic appeal. The integration of text-to-image models is a key innovation, enabling the creation of graphics that are not only informative but also visually captivating. Finally, a VLM performs quality control, ensuring that the visual enhancements align with the informational accuracy. This comprehensive approach effectively combines the accuracy of structured data representation, the creative potential of text-to-image models, and the validation capabilities of VLMs. The result is a new standard in information graphic creation, suitable for diverse applications ranging from education to scientific communication, where both information integrity and visual engagement are essential.",
                "authors": "Chao-Ting Chen, Hen-Hsen Huang",
                "citations": 0
            },
            {
                "title": "OSPC: Artificial VLM Features for Hateful Meme Detection",
                "abstract": "The digital revolution and the advent of the world wide web have transformed human communication, notably through the emergence of memes. While memes are a popular and straightforward form of expression, they can also be used to spread misinformation and hate due to their anonymity and ease of use. In response to these challenges, this paper introduces a solution developed by team 'Baseline' for the AI Singapore Online Safety Prize Challenge. Focusing on computational efficiency and feature engineering, the solution achieved an AUROC of 0.76 and an accuracy of 0.69 on the test dataset. As key features, the solution leverages the inherent probabilistic capabilities of large Vision-Language Models (VLMs) to generate task-adapted feature encodings from text, and applies a distilled quantization tailored to the specific cultural nuances present in Singapore. This type of processing and fine-tuning can be adapted to various visual and textual understanding and classification tasks, and even applied on private VLMs such as OpenAI's GPT. Finally it can eliminate the need for extensive model training on large GPUs for resource constrained applications, also offering a solution when little or no data is available.",
                "authors": "Peter Grönquist",
                "citations": 0
            },
            {
                "title": "MEDIFICS: Model Calling Enhanced VLM for Medical VQA",
                "abstract": "The integration of multimodal data has revolutionized medical diagnostics by combining visual and textual information for a comprehensive understanding of patient conditions. This paper introduces MEDIFICS, an innovative solution that leverages the IDEFICS-9b-instruct model, fine-tuned with QLoRA, to transform image and text data into actionable medical insights. Synthetic data generation enhances model training by creating structured datasets from multiple sources, thereby improving accuracy and generalization. The system ensures precise diagnostics through specialized models for image classification and object detection, tailored to medical specialties like dermatology, radiology, and oncology, using a new functionality called model calling. Stringent data security measures protect sensitive information, and evaluation metrics show significant accuracy improvements post-fine-tuning. Practical applications in diagnosing skin lesions, interpreting radiographs, and identifying breast masses demonstrate MEDIFICS’ potential to revolutionize medical diagnostics, offering time-efficient, accessible, and free consultations. This study highlights the effectiveness of multimodal data analysis and synthetic data generation, setting the stage for future advancements in AI-driven healthcare solutions.",
                "authors": "Et-Tousy Said, Ait El Aouad Soufiane, Et-Tousy Jamal",
                "citations": 0
            },
            {
                "title": "VLM-driven Behavior Tree for Context-aware Task Planning",
                "abstract": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs) has recently gained attention in the robotics community, yet remains in its early stages of development. In this paper, we propose a novel framework that leverages Vision-Language Models (VLMs) to interactively generate and edit BTs that address visual conditions, enabling context-aware robot operations in visually complex environments. A key feature of our approach lies in the conditional control through self-prompted visual conditions. Specifically, the VLM generates BTs with visual condition nodes, where conditions are expressed as free-form text. Another VLM process integrates the text into its prompt and evaluates the conditions against real-world images during robot execution. We validated our framework in a real-world cafe scenario, demonstrating both its feasibility and limitations.",
                "authors": "Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Kazuhiro Sasabuchi, Katsushi Ikeuchi",
                "citations": 0
            },
            {
                "title": "BMIP: Bi-directional Modality Interaction Prompt Learning for VLM",
                "abstract": "Vision-language models (VLMs) have exhibited remarkable generalization capabilities, and prompt learning for VLMs has attracted great attention for the ability to adapt pre-trained VLMs to specific downstream tasks. However, existing studies mainly focus on single-modal prompts or uni-directional modality interaction, overlooking the powerful alignment effects resulting from the interaction between the vision and language modalities. To this end, we propose a novel prompt learning method called $\\underline{\\textbf{B}}i-directional \\underline{\\textbf{M}}odality \\underline{\\textbf{I}}nteraction \\underline{\\textbf{P}}rompt (BMIP)$, which dynamically weights bi-modal information through learning the information of the attention layer, enhancing trainability and inter-modal consistency compared to simple information aggregation methods. To evaluate the effectiveness of prompt learning methods, we propose a more realistic evaluation paradigm called open-world generalization complementing the widely adopted cross-dataset transfer and domain generalization tasks. Comprehensive experiments on various datasets reveal that BMIP not only outperforms current state-of-the-art methods across all three evaluation paradigms but is also flexible enough to be combined with other prompt-based methods for consistent performance enhancement.",
                "authors": "Song-Lin Lv, Yu-Yang Chen, Zhi Zhou, Ming Yang, Lan-Zhe Guo",
                "citations": 0
            },
            {
                "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
                "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. Code and models are available at https://github.com/dvlab-research/MiniGemini.",
                "authors": "Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia",
                "citations": 150
            },
            {
                "title": "What matters when building vision-language models?",
                "abstract": "The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.",
                "authors": "Hugo Laurençon, Léo Tronchon, Matthieu Cord, Victor Sanh",
                "citations": 94
            },
            {
                "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities",
                "abstract": "Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size difference. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in training recipe including data quality, training pipeline and VLM architecture. Our workfeatures the first Internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qual-itative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of thought spatial reasoning and robotics due to its quantitative estimation capability. Website: https://spatial-vlm.github.iol",
                "authors": "Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas J. Guibas, Fei Xia",
                "citations": 93
            },
            {
                "title": "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model",
                "abstract": "We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs' performance. Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale. Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our models will be released at https://github.com/Meituan-AutoML/MobileVLM .",
                "authors": "Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, Chunhua Shen",
                "citations": 75
            },
            {
                "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
                "abstract": "Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization, and challenge sets that probe properties such as hallucination; evaluations that provide fine-grained insight VLM capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and training from base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible training code, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open VLMs.",
                "authors": "Siddharth Karamcheti, Suraj Nair, A. Balakrishna, Percy Liang, Thomas Kollar, Dorsa Sadigh",
                "citations": 64
            },
            {
                "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
                "abstract": "Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
                "authors": "Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Q. Vuong, Tingnan Zhang, T. Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, N. Heess, Chelsea Finn, Sergey Levine, Brian Ichter",
                "citations": 60
            },
            {
                "title": "Multi-Modal Hallucination Control by Visual Information Grounding",
                "abstract": "Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image. We investigate this phenomenon, usually referred to as “hallucination” and show that it stems from an excessive reliance on the language prior. In particular, we show that as more tokens are generated, the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations. To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification. M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher mutual information with the visual prompt. M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead. If training is an option, we show that M3ID can be paired with Direct Preference Optimization (DPO) to improve the model's reliance on the prompt image without requiring any labels. Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24%.",
                "authors": "Alessandro Favero, L. Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, A. Achille, Ashwin Swaminathan, S. Soatto",
                "citations": 34
            },
            {
                "title": "An Introduction to Vision-Language Modeling",
                "abstract": "Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos.",
                "authors": "Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Huijuan Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Q. Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, Vikas Chandra",
                "citations": 33
            },
            {
                "title": "Vision-language models for medical report generation and visual question answering: a review",
                "abstract": "Medical vision-language models (VLMs) combine computer vision (CV) and natural language processing (NLP) to analyze visual and textual medical data. Our paper reviews recent advancements in developing VLMs specialized for healthcare, focusing on publicly available models designed for medical report generation and visual question answering (VQA). We provide background on NLP and CV, explaining how techniques from both fields are integrated into VLMs, with visual and language data often fused using Transformer-based architectures to enable effective learning from multimodal data. Key areas we address include the exploration of 18 public medical vision-language datasets, in-depth analyses of the architectures and pre-training strategies of 16 recent noteworthy medical VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs' performance in medical report generation and VQA. We also highlight current challenges facing medical VLM development, including limited data availability, concerns with data privacy, and lack of proper evaluation metrics, among others, while also proposing future directions to address these obstacles. Overall, our review summarizes the recent progress in developing VLMs to harness multimodal medical data for improved healthcare applications.",
                "authors": "Iryna Hartsock, Ghulam Rasool",
                "citations": 28
            },
            {
                "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning",
                "abstract": "Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual instruction tuning framework and achieves the state-of-the-art performance across a wide range of multi-modal evaluation benchmarks. Finally, we conduct in-depth analyses to understand visual instruction tuning and our findings reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs' capabilities but rather modulates the model's responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can effectively align VLM responses with human-preference; (3) Visual instruction tuning mainly helps large-language models (LLMs) to understand visual features.",
                "authors": "Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, dingnan jin, Yu Cheng, Qifan Wang, Lifu Huang",
                "citations": 26
            },
            {
                "title": "Multimodal Healthcare AI: Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology",
                "abstract": "Recent advances in AI combine large language models (LLMs) with vision encoders that bring forward unprecedented technical capabilities to leverage for a wide range of healthcare applications. Focusing on the domain of radiology, vision-language models (VLMs) achieve good performance results for tasks such as generating radiology findings based on a patient’s medical image, or answering visual questions (e.g., “Where are the nodules in this chest X-ray?”). However, the clinical utility of potential applications of these capabilities is currently underexplored. We engaged in an iterative, multidisciplinary design process to envision clinically relevant VLM interactions, and co-designed four VLM use concepts: Draft Report Generation, Augmented Report Review, Visual Search and Querying, and Patient Imaging History Highlights. We studied these concepts with 13 radiologists and clinicians who assessed the VLM concepts as valuable, yet articulated many design considerations. Reflecting on our findings, we discuss implications for integrating VLM capabilities in radiology, and for healthcare AI more generally.",
                "authors": "Nur Yildirim, Hannah Richardson, M. Wetscherek, Junaid Bajwa, Joseph Jacob, Mark A. Pinnock, Stephen Harris, Daniel Coelho De Castro, Shruthi Bannur, Stephanie L. Hyland, Pratik Ghosh, M. Ranjit, Kenza Bouzid, Anton Schwaighofer, Fernando P'erez-Garc'ia, Harshita Sharma, O. Oktay, M. Lungren, Javier Alvarez-Valle, A. Nori, Anja Thieme",
                "citations": 24
            },
            {
                "title": "MOKA: Open-World Robotic Manipulation through Mark-Based Visual Prompting",
                "abstract": "Open-world generalization requires robotic systems to have a profound understanding of the physical world and the user command to solve diverse and complex tasks. While the recent advancement in vision-language models (VLMs) has offered unprecedented opportunities to solve open-world problems, how to leverage their capabilities to control robots remains a grand challenge. In this paper, we introduce Marking Open-world Keypoint Affordances (MOKA), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language instructions. Central to our approach is a compact point-based representation of affordance, which bridges the VLM's predictions on observed images and the robot's actions in the physical world. By prompting the pre-trained VLM, our approach utilizes the VLM's commonsense knowledge and concept understanding acquired from broad data sources to predict affordances and generate motions. To facilitate the VLM's reasoning in zero-shot and few-shot manners, we propose a visual prompting technique that annotates marks on images, converting affordance reasoning into a series of visual question-answering problems that are solvable by the VLM. We further explore methods to enhance performance with robot experiences collected by MOKA through in-context learning and policy distillation. We evaluate and analyze MOKA's performance on various table-top manipulation tasks including tool use, deformable body manipulation, and object rearrangement.",
                "authors": "Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine",
                "citations": 23
            },
            {
                "title": "Building and better understanding vision-language models: insights and future directions",
                "abstract": "The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a VLM. We begin by providing a comprehensive overview of the current state-of-the-art approaches, highlighting the strengths and weaknesses of each, addressing the major challenges in the field, and suggesting promising research directions for underexplored areas. We then walk through the practical steps to build Idefics3-8B, a powerful VLM that significantly outperforms its predecessor Idefics2-8B, while being trained efficiently, exclusively on open datasets, and using a straightforward pipeline. These steps include the creation of Docmatix, a dataset for improving document understanding capabilities, which is 240 times larger than previously available datasets. We release the model along with the datasets created for its training.",
                "authors": "Hugo Laurençon, Andr'es Marafioti, Victor Sanh, Léo Tronchon",
                "citations": 22
            },
            {
                "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset",
                "abstract": "Using vision-language models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML. Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored. We posit that this is mainly due to the absence of a suitable, high-quality dataset. This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots. We fine-tune a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code. To accelerate the research in this area, we open-source WebSight.",
                "authors": "Hugo Laurençon, Léo Tronchon, Victor Sanh",
                "citations": 20
            },
            {
                "title": "Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases",
                "abstract": "Large Vision-Language Models (LVLMs) have received widespread attention for advancing the interpretable self-driving. Existing evaluations of LVLMs primarily focus on multi-faceted capabilities in natural circumstances, lacking automated and quantifiable assessment for self-driving, let alone the severe road corner cases. In this work, we propose CODA-LM, the very first benchmark for the automatic evaluation of LVLMs for self-driving corner cases. We adopt a hierarchical data structure and prompt powerful LVLMs to analyze complex driving scenes and generate high-quality pre-annotations for the human annotators, while for LVLM evaluation, we show that using the text-only large language models (LLMs) as judges reveals even better alignment with human preferences than the LVLM judges. Moreover, with our CODA-LM, we build CODA-VLM, a new driving LVLM surpassing all open-sourced counterparts on CODA-LM. Our CODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V by +21.42% on the regional perception task. We hope CODA-LM can become the catalyst to promote interpretable self-driving empowered by LVLMs.",
                "authors": "Yanze Li, Wenhua Zhang, Kai Chen, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, Dit-Yan Yeung, Huchuan Lu, Xu Jia",
                "citations": 20
            },
            {
                "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning",
                "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.",
                "authors": "Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, Sergey Levine",
                "citations": 20
            },
            {
                "title": "RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics",
                "abstract": "From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. Using the pipeline, we train RoboPoint, a VLM that predicts image keypoint affordances given language instructions. Compared to alternative approaches, our method requires no real-world data collection or human demonstration, making it much more scalable to diverse environments and viewpoints. In addition, RoboPoint is a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance. Our experiments demonstrate that RoboPoint outperforms state-of-the-art VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy of predicting spatial affordance and by 30.5% in the success rate of downstream tasks. Project website: https://robo-point.github.io.",
                "authors": "Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, A. Mousavian, Dieter Fox",
                "citations": 20
            },
            {
                "title": "Negative Label Guided OOD Detection with Pretrained Vision-Language Models",
                "abstract": "Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels. Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM architectures. Furthermore, our method NegLabel exhibits remarkable robustness against diverse domain shifts. The codes are available at https://github.com/tmlr-group/NegLabel.",
                "authors": "Xue Jiang, Feng Liu, Zhengfeng Fang, Hong Chen, Tongliang Liu, Feng Zheng, Bo Han",
                "citations": 17
            },
            {
                "title": "Explore until Confident: Efficient Exploration for Embodied Question Answering",
                "abstract": "We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's question answering confidence, allowing the robot to know when to stop exploration - leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence. Webpage with experiment videos and code: https://explore-eqa.github.io/",
                "authors": "Allen Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, Dorsa Sadigh",
                "citations": 16
            },
            {
                "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations",
                "abstract": "Vision-Language Models (VLMs) have demonstrated their broad effectiveness thanks to extensive training in aligning visual instructions to responses. However, such training of conclusive alignment leads models to ignore essential visual reasoning, further resulting in failures in meticulous visual problems and unfaithful responses. Drawing inspiration from human cognition in solving visual problems (e.g., marking, zoom in), this paper introduces Chain of Manipulations, a mechanism that enables VLMs to solve problems step-by-step with evidence. After training, models can solve various visual problems by eliciting intrinsic manipulations (e.g., grounding, zoom in) with results (e.g., boxes, image) actively without involving external tools, while also allowing users to trace error causes. We study the roadmap to implement this mechanism, including (1) a flexible design of manipulations upon extensive analysis, (2) an efficient automated data generation pipeline, (3) a compatible VLM architecture capable of multi-turn multi-image, and (4) a model training process for versatile capabilities. With the design, we also manually annotate 6K high-quality samples for the challenging graphical mathematical problems. Our trained model, \\textbf{CogCoM}, equipped with this mechanism with 17B parameters achieves state-of-the-art performance across 9 benchmarks from 4 categories, demonstrating the effectiveness while preserving the interpretability. Our code, model weights, and collected data are publicly available at https://github.com/THUDM/CogCoM.",
                "authors": "Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang",
                "citations": 15
            },
            {
                "title": "RePLan: Robotic Replanning with Perception and Language Models",
                "abstract": "Advancements in large language models (LLMs) have demonstrated their potential in facilitating high-level reasoning, logical reasoning and robotics planning. Recently, LLMs have also been able to generate reward functions for low-level robot actions, effectively bridging the interface between high-level planning and low-level robot control. However, the challenge remains that even with syntactically correct plans, robots can still fail to achieve their intended goals due to imperfect plans or unexpected environmental issues. To overcome this, Vision Language Models (VLMs) have shown remarkable success in tasks such as visual question answering. Leveraging the capabilities of VLMs, we present a novel framework called Robotic Replanning with Perception and Language Models (RePLan) that enables online replanning capabilities for long-horizon tasks. This framework utilizes the physical grounding provided by a VLM's understanding of the world's state to adapt robot actions when the initial plan fails to achieve the desired goal. We developed a Reasoning and Control (RC) benchmark with eight long-horizon tasks to test our approach. We find that RePLan enables a robot to successfully adapt to unforeseen obstacles while accomplishing open-ended, long-horizon goals, where baseline models cannot, and can be readily applied to real robots. Find more information at https://replan-lm.github.io/replan.github.io/",
                "authors": "Marta Skreta, Zihan Zhou, Jia Lin Yuan, Kourosh Darvish, Alán Aspuru-Guzik, Animesh Garg",
                "citations": 18
            },
            {
                "title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
                "abstract": "Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphical User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing daily computer tasks. Finally, we train a model, ScreenAgent, which achieves comparable computer control capabilities to GPT-4V and demonstrated more precise UI positioning capabilities. Our attempts could inspire further research on building a generalist LLM agent. The code and more detailed information are at https://github.com/niuzaisheng/ScreenAgent.",
                "authors": "Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, Qi Wang",
                "citations": 19
            },
            {
                "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning",
                "abstract": "Training corpuses for vision language models (VLMs) typically lack sufficient amounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal for decision-making tasks such as in-the-wild device control through graphical user interfaces (GUIs). While training with static demonstrations has shown some promise, we show that such methods fall short for controlling real GUIs due to their failure to deal with real-world stochasticity and non-stationarity not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline RL to initialize the model, followed by offline-to-online RL. To do this, we build a scalable and parallelizable Android learning environment equipped with a VLM-based evaluator and develop a simple yet effective RL approach for learning in this domain. Our approach runs advantage-weighted RL with advantage estimators enhanced to account for stochasticity along with an automatic curriculum for deriving maximal learning signal. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.3B VLM trained with RL achieves a 49.5% absolute improvement -- from 17.7 to 67.2% success rate -- over supervised fine-tuning with static human demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent trained with AitW data (38.5%), but also the prior best autonomous RL approach based on filtered behavior cloning (57.8%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.",
                "authors": "Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, Aviral Kumar",
                "citations": 15
            },
            {
                "title": "BRAVE: Broadening the visual encoding of vision-language models",
                "abstract": "Vision-language models (VLMs) are typically composed of a vision encoder, e.g. CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks. Despite remarkable progress, VLMs are subject to several shortcomings due to the limited capabilities of vision encoders, e.g.\"blindness\"to certain image features, visual hallucination, etc. To address these issues, we study broadening the visual encoding capabilities of VLMs. We first comprehensively benchmark several vision encoders with different inductive biases for solving VLM tasks. We observe that there is no single encoding configuration that consistently achieves top performance across different tasks, and encoders with different biases can perform surprisingly similarly. Motivated by this, we introduce a method, named BRAVE, that consolidates features from multiple frozen encoders into a more versatile representation that can be directly fed as the input to a frozen LM. BRAVE achieves state-of-the-art performance on a broad range of captioning and VQA benchmarks and significantly reduces the aforementioned issues of VLMs, while requiring a smaller number of trainable parameters than existing methods and having a more compressed representation. Our results highlight the potential of incorporating different visual biases for a more broad and contextualized visual understanding of VLMs.",
                "authors": "Ouguzhan Fatih Kar, A. Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, Federico Tombari",
                "citations": 15
            },
            {
                "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
                "abstract": "Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that encode semantic features of visual observations based on the VLM's internal knowledge and reasoning capabilities, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings from off-the-shelf, general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings. Finally, we show that our approach can use chain-of-thought prompting to produce representations of common-sense semantic reasoning, improving policy performance in novel scenes by 1.5 times.",
                "authors": "William Chen, Oier Mees, Aviral Kumar, Sergey Levine",
                "citations": 15
            },
            {
                "title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views",
                "abstract": "Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.",
                "authors": "Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Marc Pollefeys, Federico Tombari",
                "citations": 15
            },
            {
                "title": "WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences",
                "abstract": "Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar. Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.",
                "authors": "Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, Bill Yuchen Lin",
                "citations": 14
            },
            {
                "title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation",
                "abstract": "Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision",
                "authors": "Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, Minjoon Seo",
                "citations": 13
            },
            {
                "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
                "abstract": "LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity to process state information as visual-textual prompts and respond with policy decisions in text. We propose LLaRA: Large Language and Robotics Assistant, a framework that formulates robot action policy as conversations and provides improved action outputs when trained with auxiliary data that complements policy learning. We first introduce an automated pipeline to generate conversation-style instruction tuning data from existing behavior cloning data. Then we enrich the dataset in a self-supervised fashion by formulating six auxiliary tasks. A VLM finetuned with the resulting collection of datasets can generate meaningful robot action policy decisions. Our experiments across multiple simulated and real-world environments demonstrate the state-of-the-art performance of the proposed LLaRA framework. The code, datasets, and pretrained models are available at https://github.com/LostXine/LLaRA.",
                "authors": "Xiang Li, Cristina Mata, Jong Sung Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, R. Burgert, Mu Cai, Yong Jae Lee, M. Ryoo",
                "citations": 13
            },
            {
                "title": "Why are Visually-Grounded Language Models Bad at Image Classification?",
                "abstract": "Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet. To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs. Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data. Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM's performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models. Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset.",
                "authors": "Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, S. Yeung-Levy",
                "citations": 12
            },
            {
                "title": "Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data",
                "abstract": "Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess world knowledge that leads to strong performance on location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting. Our benchmark is publicly available on this website. A full version of this paper can be found here.",
                "authors": "Chenhui Zhang, Sherrie Wang",
                "citations": 12
            },
            {
                "title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography",
                "abstract": "Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current shortage of both general and specialized radiologists, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies while simultaneously using the images to extract novel physiological insights. Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs) that utilize both the image and the corresponding textual radiology reports. However, current medical VLMs are generally limited to 2D images and short reports. To overcome these shortcomings for abdominal CT interpretation, we introduce Merlin - a 3D VLM that leverages both structured electronic health records (EHR) and unstructured radiology reports for pretraining without requiring additional manual annotations. We train Merlin using a high-quality clinical dataset of paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens) for training. We comprehensively evaluate Merlin on 6 task types and 752 individual tasks. The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year chronic disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs). We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines. We derive data scaling laws to empirically assess training data needs for requisite downstream task performance. Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU. This computationally efficient design can help democratize foundation model training, especially for health systems with compute constraints. We plan to release our trained models, code, and dataset, pending manual removal of all protected health information.",
                "authors": "Louis Blankemeier, Joseph Paul Cohen, Ashwin Kumar, Dave Van Veen, Syed Jamal Safdar Gardezi, Magdalini Paschali, Zhihong Chen, Jean-Benoit Delbrouck, E. Reis, C. Truyts, Christian Bluethgen, Malte E. K. Jensen, Sophie Ostmeier, Maya Varma, Jeya Maria Jose Valanarasu, Zhongnan Fang, Zepeng Huo, Zaid Nabulsi, Diego Ardila, Wei-Hung Weng, Edson Amaro Junior, Neera Ahuja, J. Fries, Nigam H. Shah, Andrew Johnston, Robert D. Boutin, Andrew Wentland, C. Langlotz, Jason Hom, S. Gatidis, Akshay S. Chaudhari",
                "citations": 11
            },
            {
                "title": "Benchmarking Vision Language Models for Cultural Understanding",
                "abstract": "Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM’s geo-diverse cultural understanding. We curate a diverse collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly weaker capabilities for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.",
                "authors": "Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stańczak, Aishwarya Agrawal",
                "citations": 11
            },
            {
                "title": "Fasting-activated ventrolateral medulla neurons regulate T cell homing and suppress autoimmune disease in mice",
                "abstract": null,
                "authors": "Liang Wang, Mingxiu Cheng, Yuchen Wang, Jing Chen, Famin Xie, Li-Hao Huang, Cheng Zhan",
                "citations": 10
            },
            {
                "title": "CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor and Indoor Environments",
                "abstract": "We present CoNVOI, a novel method for autonomous robot navigation in real-world indoor and outdoor environments using Vision Language Models (VLMs). We employ VLMs in two ways: first, we leverage their zero-shot image classification capability to identify the context or scenario (e.g., indoor corridor, outdoor terrain, crosswalk, etc) of the robot’s surroundings, and formulate context-based navigation behaviors as simple text prompts (e.g. \"stay on the pavement\"). Second, we utilize their state-of-the-art semantic understanding and logical reasoning capabilities to compute a suitable trajectory given the identified context. To this end, we propose a novel multi-modal visual marking approach to annotate the obstacle-free regions in the RGB image used as input to the VLM with numbers, by correlating it with a local occupancy map of the environment. The marked numbers ground image locations in the real-world, direct the VLM’s attention solely to navigable locations, and elucidate the spatial relationships between them and terrains depicted in the image to the VLM. Next, we query the VLM to select numbers on the marked image that satisfy the context-based behavior text prompt, and construct a reference path using the selected numbers. Finally, we propose a method to extrapolate the reference trajectory when the robot’s environmental context has not changed to prevent unnecessary VLM queries. We use the reference trajectory to guide a motion planner, and demonstrate that it leads to human-like behaviors (e.g. not cutting through a group of people, using crosswalks, etc.) in various real-world indoor and outdoor scenarios. We perform several ablations and navigation comparisons and demonstrate that CoNVOI’s trajectories are most similar to human teleoperated ground truth in terms of Fréchet distance (9.7-58.2% closer), lowest path errors (up to 88.13% lower), and up to 86.09% lower % of unacceptable paths.",
                "authors": "A. Sathyamoorthy, Kasun Weerakoon, Mohamed Bashir Elnoor, Anuj Zore, Brian Ichter, Fei Xia, Jie Tan, Wenhao Yu, Dinesh Manocha",
                "citations": 11
            },
            {
                "title": "Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection",
                "abstract": "Data selection in instruction tuning emerges as a pivotal process for acquiring high-quality data and training instruction-following large language models (LLMs), but it is still a new and unexplored research area for vision-language models (VLMs). Existing data selection approaches on LLMs either rely on single unreliable scores, or use downstream tasks for selection, which is time-consuming and can lead to potential over-fitting on the chosen evaluation datasets. To address this challenge, we introduce a novel dataset selection method, Self-Filter, that utilizes the VLM itself as a filter. This approach is inspired by the observation that VLMs benefit from training with the most challenging instructions. Self-Filter operates in two stages. In the first stage, we devise a scoring network to evaluate the difficulty of training instructions, which is co-trained with the VLM. In the second stage, we use the trained score net to measure the difficulty of each instruction, select the most challenging samples, and penalize similar samples to encourage diversity. Comprehensive experiments on LLaVA and MiniGPT-4 show that Self-Filter can reach better results compared to full data settings with merely about 15% samples, and can achieve superior performance against competitive baselines.",
                "authors": "Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu, Qi He, Tianyi Xiong, Chenxi Liu, Junfeng Guo, Heng Huang",
                "citations": 10
            },
            {
                "title": "VILA2: VILA Augmented VILA",
                "abstract": "Visual language models (VLMs) have rapidly progressed, driven by the success of large language models (LLMs). While model architectures and training infrastructures advance rapidly, data curation remains under-explored. When data quantity and quality become a bottleneck, existing work either directly crawls more raw data from the Internet that does not have a guarantee of data quality or distills from black-box commercial models ( e.g ., GPT-4V [1] / Gemini [2]) causing the performance upper bounded by that model. In this work, we introduce a novel approach that includes a self-augment step and a specialist-augment step to iteratively improve data quality and model performance. In the self-augment step, a VLM recaptions its own pretraining data to enhance data quality, and then retrains from scratch using this refined dataset to improve model performance. This process can iterate for several rounds. Once self-augmentation saturates, we employ several specialist VLMs finetuned from the self-augmented VLM with domain-specific expertise, to further infuse specialist knowledge into the generalist VLM through task-oriented recaptioning and retraining. With the combined self-augmented and specialist-augmented training, we introduce VILA 2 ( VILA-augmented-VILA ), a VLM family that consistently improves the accuracy on a wide range of tasks over prior art, and achieves new state-of-the-art results on MMMU leaderboard [3] among open-sourced models.",
                "authors": "Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, Hongxu Yin",
                "citations": 8
            },
            {
                "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging VLMs' text generation capabilities to craft persuasive and seemingly rational narratives for misinformation, such as portraying junk food as healthy. We show that Shadowcast effectively achieves the attacker's intentions using as few as 50 poison samples. Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings. Moreover, Shadowcast remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. This work reveals how poisoned VLMs can disseminate convincing yet deceptive misinformation to everyday, benign users, emphasizing the importance of data integrity for responsible VLM deployments. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.",
                "authors": "Yuancheng Xu, Jiarui Yao, Manli Shu, Yanchao Sun, Zichu Wu, Ning Yu, Tom Goldstein, Furong Huang",
                "citations": 9
            },
            {
                "title": "SpatialBot: Precise Spatial Understanding with Vision Language Models",
                "abstract": "Vision Language Models (VLMs) have achieved impressive performance in 2D image understanding, however they are still struggling with spatial understanding which is the foundation of Embodied AI. In this paper, we propose SpatialBot for better spatial understanding by feeding both RGB and depth images. Additionally, we have constructed the SpatialQA dataset, which involves multi-level depth-related questions to train VLMs for depth understanding. Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilities in spatial understanding at different levels. Extensive experiments on our spatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks, demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The model, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.",
                "authors": "Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao",
                "citations": 9
            },
            {
                "title": "LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors",
                "abstract": "Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text description of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.",
                "authors": "Sheng Jin, Xueying Jiang, Jiaxing Huang, Lewei Lu, Shijian Lu",
                "citations": 9
            },
            {
                "title": "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs",
                "abstract": "Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multi-modal data containing mostly captions without explicit spa-tial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any supervised detection data. To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spa-tial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons.",
                "authors": "Michael Dorkenwald, Nimrod Barazani, Cees G. M. Snoek, Yuki M. Asano",
                "citations": 9
            },
            {
                "title": "Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs",
                "abstract": "Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of category-specific prompts resulting in a strong zero-shot classifier. MPVR generalizes effectively across various popular zero-shot image recognition benchmarks belonging to widely different domains when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively",
                "authors": "M. J. Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub Micorek, M. Koziński, Hilde Kuhene, Horst Possegger",
                "citations": 9
            },
            {
                "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs",
                "abstract": "Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We propose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-art performance when applied on the PaLI3-5B VLM by \\citet{chen2023pali3}, while also enabling much better performance on PlotQA and FigureQA. We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by \\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model is fine-tuned using the multitask loss introduced by \\citet{hsieh2023distilling}. Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream OCR system, while keeping inference time constant compared to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought prompt \\cite{chen2023program}, our model outperforms the recently introduced Gemini Ultra and GPT-4V.",
                "authors": "Victor Carbune, Hassan Mansoor, Fangyu Liu, Rahul Aralikatte, Gilles Baechler, Jindong Chen, Abhanshu Sharma",
                "citations": 8
            },
            {
                "title": "Vision-Language Models for Feature Detection of Macular Diseases on Optical Coherence Tomography.",
                "abstract": "Importance\nVision-language models (VLMs) are a novel artificial intelligence technology capable of processing image and text inputs. While demonstrating strong generalist capabilities, their performance in ophthalmology has not been extensively studied.\n\n\nObjective\nTo assess the performance of the Gemini Pro VLM in expert-level tasks for macular diseases from optical coherence tomography (OCT) scans.\n\n\nDesign, Setting, and Participants\nThis was a cross-sectional diagnostic accuracy study evaluating a generalist VLM on ophthalmology-specific tasks using the open-source Optical Coherence Tomography Image Database. The dataset included OCT B-scans from 50 unique patients: healthy individuals and those with macular hole, diabetic macular edema, central serous chorioretinopathy, and age-related macular degeneration. Each OCT scan was labeled for 10 key pathological features, referral recommendations, and treatments. The images were captured using a Cirrus high definition OCT machine (Carl Zeiss Meditec) at Sankara Nethralaya Eye Hospital, Chennai, India, and the dataset was published in December 2018. Image acquisition dates were not specified.\n\n\nExposures\nGemini Pro, using a standard prompt to extract structured responses on December 15, 2023.\n\n\nMain Outcomes and Measures\nThe primary outcome was model responses compared against expert labels, calculating F1 scores for each pathological feature. Secondary outcomes included accuracy in diagnosis, referral urgency, and treatment recommendation. The model's internal concordance was evaluated by measuring the alignment between referral and treatment recommendations, independent of diagnostic accuracy.\n\n\nResults\nThe mean F1 score was 10.7% (95% CI, 2.4-19.2). Measurable F1 scores were obtained for macular hole (36.4%; 95% CI, 0-71.4), pigment epithelial detachment (26.1%; 95% CI, 0-46.2), subretinal hyperreflective material (24.0%; 95% CI, 0-45.2), and subretinal fluid (20.0%; 95% CI, 0-45.5). A correct diagnosis was achieved in 17 of 50 cases (34%; 95% CI, 22-48). Referral recommendations varied: 28 of 50 were correct (56%; 95% CI, 42-70), 10 of 50 were overcautious (20%; 95% CI, 10-32), and 12 of 50 were undercautious (24%; 95% CI, 12-36). Referral and treatment concordance were very high, with 48 of 50 (96%; 95 % CI, 90-100) and 48 of 49 (98%; 95% CI, 94-100) correct answers, respectively.\n\n\nConclusions and Relevance\nIn this study, a generalist VLM demonstrated limited vision capabilities for feature detection and management of macular disease. However, it showed low self-contradiction, suggesting strong language capabilities. As VLMs continue to improve, validating their performance on large benchmarking datasets will help ascertain their potential in ophthalmology.",
                "authors": "F. Antaki, Reena Chopra, P. Keane",
                "citations": 8
            },
            {
                "title": "MyVLM: Personalizing VLMs for User-Specific Queries",
                "abstract": "Recent large-scale vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and generating textual descriptions for visual content. However, these models lack an understanding of user-specific concepts. In this work, we take a first step toward the personalization of VLMs, enabling them to learn and reason over user-provided concepts. For example, we explore whether these models can learn to recognize you in an image and communicate what you are doing, tailoring the model to reflect your personal experiences and relationships. To effectively recognize a variety of user-specific concepts, we augment the VLM with external concept heads that function as toggles for the model, enabling the VLM to identify the presence of specific target concepts in a given image. Having recognized the concept, we learn a new concept embedding in the intermediate feature space of the VLM. This embedding is tasked with guiding the language model to naturally integrate the target concept in its generated response. We apply our technique to BLIP-2 and LLaVA for personalized image captioning and further show its applicability for personalized visual question-answering. Our experiments demonstrate our ability to generalize to unseen images of learned concepts while preserving the model behavior on unrelated inputs.",
                "authors": "Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, D. Cohen-Or",
                "citations": 6
            },
            {
                "title": "Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback",
                "abstract": "Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). The previous approaches for VLMMs involved Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and adding additional learnable modules. Video and text multimodal alignment remains challenging, primarily due to the deficient volume and quality of multimodal instruction-tune data compared to text-only data. We present a novel alignment strategy that employs multimodal AI system to oversee itself called Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. In specific, we propose context-aware reward modeling by providing detailed video descriptions as context during the generation of preference feedback in order to enrich the understanding of video content. Demonstrating enhanced performance across diverse video benchmarks, our multimodal RLAIF approach, VLM-RLAIF, outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.",
                "authors": "Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, Jonghyun Choi",
                "citations": 6
            },
            {
                "title": "LOC-ZSON: Language-driven Object-Centric Zero-Shot Object Retrieval and Navigation",
                "abstract": "In this paper, we present LOC-ZSON, a novel Language-driven Object-Centric image representation for object navigation task within complex scenes. We propose an object-centric image representation and corresponding losses for visual-language model (VLM) fine-tuning, which can handle complex object-level queries. In addition, we design a novel LLM-based augmentation and prompt templates for stability during training and zero-shot inference. We implement our method on Astro robot and deploy it in both simulated and real-world environments for zero-shot object navigation. We show that our proposed method can achieve an improvement of 1.38 - 13.38% in terms of text-to-image recall on different benchmark settings for the retrieval task. For object navigation, we show the benefit of our approach in simulation and real world, showing 5% and 16.67% improvement in terms of navigation success rate, respectively.",
                "authors": "Tianrui Guan, Yurou Yang, Harry Cheng, Muyuan Lin, Richard Kim, R. Madhivanan, Arnie Sen, Dinesh Manocha",
                "citations": 6
            },
            {
                "title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)",
                "abstract": "With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their prompt faithfulness -- the semantic coherence of generated images to the prompts they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs). However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines. We introduce T2IScoreScore, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images. These allow us to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple (and supposedly worse) feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.",
                "authors": "Michael Stephen Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, William Yang Wang",
                "citations": 6
            },
            {
                "title": "Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?",
                "abstract": "Data science and engineering workflows often span multiple stages, from warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As vision language models (VLMs) advance in multimodal understanding and code generation, VLM-based agents could potentially automate these workflows by generating SQL queries, Python code, and GUI operations. This automation can improve the productivity of experts while democratizing access to large-scale data analysis. In this paper, we introduce Spider2-V, the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications. These tasks, derived from real-world use cases, evaluate the ability of a multimodal agent to perform data-related tasks by writing code and managing the GUI in enterprise data software systems. To balance realistic simulation with evaluation simplicity, we devote significant effort to developing automatic configurations for task setup and carefully crafting evaluation metrics for each task. Furthermore, we supplement multimodal agents with comprehensive documents of these enterprise data software systems. Our empirical evaluation reveals that existing state-of-the-art LLM/VLM-based agents do not reliably automate full data workflows (14.0% success). Even with step-by-step guidance, these agents still underperform in tasks that require fine-grained, knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted workspaces (10.6%). We hope that Spider2-V paves the way for autonomous multimodal agents to transform the automation of data science and engineering workflow. Our code and data are available at https://spider2-v.github.io.",
                "authors": "Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu",
                "citations": 6
            },
            {
                "title": "Descriptive Image Quality Assessment in the Wild",
                "abstract": "With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Depicted image Quality Assessment in the Wild (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released in https://depictqa.github.io/depictqa-wild/.",
                "authors": "Zhiyuan You, Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Tianfan Xue, Chao Dong",
                "citations": 6
            },
            {
                "title": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step",
                "abstract": "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-CoT, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-CoT independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-CoT to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-CoT-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-CoT not only outperforms its base model by 7.4% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct.",
                "authors": "Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan",
                "citations": 7
            },
            {
                "title": "Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts",
                "abstract": "Large Vision Language Models (VLMs) extend and enhance the perceptual abilities of Large Language Models (LLMs). Despite offering new possibilities for LLM applications, these advancements raise significant security and ethical concerns, particularly regarding the generation of harmful content. While LLMs have undergone extensive security evaluations with the aid of red teaming frameworks, VLMs currently lack a well-developed one. To fill this gap, we introduce Arondight, a standardized red team framework tailored specifically for VLMs. Arondight is dedicated to resolving issues related to the absence of visual modality and inadequate diversity encountered when transitioning existing red teaming methodologies from LLMs to VLMs. Our framework features an automated multi-modal jailbreak attack, wherein visual jailbreak prompts are produced by a red team VLM, and textual prompts are generated by a red team LLM guided by a reinforcement learning agent. To enhance the comprehensiveness of VLM security evaluation, we integrate entropy bonuses and novelty reward metrics. These elements incentivize the RL agent to guide the red team LLM in creating a wider array of diverse and previously unseen test cases. Our evaluation of ten cutting-edge VLMs exposes significant security vulnerabilities, particularly in generating toxic images and aligning multi-modal prompts. In particular, our Arondight achieves an average attack success rate of 84.5\\% on GPT-4 in all fourteen prohibited scenarios defined by OpenAI in terms of generating toxic text. For a clearer comparison, we also categorize existing VLMs based on their safety levels and provide corresponding reinforcement recommendations. Our multimodal prompt dataset and red team code will be released after ethics committee approval. CONTENT WARNING: THIS PAPER CONTAINS HARMFUL MODEL RESPONSES.",
                "authors": "Yi Liu, Chengjun Cai, Xiaoli Zhang, Xingliang Yuan, Cong Wang",
                "citations": 6
            },
            {
                "title": "BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis",
                "abstract": "This paper presents BattleAgent, a detailed emulation demonstration system that combines the Large Vision-Language Model (VLM) and Multi-Agent System (MAS). This novel system aims to emulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time. The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes. It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging. These components collaborate to recreate historical events in a lively and comprehensive manner. This methodology holds the potential to substantially improve visualization of historical events and deepen our understanding of historical events especially from the perspective of decision making. The data and code for this project are accessible at https://github.com/agiresearch/battleagent and the demo is accessible at https://drive.google.com/file/d/1I5B3KWiYCSSP1uMiPGNmXlTmild-MzRJ/view?usp=sharing.",
                "authors": "Shuhang Lin, Wenyue Hua, Lingyao Li, Che-Jui Chang, Lizhou Fan, Jianchao Ji, Hang Hua, Mingyu Jin, Jiebo Luo, Yongfeng Zhang",
                "citations": 6
            },
            {
                "title": "R+X: Retrieval and Execution from Everyday Human Videos",
                "abstract": "We present R+X, a framework which enables robots to learn skills from long, unlabelled, first-person videos of humans performing everyday tasks. Given a language command from a human, R+X first retrieves short video clips containing relevant behaviour, and then executes the skill by conditioning an in-context imitation learning method on this behaviour. By leveraging a Vision Language Model (VLM) for retrieval, R+X does not require any manual annotation of the videos, and by leveraging in-context learning for execution, robots can perform commanded skills immediately, without requiring a period of training on the retrieved videos. Experiments studying a range of everyday household tasks show that R+X succeeds at translating unlabelled human videos into robust robot skills, and that R+X outperforms several recent alternative methods. Videos are available at https://www.robot-learning.uk/r-plus-x.",
                "authors": "Georgios Papagiannis, Norman Di Palo, Pietro Vitiello, Edward Johns",
                "citations": 7
            },
            {
                "title": "Harnessing Large Language Models for Training-Free Video Anomaly Detection",
                "abstract": "Video anomaly detection (VAD) aims to temporally locate abnormal events in a video. Existing works mostly rely on training deep models to learn the distribution of normality with either video-level supervision, one-class supervision, or in an unsupervised setting. Training-based methods are prone to be domain-specific, thus being costly for practical deployment as any domain change will involve data collection and model training. In this paper, we radically depart from previous efforts and propose LAnguage-based VAD (LAVAD), a method tackling VAD in a novel, training-free paradigm, exploiting the capabilities of pre-trained large language models (LLMs) and existing vision-language models (VLMs). We leverage VLM-based captioning models to generate textual descriptions for each frame of any test video. With the textual scene description, we then devise a prompting mechanism to unlock the capability of LLMs in terms of temporal aggregation and anomaly score estimation, turning LLMs into an effective video anomaly detector. We further leverage modality-aligned VLMs and propose effective techniques based on cross-modal similarity for cleaning noisy captions and refining the LLM-based anomaly scores. We evaluate LAVAD on two large datasets featuring real-world surveillance scenarios (UCF-Crime and XD- Violence), showing that it outperforms both unsupervised and one-class methods without requiring any training or data collection.",
                "authors": "Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, Elisa Ricci",
                "citations": 6
            },
            {
                "title": "Falcon2-11B Technical Report",
                "abstract": "We introduce Falcon2-11B, a foundation model trained on over five trillion tokens, and its multimodal counterpart, Falcon2-11B-vlm, which is a vision-to-text model. We report our findings during the training of the Falcon2-11B which follows a multi-stage approach where the early stages are distinguished by their context length and a final stage where we use a curated, high-quality dataset. Additionally, we report the effect of doubling the batch size mid-training and how training loss spikes are affected by the learning rate. The downstream performance of the foundation model is evaluated on established benchmarks, including multilingual and code datasets. The foundation model shows strong generalization across all the tasks which makes it suitable for downstream finetuning use cases. For the vision language model, we report the performance on several benchmarks and show that our model achieves a higher average score compared to open-source models of similar size. The model weights and code of both Falcon2-11B and Falcon2-11B-vlm are made available under a permissive license.",
                "authors": "Quentin Malartic, Nilabhra Roy Chowdhury, Ruxandra-Aimée Cojocaru, Mugariya Farooq, Giulia Campesan, Y. A. D. Djilali, Sanath Narayan, Ankit Singh, Maksim Velikanov, Basma El Amel Boussaha, Mohammed Al-Yafeai, Hamza Alobeidli, Leen Al Qadi, M. Seddik, Kirill Fedyanin, Réda Alami, Hakim Hacid",
                "citations": 6
            },
            {
                "title": "Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?",
                "abstract": "Rapid advancements in Autonomous Driving (AD) tasks turned a significant shift toward end-to-end fashion, particularly in the utilization of vision-language models (VLMs) that integrate robust logical reasoning and cognitive abilities to enable comprehensive end-to-end planning. However, these VLM-based approaches tend to integrate 2D vision tokenizers and a large language model (LLM) for ego-car planning, which lack 3D geometric priors as a cornerstone of reliable planning. Naturally, this observation raises a critical concern: Can a 2D-tokenized LLM accurately perceive the 3D environment? Our evaluation of current VLM-based methods across 3D object detection, vectorized map construction, and environmental caption suggests that the answer is, unfortunately, NO. In other words, 2D-tokenized LLM fails to provide reliable autonomous driving. In response, we introduce DETR-style 3D perceptrons as 3D tokenizers, which connect LLM with a one-layer linear projector. This simple yet elegant strategy, termed Atlas, harnesses the inherent priors of the 3D physical world, enabling it to simultaneously process high-resolution multi-view images and employ spatiotemporal modeling. Despite its simplicity, Atlas demonstrates superior performance in both 3D detection and ego planning tasks on nuScenes dataset, proving that 3D-tokenized LLM is the key to reliable autonomous driving. The code and datasets will be released.",
                "authors": "Yifan Bai, Dongming Wu, Yingfei Liu, Fan Jia, Weixin Mao, Ziheng Zhang, Yucheng Zhao, Jianbing Shen, Xing Wei, Tiancai Wang, Xiangyu Zhang",
                "citations": 7
            },
            {
                "title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
                "abstract": "Vision Language Models (VLMs) demonstrate remarkable proficiency in addressing a wide array of visual questions, which requires strong perception and reasoning faculties. Assessing these two competencies independently is crucial for model refinement, despite the inherent difficulty due to the intertwined nature of seeing and reasoning in existing VLMs. To tackle this issue, we present Prism, an innovative framework designed to disentangle the perception and reasoning processes involved in visual question solving. Prism comprises two distinct stages: a perception stage that utilizes a VLM to extract and articulate visual information in textual form, and a reasoning stage that formulates responses based on the extracted visual information using a Large Language Model (LLM). This modular design enables the systematic comparison and assessment of both proprietary and open-source VLM for their perception and reasoning strengths. Our analytical framework provides several valuable insights, underscoring Prism's potential as a cost-effective solution for vision-language tasks. By combining a streamlined VLM focused on perception with a powerful LLM tailored for reasoning, Prism achieves superior results in general vision-language tasks while substantially cutting down on training and operational expenses. Quantitative evaluations show that Prism, when configured with a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on par with VLMs $10 \\times$ larger on the rigorous multimodal benchmark MMStar. The project is released at: https://github.com/SparksJoe/Prism.",
                "authors": "Yu Qiao, Haodong Duan, Xinyu Fang, Junming Yang, Lin Chen, Songyang Zhang, Jiaqi Wang, Dahua Lin, Kai Chen",
                "citations": 7
            },
            {
                "title": "CarLLaVA: Vision language models for camera-only closed-loop driving",
                "abstract": "In this technical report, we present CarLLaVA, a Vision Language Model (VLM) for autonomous driving, developed for the CARLA Autonomous Driving Challenge 2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA architecture as backbone, achieving state-of-the-art closed-loop driving performance with only camera input and without the need for complex or expensive labels. Additionally, we show preliminary results on predicting language commentary alongside the driving output. CarLLaVA uses a semi-disentangled output representation of both path predictions and waypoints, getting the advantages of the path for better lateral control and the waypoints for better longitudinal control. We propose an efficient training recipe to train on large driving datasets without wasting compute on easy, trivial data. CarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving Challenge 2.0 outperforming the previous state of the art by 458% and the best concurrent submission by 32.6%.",
                "authors": "Katrin Renz, Long Chen, Ana-Maria Marcu, Jan Hünermann, Benoît Hanotte, Alice Karnsund, Jamie Shotton, Elahe Arani, Oleg Sinavski",
                "citations": 6
            },
            {
                "title": "UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling",
                "abstract": "Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress. To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations. Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU.",
                "authors": "Haider Al-Tahan, Q. Garrido, Randall Balestriero, Diane Bouchacourt, C. Hazirbas, Mark Ibrahim",
                "citations": 6
            },
            {
                "title": "Optical-OFDM VLC System: Peak-to-Average Power Ratio Enhancement and Performance Evaluation",
                "abstract": "Visible Light Communication (VLC) systems are favoured for numerous applications due to their extensive bandwidth and resilience to electromagnetic interference. This study delineates various constructions of Optical Orthogonal Frequency Division Multiplexing (O-OFDM) approaches employed in VLC systems. Various factors are elaborated within this context to ascertain a more effective O-OFDM approach, including constellation size, data arrangement and spectral efficiency, power efficiency, computational complexity, bit error rate (BER), and peak-to-average power ratio (PAPR). This paper seeks to assess these approaches’ BER and PAPR performance across varying modulation orders. Regrettably, in VLC systems based on OFDM methodology, the superposition of multiple subcarriers results in a high PAPR. Therefore, this study aims to diminish the PAPR in VLC systems, enhancing system performance. We propose a non-distorting PAPR reduction technique, namely the Vandermonde-Like Matrix (VLM) precoding technique. The suggested technique is implemented across various O-OFDM approaches, including DCO-OFDM, ADO-OFDM, ACO-OFDM, FLIP-OFDM, ASCO-OFDM, and LACO-OFDM. Notably, this method does not affect the system’s data rate because it does not require the mandatory transmission of side information. Furthermore, this technique can decrease the PAPR without impacting the system’s BER performance. This study compares the proposed PAPR reduction technique against established methods documented in the literature to evaluate their efficacy and validity rigorously.",
                "authors": "Yasser A. Zenhom, Ehab K. I. Hamad, Mohammed A. Alghassab, Mohamed M. Elnabawy",
                "citations": 6
            },
            {
                "title": "Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation",
                "abstract": "Learned language-conditioned robot policies often struggle to effectively adapt to new real-world tasks even when pre-trained across a diverse set of instructions. We propose a novel approach for few-shot adaptation to unseen tasks that exploits the semantic understanding of task decomposition provided by vision-language models (VLMs). Our method, Policy Adaptation via Language Optimization (PALO), combines a handful of demonstrations of a task with proposed language decompositions sampled from a VLM to quickly enable rapid nonparametric adaptation, avoiding the need for a larger fine-tuning dataset. We evaluate PALO on extensive real-world experiments consisting of challenging unseen, long-horizon robot manipulation tasks. We find that PALO is able of consistently complete long-horizon, multi-tier tasks in the real world, outperforming state of the art pre-trained generalist policies, and methods that have access to the same demonstrations.",
                "authors": "Vivek Myers, Bill Chunyuan Zheng, Oier Mees, Sergey Levine, Kuan Fang",
                "citations": 6
            },
            {
                "title": "From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models",
                "abstract": "Scene graph generation (SGG) aims to parse a visual scene into an intermediate graph representation for downstream reasoning tasks. Despite recent advancements, existing methods struggle to generate scene graphs with novel visual relation concepts. To address this challenge, we introduce a new open-vocabulary SGG framework based on sequence generation. Our framework leverages vision-language pre-trained models (VLM) by incorporating an image-to-graph generation paradigm. Specifically, we generate scene graph sequences via image-to-text generation with VLM and then construct scene graphs from these sequences. By doing so, we harness the strong capabilities of VLM for open-vocabulary SGG and seamlessly integrate explicit relational modeling for enhancing the VL tasks. Experimental results demonstrate that our design not only achieves superior performance with an open vocabulary but also enhances downstream vision-language task performance through explicit relation modeling knowledge.",
                "authors": "Rongjie Li, Songyang Zhang, Dahua Lin, Kai Chen, Xuming He",
                "citations": 6
            },
            {
                "title": "Failures to Find Transferable Image Jailbreaks Between Vision-Language Models",
                "abstract": "The integration of new modalities into frontier AI systems offers exciting capabilities, but also increases the possibility such systems can be adversarially manipulated in undesirable ways. In this work, we focus on a popular class of vision-language models (VLMs) that generate text outputs conditioned on visual and textual inputs. We conducted a large-scale empirical study to assess the transferability of gradient-based universal image ``jailbreaks\"using a diverse set of over 40 open-parameter VLMs, including 18 new VLMs that we publicly release. Overall, we find that transferable gradient-based image jailbreaks are extremely difficult to obtain. When an image jailbreak is optimized against a single VLM or against an ensemble of VLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits little-to-no transfer to any other VLMs; transfer is not affected by whether the attacked and target VLMs possess matching vision backbones or language models, whether the language model underwent instruction-following and/or safety-alignment training, or many other factors. Only two settings display partially successful transfer: between identically-pretrained and identically-initialized VLMs with slightly different VLM training data, and between different training checkpoints of a single VLM. Leveraging these results, we then demonstrate that transfer can be significantly improved against a specific target VLM by attacking larger ensembles of ``highly-similar\"VLMs. These results stand in stark contrast to existing evidence of universal and transferable text jailbreaks against language models and transferable adversarial attacks against image classifiers, suggesting that VLMs may be more robust to gradient-based transfer attacks.",
                "authors": "Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristobal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez",
                "citations": 6
            },
            {
                "title": "Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models",
                "abstract": "Legged robots are physically capable of navigating a diverse variety of environments and overcoming a wide range of obstructions. For example, in a search and rescue mission, a legged robot could climb over debris, crawl through gaps, and navigate out of dead ends. However, the robot's controller needs to respond intelligently to such varied obstacles, and this requires handling unexpected and unusual scenarios successfully. This presents an open challenge to current learning methods, which often struggle with generalization to the long tail of unexpected situations without heavy human supervision. To address this issue, we investigate how to leverage the broad knowledge about the structure of the world and commonsense reasoning capabilities of vision-language models (VLMs) to aid legged robots in handling difficult, ambiguous situations. We propose a system, VLM-Predictive Control (VLM-PC), combining two key components that we find to be crucial for eliciting on-the-fly, adaptive behavior selection with VLMs: (1) in-context adaptation over previous robot interactions and (2) planning multiple skills into the future and replanning. We evaluate VLM-PC on several challenging real-world obstacle courses, involving dead ends and climbing and crawling, on a Go1 quadruped robot. Our experiments show that by reasoning over the history of interactions and future plans, VLMs enable the robot to autonomously perceive, navigate, and act in a wide range of complex scenarios that would otherwise require environment-specific engineering or human guidance.",
                "authors": "Annie S. Chen, Alec M. Lessing, Andy Tang, Govind Chada, Laura M. Smith, Sergey Levine, Chelsea Finn",
                "citations": 6
            },
            {
                "title": "SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model",
                "abstract": "In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved.",
                "authors": "Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao",
                "citations": 5
            },
            {
                "title": "Bi-LORA: A Vision-Language Approach for Synthetic Image Detection",
                "abstract": "Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT.",
                "authors": "Mamadou Keita, W. Hamidouche, Hessen Bougueffa Eutamene, Abdenour Hadid, Abdelmalik Taleb-Ahmed",
                "citations": 5
            },
            {
                "title": "IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models",
                "abstract": "The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning. This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization. GPT4V, the best performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization. We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially degrade the performance of Gemini-Pro in the localization task. Tangentially, we discover a potential weakness in the ICL capabilities of VLMs: they fail to locate optical illusions even when the correct answer is in the context window as a few-shot example.",
                "authors": "H. S. Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, Rifat Shahriyar",
                "citations": 5
            },
            {
                "title": "Self-Supervised Visual Preference Alignment",
                "abstract": "This paper makes the first attempt towards unsupervised preference alignment in Vision-Language Models (VLMs). We generate chosen and rejected responses with regard to the original and augmented image pairs, and conduct preference alignment with direct preference optimization. It is based on a core idea: properly designed augmentation to the image input will induce VLM to generate false but hard negative responses, which helps the model to learn from and produce more robust and powerful answers. The whole pipeline no longer hinges on supervision from GPT-4 or human involvement during alignment, and is highly efficient with few lines of code. With only 8k randomly sampled unsupervised data, it achieves 90\\% relative score to GPT-4 on complex reasoning in LLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex multi-modal benchmark MM-Vet. Visualizations shows its improved ability to align with user-intentions. A series of ablations are firmly conducted to reveal the latent mechanism of the approach, which also indicates its potential towards further scaling. Code are available in https://github.com/Kevinz-code/SeVa.",
                "authors": "Ke Zhu, Liang Zhao, Zheng Ge, Xiangyu Zhang",
                "citations": 5
            },
            {
                "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning",
                "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem of reward misalignment when applying VLM as a reward in RL tasks. To address this issue, we introduce a lightweight fine-tuning method, named Fuzzy VLM reward-aided RL (FuRL), based on reward alignment and relay RL. Specifically, we enhance the performance of SAC/DrQ baseline agents on sparse reward tasks by fine-tuning VLM representations and using relay RL to avoid local minima. Extensive experiments on the Meta-world benchmark tasks demonstrate the efficacy of the proposed method. Code is available at: https://github.com/fuyw/FuRL.",
                "authors": "Yuwei Fu, Haichao Zhang, Di Wu, Wei Xu, Benoit Boulet",
                "citations": 5
            },
            {
                "title": "CLIP-Loc: Multi-modal Landmark Association for Global Localization in Object-based Maps",
                "abstract": "This paper describes a multi-modal data association method for global localization using object-based maps and camera images. In global localization, or relocalization, using object-based maps, existing methods typically resort to matching all possible combinations of detected objects and landmarks with the same object category, followed by inlier extraction using RANSAC or brute-force search. This approach becomes infeasible as the number of landmarks increases due to the exponential growth of correspondence candidates. In this paper, we propose labeling landmarks with natural language descriptions and extracting correspondences based on conceptual similarity with image observations using a Vision Language Model (VLM). By leveraging detailed text information, our approach efficiently extracts correspondences compared to methods using only object categories. Through experiments, we demonstrate that the proposed method enables more accurate global localization with fewer iterations compared to baseline methods, exhibiting its efficiency.",
                "authors": "Shigemichi Matsuzaki, Takuma Sugino, Kazuhito Tanaka, Zijun Sha, Shintaro Nakaoka, Shintaro Yoshizawa, Kazuhiro Shintani",
                "citations": 5
            },
            {
                "title": "Empowering Vision-Language Models for Reasoning Ability through Large Language Models",
                "abstract": "Vision-language models (VLM) have shown excellent performance in vision-language tasks. However, they sometimes lack sufficient reasoning ability. In contrast, large language models (LLMs) have emerged with powerful reasoning capabilities. Therefore, we propose a framework called TReE, which transfers the reasoning ability of the LLM to the VLM in learning-free settings. TReE is a three-stage framework: observation, thinking, and re-thinking. The observation stage requires the VLM to obtain overall visual information about the image. Then, the thinking stage combines the visual information and task description as the prompt for the LLM, allowing it to present the thinking process (namely, rationale). Lastly, the re-thinking stage learns useful information from the rationale and then predicts the final result using the VLM. We are the first to explore enhancing the VLM’s reasoning ability without any training, finetuning, or access to the LLM’s parameters, which we refer to as a plug-in mode, leading to the model-agnostic feature. Experiments show that TReE performed well on general visual questionanswering (VQA) tasks and outperformed KOSMOS-1 on the challenging Raven IQ test dataset by 6%. Furthermore, with additional lightweight finetuning using a smaller amount of parameters, TReE achieved a high accuracy of 81.7% on GQA and 67.3% on VQAv2.",
                "authors": "Yueting Yang, Xintong Zhang, Jinan Xu, Wenjuan Han",
                "citations": 5
            },
            {
                "title": "Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models",
                "abstract": "This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of more practical and reliable VLMs.",
                "authors": "Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei Liu, Kiyoharu Aizawa",
                "citations": 5
            },
            {
                "title": "Prompt learning in computer vision: a survey",
                "abstract": "Prompt learning has attracted broad attention in computer vision since the large pre-trained vision-language models (VLMs) exploded. Based on the close relationship between vision and language information built by VLM, prompt learning becomes a crucial technique in many important applications such as artificial intelligence generated content (AIGC). In this survey, we provide a progressive and comprehensive review of visual prompt learning as related to AIGC. We begin by introducing VLM, the foundation of visual prompt learning. Then, we review the vision prompt learning methods and prompt-guided generative models, and discuss how to improve the efficiency of adapting AIGC models to specific downstream tasks. Finally, we provide some promising research directions concerning prompt learning. 自大型预训练视觉—语言模型(VLM)爆发以来,提示学习已在计算机视觉领域引发广泛关注。基于VLM构建的视觉和语言信息之间的密切关系,提示学习成为许多重要应用领域(如人工智能内容生成(AIGC))中的关键技术。本综述循序渐进且全面地总结了与AIGC相关的视觉提示学习。首先介绍了VLM,它是视觉提示学习的基础。然后,回顾了视觉提示学习方法和提示引导生成模型,并讨论了如何提高将AIGC模型适用于下游特定任务的效率。最后,提供了一些有前景的关于提示学习的研究方向。",
                "authors": "Yiming Lei, Jingqi Li, Zilong Li, Yuan Cao, Hongming Shan",
                "citations": 5
            },
            {
                "title": "Agent3D-Zero: An Agent for Zero-shot 3D Understanding",
                "abstract": "The ability to understand and reason the 3D real world is a crucial milestone towards artificial general intelligence. The current common practice is to finetune Large Language Models (LLMs) with 3D data and texts to enable 3D understanding. Despite their effectiveness, these approaches are inherently limited by the scale and diversity of the available 3D data. Alternatively, in this work, we introduce Agent3D-Zero, an innovative 3D-aware agent framework addressing the 3D scene understanding in a zero-shot manner. The essence of our approach centers on reconceptualizing the challenge of 3D scene perception as a process of understanding and synthesizing insights from multiple images, inspired by how our human beings attempt to understand 3D scenes. By consolidating this idea, we propose a novel way to make use of a Large Visual Language Model (VLM) via actively selecting and analyzing a series of viewpoints for 3D understanding. Specifically, given an input 3D scene, Agent3D-Zero first processes a bird's-eye view image with custom-designed visual prompts, then iteratively chooses the next viewpoints to observe and summarize the underlying knowledge. A distinctive advantage of Agent3D-Zero is the introduction of novel visual prompts, which significantly unleash the VLMs' ability to identify the most informative viewpoints and thus facilitate observing 3D scenes. Extensive experiments demonstrate the effectiveness of the proposed framework in understanding diverse and previously unseen 3D environments.",
                "authors": "Sha Zhang, Di Huang, Jiajun Deng, Shixiang Tang, Wanli Ouyang, Tong He, Yanyong Zhang",
                "citations": 5
            },
            {
                "title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data",
                "abstract": "Recently, Vision-Language Models (VLMs) have achieved remarkable progress in multimodal tasks, and multimodal instruction data serves as the foundation for enhancing VLM capabilities. Despite the availability of several open-source multimodal datasets, limitations in the scale and quality of open-source instruction data hinder the performance of VLMs trained on these datasets, leading to a significant gap compared to models trained on closed-source data. To address this challenge, we introduce Infinity-MM, a large-scale multimodal instruction dataset. We collected the available multimodal instruction datasets and performed unified preprocessing, resulting in a dataset with over 40 million samples that ensures diversity and accuracy. Furthermore, to enable large-scale expansion of instruction data and support the continuous acquisition of high-quality data, we propose a synthetic instruction generation method based on a tagging system and open-source VLMs. By establishing correspondences between different types of images and associated instruction types, this method can provide essential guidance during data synthesis. Leveraging this high-quality data, we have trained a 2-billion-parameter Vision-Language Model, Aquila-VL-2B, which achieves state-of-the-art (SOTA) performance among models of similar scale. The data is available at: https://huggingface.co/datasets/BAAI/Infinity-MM.",
                "authors": "Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, Zhenchong Hu, Bo-Wen Zhang, Jijie Li, Dong Liang, Yingli Zhao, Yulong Ao, Yaoqi Liu, Fangxiang Feng, Guang Liu",
                "citations": 5
            },
            {
                "title": "ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation",
                "abstract": "Navigating and understanding complex environments over extended periods of time is a significant challenge for robots. People interacting with the robot may want to ask questions like where something happened, when it occurred, or how long ago it took place, which would require the robot to reason over a long history of their deployment. To address this problem, we introduce a Retrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed for long-horizon video question answering for robot navigation. To evaluate ReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal, and descriptive questions to long-horizon robot navigation videos. ReMEmbR employs a structured approach involving a memory building and a querying phase, leveraging temporal information, spatial information, and images to efficiently handle continuously growing robot histories. Our experiments demonstrate that ReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve effective long-horizon reasoning with low latency. Additionally, we deploy ReMEmbR on a robot and show that our approach can handle diverse queries. The dataset, code, videos, and other material can be found at the following link: https://nvidia-ai-iot.github.io/remembr",
                "authors": "Abrar Anwar, John Welsh, Joydeep Biswas, Soha Pouya, Yan Chang",
                "citations": 5
            },
            {
                "title": "OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding",
                "abstract": "In order for robots to interact with objects effectively, they must understand the form and function of each object they encounter. Essentially, robots need to understand which actions each object affords, and where those affordances can be acted on. Robots are ultimately expected to operate in unstructured human environments, where the set of objects and affordances is not known to the robot before deployment (i.e. the open-vocabulary setting). In this work, we introduce OVAL-Prompt, a prompt-based approach for open-vocabulary affordance localization in RGB-D images. By leveraging a Vision Language Model (VLM) for open-vocabulary object part segmentation and a Large Language Model (LLM) to ground each part-segment-affordance, OVAL-Prompt demonstrates generalizability to novel object instances, categories, and affordances without domain-specific finetuning. Quantitative experiments demonstrate that without any finetuning, OVAL-Prompt achieves localization accuracy that is competitive with supervised baseline models. Moreover, qualitative experiments show that OVAL-Prompt enables affordance-based robot manipulation of open-vocabulary object instances and categories. Project Page: https://ekjt.github.io/OVAL-Prompt/",
                "authors": "Edmond Tong, A. Opipari, Stanley Lewis, Zhen Zeng, O. C. Jenkins",
                "citations": 5
            },
            {
                "title": "Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese",
                "abstract": "Vision Language Models (VLMs) have undergone a rapid evolution, giving rise to significant advancements in the realm of multimodal understanding tasks. However, the majority of these models are trained and evaluated on English-centric datasets, leaving a gap in the development and evaluation of VLMs for other languages, such as Japanese. This gap can be attributed to the lack of methodologies for constructing VLMs and the absence of benchmarks to accurately measure their performance. To address this issue, we introduce a novel benchmark, Japanese Heron-Bench, for evaluating Japanese capabilities of VLMs. The Japanese Heron-Bench consists of a variety of imagequestion answer pairs tailored to the Japanese context. Additionally, we present a baseline Japanese VLM that has been trained with Japanese visual instruction tuning datasets. Our Heron-Bench reveals the strengths and limitations of the proposed VLM across various ability dimensions. Furthermore, we clarify the capability gap between strong closed models like GPT-4V and the baseline model, providing valuable insights for future research in this domain. We release the benchmark dataset and training code to facilitate further developments in Japanese VLM research.",
                "authors": "Yuichi Inoue, Kento Sasaki, Yuma Ochi, Kazuki Fujii, Kotaro Tanahashi, Yu Yamaguchi",
                "citations": 4
            },
            {
                "title": "VILA$^2$: VILA Augmented VILA",
                "abstract": "While visual language model architectures and training infrastructures advance rapidly, data curation remains under-explored where quantity and quality become a bottleneck. Existing work either crawls extra Internet data with a loose guarantee of quality or distills from black-box proprietary models, e.g., GPT-4V / Gemini that are API frequency and performance bounded. This work enables a VLM to improve itself via data enhancement, exploiting its generative nature. We introduce a simple yet effective VLM augmentation scheme that includes a self-augment step and a specialist-augment step to iteratively improve data quality and hence, model performance. In the self-augment step, the instruction-finetuned VLM recaptions its pretraining caption datasets and then retrains from scratch leveraging refined data. Without any expensive human-in-the-loop annotation, we observe improvements in data quality and downstream accuracy boosts with three self-augmentation rounds -- a viable free lunch to the current VLM training recipe. When self-augmentation saturates, we augment the caption diversity by leveraging specialty skills picked up from instruction finetuning. We finetune VLM specialists from the self-augmented VLM with domain-specific experts, including spatial, grounding, and OCR, to fuse task-aware synthetic data into the pretraining stage. Data quality improvements and hallucination reductions are cross-checked by VLM (GPT-4V, Gemini) and human judges. Combining self-augmentation and specialist-augmented training, VILA$^2$ consistently improves the accuracy on a wide range of benchmarks over the prior art, producing a reusable pretraining dataset that is 300x more cost-efficient than human labeling.",
                "authors": "Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jan Kautz, Jang Hyun Cho, Marco Pavone, Song Han, Hongxu Yin",
                "citations": 4
            },
            {
                "title": "AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection",
                "abstract": "Zero-shot anomaly detection (ZSAD) targets the identification of anomalies within images from arbitrary novel categories. This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP. AdaCLIP incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data. Two types of learnable prompts are proposed: static and dynamic. Static prompts are shared across all images, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic prompts are generated for each test image, providing CLIP with dynamic adaptation capabilities. The combination of static and dynamic prompts is referred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive experiments conducted across 14 real-world anomaly detection datasets from industrial and medical domains indicate that AdaCLIP outperforms other ZSAD methods and can generalize better to different categories and even domains. Finally, our analysis highlights the importance of diverse auxiliary data and optimized prompts for enhanced generalization capacity. Code is available at https://github.com/caoyunkang/AdaCLIP.",
                "authors": "Yunkang Cao, Jiangning Zhang, Luca Frittoli, Yuqi Cheng, Weiming Shen, Giacomo Boracchi",
                "citations": 4
            },
            {
                "title": "Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks",
                "abstract": "Augmenting Large Language Models (LLMs) with image-understanding capabilities has resulted in a boom of high-performing Vision-Language models (VLMs). While studying the alignment of LLMs to human values has received widespread attention, the safety of VLMs has not received the same attention. In this paper, we explore the impact of jailbreaking on three state-of-the-art VLMs, each using a distinct modeling approach. By comparing each VLM to their respective LLM backbone, we find that each VLM is more susceptible to jailbreaking. We consider this as an undesirable outcome from visual instruction-tuning, which imposes a forgetting effect on an LLM’s safety guardrails. Therefore, we provide recommendations for future work based on evaluation strategies that aim to highlight the weaknesses of a VLM, as well as take safety measures into account during visual instruction tuning.",
                "authors": "Georgios Pantazopoulos, Amit Parekh, Malvina Nikandrou, Alessandro Suglia",
                "citations": 4
            },
            {
                "title": "The Significance of Interseismic Vertical Land Movement at Convergent Plate Boundaries in Probabilistic Sea‐Level Projections for AR6 Scenarios: The New Zealand Case",
                "abstract": "Anticipating and managing the impacts of sea‐level rise for nations astride active tectonic margins requires understanding of rates of sea surface elevation change in relation to coastal land elevation. Vertical land motion (VLM) can either exacerbate or reduce sea‐level changes with impacts varying significantly along a coastline. Determining rate, pattern, and variability of VLM near coasts leads to a direct improvement of location‐specific relative sea level (RSL) estimates for coastal hazard risk assessment. Here, we utilize vertical velocity field from interferometric synthetic aperture radar (InSAR) data, calibrated with campaign and continuous Global Navigation Satellite System data, to determine the VLM for the entire coastline of New Zealand. Guided by available knowledge of the seismic cycle, the VLM data infer secular, interseismic rates of land surface deformation. Using the Framework for Assessing Changes to Sea‐level (FACTS), we build probabilistic RSL projections using the same emissions scenarios employed in IPCC Assessment Report 6 and local VLM data at 8,179 sites, thereby enhancing spatial coverage that was previously limited to four tide gauges. We present ensembles of probability distributions of RSL for each scenario to 2150, and for low confidence sea‐level processes to 2300. Where land subsidence is occurring at rates >2 mm/y VLM makes a significant contribution to RSL projections for all scenarios out to 2150. Our approach can be applied to similar locations across the world and has significant implications for adaptation planning, as timing of threshold exceedance for coastal inundation can be brought forward (or delayed) by decades.",
                "authors": "T. Naish, R. Levy, I. Hamling, S. Hreinsdóttir, P. Kumar, G. G. Garner, R. Kopp, N. Golledge, R. Bell, R. Paulik, J. Lawrence, P. Denys, T. Gillies, S. Bengtson, A. Howell, K. Clark, D. King, N. Litchfield, R. Newnham",
                "citations": 4
            },
            {
                "title": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification",
                "abstract": "Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology. However, these methods heavily depend on a substantial number of bag-level labels and solely learn from the original slides, which are easily affected by variations in data distribution. Recently, vision language model (VLM)-based methods introduced the language prior by pre-training on large-scale pathological image-text pairs. However, the previous text prompt lacks the consideration of pathological prior knowledge, there-fore does not substantially boost the model's performance. Moreover, the collection of such pairs and the pre-training process are very time-consuming and source-intensive. To solve the above problems, we propose a dual-scale vision-language multiple instance learning (ViLa-MIL) framework for whole slide image classification. Specifically, we propose a dual-scale visual descriptive text prompt based on the frozen large language model (LLM) to boost the performance of VLM effectively. To transfer the VLM to process WSI efficiently, for the image branch, we propose a prototype-guided patch decoder to aggregate the patch features progressively by grouping similar patches into the same prototype; for the text branch, we introduce a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts. Extensive studies on three multi-cancer and multi-center subtyping datasets demonstrate the superiority of ViLa-MIL.",
                "authors": "Jiangbo Shi, Chen Li, Tieliang Gong, Yefeng Zheng, Huazhu Fu",
                "citations": 4
            },
            {
                "title": "π0: A Vision-Language-Action Flow Model for General Robot Control",
                "abstract": "Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.",
                "authors": "Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky",
                "citations": 4
            },
            {
                "title": "Enhancing Interactive Image Retrieval With Query Rewriting Using Large Language Models and Vision Language Models",
                "abstract": "Image search stands as a pivotal task in multimedia and computer vision, finding applications across diverse domains, ranging from internet search to medical diagnostics. Conventional image search systems operate by accepting textual or visual queries, retrieving the top-relevant candidate results from the database. However, prevalent methods often rely on single-turn procedures, introducing potential inaccuracies and limited recall. These methods also face the challenges, such as vocabulary mismatch and the semantic gap, constraining their overall effectiveness. To address these issues, we propose an interactive image retrieval system capable of refining queries based on user relevance feedback in a multi-turn setting. This system incorporates a vision language model (VLM) based image captioner to enhance the quality of text-based queries, resulting in more informative queries with each iteration. Moreover, we introduce a large language model (LLM) based denoiser to refine text-based query expansions, mitigating inaccuracies in image descriptions generated by captioning models. To evaluate our system, we curate a new dataset by adapting the MSR-VTT video retrieval dataset to the image retrieval task, offering multiple relevant ground truth images for each query. Through comprehensive experiments, we validate the effectiveness of our proposed system against baseline methods, achieving state-of-the-art performance with a notable 10% improvement in terms of recall. Our contributions encompass the development of an innovative interactive image retrieval system, the integration of an LLM-based denoiser, the curation of a meticulously designed evaluation dataset, and thorough experimental validation.",
                "authors": "Hongyi Zhu, Jia-Hong Huang, S. Rudinac, E. Kanoulas",
                "citations": 4
            },
            {
                "title": "Synth2: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings",
                "abstract": "The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). In this work, we investigate an approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs a pretrained text-to-image model to synthesize image embeddings from captions generated by an LLM. Despite the text-to-image model and VLM initially being trained on the same data, our approach leverages the image generator's ability to create novel compositions, resulting in synthetic image embeddings that expand beyond the limitations of the original dataset. Extensive experiments demonstrate that our VLM, finetuned on synthetic data achieves comparable performance to models trained solely on human-annotated data, while requiring significantly less data. Furthermore, we perform a set of analyses on captions which reveals that semantic diversity and balance are key aspects for better downstream performance. Finally, we show that synthesizing images in the image embedding space is 25\\% faster than in the pixel space. We believe our work not only addresses a significant challenge in VLM training but also opens up promising avenues for the development of self-improving multi-modal models.",
                "authors": "Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, D. Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, Andrea Banino",
                "citations": 4
            },
            {
                "title": "Harmonizing Generalization and Personalization in Federated Prompt Learning",
                "abstract": "Federated Prompt Learning (FPL) incorporates large pre-trained Vision-Language models (VLM) into federated learning through prompt tuning. The transferable representations and remarkable generalization capacity of VLM make them highly compatible with the integration of federated learning. Addressing data heterogeneity in federated learning requires personalization, but excessive focus on it across clients could compromise the model's ability to generalize effectively. To preserve the impressive generalization capability of VLM, it is crucial to strike a balance between personalization and generalization in FPL. To tackle this challenge, we proposed Federated Prompt Learning with CLIP Generalization and low-rank Personalization (FedPGP), which employs pre-trained CLIP to provide knowledge-guidance on the global prompt for improved generalization and incorporates a low-rank adaptation term to personalize the global prompt. Further, FedPGP integrates a prompt-wise contrastive loss to achieve knowledge guidance and personalized adaptation simultaneously, enabling a harmonious balance between personalization and generalization in FPL. We conduct extensive experiments on various datasets to explore base-to-novel generalization in both category-level and domain-level scenarios with heterogeneous data, showing the superiority of FedPGP in balancing generalization and personalization.",
                "authors": "Tianyu Cui, Hongxia Li, Jingya Wang, Ye Shi",
                "citations": 4
            },
            {
                "title": "Code as Reward: Empowering Reinforcement Learning with VLMs",
                "abstract": "Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named Code as Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through code generation, thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, and can be more effective in training RL policies than the original sparse environment rewards.",
                "authors": "David Venuto, Sami Nur Islam, Martin Klissarov, D. Precup, Sherry Yang, Ankit Anand",
                "citations": 4
            },
            {
                "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
                "abstract": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the safety of machine learning systems and has shaped the field of OOD detection. Meanwhile, several other problems are closely related to OOD detection, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD). To unify these problems, a generalized OOD detection framework was proposed, taxonomically categorizing these five problems. However, Vision Language Models (VLMs) such as CLIP have significantly changed the paradigm and blurred the boundaries between these fields, again confusing researchers. In this survey, we first present a generalized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD detection, and OD in the VLM era. Our framework reveals that, with some field inactivity and integration, the demanding challenges have become OOD detection and AD. In addition, we also highlight the significant shift in the definition, problem settings, and benchmarks; we thus feature a comprehensive review of the methodology for OOD detection, including the discussion over other related tasks to clarify their relationship to OOD detection. Finally, we explore the advancements in the emerging Large Vision Language Model (LVLM) era, such as GPT-4V. We conclude this survey with open challenges and future directions.",
                "authors": "Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin, Qing Yu, Go Irie, Shafiq R. Joty, Yixuan Li, Hai Li, Ziwei Liu, T. Yamasaki, Kiyoharu Aizawa",
                "citations": 4
            },
            {
                "title": "MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion",
                "abstract": "Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Mu ltimodal-L LM a ge n t (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on https:github.com/ measure-infinity/mulan-code .",
                "authors": "Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou",
                "citations": 4
            },
            {
                "title": "Highlighting the Safety Concerns of Deploying LLMs/VLMs in Robotics",
                "abstract": "In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works focus on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation and navigation. Despite these improvements, analyzing the safety of such systems remains underexplored yet extremely critical. LLMs and VLMs are highly susceptible to adversarial inputs, prompting a significant inquiry into the safety of robotic systems. This concern is important because robotics operate in the physical world where erroneous actions can result in severe consequences. This paper explores this issue thoroughly, presenting a mathematical formulation of potential attacks on LLM/VLM-based robotic systems and offering experimental evidence of the safety challenges. Our empirical findings highlight a significant vulnerability: simple modifications to the input can drastically reduce system effectiveness. Specifically, our results demonstrate an average performance deterioration of 19.4% under minor input prompt modifications and a more alarming 29.1% under slight perceptual changes. These findings underscore the urgent need for robust countermeasures to ensure the safe and reliable deployment of advanced LLM/VLM-based robotic systems.",
                "authors": "Xiyang Wu, Ruiqi Xian, Tianrui Guan, Jing Liang, Souradip Chakraborty, Fuxiao Liu, Brian M. Sadler, Dinesh Manocha, A. S. Bedi",
                "citations": 4
            },
            {
                "title": "LLMC: Benchmarking Large Language Model Quantization with a Versatile Compression Toolkit",
                "abstract": "Recent advancements in large language models (LLMs) are propelling us toward artificial general intelligence with their remarkable emergent abilities and reasoning capabilities. However, the substantial computational and memory requirements limit the widespread adoption. Quantization, a key compression technique, can effectively mitigate these demands by compressing and accelerating LLMs, albeit with potential risks to accuracy. Numerous studies have aimed to minimize the accuracy loss associated with quantization. However, their quantization configurations vary from each other and cannot be fairly compared. In this paper, we present LLMC, a plug-and-play compression toolkit, to fairly and systematically explore the impact of quantization. LLMC integrates dozens of algorithms, models, and hardware, offering high extensibility from integer to floating-point quantization, from LLM to vision-language (VLM) model, from fixed-bit to mixed precision, and from quantization to sparsification. Powered by this versatile toolkit, our benchmark covers three key aspects: calibration data, algorithms (three strategies), and data formats, providing novel insights and detailed analyses for further research and practical guidance for users. Our toolkit is available at https://github.com/ModelTC/llmc.",
                "authors": "Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Yunchen Zhang, Xianglong Liu, Dacheng Tao",
                "citations": 4
            },
            {
                "title": "GPT4Ego: Unleashing the Potential of Pre-Trained Models for Zero-Shot Egocentric Action Recognition",
                "abstract": "Vision-Language Models (VLMs), pre-trained on large-scale datasets, have shown impressive performance in various visual recognition tasks. This advancement paves the way for notable performance in some egocentric tasks, Zero-Shot Egocentric Action Recognition (ZS-EAR), entailing VLMs zero-shot to recognize actions from first-person videos enriched in more realistic human-environment interactions. Typically, VLMs handle ZS-EAR as a global video-text matching task, which often leads to suboptimal alignment of vision and linguistic knowledge. We propose a refined approach for ZS-EAR using VLMs, emphasizing fine-grained concept-description alignment that capitalizes on the rich semantic and contextual details in egocentric videos. In this work, we introduce a straightforward yet remarkably potent VLM framework, <italic>aka</italic> GPT4Ego, designed to enhance the fine-grained alignment of concept and description between vision and language. Specifically, we first propose a new Ego-oriented Text Prompting (EgoTP<inline-formula><tex-math notation=\"LaTeX\">$\\spadesuit$</tex-math></inline-formula>) scheme, which effectively prompts action-related text-contextual semantics by evolving word-level class names to sentence-level contextual descriptions by ChatGPT with well-designed chain-of-thought textual prompts. Moreover, we design a new Ego-oriented Visual Parsing (EgoVP<inline-formula><tex-math notation=\"LaTeX\">$\\clubsuit$</tex-math></inline-formula>) strategy that learns action-related vision-contextual semantics by refining global-level images to part-level contextual concepts with the help of SAM. Extensive experiments demonstrate GPT4Ego significantly outperforms existing VLMs on three large-scale egocentric video benchmarks, i.e., EPIC-KITCHENS-100 (33.2%<inline-formula><tex-math notation=\"LaTeX\">$\\uparrow$</tex-math></inline-formula><inline-formula><tex-math notation=\"LaTeX\">$_{\\bm {+9.4}}$</tex-math></inline-formula>), EGTEA (39.6%<inline-formula><tex-math notation=\"LaTeX\">$\\uparrow$</tex-math></inline-formula><inline-formula><tex-math notation=\"LaTeX\">$_{\\bm {+5.5}}$</tex-math></inline-formula>), and CharadesEgo (31.5%<inline-formula><tex-math notation=\"LaTeX\">$\\uparrow$</tex-math></inline-formula><inline-formula><tex-math notation=\"LaTeX\">$_{\\bm {+2.6}}$</tex-math></inline-formula>). In addition, benefiting from the novel mechanism of fine-grained concept and description alignment, GPT4Ego can sustainably evolve with the advancement of ever-growing pre-trained foundational models. We hope this work can encourage the egocentric community to build more investigation into pre-trained vision-language models.",
                "authors": "Guangzhao Dai, Xiangbo Shu, Wenhao Wu, Rui Yan, Jiachao Zhang",
                "citations": 4
            },
            {
                "title": "Improve Vision Language Model Chain-of-thought Reasoning",
                "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.",
                "authors": "Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, Yiming Yang",
                "citations": 4
            },
            {
                "title": "MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models",
                "abstract": "We introduce MultiMedEval, an open-source toolkit for fair and reproducible evaluation of large, medical vision-language models (VLM). MultiMedEval comprehensively assesses the models' performance on a broad array of six multi-modal tasks, conducted over 23 datasets, and spanning over 11 medical domains. The chosen tasks and performance metrics are based on their widespread adoption in the community and their diversity, ensuring a thorough evaluation of the model's overall generalizability. We open-source a Python toolkit (github.com/corentin-ryr/MultiMedEval) with a simple interface and setup process, enabling the evaluation of any VLM in just a few lines of code. Our goal is to simplify the intricate landscape of VLM evaluation, thus promoting fair and uniform benchmarking of future models.",
                "authors": "Corentin Royer, Bjoern H Menze, A. Sekuboyina",
                "citations": 4
            },
            {
                "title": "Bridge the Modality and Capability Gaps in Vision-Language Model Selection",
                "abstract": "Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. To better reuse the VLM resource and fully leverage its potential on different zero-shot image classification tasks, a promising strategy is selecting appropriate Pre-Trained VLMs from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the\"Modality Gap\"- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the\"Capability Gap\"- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the negative impact of two gaps. SWAB first adopts optimal transport to capture the relevance between open-source and target datasets with a transportation matrix. It then uses this matrix to transfer useful statistics of VLMs from open-source datasets to the target dataset for bridging two gaps. By bridging two gaps to obtain better substitutes for test images, SWAB can accurately predict the performance ranking of different VLMs on the target task without the need for the dataset's images. Experiments across various VLMs and image classification datasets validate SWAB's effectiveness.",
                "authors": "Chao Yi, Yu-Hang He, De-chuan Zhan, Han-Jia Ye",
                "citations": 4
            },
            {
                "title": "SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-Supervised Change Detector",
                "abstract": "Change detection (CD) aims to identify pixels with semantic changes between images. However, annotating massive numbers of pixel-level images is labor-intensive and costly, especially for multitemporal images, which require pixel-wise comparisons by human experts. Considering the excellent performance of visual-language models (VLMs) for zero-shot, OV, etc., with prompt-based reasoning, it is promising to utilize VLMs to make better CD under limited labeled data. In this article, we propose a VLM guidance-based semi-supervised CD method, namely SemiCD-VL. The insight of SemiCD-VL is to synthesize free change labels using VLMs to provide additional supervision signals for unlabeled data. However, almost all current VLMs are designed for single-temporal images and cannot be directly applied to bi- or multitemporal images. Motivated by this, we first propose a VLM-based mixed change event generation (CEG) strategy to yield pseudo-labels for unlabeled CD data. Since the additional supervised signals provided by these VLM-driven pseudo-labels may conflict with the original pseudo-labels from the consistency regularization paradigm (e.g., FixMatch), we propose the dual projection head for de-entangling different signal sources. Further, we explicitly decouple the bitemporal images semantic representation through two auxiliary segmentation decoders, which are also guided by VLM. Finally, to make the model more adequately capture change representations, we introduce contrastive consistency regularization (CCR) by constructing feature-level contrastive loss in auxiliary branches. Extensive experiments show the advantage of SemiCD-VL. For instance, SemiCD-VL improves the FixMatch baseline by $+ 5.3~\\text {IoU}^{c}$ on WHU-CD and by $+ 2.4~\\text {IoU}^{c}$ on LEVIR-CD with 5% labels, and SemiCD-VL requires only 5%–10% of the labels to achieve performance similar to the supervised methods. In addition, our CEG strategy, in an unsupervised manner, can achieve performance far superior to state-of-the-art (SOTA) unsupervised CD methods (e.g., IoU improved from 18.8% to 46.3% on LEVIR-CD dataset). The code is available at https://github.com/likyoo/SemiCD-VL.",
                "authors": "Kaiyu Li, Xiangyong Cao, Yupeng Deng, Jiayi Song, Junmin Liu, Deyu Meng, Zhi Wang",
                "citations": 4
            },
            {
                "title": "VR-GPT: Visual Language Model for Intelligent Virtual Reality Applications",
                "abstract": "The advent of immersive Virtual Reality applications has transformed various domains, yet their integration with advanced artificial intelligence technologies like Visual Language Models remains underexplored. This study introduces a pioneering approach utilizing VLMs within VR environments to enhance user interaction and task efficiency. Leveraging the Unity engine and a custom-developed VLM, our system facilitates real-time, intuitive user interactions through natural language processing, without relying on visual text instructions. The incorporation of speech-to-text and text-to-speech technologies allows for seamless communication between the user and the VLM, enabling the system to guide users through complex tasks effectively. Preliminary experimental results indicate that utilizing VLMs not only reduces task completion times but also improves user comfort and task engagement compared to traditional VR interaction methods.",
                "authors": "Mikhail Konenkov, Artem Lykov, Daria Trinitatova, D. Tsetserukou",
                "citations": 4
            },
            {
                "title": "Advancements in Vision–Language Models for Remote Sensing: Datasets, Capabilities, and Enhancement Techniques",
                "abstract": "Recently, the remarkable success of ChatGPT has sparked a renewed wave of interest in artificial intelligence (AI), and the advancements in Vision–Language Models (VLMs) have pushed this enthusiasm to new heights. Differing from previous AI approaches that generally formulated different tasks as discriminative models, VLMs frame tasks as generative models and align language with visual information, enabling the handling of more challenging problems. The remote sensing (RS) field, a highly practical domain, has also embraced this new trend and introduced several VLM-based RS methods that have demonstrated promising performance and enormous potential. In this paper, we first review the fundamental theories related to VLM, then summarize the datasets constructed for VLMs in remote sensing and the various tasks they address. Finally, we categorize the improvement methods into three main parts according to the core components of VLMs and provide a detailed introduction and comparison of these methods.",
                "authors": "Lijie Tao, Haokui Zhang, Haizhao Jing, Yu Liu, Dawei Yan, Guoting Wei, Xizhe Xue",
                "citations": 1
            },
            {
                "title": "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding",
                "abstract": "Recently, mobile AI agents based on VLMs have been gaining increasing attention. These works typically utilize VLM as a foundation, fine-tuning it with instruction-based mobile datasets. However, these VLMs are typically pre-trained on general-domain data, which often results in a lack of fundamental capabilities specific to the mobile domain. Therefore, they may struggle to recognize specific UI elements and understand intra-UI fine-grained information. In addition, the current fine-tuning task focuses on interacting with the most relevant element for the given instruction. These fine-tuned VLMs may still ignore the relationships between UI pages, neglect the roles of elements in page transitions and lack inter-UI understanding. To address issues, we propose a VLM called MobileVLM, which includes two additional pre-training stages to enhance both intra- and inter-UI understanding. We defined four UI-based pre-training tasks, enabling the model to better perceive fine-grained elements and capture page transition actions. To address the lack of mobile pre-training data, we built a large Chinese mobile dataset Mobile3M from scratch, which contains 3 million UI pages, and real-world transition actions, forming a directed graph structure. Experimental results show MobileVLM excels on both our test set and public mobile benchmarks, outperforming existing VLMs.",
                "authors": "Qinzhuo Wu, Weikai Xu, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, Shuo Shang",
                "citations": 3
            },
            {
                "title": "Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning",
                "abstract": "Few-shot recognition (FSR) aims to train a classification model with only a few labeled examples of each concept concerned by a downstream task, where data annotation cost can be prohibitively high. We develop methods to solve FSR by leveraging a pretrained Vision-Language Model (VLM). We particularly explore retrieval-augmented learning (RAL), which retrieves data from the VLM's pretraining set to learn better models for serving downstream tasks. RAL has been widely studied in zero-shot recognition but remains under-explored in FSR. Although applying RAL to FSR may seem straightforward, we observe interesting and novel challenges and opportunities. First, somewhat surprisingly, finetuning a VLM on a large amount of retrieved data underperforms state-of-the-art zero-shot methods. This is due to the imbalanced distribution of retrieved data and its domain gaps with the few-shot examples in the downstream task. Second, more surprisingly, we find that simply finetuning a VLM solely on few-shot examples significantly outperforms previous FSR methods, and finetuning on the mix of retrieved and few-shot data yields even better results. Third, to mitigate the imbalanced distribution and domain gap issues, we propose Stage-Wise retrieval-Augmented fineTuning (SWAT), which involves end-to-end finetuning on mixed data in the first stage and retraining the classifier on the few-shot data in the second stage. Extensive experiments on nine popular benchmarks demonstrate that SWAT significantly outperforms previous methods by $>$6% accuracy.",
                "authors": "Tian Liu, Huixin Zhang, Shubham Parashar, Shu Kong",
                "citations": 3
            },
            {
                "title": "IQAGPT: computed tomography image quality assessment with vision-language and ChatGPT models",
                "abstract": null,
                "authors": "Zhihao Chen, Bin Hu, Chuang Niu, Tao Chen, Yuxin Li, Hongming Shan, Ge Wang",
                "citations": 3
            },
            {
                "title": "GameVLM: A Decision-making Framework for Robotic Task Planning Based on Visual Language Models and Zero-sum Games",
                "abstract": "With their prominent scene understanding and reasoning capabilities, pre-trained visual-language models (VLMs) such as GPT-4V have attracted increasing attention in robotic task planning. Compared with traditional task planning strategies, VLMs are strong in multimodal information parsing and code generation and show remarkable efficiency. Although VLMs demonstrate great potential in robotic task planning, they suffer from challenges like hallucination, semantic complexity, and limited context. To handle such issues, this paper proposes a multi-agent framework, i.e., GameVLM, to enhance the decision-making process in robotic task planning. In this study, VLM-based decision and expert agents are presented to conduct the task planning. Specifically, decision agents are used to plan the task, and the expert agent is employed to evaluate these task plans. Zero-sum game theory is introduced to resolve inconsistencies among different agents and determine the optimal solution. Experimental results on real robots demonstrate the efficacy of the proposed framework, with an average success rate of 83.3%. Videos of our experiments are available at https://youtu.be/sam-MKCPP7Y.",
                "authors": "Aoran Mei, Jianhua Wang, Guo-Niu Zhu, Zhongxue Gan",
                "citations": 3
            },
            {
                "title": "AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description",
                "abstract": "Our objective is to generate Audio Descriptions (ADs) for both movies and TV series in a training-free manner. We use the power of off-the-shelf Visual-Language Models (VLMs) and Large Language Models (LLMs), and develop visual and text prompting strategies for this task. Our contributions are three-fold: (i) We demonstrate that a VLM can successfully name and refer to characters if directly prompted with character information through visual indications without requiring any fine-tuning; (ii) A two-stage process is developed to generate ADs, with the first stage asking the VLM to comprehensively describe the video, followed by a second stage utilising a LLM to summarise dense textual information into one succinct AD sentence; (iii) A new dataset for TV audio description is formulated. Our approach, named AutoAD-Zero, demonstrates outstanding performance (even competitive with some models fine-tuned on ground truth ADs) in AD generation for both movies and TV series, achieving state-of-the-art CRITIC scores.",
                "authors": "Junyu Xie, Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman",
                "citations": 3
            },
            {
                "title": "Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models",
                "abstract": "Language and Vision-Language Models (LLMs/VLMs) have revolutionized the field of AI by their ability to generate human-like text and understand images, but ensuring their reliability is crucial. This paper aims to evaluate the ability of LLMs (GPT4, GPT-3.5, LLaMA2, and PaLM 2) and VLMs (GPT4V and Gemini Pro Vision) to estimate their verbalized uncertainty via prompting. We propose the new Japanese Uncertain Scenes (JUS) dataset, aimed at testing VLM capabilities via difficult queries and object counting, and the Net Calibration Error (NCE) to measure direction of miscalibration.Results show that both LLMs and VLMs have a high calibration error and are overconfident most of the time, indicating a poor capability for uncertainty estimation. Additionally we develop prompts for regression tasks, and we show that VLMs have poor calibration when producing mean/standard deviation and 95% confidence intervals.",
                "authors": "Tobias Groot, Matias Valdenegro-Toro",
                "citations": 3
            },
            {
                "title": "Selective \"Selective Prediction\": Reducing Unnecessary Abstention in Vision-Language Reasoning",
                "abstract": "Selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain. However, when deploying a vision-language system with low tolerance for inaccurate predictions, selective prediction may be over-cautious and abstain too frequently, even on many correct predictions. We introduce ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without increasing the error rate of the system's predictions. When the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries to find relevant clues in the image that provide additional evidence for the prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining. ReCoVERR enables three VLMs (BLIP2, InstructBLIP, and LLaVA-1.5) to answer up to 20% more questions on the VQAv2 and A-OKVQA tasks without decreasing system accuracy, thus improving overall system reliability. Our code is available at https://github.com/tejas1995/ReCoVERR.",
                "authors": "Tejas Srinivasan, Jack Hessel, Tanmay Gupta, Bill Yuchen Lin, Yejin Choi, Jesse Thomason, Khyathi Raghavi Chandu",
                "citations": 3
            },
            {
                "title": "Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",
                "abstract": "The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA along with human annotations for the categories as mentioned earlier.",
                "authors": "Anku Rani, Vipula Rawte, Harshad Sharma, Neeraj Anand, Krishnav Rajbangshi, Amit P. Sheth, Amitava Das",
                "citations": 3
            },
            {
                "title": "Image Safeguarding: Reasoning with Conditional Vision Language Model and Obfuscating Unsafe Content Counterfactually",
                "abstract": "Social media platforms are being increasingly used by malicious actors to share unsafe content, such as images depicting sexual activity, cyberbullying, and self-harm. Consequently, major platforms use artificial intelligence (AI) and human moderation to obfuscate such images to make them safer. Two critical needs for obfuscating unsafe images is that an accurate rationale for obfuscating image regions must be provided, and the sensitive regions should be obfuscated (e.g. blurring) for users' safety. This process involves addressing two key problems: (1) the reason for obfuscating unsafe images demands the platform to provide an accurate rationale that must be grounded in unsafe image-specific attributes, and (2) the unsafe regions in the image must be minimally obfuscated while still depicting the safe regions. In this work, we address these key issues by first performing visual reasoning by designing a visual reasoning model (VLM) conditioned on pre-trained unsafe image classifiers to provide an accurate rationale grounded in unsafe image attributes, and then proposing a counterfactual explanation algorithm that minimally identifies and obfuscates unsafe regions for safe viewing, by first utilizing an unsafe image classifier attribution matrix to guide segmentation for a more optimal subregion segmentation followed by an informed greedy search to determine the minimum number of subregions required to modify the classifier's output based on attribution score. Extensive experiments on uncurated data from social networks emphasize the efficacy of our proposed method. We make our code available at: https://github.com/SecureAIAutonomyLab/ConditionalVLM",
                "authors": "Mazal Bethany, Brandon Wherry, Nishant Vishwamitra, Peyman Najafirad",
                "citations": 3
            },
            {
                "title": "CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models",
                "abstract": "Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as GPT-4V, identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on compositionality benchmarks, while maintaining or improving the performance on standard object-recognition and retrieval benchmarks. Our code and pre-trained models are publicly available at https://github.com/netflix/clove.",
                "authors": "Santiago Castro, Amir Ziai, Avneesh Saluja, Zhuoning Yuan, Rada Mihalcea",
                "citations": 3
            },
            {
                "title": "HPE-CogVLM: New Head Pose Grounding Task Exploration on Vision Language Model",
                "abstract": "Head pose estimation (HPE) task requires a sophisticated understanding of 3D spatial relationships and precise numerical output of yaw, pitch, and roll Euler angles. Previous HPE studies are mainly based on Non-large language models (Non-LLMs), which rely on close-up human heads cropped from the full image as inputs and lack robustness in real-world scenario. In this paper, we present a novel framework to enhance the HPE prediction task by leveraging the visual grounding capability of CogVLM. CogVLM is a vision language model (VLM) with grounding capability of predicting object bounding boxes (BBoxes), which enables HPE training and prediction using full image information input. To integrate the HPE task into the VLM, we first cop with the catastrophic forgetting problem in large language models (LLMs) by investigating the rehearsal ratio in the data rehearsal method. Then, we propose and validate a LoRA layer-based model merging method, which keeps the integrity of parameters, to enhance the HPE performance in the framework. The results show our HPE-CogVLM achieves a 31.5% reduction in Mean Absolute Error for HPE prediction over the current Non-LLM based state-of-the-art in cross-dataset evaluation. Furthermore, we compare our LoRA layer-based model merging method with LoRA fine-tuning only and other merging methods in CogVLM. The results demonstrate our framework outperforms them in all HPE metrics.",
                "authors": "Yu Tian, Tianqi Shao, Tsukasa Demizu, Xuyang Wu, Hsin-Tai Wu",
                "citations": 3
            },
            {
                "title": "VLMPC: Vision-Language Model Predictive Control for Robotic Manipulation",
                "abstract": "Although Model Predictive Control (MPC) can effectively predict the future states of a system and thus is widely used in robotic manipulation tasks, it does not have the capability of environmental perception, leading to the failure in some complex scenarios. To address this issue, we introduce Vision-Language Model Predictive Control (VLMPC), a robotic manipulation framework which takes advantage of the powerful perception capability of vision language model (VLM) and integrates it with MPC. Specifically, we propose a conditional action sampling module which takes as input a goal image or a language instruction and leverages VLM to sample a set of candidate action sequences. Then, a lightweight action-conditioned video prediction model is designed to generate a set of future frames conditioned on the candidate action sequences. VLMPC produces the optimal action sequence with the assistance of VLM through a hierarchical cost function that formulates both pixel-level and knowledge-level consistence between the current observation and the goal image. We demonstrate that VLMPC outperforms the state-of-the-art methods on public benchmarks. More importantly, our method showcases excellent performance in various real-world tasks of robotic manipulation. Code is available at~\\url{https://github.com/PPjmchen/VLMPC}.",
                "authors": "Wentao Zhao, Jiaming Chen, Ziyu Meng, Donghui Mao, Ran Song, Wei Zhang",
                "citations": 3
            },
            {
                "title": "Multimodal Deep Learning for Low-Resource Settings: A Vector Embedding Alignment Approach for Healthcare Applications",
                "abstract": "Objective: Large-scale multi-modal deep learning models and datasets have revolutionized various domains such as healthcare, underscoring the critical role of computational power. However, in resource-constrained regions like Low and Middle-Income Countries (LMICs), GPU and data access is limited, leaving many dependent solely on CPUs. To address this, we advocate leveraging vector embeddings for flexible and efficient computational methodologies, aiming to democratize multimodal deep learning across diverse contexts. Background and Significance: Our paper investigates the computational efficiency and effectiveness of leveraging vector embeddings, extracted from single-modal foundation models and multi-modal Vision-Language Models (VLM), for multimodal deep learning in low-resource environments, particularly in healthcare applications. Additionally, we propose an easy but effective inference-time method to enhance performance by further aligning image-text embeddings. Materials and Methods: By comparing these approaches with traditional multimodal deep learning methods, we assess their impact on computational efficiency and model performance using accuracy, F1-score, inference time, training time, and memory usage across 3 medical modalities such as BRSET (ophthalmology), HAM10000 (dermatology), and SatelliteBench (public health). Results: Our findings indicate that embeddings reduce computational demands without compromising the model's performance, and show that our embedding alignment method improves the performance of the models in medical tasks. Discussion: This research contributes to sustainable AI practices by optimizing computational resources in resource-constrained environments. It highlights the potential of embedding-based approaches for efficient multimodal learning. Conclusion: Vector embeddings democratize multimodal deep learning in LMICs, especially in healthcare. Our study showcases their effectiveness, enhancing AI adaptability in varied use cases.",
                "authors": "David Restrepo, Chenwei Wu, Sebastián Andrés Cajas, L. Nakayama, L. A. Celi, Diego M L'opez",
                "citations": 3
            },
            {
                "title": "Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and Semantically-Rich Vision-Language Models",
                "abstract": "Vision-language models (VLMs) have achieved significant strides in recent times specially in multimodal tasks, yet they remain susceptible to adversarial attacks on their vision components. To address this, we propose Sim-CLIP, an unsupervised adversarial fine-tuning method that enhances the robustness of the widely-used CLIP vision encoder against such attacks while maintaining semantic richness and specificity. By employing a Siamese architecture with cosine similarity loss, Sim-CLIP learns semantically meaningful and attack-resilient visual representations without requiring large batch sizes or momentum encoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned CLIP encoder exhibit significantly enhanced robustness against adversarial attacks, while preserving semantic meaning of the perturbed images. Notably, Sim-CLIP does not require additional training or fine-tuning of the VLM itself; replacing the original vision encoder with our fine-tuned Sim-CLIP suffices to provide robustness. This work underscores the significance of reinforcing foundational models like CLIP to safeguard the reliability of downstream VLM applications, paving the way for more secure and effective multimodal systems.",
                "authors": "Md. Zarif Hossain, Ahmed Imteaj",
                "citations": 3
            },
            {
                "title": "Prompt Injection Attacks on Large Language Models in Oncology",
                "abstract": "Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be attacked by prompt injection attacks, which can be used to output harmful information just by interacting with the VLM, without any access to its parameters. We performed a quantitative study to evaluate the vulnerabilities to these attacks in four state of the art VLMs which have been proposed to be of utility in healthcare: Claude 3 Opus, Claude 3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N=297 attacks, we show that all of these models are susceptible. Specifically, we show that embedding sub-visual prompts in medical imaging data can cause the model to provide harmful output, and that these prompts are non-obvious to human observers. Thus, our study demonstrates a key vulnerability in medical VLMs which should be mitigated before widespread clinical adoption.",
                "authors": "J. Clusmann, Dyke Ferber, I. Wiest, C. V. Schneider, T. Brinker, S. Foersch, Daniel Truhn, J. N. Kather",
                "citations": 3
            },
            {
                "title": "DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning",
                "abstract": "Vision-language models (VLMs) have been applied to robot task planning problems, where the robot receives a task in natural language and generates plans based on visual inputs. While current VLMs have demonstrated strong vision-language understanding capabilities, their performance is still far from being satisfactory in planning tasks. At the same time, although classical task planners, such as PDDL-based, are strong in planning for long-horizon tasks, they do not work well in open worlds where unforeseen situations are common. In this paper, we propose a novel task planning and execution framework, called DKPROMPT, which automates VLM prompting using domain knowledge in PDDL for classical planning in open worlds. Results from quantitative experiments show that DKPROMPT outperforms classical planning, pure VLM-based and a few other competitive baselines in task completion rate.",
                "authors": "Xiaohan Zhang, Zainab Altaweel, Yohei Hayamizu, Yan Ding, S. Amiri, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang",
                "citations": 3
            },
            {
                "title": "Unified Embedding Alignment for Open-Vocabulary Video Instance Segmentation",
                "abstract": "Open-Vocabulary Video Instance Segmentation (VIS) is attracting increasing attention due to its ability to segment and track arbitrary objects. However, the recent Open-Vocabulary VIS attempts obtained unsatisfactory results, especially in terms of generalization ability of novel categories. We discover that the domain gap between the VLM features (e.g., CLIP) and the instance queries and the underutilization of temporal consistency are two central causes. To mitigate these issues, we design and train a novel Open-Vocabulary VIS baseline called OVFormer. OVFormer utilizes a lightweight module for unified embedding alignment between query embeddings and CLIP image embeddings to remedy the domain gap. Unlike previous image-based training methods, we conduct video-based model training and deploy a semi-online inference scheme to fully mine the temporal consistency in the video. Without bells and whistles, OVFormer achieves 21.9 mAP with a ResNet-50 backbone on LV-VIS, exceeding the previous state-of-the-art performance by 7.7. Extensive experiments on some Close-Vocabulary VIS datasets also demonstrate the strong zero-shot generalization ability of OVFormer (+ 7.6 mAP on YouTube-VIS 2019, + 3.9 mAP on OVIS). Code is available at https://github.com/fanghaook/OVFormer.",
                "authors": "Hao Fang, Peng Wu, Yawei Li, Xinxin Zhang, Xiankai Lu",
                "citations": 3
            },
            {
                "title": "3rd Place Solution for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation",
                "abstract": "Referring video object segmentation (RVOS) relies on natural language expressions to segment target objects in video, emphasizing modeling dense text-video relations. The current RVOS methods typically use independently pre-trained vision and language models as backbones, resulting in a significant domain gap between video and text. In cross-modal feature interaction, text features are only used as query initialization and do not fully utilize important information in the text. In this work, we propose using frozen pre-trained vision-language models (VLM) as backbones, with a specific emphasis on enhancing cross-modal feature interaction. Firstly, we use frozen convolutional CLIP backbone to generate feature-aligned vision and text features, alleviating the issue of domain gap and reducing training costs. Secondly, we add more cross-modal feature fusion in the pipeline to enhance the utilization of multi-modal information. Furthermore, we propose a novel video query initialization method to generate higher quality video queries. Without bells and whistles, our method achieved 51.5 J&F on the MeViS test set and ranked 3rd place for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation.",
                "authors": "Feiyu Pan, Hao Fang, Xiankai Lu",
                "citations": 3
            },
            {
                "title": "Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?",
                "abstract": "Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. In this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that most tested VLMs are less self-consistent than LLMs. Text contributions in all tested VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to VL encoders. We find that the tested VL decoders still struggle with most phenomena tested by VALSE.",
                "authors": "Letitia Parcalabescu, Anette Frank",
                "citations": 3
            },
            {
                "title": "Hawk: Learning to Understand Open-World Video Anomalies",
                "abstract": "Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios. In this paper, we introduce Hawk, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, Hawk explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that Hawk achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.",
                "authors": "Jiaqi Tang, Hao Lu, Ruizheng Wu, Xiaogang Xu, Ke Ma, Cheng Fang, Bin Guo, Jiangbo Lu, Qifeng Chen, Ying-Cong Chen",
                "citations": 3
            },
            {
                "title": "Advancing Cross-domain Discriminability in Continual Learning of Vison-Language Models",
                "abstract": "Continual learning (CL) with Vision-Language Models (VLMs) has overcome the constraints of traditional CL, which only focuses on previously encountered classes. During the CL of VLMs, we need not only to prevent the catastrophic forgetting on incrementally learned knowledge but also to preserve the zero-shot ability of VLMs. However, existing methods require additional reference datasets to maintain such zero-shot ability and rely on domain-identity hints to classify images across different domains. In this study, we propose Regression-based Analytic Incremental Learning (RAIL), which utilizes a recursive ridge regression-based adapter to learn from a sequence of domains in a non-forgetting manner and decouple the cross-domain correlations by projecting features to a higher-dimensional space. Cooperating with a training-free fusion module, RAIL absolutely preserves the VLM's zero-shot ability on unseen domains without any reference data. Additionally, we introduce Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting. In this setting, a CL learner is required to incrementally learn from multiple domains and classify test images from both seen and unseen domains without any domain-identity hint. We theoretically prove RAIL's absolute memorization on incrementally learned domains. Experiment results affirm RAIL's state-of-the-art performance in both X-TAIL and existing Multi-domain Task-Incremental Learning settings. The code is released at https://github.com/linghan1997/Regression-based-Analytic-Incremental-Learning.",
                "authors": "Yicheng Xu, Yuxin Chen, Jiahao Nie, Yusong Wang, Huiping Zhuang, Manabu Okumura",
                "citations": 3
            },
            {
                "title": "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions",
                "abstract": "Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize textual features that are important for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate features that are important for the VLM. Then, we inspect the descriptions to identify features that contribute to VLM representations. Using EX2, we find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat (e.g., North America) to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations.",
                "authors": "Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach",
                "citations": 3
            },
            {
                "title": "RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning",
                "abstract": "Developing robust and correctable visuomotor policies for robotic manipulation is challenging due to the lack of self-recovery mechanisms from failures and the limitations of simple language instructions in guiding robot actions. To address these issues, we propose a scalable data generation pipeline that automatically augments expert demonstrations with failure recovery trajectories and fine-grained language annotations for training. We then introduce Rich languAge-guided failure reCovERy (RACER), a supervisor-actor framework, which combines failure recovery data with rich language descriptions to enhance robot control. RACER features a vision-language model (VLM) that acts as an online supervisor, providing detailed language guidance for error correction and task execution, and a language-conditioned visuomotor policy as an actor to predict the next actions. Our experimental results show that RACER outperforms the state-of-the-art Robotic View Transformer (RVT) on RLbench across various evaluation settings, including standard long-horizon tasks, dynamic goal-change tasks and zero-shot unseen tasks, achieving superior performance in both simulated and real world environments. Videos and code are available at: https://rich-language-failure-recovery.github.io.",
                "authors": "Yinpei Dai, Jayjun Lee, Nima Fazeli, Joyce Chai",
                "citations": 3
            },
            {
                "title": "Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning",
                "abstract": "This paper proposes a novel framework for multi-label image recognition without any training data, called data-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt pretrained Vision-Language Model (VLM) like CLIP to multilabel classification. Through asking LLM by well-designed questions, we acquire comprehensive knowledge about characteristics and contexts of objects, which provides valuable text descriptions for learning prompts. Then we propose a hierarchical prompt learning method by taking the multi-label dependency into consideration, wherein a subset of category-specific prompt tokens are shared when the corresponding objects exhibit similar attributes or are more likely to co-occur. Benefiting from the remarkable alignment between visual and linguistic semantics of CLIP, the hierarchical prompts learned from text descriptions are applied to perform classification of images during inference. Our framework presents a new way to explore the synergies between multiple pre-trained models for novel category recognition. Extensive experiments on three public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that our method achieves better results than the state-of-the-art methods, especially outperforming the zero-shot multi-label recognition methods by 4.7% in mAP on MS-COCO.",
                "authors": "Shuo Yang, Zirui Shang, Yongqi Wang, Derong Deng, Hongwei Chen, Qiyuan Cheng, Xinxiao Wu",
                "citations": 2
            },
            {
                "title": "WATT: Weight Average Test-Time Adaptation of CLIP",
                "abstract": "Vision-Language Models (VLMs) such as CLIP have yielded unprecedented performance for zero-shot image classification, yet their generalization capability may still be seriously challenged when confronted to domain shifts. In response, we present Weight Average Test-Time Adaptation (WATT) of CLIP, a pioneering approach facilitating full test-time adaptation (TTA) of this VLM. Our method employs a diverse set of templates for text prompts, augmenting the existing framework of CLIP. Predictions are utilized as pseudo labels for model updates, followed by weight averaging to consolidate the learned information globally. Furthermore, we introduce a text ensemble strategy, enhancing overall test performance by aggregating diverse textual cues. Our findings underscore the efficacy of WATT in enhancing performance across diverse datasets, including CIFAR-10-C, CIFAR-10.1, CIFAR-100-C, VisDA-C, and several other challenging datasets, effectively covering a wide range of domain shifts. Notably, these enhancements are achieved without necessitating additional model transformations or trainable modules. Moreover, compared to other Test-Time Adaptation methods, our approach can operate effectively with just a single image. Highlighting the potential of innovative test-time strategies, this research emphasizes their role in fortifying the adaptability of VLMs. The implementation is available at: \\url{https://github.com/Mehrdad-Noori/WATT.git}.",
                "authors": "David Osowiechi, Mehrdad Noori, G. A. V. Hakim, Moslem Yazdanpanah, Ali Bahri, Milad Cheraghalikhani, Sahar Dastani, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers",
                "citations": 2
            },
            {
                "title": "Interpretation and Attribution of Coastal Land Subsidence: An InSAR and Machine Learning Perspective",
                "abstract": "Subsidence, the downward vertical land motion (VLM), plays a pivotal role in contributing to the risk of coastal flooding. Accurately estimating VLM and identifying its potential features related to subsidence can provide crucial information for stakeholders to make better-informed decisions. This study aimed to estimate large-scale subsidence at the Texas Gulf Coast and identify potential subsidence features using explainable models. Nine potential features were considered for modeling the VLM, ranging from natural terrain variations to anthropogenic activities. These features were used to train a random forest (RF) machine learning model. Explainable artificial intelligence (XAI) techniques including SHapley Additive exPlanations (SHAP) and impurity- and permutation-based feature importance were used to identify the contributions to subsidence. The results demonstrated favorable performance of the RF model, achieving an $R^{2}$ value of 0.56 during validation. XAI results underscored the significance of the digital elevation model in explaining subsidence at the Texas Coast. Additionally, XAI analysis highlighted the overall contribution of subsidence from anthropogenic activities, such as hydrocarbon extraction and groundwater withdrawal. Furthermore, the sample-level SHAP map provided detailed and reasonable subsidence-attribution results across the study area, showing potential for automatic and data-driven explanations of the VLM.",
                "authors": "Xiaojun Qiao, T. Chu, Evan Krell, Philippe Tissot, Seneca Holland, Mohamed Ahmed, Danielle Smilovsky",
                "citations": 2
            },
            {
                "title": "ChangeChat: An Interactive Model for Remote Sensing Change Analysis via Multimodal Instruction Tuning",
                "abstract": "Remote sensing (RS) change analysis is vital for monitoring Earth's dynamic processes by detecting alterations in images over time. Traditional change detection excels at identifying pixel-level changes but lacks the ability to contextualize these alterations. While recent advancements in change captioning offer natural language descriptions of changes, they do not support interactive, user-specific queries. To address these limitations, we introduce ChangeChat, the first bitemporal vision-language model (VLM) designed specifically for RS change analysis. ChangeChat utilizes multimodal instruction tuning, allowing it to handle complex queries such as change captioning, category-specific quantification, and change localization. To enhance the model's performance, we developed the ChangeChat-87k dataset, which was generated using a combination of rule-based methods and GPT-assisted techniques. Experiments show that ChangeChat offers a comprehensive, interactive solution for RS change analysis, achieving performance comparable to or even better than state-of-the-art (SOTA) methods on specific tasks, and significantly surpassing the latest general-domain model, GPT-4. Code and pre-trained weights are available at https://github.com/hanlinwu/ChangeChat.",
                "authors": "Pei Deng, Wenqian Zhou, Hanlin Wu",
                "citations": 2
            },
            {
                "title": "Learning Visual Grounding from Generative Vision and Language Model",
                "abstract": "Visual grounding tasks aim to localize image regions based on natural language references. In this work, we explore whether generative VLMs predominantly trained on image-text data could be leveraged to scale up the text annotation of visual grounding data. We find that grounding knowledge already exists in generative VLM and can be elicited by proper prompting. We thus prompt a VLM to generate object-level descriptions by feeding it object regions from existing object detection datasets. We further propose attribute modeling to explicitly capture the important object attributes, and spatial relation modeling to capture inter-object relationship, both of which are common linguistic pattern in referring expression. Our constructed dataset (500K images, 1M objects, 16M referring expressions) is one of the largest grounding datasets to date, and the first grounding dataset with purely model-generated queries and human-annotated objects. To verify the quality of this data, we conduct zero-shot transfer experiments to the popular RefCOCO benchmarks for both referring expression comprehension (REC) and segmentation (RES) tasks. On both tasks, our model significantly outperform the state-of-the-art approaches without using human annotated visual grounding data. Our results demonstrate the promise of generative VLM to scale up visual grounding in the real world. Code and models will be released.",
                "authors": "Shijie Wang, Dahun Kim, A. Taalimi, Chen Sun, Weicheng Kuo",
                "citations": 2
            },
            {
                "title": "Concept-based Analysis of Neural Networks via Vision-Language Models",
                "abstract": "The analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\\texttt{Con}_{\\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\\texttt{Con}_{\\texttt{spec}}$ specifications, we build a map between the internal representations of a given vision model and a VLM, leading to an efficient verification procedure of natural-language properties for vision models. We demonstrate our techniques on a ResNet-based classifier trained on the RIVAL-10 dataset using CLIP as the multimodal model.",
                "authors": "Ravi Mangal, Nina Narodytska, Divya Gopinath, Boyue Caroline Hu, Anirban Roy, Susmit Jha, Corina S. Pasareanu",
                "citations": 2
            },
            {
                "title": "ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation",
                "abstract": "This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to chance (at 12% and 30%, respectively), although the non-contrastive BLIP ITM model is stronger (87%). We also find that finetuning on fewer than 2,000 examples yields significant performance gains on this out-of-distribution word-order understanding task. The dataset is here: https://github.com/Top34051/colorswap and here: https://huggingface.co/datasets/stanfordnlp/colorswap.",
                "authors": "Jirayu Burapacheep, Ishan Gaur, Agam Bhatia, Tristan Thrush",
                "citations": 2
            },
            {
                "title": "KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data",
                "abstract": "Building generalist robotic systems involves effectively endowing robots with the capabilities to handle novel objects in an open-world setting. Inspired by the advances of large pre-trained models, we propose Keypoint Affordance Learning from Imagined Environments (KALIE), which adapts pre-trained Vision Language Models (VLMs) for robotic control in a scalable manner. Instead of directly producing motor commands, KALIE controls the robot by predicting point-based affordance representations based on natural language instructions and visual observations of the scene. The VLM is trained on 2D images with affordances labeled by humans, bypassing the need for training data collected on robotic systems. Through an affordance-aware data synthesis pipeline, KALIE automatically creates massive high-quality training data based on limited example data manually collected by humans. We demonstrate that KALIE can learn to robustly solve new manipulation tasks with unseen objects given only 50 example data points. Compared to baselines using pre-trained VLMs, our approach consistently achieves superior performance.",
                "authors": "Grace Tang, Swetha Rajkumar, Yifei Zhou, Homer Walke, Sergey Levine, Kuan Fang",
                "citations": 2
            },
            {
                "title": "DexGANGrasp: Dexterous Generative Adversarial Grasping Synthesis for Task-Oriented Manipulation",
                "abstract": "We introduce DexGanGrasp, a dexterous grasp synthesis method that generates and evaluates grasps with a single view in real-time. DexGanGrasp comprises a Conditional Generative Adversarial Network (cGAN)-based DexGenerator to generate dexterous grasps and a discriminator-like DexEvalautor to assess the stability of these grasps. Extensive simulation and real-world experiments showcase the effectiveness of our proposed method, outperforming the baseline FFHNet with an $18.57 \\%$ higher success rate in real-world evaluation. To further achieve task-oriented grasping, we extend DexGanGrasp to DexAfford-Prompt, an open-vocabulary affordance grounding pipeline for dexterous grasping leveraging Multimodal Large Language Models (MLLM) and Vision Language Models (VLM) with successful real-world deployments. For the code and data, visit our website.",
                "authors": "Qian Feng, David S. Martinez Lema, M. Malmir, Hang Li, Jianxiang Feng, Zhaopeng Chen, Alois Knoll",
                "citations": 2
            },
            {
                "title": "AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation",
                "abstract": "Robotic manipulation in open-world settings requires not only task execution but also the ability to detect and learn from failures. While recent advances in vision-language models (VLMs) and large language models (LLMs) have improved robots' spatial reasoning and problem-solving abilities, they still struggle with failure recognition, limiting their real-world applicability. We introduce AHA, an open-source VLM designed to detect and reason about failures in robotic manipulation using natural language. By framing failure detection as a free-form reasoning task, AHA identifies failures and provides detailed, adaptable explanations across different robots, tasks, and environments. We fine-tuned AHA using FailGen, a scalable framework that generates the first large-scale dataset of robotic failure trajectories, the AHA dataset. FailGen achieves this by procedurally perturbing successful demonstrations from simulation. Despite being trained solely on the AHA dataset, AHA generalizes effectively to real-world failure datasets, robotic systems, and unseen tasks. It surpasses the second-best model (GPT-4o in-context learning) by 10.3% and exceeds the average performance of six compared models including five state-of-the-art VLMs by 35.3% across multiple metrics and datasets. We integrate AHA into three manipulation frameworks that utilize LLMs/VLMs for reinforcement learning, task and motion planning, and zero-shot trajectory generation. AHA's failure feedback enhances these policies' performances by refining dense reward functions, optimizing task planning, and improving sub-task verification, boosting task success rates by an average of 21.4% across all three tasks compared to GPT-4 models.",
                "authors": "Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, Yijie Guo",
                "citations": 2
            },
            {
                "title": "FlexCap: Generating Rich, Localized, and Flexible Captions in Images",
                "abstract": "We introduce a versatile $\\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications. First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQA datasets. We also demonstrate a $\\textit{localize-then-describe}$ approach with FlexCap can be better at open-ended object detection than a $\\textit{describe-then-localize}$ approach with other VLMs. We highlight a novel characteristic of FlexCap, which is its ability to extract diverse visual information through prefix conditioning. Finally, we qualitatively demonstrate FlexCap's broad applicability in tasks such as image labeling, object attribute recognition, and visual dialog. Project webpage: https://flex-cap.github.io .",
                "authors": "Debidatta Dwibedi, Vidhi Jain, Jonathan Tompson, A. Zisserman, Y. Aytar",
                "citations": 2
            },
            {
                "title": "Envisioning Medclip: A Deep Dive into Explainability for Medical Vision-Language Models",
                "abstract": "Explaining Deep Learning models is becoming increasingly important in the face of daily emerging multimodal models, particularly in safety-critical domains like medical imaging. However, the lack of detailed investigations into the performance of explainability methods on these models is widening the gap between their development and safe deployment. In this work, we analyze the performance of various explainable AI methods on a vision-language model, MedCLIP, to demystify its inner workings. We also provide a simple methodology to overcome the shortcomings of these methods. Our work offers a different new perspective on the explainability of a recent well-known VLM in the medical domain and our assessment method is generalizable to other current and possible future VLMs.",
                "authors": "Anees Ur Rehman Hashmi, Dwarikanath Mahapatra, Mohammad Yaqub",
                "citations": 2
            },
            {
                "title": "HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision",
                "abstract": "Large Vision Language Models (VLMs) are now the de facto state-of-the-art for a number of tasks including visual question answering, recognising objects, and spatial referral. In this work, we propose the HOI-Ref task for egocentric images that aims to understand interactions between hands and objects using VLMs. To enable HOI-Ref, we curate the HOI-QA dataset that consists of 3.9M question-answer pairs for training and evaluating VLMs. HOI-QA includes questions relating to locating hands, objects, and critically their interactions (e.g. referring to the object being manipulated by the hand). We train the first VLM for HOI-Ref on this dataset and call it VLM4HOI. Our results demonstrate that VLMs trained for referral on third person images fail to recognise and refer hands and objects in egocentric images. When fine-tuned on our egocentric HOI-QA dataset, performance improves by 27.9% for referring hands and objects, and by 26.7% for referring interactions.",
                "authors": "Siddhant Bansal, Michael Wray, D. Damen",
                "citations": 2
            },
            {
                "title": "Mammo-CLIP: A Vision Language Foundation Model to Enhance Data Efficiency and Robustness in Mammography",
                "abstract": "The lack of large and diverse training data on Computer-Aided Diagnosis (CAD) in breast cancer detection has been one of the concerns that impedes the adoption of the system. Recently, pre-training with large-scale image text datasets via Vision-Language models (VLM) (\\eg CLIP) partially addresses the issue of robustness and data efficiency in computer vision (CV). This paper proposes Mammo-CLIP, the first VLM pre-trained on a substantial amount of screening mammogram-report pairs, addressing the challenges of dataset diversity and size. Our experiments on two public datasets demonstrate strong performance in classifying and localizing various mammographic attributes crucial for breast cancer detection, showcasing data efficiency and robustness similar to CLIP in CV. We also propose Mammo-FActOR, a novel feature attribution method, to provide spatial interpretation of representation with sentence-level granularity within mammography reports. Code is available publicly: \\url{https://github.com/batmanlab/Mammo-CLIP}.",
                "authors": "Shantanu Ghosh, Clare B. Poynton, Shyam Visweswaran, K. Batmanghelich",
                "citations": 2
            },
            {
                "title": "Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation",
                "abstract": "The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of large Vision-and-Language Models (VLMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to develop a VLM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. To this end, we developed a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. The dataset covers a range of tasks, from common ones like caption generation to specialized VQA tasks that require expert knowledge. Furthermore, using the dataset we created, we fine-tuned an existing VLM. This training enabled the models to generate questions and perform iterative reasoning during inference. The results demonstrated a stride toward a more robust, accurate, and interpretable VLM, capable of reasoning explicitly and seeking information proactively when confronted with ambiguous visual input.",
                "authors": "Kohei Uehara, Nabarun Goswami, Hanqin Wang, Toshiaki Baba, Kohtaro Tanaka, Tomohiro Hashimoto, Kai Wang, Rei Ito, Takagi Naoya, Ryo Umagami, Yingyi Wen, Tanachai Anakewat, Tatsuya Harada",
                "citations": 2
            },
            {
                "title": "Towards Interpreting Visual Information Processing in Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) are powerful tools for processing and understanding text and images. We study the processing of visual tokens in the language model component of LLaVA, a prominent VLM. Our approach focuses on analyzing the localization of object information, the evolution of visual token representations across layers, and the mechanism of integrating visual information for predictions. Through ablation studies, we demonstrated that object identification accuracy drops by over 70\\% when object-specific tokens are removed. We observed that visual token representations become increasingly interpretable in the vocabulary space across layers, suggesting an alignment with textual tokens corresponding to image content. Finally, we found that the model extracts object information from these refined representations at the last token position for prediction, mirroring the process in text-only language models for factual association tasks. These findings provide crucial insights into how VLMs process and integrate visual information, bridging the gap between our understanding of language and vision models, and paving the way for more interpretable and controllable multimodal systems.",
                "authors": "Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, Fazl Barez",
                "citations": 2
            },
            {
                "title": "Are Bigger Encoders Always Better in Vision Large Models?",
                "abstract": "In recent years, multimodal large language models (MLLMs) have shown strong potential in real-world applications. They are developing rapidly due to their remarkable ability to comprehend multimodal information and their inherent powerful cognitive and reasoning capabilities. Among MLLMs, vision language models (VLM) stand out for their ability to understand vision information. However, the scaling trend of VLMs under the current mainstream paradigm has not been extensively studied. Whether we can achieve better performance by training even larger models is still unclear. To address this issue, we conducted experiments on the pretraining stage of MLLMs. We conduct our experiment using different encoder sizes and large language model (LLM) sizes. Our findings indicate that merely increasing the size of encoders does not necessarily enhance the performance of VLMs. Moreover, we analyzed the effects of LLM backbone parameter size and data quality on the pretraining outcomes. Additionally, we explored the differences in scaling laws between LLMs and VLMs.",
                "authors": "Bozhou Li, Hao Liang, Zimo Meng, Wentao Zhang",
                "citations": 2
            },
            {
                "title": "Exploring the Zero-Shot Capabilities of Vision-Language Models for Improving Gaze Following",
                "abstract": "Contextual cues related to a person’s pose and interactions with objects and other people in the scene can provide valuable information for gaze following. While existing methods have focused on dedicated cue extraction methods, in this work we investigate the zero-shot capabilities of Vision-Language Models (VLMs) for extracting a wide array of contextual cues to improve gaze following performance. We first evaluate various VLMs, prompting strategies, and in-context learning (ICL) techniques for zero-shot cue recognition performance. We then use these insights to extract contextual cues for gaze following, and investigate their impact when incorporated into a state of the art model for the task. Our analysis indicates that BLIP-2 is the overall top performing VLM and that ICL can improve performance. We also observe that VLMs are sensitive to the choice of the text prompt although ensembling over multiple text prompts can provide more robust performance. Additionally, we discover that using the entire image along with an ellipse drawn around the target person is the most effective strategy for visual prompting. For gaze following, incorporating the extracted cues results in better generalization performance, especially when considering a larger set of cues, highlighting the potential of this approach.",
                "authors": "Anshul Gupta, Pierre Vuillecard, Arya Farkhondeh, J. Odobez",
                "citations": 2
            },
            {
                "title": "SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving",
                "abstract": "Many fields could benefit from the rapid development of the large language models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the typically fields facing new opportunities as the LLMs have supported more and more modalities. Here, by utilizing vision-language model (VLM), we proposed an e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided into four stages, which are perception, prediction, planning, and behavior. Each stage consists of several visual question answering (VQA) pairs and VQA pairs interconnect with each other constructing a graph called Graph VQA (GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our method could achieve e2e driving with language. In our method, vision transformers (ViT) models are employed to process nuScenes visual data, while VLM are utilized to interpret and reason about the information extracted from the visual inputs. In the perception stage, the system identifies and classifies objects from the driving environment. The prediction stage involves forecasting the potential movements of these objects. The planning stage utilizes the gathered information to develop a driving strategy, ensuring the safety and efficiency of the autonomous vehicle. Finally, the behavior stage translates the planned actions into executable commands for the vehicle. Our experiments demonstrate that SimpleLLM4AD achieves competitive performance in complex driving scenarios.",
                "authors": "Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, Shaohua Wu",
                "citations": 2
            },
            {
                "title": "SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors",
                "abstract": "Current state-of-the-art spatial reasoning-enhanced VLMs are trained to excel at spatial visual question answering (VQA). However, we believe that higher-level 3D-aware tasks, such as articulating dynamic scene changes and motion planning, require a fundamental and explicit 3D understanding beyond current spatial VQA datasets. In this work, we present SpatialPIN, a framework designed to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with priors from multiple 3D foundation models in a zero-shot, training-free manner. Extensive experiments demonstrate that our spatial reasoning-imbued VLM performs well on various forms of spatial VQA and can extend to help in various downstream robotics tasks such as pick and stack and trajectory planning.",
                "authors": "Chenyang Ma, Kai Lu, Ta-Ying Cheng, Niki Trigoni, Andrew Markham",
                "citations": 2
            },
            {
                "title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models",
                "abstract": "Despite recent advances demonstrating vision- language models’ (VLMs) abilities to describe complex relationships among objects in images using natural language, their capability to quantitatively reason about object sizes and distances remains underexplored. In this work, we introduce a manually annotated benchmark of 241 questions across five categories specifically designed for quantitative spatial reasoning, and systematically investigate the performance of SoTA VLMs on this task. Our analysis reveals that questions involving reasoning about distances between objects are particularly challenging for SoTA VLMs; however, some VLMs perform significantly better at this task than others, with an almost 40 points gap between the two best performing models. We also make the surprising observation that the success rate of the top-performing VLM increases by 19 points when a reasoning path using a reference object emerges naturally in the response. Inspired by this observation, we develop a zero-shot prompting technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial questions using references objects as visual cues. Specifically, we demonstrate that instruct- ing VLMs to use reference objects in their reasoning paths significantly improves their quantitative spatial reasoning performance, bypassing the need for external data, architectural modifications, or fine-tuning. Remarkably, by solely using SpatialPrompt, Gemini 1.5 Pro, GPT-4V, and GPT-4o improve by 56.2, 28.5, and 6.7 points on average in Q-Spatial Bench without the need for more data, model architectural modifications, or fine-tuning.",
                "authors": "Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna",
                "citations": 2
            },
            {
                "title": "NEUI at MEDIQA-M3G 2024: Medical VQA through consensus",
                "abstract": "This document describes our solution to the MEDIQA-M3G: Multilingual & Multimodal Medical Answer Generation. To build our solution, we leveraged two pre-trained models, a Visual Language Model (VLM) and a Large Language Model (LLM). We fine-tuned both models using the MEDIQA-M3G and MEDIQA-CORR training datasets, respectively. In the first stage, the VLM provides singular responses for each pair of image & text inputs in a case. In the second stage, the LLM consolidates the VLM responses using it as context among the original text input. By changing the original English case content field in the context component of the second stage to the one in Spanish, we adapt the pipeline to generate submissions in English and Spanish. We performed an ablation study to explore the impact of the different models’ capabilities, such as multimodality and reasoning, on the MEDIQA-M3G task. Our approach favored privacy and feasibility by adopting open-source and self-hosted small models and ranked 4th in English and 2nd in Spanish.",
                "authors": "Ricardo García, Oscar Lithgow-Serrano",
                "citations": 2
            },
            {
                "title": "Toward Open-Set Human Object Interaction Detection",
                "abstract": "This work is oriented toward the task of open-set Human Object Interaction (HOI) detection. The challenge lies in identifying completely new, out-of-domain relationships, as opposed to in-domain ones which have seen improvements in zero-shot HOI detection. To address this challenge, we introduce a simple Disentangled HOI Detection (DHD) model for detecting novel relationships by integrating an open-set object detector with a Visual Language Model (VLM). We utilize a disentangled image-text contrastive learning metric for training and connect the bottom-up visual features to text embeddings through lightweight unary and pair-wise adapters. Our model can benefit from the open-set object detector and the VLM to detect novel action categories and combine actions with novel object categories. We further present the VG-HOI dataset, a comprehensive benchmark with over 17k HOI relationships for open-set scenarios. Experimental results show that our model can detect unknown action classes and combine unknown object classes. Furthermore, it can generalize to over 17k HOI classes while being trained on just 600 HOI classes.",
                "authors": "Ming-Kuan Wu, Yuqi Liu, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji",
                "citations": 2
            },
            {
                "title": "Details Make a Difference: Object State-Sensitive Neurorobotic Task Planning",
                "abstract": "The state of an object reflects its current status or condition and is important for a robot's task planning and manipulation. However, detecting an object's state and generating a state-sensitive plan for robots is challenging. Recently, pre-trained Large Language Models (LLMs) and Vision-Language Models (VLMs) have shown impressive capabilities in generating plans. However, to the best of our knowledge, there is hardly any investigation on whether LLMs or VLMs can also generate object state-sensitive plans. To study this, we introduce an Object State-Sensitive Agent (OSSA), a task-planning agent empowered by pre-trained neural networks. We propose two methods for OSSA: (i) a modular model consisting of a pre-trained vision processing module (dense captioning model, DCM) and a natural language processing model (LLM), and (ii) a monolithic model consisting only of a VLM. To quantitatively evaluate the performances of the two methods, we use tabletop scenarios where the task is to clear the table. We contribute a multimodal benchmark dataset that takes object states into consideration. Our results show that both methods can be used for object state-sensitive tasks, but the monolithic approach outperforms the modular approach. The code for OSSA is available at https://github.com/Xiao-wen-Sun/OSSA",
                "authors": "Xiaowen Sun, Xufeng Zhao, Jae Hee Lee, Wenhao Lu, Matthias Kerzel, Stefan Wermter",
                "citations": 2
            },
            {
                "title": "Smart Vision-Language Reasoners",
                "abstract": "In this article, we investigate vision-language models (VLM) as reasoners. The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning. Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in \\cite{cherian2022deep} as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern. We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to $48\\%$ gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills. End-to-end code is available at https://github.com/smarter-vlm/smarter.",
                "authors": "Denisa Roberts, Lucas Roberts",
                "citations": 2
            },
            {
                "title": "DST-Det: Open-Vocabulary Object Detection via Dynamic Self-Training",
                "abstract": "Open-vocabulary object detection (OVOD) aims to detect the objects beyond the set of classes observed during training. This work presents a simple yet effective strategy that leverages the zero-shot classification ability of pre-trained vision-language models (VLM), such as CLIP, to directly discover proposals of possible novel classes. Unlike previous works that ignore novel classes during training and rely solely on the region proposal network (RPN) for novel object detection, our method selectively filters proposals based on specific design criteria. The resulting sets of identified proposals serve as pseudo-labels of potential novel classes during the training phase. This self-training strategy improves the recall and accuracy of novel classes without requiring additional annotations or datasets. We further propose a simple offline pseudo-label generation strategy to refine the object detector. Empirical evaluations on three datasets, including LVIS, V3Det, and COCO, demonstrate significant improvements over the baseline performance without incurring additional parameters or computational costs during inference. In particular, compared with previous F-VLM, our method achieves a 1.7% improvement on the LVIS dataset. We also achieve over 6.5% improvement on the recent challenging V3Det dataset. When combined with the recent method CLIPSelf, our method also achieves 46.7 novel class AP on COCO without introducing extra data for pertaining.",
                "authors": "Shilin Xu, Xiangtai Li, Size Wu, Wenwei Zhang, Yining Li, Guangliang Cheng, Yunhai Tong, Chen Change Loy",
                "citations": 2
            },
            {
                "title": "NODE-Adapter: Neural Ordinary Differential Equations for Better Vision-Language Reasoning",
                "abstract": "In this paper, we consider the problem of prototype-based vision-language reasoning problem. We observe that existing methods encounter three major challenges: 1) escalating resource demands and prolonging training times, 2) contending with excessive learnable parameters, and 3) fine-tuning based only on a single modality. These challenges will hinder their capability to adapt Vision-Language Models (VLMs) to downstream tasks. Motivated by this critical observation, we propose a novel method called NODE-Adapter, which utilizes Neural Ordinary Differential Equations for better vision-language reasoning. To fully leverage both visual and textual modalities and estimate class prototypes more effectively and accurately, we divide our method into two stages: cross-modal prototype construction and cross-modal prototype optimization using neural ordinary differential equations. Specifically, we exploit VLM to encode hand-crafted prompts into textual features and few-shot support images into visual features. Then, we estimate the textual prototype and visual prototype by averaging the textual features and visual features, respectively, and adaptively combine the textual prototype and visual prototype to construct the cross-modal prototype. To alleviate the prototype bias, we then model the prototype optimization process as an initial value problem with Neural ODEs to estimate the continuous gradient flow. Our extensive experimental results, which cover few-shot classification, domain generalization, and visual reasoning on human-object interaction, demonstrate that the proposed method significantly outperforms existing state-of-the-art approaches.",
                "authors": "Yi Zhang, Chun-Wun Cheng, Ke Yu, Zhihai He, C. Schönlieb, Angelica I. Avilés-Rivero",
                "citations": 2
            },
            {
                "title": "Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image",
                "abstract": "Open-vocabulary 3D object detection (OV-3DDet) aims to localize and recognize both seen and previously unseen object categories within any new 3D scene. While language and vision foundation models have achieved success in handling various open-vocabulary tasks with abundant training data, OV-3DDet faces a significant challenge due to the limited availability of training data. Although some pioneering efforts have integrated vision-language models (VLM) knowledge into OV-3DDet learning, the full potential of these foundational models has yet to be fully exploited. In this paper, we unlock the textual and visual wisdom to tackle the open-vocabulary 3D detection task by leveraging the language and vision foundation models. We leverage a vision foundation model to provide image-wise guidance for discovering novel classes in 3D scenes. Specifically, we utilize a object detection vision foundation model to enable the zero-shot discovery of objects in images, which serves as the initial seeds and filtering guidance to identify novel 3D objects. Additionally, to align the 3D space with the powerful vision-language space, we introduce a hierarchical alignment approach, where the 3D feature space is aligned with the vision-language feature space using a pre-trained VLM at the instance, category, and scene levels. Through extensive experimentation, we demonstrate significant improvements in accuracy and generalization, highlighting the potential of foundation models in advancing open-vocabulary 3D object detection in real-world scenarios.",
                "authors": "Pengkun Jiao, Na Zhao, Jingjing Chen, Yugang Jiang",
                "citations": 2
            },
            {
                "title": "Long-Term and Decadal Sea-Level Trends of the Baltic Sea Using Along-Track Satellite Altimetry",
                "abstract": "One of the main effects of climate change is rising sea levels, which presents challenges due to its geographically heterogenous nature. Often, contradictory results arise from examining different sources of measurement and time spans. This study addresses these issues by analysing both long-term (1995–2022) and decadal (2000–2009 and 2010–2019) sea-level trends in the Baltic Sea. Two independent sources of data, which consist of 13 tide gauge (TG) stations and multi-mission along-track satellite altimetry (SA), are utilized to calculate sea-level trends using the ordinary least-squares method. Given that the Baltic Sea is influenced by geographically varying vertical land motion (VLM), both relative sea level (RSL) and absolute sea level (ASL) trends were examined for the long-term assessment. The results for the long-term ASL show estimates for TG and SA to be 3.3 mm/yr and 3.9 mm/yr, respectively, indicating agreement between sources. Additionally, the comparison of long-term RSL ranges from −2 to 4.5 mm/yr, while ASL varies between 2 and 5.4 mm/yr, as expected due to the VLM. Spatial variation in long-term ASL trends is observed, with higher rates in the northern and eastern regions. Decadal sea-level trends show higher rates, particularly the decade 2000–2009. Comparison with other available sea-level datasets (gridded models) yields comparable results. Therefore, this study evaluates the ability of SA as a reliable source for determining reginal sea-level trends in comparison with TG data.",
                "authors": "M. Mostafavi, A. Ellmann, N. Delpeche-Ellmann",
                "citations": 2
            },
            {
                "title": "LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving",
                "abstract": "Existing Vision-Language Models (VLMs) produce long-term trajectory waypoints or directly control actions based on their perception input and language prompt. However, these VLMs are not explicitly aware of the constraints imposed by the scene or kinematics of the vehicle. As a result, the generated trajectories or control inputs are likely to be unsafe and/or infeasible. In this paper, we introduce LeGo-Drive†, which aims to address these issues. Our key idea is to use the VLM to just predict a goal location based on the given language command and perception input, which is then fed to a downstream differentiable trajectory optimizer with learnable components. We train the VLM and the trajectory optimizer in an end-to-end fashion using a loss function that captures the ego-vehicle’s ability to reach the predicted goal while satisfying safety and kinematic constraints. The gradients during the back-propagation flow through the optimization layer and make the VLM aware of the planner’s capabilities, making more feasible goal predictions. We compare our end-to-end approach with a decoupled framework where the planner is just used at the inference time to drive to the VLM-predicted goal location and report a goal reaching Success Rate of 81%. We demonstrate the versatility of LeGo-Drive† across various driving scenarios and navigation commands, highlighting its potential for practical deployment in autonomous vehicles.",
                "authors": "Pranjal Paul, Anant Garg, Tushar Choudhary, Arun Kumar Singh, K. M. Krishna",
                "citations": 2
            },
            {
                "title": "Vertical Land Motion Due To Present‐Day Ice Loss From Greenland's and Canada's Peripheral Glaciers",
                "abstract": "Greenland's bedrock responds to ongoing ice loss with an elastic vertical land motion (VLM) that is measured by Greenland's Global Navigation Satellite System (GNSS) Network (GNET). The measured VLM also contains other contributions, including the long‐term viscoelastic response of the Earth to the deglaciation of the last glacial period. Greenland's ice sheet (GrIS) produces the most significant contribution to the total VLM. The contribution of peripheral glaciers (PGs) from both Greenland (GrPGs) and Arctic Canada (CanPGs) has not carefully been accounted for in previous GNSS analyses. This is a significant concern, since GNET stations are often closer to PGs than to the ice sheet. We find that, PGs produce significant elastic rebound, especially in North and East Greenland. Across these regions, the PGs produce up to 32% of the elastic rebound. For a few stations in the North, the VLM from PGs is larger than that due to the GrIS.",
                "authors": "D. Berg, V. Barletta, J. Hassan, E. Lippert, W. Colgan, M. Bevis, R. Steffen, S. A. Khan",
                "citations": 2
            },
            {
                "title": "Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension",
                "abstract": "Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs. Our value model and code are available at https://github.com/si0wang/VisVM.",
                "authors": "Wang Xiyao, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Lin Chung-Ching Lin, Lin Kevin, Furong Huang, Lijuan Wang",
                "citations": 2
            },
            {
                "title": "Visual Narratives: Large-scale Hierarchical Classification of Art-historical Images",
                "abstract": "Iconography refers to the methodical study and interpretation of thematic content in the visual arts, distinguishing it, e.g., from purely formal or aesthetic considerations. In iconographic studies, Iconclass is a widely used taxonomy that encapsulates historical, biblical, and literary themes, among others. However, given the hierarchical nature and inherent complexity of such a taxonomy, it is highly desirable to use automated methods for (Iconclass-based) image classification. Previous studies either focused narrowly on certain subsets of narratives or failed to exploit Iconclass’s hierarchical structure. In this paper, we propose a novel approach for Hierarchical Multi-label Classification (HMC) of iconographic concepts in images. We present three strategies, including Language Models (LMs), for the generation of textual image descriptions using keywords extracted from Iconclass. These descriptions are utilized to pre-train a Vision-Language Model (VLM) based on a newly introduced data set of 477,569 images with more than 20,000 Iconclass concepts, far more than considered in previous studies. Furthermore, we present five approaches to multi-label classification, including a novel transformer decoder that leverages hierarchical information from the Iconclass taxonomy. Experimental results show the superiority of this approach over reasonable baselines.",
                "authors": "Matthias Springstein, Stefanie Schneider, J. Rahnama, Julian Stalter, Maximilian Kristen, Eric Müller-Budack, Ralph Ewerth",
                "citations": 2
            },
            {
                "title": "Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination",
                "abstract": "The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on average GSM8K: +3.1%; ASDIV: +3.2%; SVAMP: +6.9%) and the majority of the general-purpose reasoning tasks by 3.2% to 6.0% on average.",
                "authors": "Syeda Nahida Akter, Aman Madaan, Sangwu Lee, Yiming Yang, Eric Nyberg",
                "citations": 2
            },
            {
                "title": "Building Vision-Language Models on Solid Foundations with Masked Distillation",
                "abstract": "Recent advancements in Vision-Language Models (VLMs) have marked a significant leap in bridging the gap between computer vision and natural language processing. However, traditional VLMs, trained through contrastive learning on limited and noisy image-text pairs, often lack the spatial and linguistic understanding to generalize well to dense vision tasks or less common languages. Our approach, Solid Foun-dation CLIP (SF-CLIP), circumvents this issue by implicitly building on the solid visual and language understanding of foundational models trained on vast amounts of unimodal data. SF-CLIP integrates contrastive image-text pretraining with a masked knowledge distillation from large foundational text and vision models. This methodology guides our VLM in developing robust text and image representations. As a result, SF-CLIP shows exceptional zero-shot classification accuracy and enhanced image and text retrieval capabilities, setting a new state of the art for ViT-B/16 trained on YFCC15M and CC12M.Moreover, the dense per-patch supervision enhances our zero-shot and linear probe performance in semantic segmentation tasks. A remarkable aspect of our model is its multilingual proficiency, evidenced by strong retrieval results in multiple languages despite being trained predominantly on English data. We achieve all of these improvements without sacrificing the training efficiency through our selective application of masked distillation and the inheritance of teacher word embeddings.",
                "authors": "Sepehr Sameni, Kushal Kafle, Hao Tan, Simon Jenni",
                "citations": 2
            },
            {
                "title": "Jailbreaking Text-to-Image Models with LLM-Based Agents",
                "abstract": "Recent advancements have significantly improved automated task-solving capabilities using autonomous agents powered by large language models (LLMs). However, most LLM-based agents focus on dialogue, programming, or specialized domains, leaving their potential for addressing generative AI safety tasks largely unexplored. In this paper, we propose Atlas, an advanced LLM-based multi-agent framework targeting generative AI models, specifically focusing on jailbreak attacks against text-to-image (T2I) models with built-in safety filters. Atlas consists of two agents, namely the mutation agent and the selection agent, each comprising four key modules: a vision-language model (VLM) or LLM brain, planning, memory, and tool usage. The mutation agent uses its VLM brain to determine whether a prompt triggers the T2I model's safety filter. It then collaborates iteratively with the LLM brain of the selection agent to generate new candidate jailbreak prompts with the highest potential to bypass the filter. In addition to multi-agent communication, we leverage in-context learning (ICL) memory mechanisms and the chain-of-thought (COT) approach to learn from past successes and failures, thereby enhancing Atlas's performance. Our evaluation demonstrates that Atlas successfully jailbreaks several state-of-the-art T2I models equipped with multi-modal safety filters in a black-box setting. Additionally, Atlas outperforms existing methods in both query efficiency and the quality of generated images. This work convincingly demonstrates the successful application of LLM-based agents in studying the safety vulnerabilities of popular text-to-image generation models. We urge the community to consider advanced techniques like ours in response to the rapidly evolving text-to-image generation field.",
                "authors": "Yingkai Dong, Zheng Li, Xiangtao Meng, Ning Yu, Shanqing Guo",
                "citations": 2
            },
            {
                "title": "LLaVA-Ultra: Large Chinese Language and Vision Assistant for Ultrasound",
                "abstract": "Multimodal Large Language Model (MLLM) has recently garnered attention as a prominent research focus. By harnessing powerful LLM, it facilitates a transition of conversational generative AI from unimodal text to performing multimodal tasks. This boom begins to significantly impact medical field. However, general visual language model (VLM) lacks sophisticated comprehension for medical visual question answering (Med-VQA). Even models specifically tailored for medical domain tend to produce vague answers with weak visual relevance. In this paper, we propose a fine-grained adaptive VLM architecture for Chinese medical visual conversations through parameter-efficient tuning. Specifically, we devise a fusion module with fine-grained vision encoders to achieve enhancement for subtle medical visual semantics. Then we note data redundancy common to medical scenes is ignored in most prior works. In cases of a single text paired with multiple figures, we utilize weighted scoring with knowledge distillation to adaptively screen valid images mirroring text descriptions. For execution, we leverage a large-scale multimodal Chinese ultrasound dataset obtained from the hospital. We create instruction-following data based on text from professional doctors, which ensures effective tuning. With enhanced model and quality data, our Large Chinese Language and Vision Assistant for Ultrasound (LLaVA-Ultra) shows strong capability and robustness to medical scenarios. On three Med-VQA datasets, LLaVA-Ultra surpasses previous state-of-the-art models on various metrics.",
                "authors": "Xuechen Guo, Wenhao Chai, Shiyan Li, Gaoang Wang",
                "citations": 2
            },
            {
                "title": "Enhancing Presentation Slide Generation by LLMs with a Multi-Staged End-to-End Approach",
                "abstract": "Generating presentation slides from a long document with multimodal elements such as text and images is an important task. This is time consuming and needs domain expertise if done manually. Existing approaches for generating a rich presentation from a document are often semi-automatic or only put a flat summary into the slides ignoring the importance of a good narrative. In this paper, we address this research gap by proposing a multi-staged end-to-end model which uses a combination of LLM and VLM. We have experimentally shown that compared to applying LLMs directly with state-of-the-art prompting, our proposed multi-staged solution is better in terms of automated metrics and human evaluation.",
                "authors": "Sambaran Bandyopadhyay, Himanshu Maheshwari, Anandhavelu Natarajan, Apoorv Saxena",
                "citations": 2
            },
            {
                "title": "Medical Image Interpretation with Large Multimodal Models",
                "abstract": "This working note documents the participation of CS_Morgan in the ImageCLEFmedical 2024 Caption subtasks, focusing on Caption Prediction and Concept Detection challenges. The primary objectives included training, validating, and testing multimodal Artificial Intelligence (AI) models intended to automate the process of generating captions and identifying multi-concepts of radiology images. The dataset used is a subset of the Radiology Objects in COntext version 2 (ROCOv2) dataset and contains image-caption pairs and corresponding Unified Medical Language System (UMLS) concepts. To address the caption prediction challenge, different variants of the Large Language and Vision Assistant (LLaVA) models were experimented with, tailoring them for the medical domain. Additionally, a lightweight Large Multimodal Model (LMM), and MoonDream2, a small Vision Language Model (VLM), were explored. The former is the instruct variant of the Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS (IDEFICS) 9B obtained through quantization. Besides LMMs, conventional encoder-decoder models like Vision Generative Pre-trained Transformer 2 (visionGPT2) and Convolutional Neural Network-Transformer (CNN-Transformer) architectures were considered. Consequently, this enabled 10 submissions for the caption prediction task, with the first submission of LLaVA 1.6 on the Mistral 7B weights securing the 2nd position among the participants. This model was adapted using 40.1M parameters and achieved the best performance on the test data across the performance metrics of BERTScore (0.628059), ROUGE (0.250801), BLEU-1 (0.209298), BLEURT (0.317385), METEOR (0.092682), CIDEr (0.245029), and RefCLIPScore (0.815534). For the concept detection task, our single submission based on the ConvMixer architecture—a hybrid approach leveraging CNN and Transformer advantages—ranked 9th with an F1-score of 0.107645. Overall, the evaluations on the test data for the caption prediction task submissions suggest that LMMs, quantized LMMs, and small VLMs, when adapted and selectively fine-tuned using fewer parameters, have ample potential for understanding medical concepts present in images.",
                "authors": "Mahmudul Hoque, Md. Rakibul Hasan, Md. Ismail Siddiqi Emon, Fahmi Khalifa, Md Mahmudur Rahman",
                "citations": 2
            },
            {
                "title": "Vision-language model-driven scene understanding and robotic object manipulation",
                "abstract": "Humans often use natural language instructions to control and interact with robots for task execution. This poses a big challenge to robots that need to not only parse and understand human instructions but also realise semantic understanding of an unknown environment and its constituent elements. To address this challenge, this study presents a vision-language model (VLM)-driven approach to scene understanding of an unknown environment to enable robotic object manipulation. Given language instructions, a pre-tained vision-language model built on open-sourced Llama2-chat (7B) as the language model backbone is adopted for image description and scene understanding, which translates visual information into text descriptions of the scene. Next, a zero-shot-based approach to fine-grained visual grounding and object detection is developed to extract and localise objects of interest from the scene task. Upon 3D reconstruction and pose estimate establishment of the object, a code-writing large language model (LLM) is adopted to generate high-level control codes and link language instructions with robot actions for downstream tasks. The performance of the developed approach is experimentally validated through table-top object manipulation by a robot.",
                "authors": "Sichao Liu, Jianjing Zhang, Robert X. Gao, X. Wang, Lihui Wang",
                "citations": 2
            },
            {
                "title": "Analyzing the Roles of Language and Vision in Learning from Limited Data",
                "abstract": "Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoning.",
                "authors": "Allison Chen, Ilia Sucholutsky, Olga Russakovsky, Thomas L. Griffiths",
                "citations": 2
            },
            {
                "title": "Decompose and Compare Consistency: Measuring VLMs’ Answer Reliability via Task-Decomposition Consistency Comparison",
                "abstract": "Despite tremendous advancements, current state-of-the-art Vision-Language Models (VLMs) are still far from perfect. They tend to hallucinate and may generate biased responses. In such circumstances, having a way to assess the reliability of a given response generated by a VLM is quite useful. Existing methods, such as estimating uncertainty using answer likelihoods or prompt-based confidence generation, often suffer from overconfidence. Other methods use self-consistency comparison but are affected by confirmation biases. To alleviate these, we propose Decompose and Compare Consistency (DeCC) for reliability measurement. By comparing the consistency between the direct answer generated using the VLM’s internal reasoning process, and the indirect answers obtained by decomposing the question into sub-questions and reasoning over the sub-answers produced by the VLM, DeCC measures the reliability of VLM’s direct answer. Experiments across six vision-language tasks with three VLMs show DeCC’s reliability estimation achieves better correlation with task accuracy compared to the existing methods.",
                "authors": "Qian Yang, Weixiang Yan, Aishwarya Agrawal",
                "citations": 2
            },
            {
                "title": "Test-Time Zero-Shot Temporal Action Localization",
                "abstract": "Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate actions in untrimmed videos unseen during training. Existing ZS-TAL methods involve fine-tuning a model on a large amount of annotated training data. While effective, training-based ZS-TAL approaches assume the availability of labeled data for supervised learning, which can be impractical in some applications. Furthermore, the training process naturally induces a domain bias into the learned model, which may adversely affect the model's generalization ability to arbitrary videos. These considerations prompt us to approach the ZS-TAL problem from a radically novel perspective, relaxing the requirement for training data. To this aim, we introduce a novel method that performs Test-Time adaptation for Temporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trained Vision and Language Model (VLM). T3AL operates in three steps. First, a video-level pseudo-label of the action category is computed by aggregating information from the entire video. Then, action localization is performed adopting a novel procedure inspired by self-supervised learning. Finally, frame-level textual descriptions extracted with a state-of-the-art captioning model are employed for refining the action region proposals. We validate the effectiveness of T3AL by conducting experiments on the THUMOS14 and the ActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantly outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the benefit of a test-time adaptation approach.",
                "authors": "Benedetta Liberatori, Alessandro Conti, P. Rota, Yiming Wang, Elisa Ricci",
                "citations": 2
            },
            {
                "title": "Navi2Gaze: Leveraging Foundation Models for Navigation and Target Gazing",
                "abstract": "Task-aware navigation continues to be a challenging area of research, especially in scenarios involving open vocabulary. Previous studies primarily focus on finding suitable locations for task completion, often overlooking the importance of the robot's pose. However, the robot's orientation is crucial for successfully completing tasks because of how objects are arranged (e.g., to open a refrigerator door). Humans intuitively navigate to objects with the right orientation using semantics and common sense. For instance, when opening a refrigerator, we naturally stand in front of it rather than to the side. Recent advances suggest that Vision-Language Models (VLMs) can provide robots with similar common sense. Therefore, we develop a VLM-driven method called Navigation-to-Gaze (Navi2Gaze) for efficient navigation and object gazing based on task descriptions. This method uses the VLM to score and select the best pose from numerous candidates automatically. In evaluations on multiple photorealistic simulation benchmarks, Navi2Gaze significantly outperforms existing approaches by precisely determining the optimal orientation relative to target objects, resulting in a 68.8% reduction in Distance to Goal (DTG). Real-world video demonstrations can be found on the supplementary website",
                "authors": "Jun Zhu, Zihao Du, Haotian Xu, Fengbo Lan, Zilong Zheng, Bo Ma, Shengjie Wang, Tao Zhang",
                "citations": 2
            },
            {
                "title": "RS-MoE: Mixture of Experts for Remote Sensing Image Captioning and Visual Question Answering",
                "abstract": "Remote Sensing Image Captioning (RSIC) presents unique challenges and plays a critical role in applications. Traditional RSIC methods often struggle to produce rich and diverse descriptions. Recently, with advancements in VLMs, efforts have emerged to integrate these models into the remote sensing domain and to introduce descriptive datasets specifically designed to enhance VLM training. This paper proposes RS-MoE, a first Mixture of Expert based VLM specifically customized for remote sensing domain. Unlike traditional MoE models, the core of RS-MoE is the MoE Block, which incorporates a novel Instruction Router and multiple lightweight Large Language Models (LLMs) as expert models. The Instruction Router is designed to generate specific prompts tailored for each corresponding LLM, guiding them to focus on distinct aspects of the RSIC task. This design not only allows each expert LLM to concentrate on a specific subset of the task, thereby enhancing the specificity and accuracy of the generated captions, but also improves the scalability of the model by facilitating parallel processing of sub-tasks. Additionally, we present a two-stage training strategy for tuning our RS-MoE model to prevent performance degradation due to sparsity. We fine-tuned our model on the RSICap dataset using our proposed training strategy. Experimental results on the RSICap dataset, along with evaluations on other traditional datasets where no additional fine-tuning was applied, demonstrate that our model achieves state-of-the-art performance in generating precise and contextually relevant captions. Notably, our RS-MoE-1B variant achieves performance comparable to 13B VLMs, demonstrating the efficiency of our model design. Moreover, our model demonstrates promising generalization capabilities by consistently achieving state-of-the-art performance on the Remote Sensing Visual Question Answering (RSVQA) task.",
                "authors": "Hui Lin, Danfeng Hong, Shuhang Ge, Chuyao Luo, Kai Jiang, Hao Jin, Congcong Wen",
                "citations": 2
            },
            {
                "title": "Sonic VisionLM: Playing Sound with Vision Language Models",
                "abstract": "There has been a growing interest in the task of generating sound for silent videos, primarily because of its prac-ticality in streamlining video post-production. However, existing methods for video-sound generation attempt to di-rectly create sound from visual representations, which can be challenging due to the difficulty of aligning visual rep-resentations with audio representations. In this paper, we present Sonic VisionLM, a novel framework aimed at gen-erating a wide range of sound effects by leveraging vision-language models(VLMs). Instead of generating audio di-rectly from video, we use the capabilities of powerful VLMs. When provided with a silent video, our approach first iden-tifies events within the video using a VLM to suggest pos-sible sounds that match the video content. This shift in approach transforms the challenging task of aligning image and audio into more well-studied sub-problems of aligning image-to-text and text-to-audio through the popular diffusion models. To improve the quality of audio recommen-dations with LLMs, we have collected an extensive dataset that maps text descriptions to specific sound effects and de-veloped a time-controlled audio adapter. Our approach surpasses current state-of-the-art methods for converting video to audio, enhancing synchronization with the visuals, and improving alignment between audio and video components. Project page: https://yusiissy.github.io/SonicVisionLM.github.io/",
                "authors": "Zhifeng Xie, Shengye Yu, Qile He, Mengtian Li",
                "citations": 2
            },
            {
                "title": "VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions",
                "abstract": "Predicting future trajectories for other road agents is an essential task for autonomous vehicles. Established trajectory prediction methods primarily use agent tracks generated by a detection and tracking system and HD map as inputs. In this work, we propose a novel method that also incorporates visual input from surround-view cameras, allowing the model to utilize visual cues such as human gazes and gestures, road conditions, vehicle turn signals, etc, which are typically hidden from the model in prior methods. Furthermore, we use textual descriptions generated by a Vision-Language Model (VLM) and refined by a Large Language Model (LLM) as supervision during training to guide the model on what to learn from the input data. Despite using these extra inputs, our method achieves a latency of 53 ms, making it feasible for real-time processing, which is significantly faster than that of previous single-agent prediction methods with similar performance. Our experiments show that both the visual inputs and the textual descriptions contribute to improvements in trajectory prediction performance, and our qualitative analysis highlights how the model is able to exploit these additional inputs. Lastly, in this work we create and release the nuScenes-Text dataset, which augments the established nuScenes dataset with rich textual annotations for every scene, demonstrating the positive impact of utilizing VLM on trajectory prediction. Our project page is at https://moonseokha.github.io/VisionTrap/",
                "authors": "Seokha Moon, Hyun Woo, Hongbeen Park, Haeji Jung, R. Mahjourian, Hyung-Gun Chi, Hyerin Lim, Sangpil Kim, Jinkyu Kim",
                "citations": 2
            },
            {
                "title": "PuzzleAvatar: Assembling 3D Avatars from Personal Albums",
                "abstract": "\n Generating\n personalized\n 3D avatars is crucial for AR/VR. However, recent text-to-3D methods that generate avatars for celebrities or fictional characters, struggle with everyday people. Methods for faithful reconstruction typically require full-body images in controlled settings. What if users could just upload their personal \"OOTD\" (Outfit Of The Day) photo collection and get a faithful avatar in return? The challenge is that such casual photo collections contain diverse poses, challenging viewpoints, cropped views, and occlusion (albeit with a consistent outfit, accessories and hairstyle). We address this novel \"\n Album2Human\n \" task by developing\n PuzzleAvatar\n , a novel model that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD album, bypassing the challenging estimation of body and camera pose. To this end, we fine-tune a foundational vision-language model (VLM) on such photos, encoding the appearance, identity, garments, hairstyles, and accessories of a person into separate learned tokens, instilling these cues into the VLM. In effect, we exploit the learned tokens as \"puzzle pieces\" from which we assemble a faithful, personalized 3D avatar. Importantly, we can customize avatars by simply inter-changing tokens. As a benchmark for this new task, we create a new dataset, called\n PuzzleIOI\n , with 41 subjects in a total of nearly 1k OOTD configurations, in challenging partial photos with paired ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique scalability to album photos, and demonstrating strong robustness. Our code and data are publicly available for research purpose at\n puzzleavatar.is.tue.mpg.de\n",
                "authors": "Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, Michael J. Black",
                "citations": 2
            },
            {
                "title": "AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation",
                "abstract": "Text-to-Image (T2I) diffusion models have achieved remarkable success in image generation. Despite their progress, challenges remain in both prompt-following ability, image quality and lack of high-quality datasets, which are essential for refining these models. As acquiring labeled data is costly, we introduce AGFSync, a framework that enhances T2I diffusion models through Direct Preference Optimization (DPO) in a fully AI-driven approach. AGFSync utilizes Vision-Language Models (VLM) to assess image quality across style, coherence, and aesthetics, generating feedback data within an AI-driven loop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and SDXL-base, our extensive experiments on the TIFA dataset demonstrate notable improvements in VQA scores, aesthetic evaluations, and performance on the HPSv2 benchmark, consistently outperforming the base models. AGFSync's method of refining T2I diffusion models paves the way for scalable alignment techniques. Our code and dataset are publicly available at https://anjingkun.github.io/AGFSync.",
                "authors": "Jingkun An, Yinghao Zhu, Zongjian Li, Haoran Feng, Bohua Chen, Yemin Shi, Chengwei Pan",
                "citations": 2
            },
            {
                "title": "VG4D: Vision-Language Model Goes 4D Video Recognition",
                "abstract": "Understanding the real world through point cloud video is a crucial aspect of robotics and autonomous driving systems. However, prevailing methods for 4D point cloud recognition have limitations due to sensor resolution, which leads to a lack of detailed information. Recent advances have shown that Vision-Language Models (VLM) pre-trained on web-scale text-image datasets can learn fine-grained visual concepts that can be transferred to various downstream tasks. However, effectively integrating VLM into the domain of 4D point clouds remains an unresolved problem. In this work, we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pretrained models to a 4D point cloud network. Our approach involves aligning the 4D encoder’s representation with a VLM learning a shared visual and text space from training on large-scale image-text pairs. By transferring the knowledge of the VLM to the 4D encoder and combining the VLM, our VG4D achieves improved recognition performance. To enhance the 4D encoder, we modernize the classic dynamic point cloud backbone and propose an improved version of PSTNet, im-PSTNet, which can efficiently model point cloud videos. Experiments demonstrate that our method achieves state-of-the-art performance for action recognition on both NTU RGB+D 60 dataset and NTU RGB+D 120 dataset.",
                "authors": "Zhichao Deng, Xiangtai Li, Xia Li, Yunhai Tong, Shen Zhao, Mengyuan Liu",
                "citations": 2
            },
            {
                "title": "Fast and Lightweight Vision-Language Model for Adversarial Traffic Sign Detection",
                "abstract": "Several attacks have been proposed against autonomous vehicles and their subsystems that are powered by machine learning (ML). Road sign recognition models are especially heavily tested under various adversarial ML attack settings, and they have proven to be vulnerable. Despite the increasing research on adversarial ML attacks against road sign recognition models, there is little to no focus on defending against these attacks. In this paper, we propose the first defense method specifically designed for autonomous vehicles to detect adversarial ML attacks targeting road sign recognition models, which is called ViLAS (Vision-Language Model for Adversarial Traffic Sign Detection). The proposed defense method is based on a custom, fast, lightweight, and salable vision-language model (VLM) and is compatible with any existing traffic sign recognition system. Thanks to the orthogonal information coming from the class label text data through the language model, ViLAS leverages image context in addition to visual data for highly effective attack detection performance. In our extensive experiments, we show that our method consistently detects various attacks against different target models with high true positive rates while satisfying very low false positive rates. When tested against four state-of-the-art attacks targeting four popular action recognition models, our proposed detector achieves an average AUC of 0.94. This result achieves a 25.3% improvement over a state-of-the-art defense method proposed for generic image attack detection, which attains an average AUC of 0.75. We also show that our custom VLM is more suitable for an autonomous vehicle compared to the popular off-the-shelf VLM and CLIP in terms of speed (4.4 vs. 9.3 milliseconds), space complexity (0.36 vs. 1.6 GB), and performance (0.94 vs. 0.43 average AUC).",
                "authors": "Furkan Mumcu, Yasin Yılmaz",
                "citations": 2
            },
            {
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration",
                "abstract": "Vision-Language Models (VLMs) have demonstrated impressive performance across a versatile set of tasks. A key challenge in accelerating VLMs is storing and accessing the large Key-Value (KV) cache that encodes long visual contexts, such as images or videos. While existing KV cache compression methods are effective for Large Language Models (LLMs), directly migrating them to VLMs yields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache, a novel KV cache compression recipe tailored for accelerating VLM inference. In this paper, we first investigate the unique sparsity pattern of VLM attention by distinguishing visual and text tokens in prefill and decoding phases. Based on these observations, we introduce a layer-adaptive sparsity-aware cache budget allocation method that effectively distributes the limited cache budget across different layers, further reducing KV cache size without compromising accuracy. Additionally, we develop a modality-aware token scoring policy to better evaluate the token importance. Empirical results on multiple benchmark datasets demonstrate that retaining only 10% of KV cache achieves accuracy comparable to that with full cache. In a speed benchmark, our method accelerates end-to-end latency of generating 100 tokens by up to 2.33x and speeds up decoding by up to 7.08x, while reducing the memory footprint of KV cache in GPU by 90%.",
                "authors": "Dezhan Tu, Danylo Vashchilenko, Yuzhe Lu, Panpan Xu",
                "citations": 1
            },
            {
                "title": "LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models",
                "abstract": "Recent advances in large vision-language models (VLMs) typically employ vision encoders based on the Vision Transformer (ViT) architecture. The division of the images into patches by ViT results in a fragmented perception, thereby hindering the visual understanding capabilities of VLMs. In this paper, we propose an innovative enhancement to address this limitation by introducing a Scene Graph Expression (SGE) module in VLMs. This module extracts and structurally expresses the complex semantic information within images, thereby improving the foundational perception and understanding abilities of VLMs. Extensive experiments demonstrate that integrating our SGE module significantly enhances the VLM's performance in vision-language tasks, indicating its effectiveness in preserving intricate semantic details and facilitating better visual understanding.",
                "authors": "Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng",
                "citations": 1
            },
            {
                "title": "Visual Grounding for Object-Level Generalization in Reinforcement Learning",
                "abstract": "Generalization is a pivotal challenge for agents following natural language instructions. To approach this goal, we leverage a vision-language model (VLM) for visual grounding and transfer its vision-language knowledge into reinforcement learning (RL) for object-centric tasks, which makes the agent capable of zero-shot generalization to unseen objects and instructions. By visual grounding, we obtain an object-grounded confidence map for the target object indicated in the instruction. Based on this map, we introduce two routes to transfer VLM knowledge into RL. Firstly, we propose an object-grounded intrinsic reward function derived from the confidence map to more effectively guide the agent towards the target object. Secondly, the confidence map offers a more unified, accessible task representation for the agent's policy, compared to language embeddings. This enables the agent to process unseen objects and instructions through comprehensible visual confidence maps, facilitating zero-shot object-level generalization. Single-task experiments prove that our intrinsic reward significantly improves performance on challenging skill learning. In multi-task experiments, through testing on tasks beyond the training set, we show that the agent, when provided with the confidence map as the task representation, possesses better generalization capabilities than language-based conditioning. The code is available at https://github.com/PKU-RL/COPL.",
                "authors": "Haobin Jiang, Zongqing Lu",
                "citations": 1
            },
            {
                "title": "Vision-Language Models Meet Meteorology: Developing Models for Extreme Weather Events Detection with Heatmaps",
                "abstract": "Real-time detection and prediction of extreme weather protect human lives and infrastructure. Traditional methods rely on numerical threshold setting and manual interpretation of weather heatmaps with Geographic Information Systems (GIS), which can be slow and error-prone. Our research redefines Extreme Weather Events Detection (EWED) by framing it as a Visual Question Answering (VQA) problem, thereby introducing a more precise and automated solution. Leveraging Vision-Language Models (VLM) to simultaneously process visual and textual data, we offer an effective aid to enhance the analysis process of weather heatmaps. Our initial assessment of general-purpose VLMs (e.g., GPT-4-Vision) on EWED revealed poor performance, characterized by low accuracy and frequent hallucinations due to inadequate color differentiation and insufficient meteorological knowledge. To address these challenges, we introduce ClimateIQA, the first meteorological VQA dataset, which includes 8,760 wind gust heatmaps and 254,040 question-answer pairs covering four question types, both generated from the latest climate reanalysis data. We also propose Sparse Position and Outline Tracking (SPOT), an innovative technique that leverages OpenCV and K-Means clustering to capture and depict color contours in heatmaps, providing ClimateIQA with more accurate color spatial location information. Finally, we present Climate-Zoo, the first meteorological VLM collection, which adapts VLMs to meteorological applications using the ClimateIQA dataset. Experiment results demonstrate that models from Climate-Zoo substantially outperform state-of-the-art general VLMs, achieving an accuracy increase from 0% to over 90% in EWED verification. The datasets and models in this study are publicly available for future climate science research: https://github.com/AlexJJJChen/Climate-Zoo.",
                "authors": "Jian Chen, Peilin Zhou, Y. Hua, Dading Chong, Meng Cao, Yaowei Li, Zixuan Yuan, Bing Zhu, Junwei Liang",
                "citations": 1
            },
            {
                "title": "Enhancing Outlier Knowledge for Few-Shot Out-of-Distribution Detection with Extensible Local Prompts",
                "abstract": "Out-of-Distribution (OOD) detection, aiming to distinguish outliers from known categories, has gained prominence in practical scenarios. Recently, the advent of vision-language models (VLM) has heightened interest in enhancing OOD detection for VLM through few-shot tuning. However, existing methods mainly focus on optimizing global prompts, ignoring refined utilization of local information with regard to outliers. Motivated by this, we freeze global prompts and introduce a novel coarse-to-fine tuning paradigm to emphasize regional enhancement with local prompts. Our method comprises two integral components: global prompt guided negative augmentation and local prompt enhanced regional regularization. The former utilizes frozen, coarse global prompts as guiding cues to incorporate negative augmentation, thereby leveraging local outlier knowledge. The latter employs trainable local prompts and a regional regularization to capture local information effectively, aiding in outlier identification. We also propose regional-related metric to empower the enrichment of OOD detection. Moreover, since our approach explores enhancing local prompts only, it can be seamlessly integrated with trained global prompts during inference to boost the performance. Comprehensive experiments demonstrate the effectiveness and potential of our method. Notably, our method reduces average FPR95 by 5.17% against state-of-the-art method in 4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot results of previous methods.",
                "authors": "Fanhu Zeng, Zhen Cheng, Fei Zhu, Xu-Yao Zhang",
                "citations": 1
            },
            {
                "title": "OVExp: Open Vocabulary Exploration for Object-Oriented Navigation",
                "abstract": "Object-oriented embodied navigation aims to locate specific objects, defined by category or depicted in images. Existing methods often struggle to generalize to open vocabulary goals without extensive training data. While recent advances in Vision-Language Models (VLMs) offer a promising solution by extending object recognition beyond predefined categories, efficient goal-oriented exploration becomes more challenging in an open vocabulary setting. We introduce OVExp, a learning-based framework that integrates VLMs for Open-Vocabulary Exploration. OVExp constructs scene representations by encoding observations with VLMs and projecting them onto top-down maps for goal-conditioned exploration. Goals are encoded in the same VLM feature space, and a lightweight transformer-based decoder predicts target locations while maintaining versatile representation abilities. To address the impracticality of fusing dense pixel embeddings with full 3D scene reconstruction for training, we propose constructing maps using low-cost semantic categories and transforming them into CLIP's embedding space via the text encoder. The simple but effective design of OVExp significantly reduces computational costs and demonstrates strong generalization abilities to various navigation settings. Experiments on established benchmarks show OVExp outperforms previous zero-shot methods, can generalize to diverse scenes, and handle different goal modalities.",
                "authors": "Meng Wei, Tai Wang, Yilun Chen, Hanqing Wang, Jiangmiao Pang, Xihui Liu",
                "citations": 1
            },
            {
                "title": "SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework with Interactive Vision-Language Models",
                "abstract": "With the continuous advancement of vision language models (VLMs) technology, remarkable research achievements have emerged in the dermatology field, the fourth most prevalent human disease category. However, despite these advancements, VLM still faces\"hallucination\"in dermatological diagnosis, and due to the inherent complexity of dermatological conditions, existing tools offer relatively limited support for user comprehension. We propose SkinGEN, a diagnosis-to-generation framework that leverages the stable diffusion (SD) method to generate reference demonstrations from diagnosis results provided by VLM, thereby enhancing the visual explainability for users. Through extensive experiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for skin condition image generation. We conduct a user study with 32 participants evaluating both the system performance and explainability. Results demonstrate that SkinGEN significantly improves users' comprehension of VLM predictions and fosters increased trust in the diagnostic process. This work paves the way for more transparent and user-centric VLM applications in dermatology and beyond.",
                "authors": "Bo Lin, Yingjing Xu, Xuanwen Bao, Zhou Zhao, Zuyong Zhang, Zhouyang Wang, Jie Zhang, Shuiguang Deng, Jianwei Yin",
                "citations": 1
            },
            {
                "title": "Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis",
                "abstract": "Vision-language models (VLMs) have shown impressive zero-and few-shot performance on real-world visual question answering (VQA) benchmarks, alluding to their capabilities as visual reasoning engines. However, the benchmarks being used conflate \"pure\" visual reasoning with world knowledge, and also have questions that involve a limited number of reasoning steps. Thus, it remains unclear whether a VLM’s apparent visual reasoning performance is due to its world knowledge, or due to actual visual reasoning capabilities.To clarify this ambiguity, we systematically benchmark and dissect the zero-shot visual reasoning capabilities of VLMs through synthetic datasets that require minimal world knowledge, and allow for analysis over a broad range of reasoning steps. We focus on two novel aspects of zero-shot visual reasoning: i) evaluating the impact of conveying scene information as either visual embeddings or purely textual scene descriptions to the underlying large language model (LLM) of the VLM, and ii) comparing the effectiveness of chain-of-thought prompting to standard prompting for zero-shot visual reasoning.We find that the underlying LLMs, when provided textual scene descriptions, consistently perform better compared to being provided visual embeddings. In particular, 18% higher accuracy is achieved on the PTR dataset. We also ∼find that CoT prompting performs marginally better than standard prompting only for the comparatively large GPT-3.5-Turbo (175B) model, and does worse for smaller-scale models. This suggests the emergence of CoT abilities for visual reasoning in LLMs at larger scales even when world knowledge is limited. Overall, we find limitations in the abilities of VLMs and LLMs for more complex visual reasoning, and highlight the important role that LLMs can play in visual reasoning.",
                "authors": "Aishik Nagar, Shantanu Jaiswal, Cheston Tan",
                "citations": 1
            },
            {
                "title": "Open-Vocabulary Action Localization with Iterative Visual Prompting",
                "abstract": "Video action localization aims to find the timings of specific actions from a long video. Although existing learning-based approaches have been successful, they require annotating videos, which comes with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging off-the-shelf vision-language models (VLMs). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames and create a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start and end of the action. Iterating this process by narrowing a sampling time window results in finding the specific frames corresponding to the start and end of an action. We demonstrate that this technique yields reasonable performance, achieving results comparable to state-of-the-art zero-shot action localization. These results illustrate a practical extension of VLMs for understanding videos. A sample code is available at https://microsoft.github.io/VLM-Video-Action-Localization/.",
                "authors": "Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi",
                "citations": 1
            },
            {
                "title": "EvoChart: A Benchmark and a Self-Training Approach Towards Real-World Chart Understanding",
                "abstract": "Chart understanding enables automated data analysis for humans, which requires models to achieve highly accurate visual comprehension. While existing Visual Language Models (VLMs) have shown progress in chart understanding, the lack of high-quality training data and comprehensive evaluation benchmarks hinders VLM chart comprehension. In this paper, we introduce EvoChart, a novel self-training method for generating synthetic chart data to enhance VLMs' capabilities in real-world chart comprehension. We also propose EvoChart-QA, a noval benchmark for measuring models' chart comprehension abilities in real-world scenarios. Specifically, EvoChart is a unique self-training data synthesis approach that simultaneously produces high-quality training corpus and a high-performance chart understanding model. EvoChart-QA consists of 650 distinct real-world charts collected from 140 different websites and 1,250 expert-curated questions that focus on chart understanding. Experimental results on various open-source and proprietary VLMs tested on EvoChart-QA demonstrate that even the best proprietary model, GPT-4o, achieves only 49.8% accuracy. Moreover, the EvoChart method significantly boosts the performance of open-source VLMs on real-world chart understanding tasks, achieving 54.2% accuracy on EvoChart-QA.",
                "authors": "Muye Huang, Lai Han, Xinyu Zhang, Wenjun Wu, Jie Ma, Lingling Zhang, Jun Liu",
                "citations": 1
            },
            {
                "title": "Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems",
                "abstract": "Recent developments in vision language models (VLM) have shown great potential for diverse applications related to image understanding. In this study, we have explored state-of-the-art VLM models for vision-based transportation engineering tasks such as image classification and object detection. The image classification task involves congestion detection and crack identification, whereas, for object detection, helmet violations were identified. We have applied open-source models such as CLIP, BLIP, OWL-ViT, Llava-Next, and closed-source GPT-4o to evaluate the performance of these state-of-the-art VLM models to harness the capabilities of language understanding for vision-based transportation tasks. These tasks were performed by applying zero-shot prompting to the VLM models, as zero-shot prompting involves performing tasks without any training on those tasks. It eliminates the need for annotated datasets or fine-tuning for specific tasks. Though these models gave comparative results with benchmark Convolutional Neural Networks (CNN) models in the image classification tasks, for object localization tasks, it still needs improvement. Therefore, this study provides a comprehensive evaluation of the state-of-the-art VLM models highlighting the advantages and limitations of the models, which can be taken as the baseline for future improvement and wide-scale implementation.",
                "authors": "Sanjita Prajapati, Tanu Singh, Chinmay Hegde, Pranamesh Chakraborty",
                "citations": 1
            },
            {
                "title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models",
                "abstract": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be degraded by the integration of the vision module compared to its LLM backbone. We investigate this phenomenon, dubbed as ''safety alignment degradation'' in this paper, and show that the challenge arises from the representation gap that emerges when introducing vision modality to VLMs. In particular, we show that the representations of multi-modal inputs shift away from that of text-only inputs which represent the distribution that the LLM backbone is optimized for. At the same time, the safety alignment capabilities, initially developed within the textual embedding space, do not successfully transfer to this new multi-modal representation space. To reduce safety alignment degradation, we introduce Cross-Modality Representation Manipulation (CMRM), an inference time representation intervention method for recovering the safety alignment ability that is inherent in the LLM backbone of VLMs, while simultaneously preserving the functional capabilities of VLMs. The empirical results show that our framework significantly recovers the alignment ability that is inherited from the LLM backbone with minimal impact on the fluency and linguistic capabilities of pre-trained VLMs even without additional training. Specifically, the unsafe rate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as 3.15% with only inference-time intervention. WARNING: This paper contains examples of toxic or harmful language.",
                "authors": "Qin Liu, Chao Shang, Ling Liu, Nikolaos Pappas, Jie Ma, Neha Ann John, Srikanth Doss, Lluís Màrquez i Villodre, Miguel Ballesteros, Yassine Benajiba",
                "citations": 1
            },
            {
                "title": "SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining",
                "abstract": "Vision-language models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets. However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text. This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images. This mismatch, particularly when non-co-occurring elements are masked, undermines the training of conventional VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby hindering the model's ability to accurately align fine-grained visual and textual features. Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text. This synchronization is accomplished by harnessing cross-attentional features obtained from a momentum model, ensuring a precise alignment between the two modalities. Additionally, we enhance grouped batch sampling with semi-hard negatives, effectively mitigating false negative issues in Image-Text Matching and Image-Text Contrastive learning objectives within fashion datasets. Our experiments demonstrate the effectiveness of the proposed approach, outper-forming existing methods in three downstream tasks.",
                "authors": "Chull Hwan Song, Taebaek Hwang, Jooyoung Yoon, Shunghyun Choi, Yeong Hyeon Gu",
                "citations": 1
            },
            {
                "title": "PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning",
                "abstract": "Medical vision-language models (Med-VLMs) trained on large datasets of medical image-text pairs and later fine-tuned for specific tasks have emerged as a mainstream paradigm in medical image analysis. However, recent studies have highlighted the susceptibility of these Med-VLMs to adversarial attacks, raising concerns about their safety and robustness. Randomized smoothing is a well-known technique for turning any classifier into a model that is certifiably robust to adversarial perturbations. However, this approach requires retraining the Med-VLM-based classifier so that it classifies well under Gaussian noise, which is often infeasible in practice. In this paper, we propose a novel framework called PromptSmooth to achieve efficient certified robustness of Med-VLMs by leveraging the concept of prompt learning. Given any pre-trained Med-VLM, PromptSmooth adapts it to handle Gaussian noise by learning textual prompts in a zero-shot or few-shot manner, achieving a delicate balance between accuracy and robustness, while minimizing the computational overhead. Moreover, PromptSmooth requires only a single model to handle multiple noise levels, which substantially reduces the computational cost compared to traditional methods that rely on training a separate model for each noise level. Comprehensive experiments based on three Med-VLMs and across six downstream datasets of various imaging modalities demonstrate the efficacy of PromptSmooth. Our code and models are available at https://github.com/nhussein/promptsmooth.",
                "authors": "Noor Hussein, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar",
                "citations": 1
            },
            {
                "title": "HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models",
                "abstract": "Robots interacting with humans through natural language can unlock numerous applications such as Referring Grasp Synthesis (RGS). Given a text query, RGS determines a stable grasp pose to manipulate the referred object in the robot's workspace. RGS comprises two steps: visual grounding and grasp pose estimation. Recent studies leverage powerful Vision-Language Models (VLMs) for visually grounding free-flowing natural language in real-world robotic execution. However, comparisons in complex, cluttered environments with multiple instances of the same object are lacking. This paper introduces HiFi-CS, featuring hierarchical application of Featurewise Linear Modulation (FiLM) to fuse image and text embeddings, enhancing visual grounding for complex attribute rich text queries encountered in robotic grasping. Visual grounding associates an object in 2D/3D space with natural language input and is studied in two scenarios: Closed and Open Vocabulary. HiFi-CS features a lightweight decoder combined with a frozen VLM and outperforms competitive baselines in closed vocabulary settings while being 100x smaller in size. Our model can effectively guide open-set object detectors like GroundedSAM to enhance open-vocabulary performance. We validate our approach through real-world RGS experiments using a 7-DOF robotic arm, achieving 90.33\\% visual grounding accuracy in 15 tabletop scenes. We include our codebase in the supplementary material.",
                "authors": "V. Bhat, P. Krishnamurthy, Ramesh Karri, F. Khorrami",
                "citations": 1
            },
            {
                "title": "Response Wide Shut: Surprising Observations in Basic Vision Language Model Capabilities",
                "abstract": "Vision-Language Models (VLMs) have emerged as general purpose tools for addressing a variety of complex computer vision problems. Such models have been shown to be highly capable, but, at the same time, also lacking some basic visual understanding skills. In this paper, we set out to understand the limitations of SoTA VLMs on fundamental visual tasks: object classification, understanding spatial arrangement, and ability to delineate individual object instances (through counting), by constructing a series of tests that probe which components of design, specifically, maybe lacking. Importantly, we go significantly beyond the current benchmarks, that simply measure final performance of VLM, by also comparing and contrasting it to performance of probes trained directly on features obtained from visual encoder (image embeddings), as well as intermediate vision-language projection used to bridge image-encoder and LLM-decoder ouput in many SoTA models (e.g., LLaVA, BLIP, InstructBLIP). In doing so, we uncover nascent shortcomings in VLMs response and make a number of important observations which could help train and develop more effective VLM models in future.",
                "authors": "Shivam Chandhok, Wan-Cyuan Fan, Leonid Sigal",
                "citations": 1
            },
            {
                "title": "Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection",
                "abstract": "The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via input perturbations, our method can reprogram a pre-trained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. First, learnable visual perturbations are used to refine feature extraction for deepfake detection. Then, we exploit information of face embedding to create sample-level adaptative text prompts, improving the performance. Extensive experiments on several popular benchmark datasets demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88\\% AUC in cross-dataset setting from FF++ to WildDeepfake); (2) the superior performances are achieved with fewer trainable parameters, making it a promising approach for real-world applications.",
                "authors": "Kaiqing Lin, Yuzhen Lin, Weixiang Li, Taiping Yao, Bin Li",
                "citations": 1
            },
            {
                "title": "iHealth-Chile-1 at RRG24: In-context Learning and Finetuning of a Large Multimodal Model for Radiology Report Generation",
                "abstract": "This paper presents the approach of the iHealth-Chile-1 team for the shared task of Large-Scale Radiology Report Generation at the BioNLP workshop, inspired by progress in large multimodal models for processing images and text. In this work, we leverage LLaVA, a Visual-Language Model (VLM), composed of a vision-encoder, a vision-language connector or adapter, and a large language model able to process text and visual embeddings. We achieve our best result by enriching the input prompt of LLaVA with the text output of a simpler report generation model. With this enriched-prompt technique, we improve our results in 4 of 5 metrics (BLEU-4, Rouge-L, BertScore and F1-RadGraph,), only doing in-context learning. Moreover, we provide details about different architecture settings, fine-tuning strategies, and dataset configurations.",
                "authors": "Diego Campanini, Oscar Loch, Pablo Messina, Rafael Elberg, Denis Parra",
                "citations": 1
            },
            {
                "title": "VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with Lightweight Blocks",
                "abstract": "Foundation Vision-Language Models (VLMs) trained using large-scale open-domain images and text pairs have recently been adapted to develop Vision-Language Segmentation Models (VLSMs) that allow providing text prompts during inference to guide image segmentation. If robust and powerful VLSMs can be built for medical images, it could aid medical professionals in many clinical tasks where they must spend substantial time delineating the target structure of interest. VLSMs for medical images resort to fine-tuning base VLM or VLSM pretrained on open-domain natural image datasets due to fewer annotated medical image datasets; this fine-tuning is resource-consuming and expensive as it usually requires updating all or a significant fraction of the pretrained parameters. Recently, lightweight blocks called adapters have been proposed in VLMs that keep the pretrained model frozen and only train adapters during fine-tuning, substantially reducing the computing resources required. We introduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained vision-language segmentation models using transformer encoders. Our experiments in widely used CLIP-based segmentation models show that with only 3 million trainable parameters, the VLSM-Adapter outperforms state-of-the-art and is comparable to the upper bound end-to-end fine-tuning. The source code is available at: https://github.com/naamiinepal/vlsm-adapter.",
                "authors": "Manish Dhakal, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal",
                "citations": 1
            },
            {
                "title": "Exploring the Spectrum of Visio-Linguistic Compositionality and Recognition",
                "abstract": "Vision and language models (VLMs) such as CLIP have showcased remarkable zero-shot recognition abilities yet face challenges in visio-linguistic compositionality, particularly in linguistic comprehension and fine-grained image-text alignment. This paper explores the intricate relationship between compositionality and recognition -- two pivotal aspects of VLM capability. We conduct a comprehensive evaluation of existing VLMs, covering both pre-training approaches aimed at recognition and the fine-tuning methods designed to improve compositionality. Our evaluation employs 12 benchmarks for compositionality, along with 21 zero-shot classification and two retrieval benchmarks for recognition. In our analysis from 274 CLIP model checkpoints, we reveal patterns and trade-offs that emerge between compositional understanding and recognition accuracy. Ultimately, this necessitates strategic efforts towards developing models that improve both capabilities, as well as the meticulous formulation of benchmarks for compositionality. We open our evaluation framework at https://github.com/ytaek-oh/vl_compo.",
                "authors": "Youngtaek Oh, Pyunghwan Ahn, Jinhyung Kim, Gwangmo Song, Soonyoung Lee, I. Kweon, Junmo Kim",
                "citations": 1
            },
            {
                "title": "UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark",
                "abstract": "Localizing unusual activities, such as human errors or surveillance incidents, in videos holds practical significance. However, current video understanding models struggle with localizing these unusual events likely because of their insufficient representation in models' pretraining datasets. To explore foundation models' capability in localizing unusual activity, we introduce UAL-Bench, a comprehensive benchmark for unusual activity localization, featuring three video datasets: UAG-OOPS, UAG-SSBD, UAG-FunQA, and an instruction-tune dataset: OOPS-UAG-Instruct, to improve model capabilities. UAL-Bench evaluates three approaches: Video-Language Models (Vid-LLMs), instruction-tuned Vid-LLMs, and a novel integration of Vision-Language Models and Large Language Models (VLM-LLM). Our results show the VLM-LLM approach excels in localizing short-span unusual events and predicting their onset (start time) more accurately than Vid-LLMs. We also propose a new metric, R@1, TD<= p, to address limitations in existing evaluation methods. Our findings highlight the challenges posed by long-duration videos, particularly in autism diagnosis scenarios, and the need for further advancements in localization techniques. Our work not only provides a benchmark for unusual activity localization but also outlines the key challenges for existing foundation models, suggesting future research directions on this important task.",
                "authors": "Hasnat Md. Abdullah, Tian Liu, Kangda Wei, Shu Kong, Ruihong Huang",
                "citations": 1
            },
            {
                "title": "Talk Through It: End User Directed Manipulation Learning",
                "abstract": "Training robots to perform a huge range of tasks in many different environments is immensely difficult. Instead, we propose selectively training robots based on end-user preferences. Given a factory model that lets an end user instruct a robot to perform lower-level actions (e.g. ‘Move left’), we show that end users can collect demonstrations using language to train their home model for higher-level tasks specific to their needs (e.g. ‘Open the top drawer and put the block inside’). We demonstrate this framework on robot manipulation tasks using RLBench environments. Our method results in a 13% improvement in task success rates compared to a baseline method. We also explore the use of the large vision-language model (VLM), Bard, to automatically break down tasks into sequences of lower-level instructions, aiming to bypass end-user involvement. The VLM is unable to break tasks down to our lowest level, but does achieve good results breaking high-level tasks into mid-level skills.",
                "authors": "Carl Winge, Adam Imdieke, Bahaa Aldeeb, Dongyeop Kang, Karthik Desingh",
                "citations": 1
            },
            {
                "title": "Training a Vision Language Model as Smartphone Assistant",
                "abstract": "Addressing the challenge of a digital assistant capable of executing a wide array of user tasks, our research focuses on the realm of instruction-based mobile device control. We leverage recent advancements in large language models (LLMs) and present a visual language model (VLM) that can fulfill diverse tasks on mobile devices. Our model functions by interacting solely with the user interface (UI). It uses the visual input from the device screen and mimics human-like interactions, encompassing gestures such as tapping and swiping. This generality in the input and output space allows our agent to interact with any application on the device. Unlike previous methods, our model operates not only on a single screen image but on vision-language sentences created from sequences of past screenshots along with corresponding actions. Evaluating our method on the challenging Android in the Wild benchmark demonstrates its promising efficacy and potential.",
                "authors": "Nicolai Dorka, Janusz Marecki, Ammar Anwar",
                "citations": 1
            },
            {
                "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
                "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal tasks, enabling more sophisticated and accurate reasoning across various applications, including image and video captioning, visual question answering, and cross-modal retrieval. Despite their superior capabilities, VLMs struggle with fine-grained image regional composition information perception. Specifically, they have difficulty accurately aligning the segmentation masks with the corresponding semantics and precisely describing the compositional aspects of the referred regions. However, compositionality - the ability to understand and generate novel combinations of known visual and textual components - is critical for facilitating coherent reasoning and understanding across modalities by VLMs. To address this issue, we propose FINECAPTION, a novel VLM that can recognize arbitrary masks as referential inputs and process high-resolution images for compositional image captioning at different granularity levels. To support this endeavor, we introduce COMPOSITIONCAP, a new dataset for multi-grained region compositional image captioning, which introduces the task of compositional attribute-aware regional image captioning. Empirical results demonstrate the effectiveness of our proposed model compared to other state-of-the-art VLMs. Additionally, we analyze the capabilities of current VLMs in recognizing various visual prompts for compositional region image captioning, highlighting areas for improvement in VLM design and training.",
                "authors": "Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Zhifei Zhang, Yilin Wang, Jianming Zhang, Jiebo Luo",
                "citations": 1
            },
            {
                "title": "Evaluating Semantic Relations in Predicting Textual Labels for Images of Abstract and Concrete Concepts",
                "abstract": "This study investigates the performance of SigLIP, a state-of-the-art Vision-Language Model (VLM), in predicting labels for images depicting 1,278 concepts. Our analysis across 300 images per concept shows that the model frequently predicts the exact user-tagged labels, but similarly, it often predicts labels that are semantically related to the exact labels in various ways: synonyms, hypernyms, co-hyponyms, and associated words, particularly for abstract concepts. We then zoom into the diversity of the user tags of images and word associations for abstract versus concrete concepts. Surprisingly, not only abstract but also concrete concepts exhibit significant variability, thus challenging the traditional view that representations of concrete concepts are less diverse.",
                "authors": "Tarun Tater, S. Schulte im Walde, Diego Frassinelli",
                "citations": 1
            },
            {
                "title": "Vision-Language Models for Robot Success Detection",
                "abstract": "In this work, we use Vision-Language Models (VLMs) as a binary success detector given a robot observation and task description, formulated as a Visual Question Answering (VQA) problem. We fine-tune the open-source MiniGPT-4 VLM to detect success on robot trajectories from the Berkeley Bridge and Berkeley AUTOLab UR5 datasets. We find that while a handful of test distribution trajectories can train an accurate detector, transferring learning between different environments is challenging due to distribution shift. In addition, while our VLM is robust to language variations, it is less robust to visual variations. In the future, more powerful VLMs such as Gemini and GPT-4 have the potential to be more accurate and robust success detectors, and success detectors can provide a sparse binary reward to improve existing policies.",
                "authors": "Fiona Luo",
                "citations": 1
            },
            {
                "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models",
                "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). These prompts are ranked according to a purity measure obtained through a fitness function. In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of text prompts preferred by the downstream VLM. Furthermore, we also explicitly steer the LLM generation process in each optimization step by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step. This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models -- showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models.",
                "authors": "M. J. Mirza, Mengjie Zhao, Zhuoyuan Mao, Sivan Doveh, Wei Lin, Paul Gavrikov, Michael Dorkenwald, Shiqi Yang, Saurav Jha, Hiromi Wakaki, Yuki Mitsufuji, Horst Possegger, Rogério Feris, Leonid Karlinsky, James Glass",
                "citations": 1
            },
            {
                "title": "How Well Can Vision Language Models See Image Details?",
                "abstract": "Large Language Model-based Vision-Language Models (LLM-based VLMs) have demonstrated impressive results in various vision-language understanding tasks. However, how well these VLMs can see image detail beyond the semantic level remains unclear. In our study, we introduce a pixel value prediction task (PVP) to explore\"How Well Can Vision Language Models See Image Details?\"and to assist VLMs in perceiving more details. Typically, these models comprise a frozen CLIP visual encoder, a large language model, and a connecting module. After fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to predict precise pixel values by only fine-tuning the connection module and LLM; and 2) prediction precision is significantly improved when the vision encoder is also adapted. Additionally, our research reveals that incorporating pixel value prediction as one of the VLM pre-training tasks and vision encoder adaptation markedly boosts VLM performance on downstream image-language understanding tasks requiring detailed image perception, such as referring image segmentation (with an average +10.19 cIoU improvement) and video game decision making (with average score improvements of +80.34 and +70.54 on two games, respectively).",
                "authors": "Chenhui Gou, Abdulwahab Felemban, Faizan Farooq Khan, Deyao Zhu, Jianfei Cai, Hamid Rezatofighi, Mohamed Elhoseiny",
                "citations": 1
            },
            {
                "title": "Assessing Current Coastal Subsidence at Continental Scale: Insights From Europe Using the European Ground Motion Service",
                "abstract": "Beside climate‐change‐induced sea‐level rise (SLR), land subsidence can strongly amplify coastal risk in flood‐prone areas. Mapping and quantifying contemporary vertical land motion (VLM) at continental scales has long been a challenge due to the absence of gridded observational products covering these large domains. Here, we fill this gap by using the new European Ground Motion Service (EGMS) to assess the current state of coastal VLM in Europe. First, we compare the InSAR‐based EGMS Ortho (Level 3) with nearby global navigation satellite systems (GNSS) vertical velocity estimates and show that the geodetic reference frame used to calibrate EGMS strongly influences coastal vertical land velocity estimates at the millimeter per year level and this needs to be considered with caution. After adjusting the EGMS vertical velocity estimates to a more updated and accurate International Terrestrial Reference Frame (ITRF2014), we performed an assessment of VLM in European low elevation coastal flood plains (CFPs). We find that nearly half of the European CFP area is, on average, subsiding at a rate faster than 1 mm/yr. More importantly, we find that urban areas and populations located in the CFP experience a near −1 mm/yr VLM on average (excluding the uplifting Fennoscandia region). For harbors, the average VLM is even larger and increases to −1.5 mm/yr on average. This demonstrates the widespread importance of continental‐scale assessments based on InSAR and GNSS to better identify areas at higher risk from relative SLR due to coastal subsidence.",
                "authors": "R. Thiéblemont, G. Le Cozannet, R. Nicholls, J. Rohmer, G. Wöppelmann, D. Raucoules, M. de Michele, A. Toimil, D. Lincke",
                "citations": 1
            },
            {
                "title": "Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding",
                "abstract": "Deep multimodal semantic understanding that goes beyond the mere superficial content relation mining has received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three experts of soft prompts: a text prompt and an image prompt that extract modality-specific features to enrich the single-modal representation, and a unified prompt to assist multi-modal interaction. Additionally, we reorganize Transformer layers into several blocks and introduce cross-modal prompt attention between adjacent blocks, which smoothens the transition from single-modal representation to multi-modal fusion. On both MSD and MSA datasets in few-shot setting, our proposed model not only surpasses the 8.2B model InstructBLIP with merely 2% parameters (150M), but also significantly outperforms other widely-used prompt methods on VLMs or task-specific methods.",
                "authors": "Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu, Yunfang Wu",
                "citations": 1
            },
            {
                "title": "Vision Language Model is NOT All You Need: Augmentation Strategies for Molecule Language Models",
                "abstract": "Recently, there has been a growing interest among researchers in understanding molecules and their textual descriptions through molecule language models (MoLM). However, despite some early promising developments, the advancement of MoLM still trails significantly behind that of vision language models (VLM). This is because unique challenges exist apart from VLM in the field of MoLM due to 1) a limited amount of molecule-text paired data and 2) missing expertise that occurred due to the specialized areas of focus among the experts. To this end, we propose AMOLE, which 1) augments molecule-text pairs with structural similarity preserving loss, and 2) transfers the expertise between the molecules. Specifically, AMOLE enriches molecule-text pairs by sharing descriptions among structurally similar molecules with a novel structural similarity preserving loss. Moreover, we propose an expertise reconstruction loss to transfer knowledge from molecules that have extensive expertise to those with less expertise. Extensive experiments on various downstream tasks demonstrate the superiority of AMOLE in comprehending molecules and their descriptions, highlighting its potential for application in real-world drug discovery. The source code for AMOLE is available at https://github.com/Namkyeong/AMOLE.",
                "authors": "Namkyeong Lee, Siddhartha Laghuvarapu, Chanyoung Park, Jimeng Sun",
                "citations": 1
            },
            {
                "title": "Lesion of NPY Receptor-expressing Neurons in Perifornical Lateral Hypothalamus Attenuates Glucoprivic Feeding.",
                "abstract": "Glucoprivic feeding is one of several counter-regulatory responses (CRRs) that facilitates restoration of euglycemia following acute glucose deficit (glucoprivation). Our previous work established that glucoprivic feeding requires ventrolateral medullary (VLM) catecholamine (CA) neurons that coexpress neuropeptide Y (NPY). However, the connections by which VLM CA/NPY neurons trigger increased feeding are uncertain. We have previously shown that glucoprivation, induced by an anti-glycolygic agent 2-Deoxy-D-glucose (2DG), activates perifornical lateral hypothalamus (PeFLH) neurons and that expression of NPY in the VLM CA/NPY neurons is required for glucoprivic feeding. We therefore hypothesized that glucoprivic feeding and possibly other CRRs require NPY-sensitive PeFLH neurons. To test this, we used the ribosomal toxin conjugate, NPY-saporin (NPY-SAP), to selectively lesion NPY receptor-expressing neurons in the PeFLH of male rats. We found that NPY-SAP destroyed a significant number of PeFLH neurons, including those expressing orexin, but not those expressing melanin-concentrating hormone. The PeFLH NPY-SAP lesions attenuated 2DG-induced feeding but did not affect 2DG-induced increase in locomotor activity, sympathoadrenal hyperglycemia, or corticosterone release. The 2DG-induced feeding response was also significantly attenuated in NPY-SAP-treated female rats. Interestingly, PeFLH NPY-SAP lesioned male rats had reduced body weights and decreased dark cycle feeding, but this effect was not seen in female rats. We conclude that a NPY projection to the PeFLH is necessary for glucoprivic feeding, but not locomotor activity, hyperglycemia, or corticosterone release, in both male and female rats.",
                "authors": "Pique P Choi, Qing Wang, L. Brenner, Ai-Jun Li, Robert C Ritter, S. Appleyard",
                "citations": 1
            },
            {
                "title": "Leveraging Multi-Primary PS-InSAR Configurations for the Robust Estimation of Coastal Subsidence",
                "abstract": "Interferometric synthetic aperture radar (InSAR) is a key technique used to constrain contributions of diverse processes to coastal subsidence, also known as vertical land motion (VLM). However, coastal environments can pose major challenges for InSAR due to natural disturbances that degrade interferogram quality. We describe a new multi-primary pairing strategy for persistent scatterer InSAR (PS-InSAR) to estimate subsidence in challenging coastal environments. Our method retains only consistent PS candidates across multi-primary substacks and solves for redundant velocity observations using SVD-based inversion, similar to the conventional small baseline subset (SBAS) method. Through simulations and a case study comparing with single-primary PS-InSAR and conventional SBAS techniques, we show that our pairing strategy reduces temporal and spatial uncertainty in subsidence estimates in the presence of strong but temporary decorrelation loss, even with increased distance from the reference point. Moreover, our method visibly dampens time-series variation and decreases standard error in our time-series fit by nearly 2x in our case study. Thus, we find that implementing a multi-primary PS-InSAR configuration is a simple method of increasing the robustness of VLM estimates in challenging coastal environments.",
                "authors": "Stacey A. Huang, J. Sauber",
                "citations": 1
            },
            {
                "title": "Generating CAD Code with Vision-Language Models for 3D Designs",
                "abstract": "Generative AI has transformed the fields of Design and Manufacturing by providing efficient and automated methods for generating and modifying 3D objects. One approach involves using Large Language Models (LLMs) to generate Computer- Aided Design (CAD) scripting code, which can then be executed to render a 3D object; however, the resulting 3D object may not meet the specified requirements. Testing the correctness of CAD generated code is challenging due to the complexity and structure of 3D objects (e.g., shapes, surfaces, and dimensions) that are not feasible in code. In this paper, we introduce CADCodeVerify, a novel approach to iteratively verify and improve 3D objects generated from CAD code. Our approach works by producing ameliorative feedback by prompting a Vision-Language Model (VLM) to generate and answer a set of validation questions to verify the generated object and prompt the VLM to correct deviations. To evaluate CADCodeVerify, we introduce, CADPrompt, the first benchmark for CAD code generation, consisting of 200 natural language prompts paired with expert-annotated scripting code for 3D objects to benchmark progress. Our findings show that CADCodeVerify improves VLM performance by providing visual feedback, enhancing the structure of the 3D objects, and increasing the success rate of the compiled program. When applied to GPT-4, CADCodeVerify achieved a 7.30% reduction in Point Cloud distance and a 5.0% improvement in success rate compared to prior work",
                "authors": "Kamel Alrashedy, Pradyumna Tambwekar, Z. Zaidi, Megan Langwasser, Wei Xu, Matthew C. Gombolay",
                "citations": 1
            },
            {
                "title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models",
                "abstract": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.",
                "authors": "Kevin Y. Li, Sachin Goyal, João Dias Semedo, J. Kolter",
                "citations": 1
            },
            {
                "title": "Sensitivity of GNSS to vertical land motion over Europe: effects of geophysical loadings and common-mode errors",
                "abstract": null,
                "authors": "Roland Hohensinn, Pia Ruttner, Yehuda Bock",
                "citations": 1
            },
            {
                "title": "Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models",
                "abstract": "In dialogue, the addressee may initially misunderstand the speaker and respond erroneously, often prompting the speaker to correct the misunderstanding in the next turn with a Third Position Repair (TPR). The ability to process and respond appropriately to such repair sequences is thus crucial in conversational AI systems. In this paper, we first collect, analyse, and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences in an instruction-following manipulation task that is, by design, rife with referential ambiguity. We employ this dataset to evaluate several state-of-the-art Vision and Language Models (VLM) across multiple settings, focusing on their capability to process and accurately respond to TPRs and thus recover from miscommunication. We find that, compared to humans, all models significantly underperform in this task. We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generalising better to new scenarios. Our results suggest that these models are not yet ready to be deployed in multi-modal collaborative settings where repairs are common, and highlight the need to design training regimes and objectives that facilitate learning from interaction. Our code and data are available at www.github.com/JChiyah/blockworld-repairs",
                "authors": "Javier Chiyah-Garcia, Alessandro Suglia, Arash Eshghi",
                "citations": 1
            },
            {
                "title": "Non‐Linear Vertical Land Motion of Coastal Chile and the Antarctic Peninsula Inferred From Combining Satellite Altimetry, Tide Gauge and GPS Data",
                "abstract": "We developed an enhanced Kalman‐based approach to quantify abrupt changes and significant non‐linearity in vertical land motion (VLM) along the coast of Chile and the Antarctic Peninsula using a combination of multi‐mission satellite altimetry (ALT), tide gauge (TG), and GPS data starting from the early 1990s. The data reveal the spatial variability of co‐seismic and post‐seismic subsidence at TGs along the Chilean subduction zone in response to the Mw8.8 Maule 2010, Mw8.1 Iquique 2014, and Mw8.3 Illapel 2015 earthquakes that are not retrievable from the interpolation of sparse GPS observations across space and time. In the Antarctic Peninsula, where continuous GPS data do not commence until ∼1998, the approach provides new insight into the ∼2002 change in VLM at the TGs of +5.3 ± 2.2 mm/yr (Palmer) and +3.5 ± 2.8 mm/yr (Vernadsky) due to the onset of ice‐mass loss following the Larsen‐B Ice Shelf breakup. We used these data to constrain viscoelastic Earth model parameters for the northern Antarctic Peninsula, obtaining a preferred lithosphere thickness of 115 km and upper mantle viscosity of 0.9 × 1018 Pa s. Our estimates of regionally‐correlated ALT systematic errors are small, typically between ∼±0.5–2.5 mm/yr over single‐mission time scales. These are consistent with competing orbit differences and the relative errors apparent in ALT crossovers. This study demonstrates that, with careful tuning, the ALT‐TG technique can provide improved temporal and spatial sampling of VLM, yielding new constraints on geodynamic models and assisting sea‐level change studies in otherwise data sparse regions and periods.",
                "authors": "Mohammad‐Hadi Rezvani, Christopher S. Watson, Matt A. King",
                "citations": 1
            },
            {
                "title": "OLiVia-Nav: An Online Lifelong Vision Language Approach for Mobile Robot Social Navigation",
                "abstract": "Service robots in human-centered environments such as hospitals, office buildings, and long-term care homes need to navigate while adhering to social norms to ensure the safety and comfortability of the people they are sharing the space with. Furthermore, they need to adapt to new social scenarios that can arise during robot navigation. In this paper, we present a novel Online Lifelong Vision Language architecture, OLiVia-Nav, which uniquely integrates vision-language models (VLMs) with an online lifelong learning framework for robot social navigation. We introduce a unique distillation approach, Social Context Contrastive Language Image Pre-training (SC-CLIP), to transfer the social reasoning capabilities of large VLMs to a lightweight VLM, in order for OLiVia-Nav to directly encode social and environment context during robot navigation. These encoded embeddings are used to generate and select robot social compliant trajectories. The lifelong learning capabilities of SC-CLIP enable OLiVia-Nav to update the lightweight VLM with robot trajectory predictions overtime as new social scenarios are encountered. We conducted extensive real-world experiments in diverse social navigation scenarios. The results showed that OLiVia-Nav outperformed existing state-of-the-art DRL and VLM methods in terms of mean squared error, Hausdorff loss, and personal space violation duration. Ablation studies also verified the design choices for OLiVia-Nav.",
                "authors": "Siddarth Narasimhan, Aaron Hao Tan, Daniel Choi, G. Nejat",
                "citations": 1
            },
            {
                "title": "Rugby Scene Classification Enhanced by Vision Language Model",
                "abstract": "This study investigates the integration of vision language models (VLM) to enhance the classification of situations within rugby match broadcasts. The importance of accurately identifying situations in sports videos is emphasized for understanding game dynamics and facilitating downstream tasks like performance evaluation and injury prevention. Utilizing a dataset comprising 18, 000 labeled images extracted at 0.2-second intervals from 100 minutes of rugby match broadcasts, scene classification tasks including contact plays (scrums, mauls, rucks, tackles, lineouts), rucks, tackles, lineouts, and multiclass classification were performed. The study aims to validate the utility of VLM outputs in improving classification performance compared to using solely image data. Experimental results demonstrate substantial performance improvements across all tasks with the incorporation of VLM outputs. Our analysis of prompts suggests that, when provided with appropriate contextual information through natural language, VLMs can effectively capture the context of a given image. The findings of our study indicate that leveraging VLMs in the domain of sports analysis holds promise for developing image processing models capable of incorpolating the tacit knowledge encoded within language models, as well as information conveyed through natural language descriptions.",
                "authors": "Naoki Nonaka, Ryo Fujihira, Toshiki Koshiba, Akira Maeda, Jun Seita",
                "citations": 1
            },
            {
                "title": "Leveraging vision-language models for fair facial attribute classification",
                "abstract": "Performance disparities of image recognition across different demographic populations are known to exist in deep learning-based models, but previous work has largely addressed such fairness problems assuming knowledge of sensitive attribute labels. To overcome this reliance, previous strategies have involved separate learning structures to expose and adjust for disparities. In this work, we explore a new paradigm that does not require sensitive attribute labels, and evades the need for extra training by leveraging general-purpose vision-language model (VLM), as a rich knowledge source for common sensitive attributes. We analyze the correspondence between VLM predicted and human defined sensitive attribute distribution. We find that VLMs can recognize samples with clear attribute information encoded in image representations, thus capture under-performed samples conflicting with attribute-related bias. We train downstream target classifiers by re-sampling and augmenting under-performed attribute groups. Extensive experiments on multiple benchmark facial attribute classification datasets show fairness gains of the model over existing unsupervised baselines that tackle with arbitrary bias. The work indicates that vision-language models can extract discriminative sensitive information prompted by language, and be used to promote model fairness.",
                "authors": "Miao Zhang, R. Chunara",
                "citations": 1
            },
            {
                "title": "A Study of Test-time Contrastive Concepts for Open-world, Open-vocabulary Semantic Segmentation",
                "abstract": "Recent VLMs, pre-trained on large amounts of image-text pairs to align both modalities, have opened the way to open-vocabulary semantic segmentation. Given an arbitrary set of textual queries, image regions are assigned the closest query in feature space. However, the usual setup expects the user to list all possible visual concepts that may occur in the image, typically all classes of benchmark datasets, that act as negatives to each other. We consider here the more challenging scenario of segmenting a single concept, given a textual prompt and nothing else. To achieve good results, besides contrasting with the generic 'background' text, we study different ways to generate query-specific test-time contrastive textual concepts, which leverage either the distribution of text in the VLM's training set or crafted LLM prompts. We show the relevance of our approach using a new, specific metric.",
                "authors": "Monika Wysocza'nska, Antonín Vobecký, Amaia Cardiel, Tomasz Trzci'nski, Renaud Marlet, Andrei Bursuc, Oriane Sim'eoni",
                "citations": 1
            },
            {
                "title": "A top-down slow breathing circuit that alleviates negative affect in mice.",
                "abstract": null,
                "authors": "Jinho Jhang, Seahyung Park, Shijia Liu, David D O'Keefe, Sung Han",
                "citations": 1
            },
            {
                "title": "Discovering Syntactic Interaction Clues for Human-Object Interaction Detection",
                "abstract": "Recently, Vision-Language Model (VLM) has greatly ad-vanced the Human-Object Interaction (HOI) detection. The existing VLM-based HOI detectors typically adopt a hand-crafted template (e.g., a photo of a person [action] a/an [object]) to acquire text knowledge through the VLM text encoder. However, such approaches, only encoding the action-specific text prompts in vocabulary level, may suffer from learning ambiguity without exploring the fine-grained clues from the perspective of interaction context. In this paper, we propose a novel method to discover Syntactic Interaction Clues for HOI detection (SICHOI) by using VLM. Specifically, we first investigate what are the essen-tial elements for an interaction context, and then establish a syntactic interaction bank from three levels: spatial relationship, action-oriented posture and situational condition. Further, to align visual features with the syntactic interaction bank, we adopt a multi-view extractor to jointly aggre-gate visual features from instance, interaction, and image levels accordingly. In addition, we also introduce a dual cross-attention decoder to perform context propagation be-tween text knowledge and visual features, thereby enhancing the HOI detection. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on HICO-DET and V-COCO.",
                "authors": "Jinguo Luo, Weihong Ren, Weibo Jiang, Xi’ai Chen, Qiang Wang, Zhi Han, Honghai Liu",
                "citations": 1
            },
            {
                "title": "SARO: Space-Aware Robot System for Terrain Crossing via Vision-Language Model",
                "abstract": "The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks. However, there are few explorations for these foundation models used in quadruped robot navigation through terrains in 3D environments. In this work, we introduce SARO (Space Aware Robot System for Terrain Crossing), an innovative system composed of a high-level reasoning module, a closed-loop sub-task execution module, and a low-level control policy. It enables the robot to navigate across 3D terrains and reach the goal position. For high-level reasoning and execution, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to effectively train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across several 3D terrains, and its generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Project page: https://saro-vlm.github.io/",
                "authors": "Shaoting Zhu, Derun Li, Linzhan Mou, Yong Liu, Ningyi Xu, Hang Zhao",
                "citations": 1
            },
            {
                "title": "Visual Landmark Map-Based Spatial Recognition Using a Monocular Camera",
                "abstract": "In this paper, we propose a computer vision-based indoor spatial recognition method. The proposed method recognizes objects in a 360-degree space and eliminates errors in the process in which the space’s visual landmark map (VLM) is constructed by considering the 360-degree spatial arrangement and relative coordinates between the recognized objects. The method divides the target space into subspaces, and individual VLMs are applied to each subspace. The VLMs for these subspaces are activated based on their arrangement. When matching images from a camera with VLMs, the system identifies the user’s location within the space. An experiment demonstrated the effectiveness of the proposed method in accurately recognizing the user’s indoor position, and the data size for maintaining the VLM is highly efficient.",
                "authors": "Haichuan Chen, Gaoyang Shan, B. Roh, Sj Kim, Junghyun Lim, Geunkyung Choi",
                "citations": 1
            },
            {
                "title": "LIT: Large Language Model Driven Intention Tracking for Proactive Human-Robot Collaboration - A Robot Sous-Chef Application",
                "abstract": "Large Language Models (LLM) and Vision Language Models (VLM) enable robots to ground natural language prompts into control actions to achieve tasks in an open world. However, when applied to a long-horizon collaborative task, this formulation results in excessive prompting for initiating or clarifying robot actions at every step of the task. We propose Language-driven Intention Tracking (LIT), leveraging LLMs and VLMs to model the human user's long-term behavior and to predict the next human intention to guide the robot for proactive collaboration. We demonstrate smooth coordination between a LIT-based collaborative robot and the human user in collaborative cooking tasks.",
                "authors": "Zhe Huang, John Pohovey, Ananya Yammanuru, K. Driggs-Campbell",
                "citations": 1
            },
            {
                "title": "Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight",
                "abstract": "Video anomaly detection (VAD) has witnessed significant advancements through the integration of large language models (LLMs) and vision-language models (VLMs), addressing critical challenges such as interpretability, temporal reasoning, and generalization in dynamic, open-world scenarios. This paper presents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024, focusing on four key aspects: (i) enhancing interpretability through semantic insights and textual explanations, making visual anomalies more understandable; (ii) capturing intricate temporal relationships to detect and localize dynamic anomalies across video frames; (iii) enabling few-shot and zero-shot detection to minimize reliance on large, annotated datasets; and (iv) addressing open-world and class-agnostic anomalies by using semantic understanding and motion features for spatiotemporal coherence. We highlight their potential to redefine the landscape of VAD. Additionally, we explore the synergy between visual and textual modalities offered by LLMs and VLMs, highlighting their combined strengths and proposing future directions to fully exploit the potential in enhancing video anomaly detection.",
                "authors": "Xi Ding, Lei Wang",
                "citations": 1
            },
            {
                "title": "ATTIQA: Generalizable Image Quality Feature Extractor Using Attribute-Aware Pretraining",
                "abstract": null,
                "authors": "Daekyu Kwon, Dongyoung Kim, Sehwan Ki, Younghyun Jo, Hyong-Euk Lee, Seon Joo Kim",
                "citations": 1
            },
            {
                "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
                "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal tasks, enabling more sophisticated and accurate reasoning across various applications, including image and video captioning, visual question answering, and cross-modal retrieval. Despite their superior capabilities, VLMs struggle with fine-grained image regional composition information perception. Specifically, they have difficulty accurately aligning the segmentation masks with the corresponding semantics and precisely describing the compositional aspects of the referred regions. However, compositionality - the ability to understand and generate novel combinations of known visual and textual components - is critical for facilitating coherent reasoning and understanding across modalities by VLMs. To address this issue, we propose FINECAPTION, a novel VLM that can recognize arbitrary masks as referential inputs and process high-resolution images for compositional image captioning at different granularity levels. To support this endeavor, we introduce COMPOSITIONCAP, a new dataset for multi-grained region compositional image captioning, which introduces the task of compositional attribute-aware regional image captioning. Empirical results demonstrate the effectiveness of our proposed model compared to other state-of-the-art VLMs. Additionally, we analyze the capabilities of current VLMs in recognizing various visual prompts for compositional region image captioning, highlighting areas for improvement in VLM design and training.",
                "authors": "Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Zhifei Zhang, Yilin Wang, Jianming Zhang, Jiebo Luo",
                "citations": 1
            },
            {
                "title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models",
                "abstract": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.",
                "authors": "Kevin Y. Li, Sachin Goyal, João Dias Semedo, J. Kolter",
                "citations": 1
            },
            {
                "title": "LIT: Large Language Model Driven Intention Tracking for Proactive Human-Robot Collaboration - A Robot Sous-Chef Application",
                "abstract": "Large Language Models (LLM) and Vision Language Models (VLM) enable robots to ground natural language prompts into control actions to achieve tasks in an open world. However, when applied to a long-horizon collaborative task, this formulation results in excessive prompting for initiating or clarifying robot actions at every step of the task. We propose Language-driven Intention Tracking (LIT), leveraging LLMs and VLMs to model the human user's long-term behavior and to predict the next human intention to guide the robot for proactive collaboration. We demonstrate smooth coordination between a LIT-based collaborative robot and the human user in collaborative cooking tasks.",
                "authors": "Zhe Huang, John Pohovey, Ananya Yammanuru, K. Driggs-Campbell",
                "citations": 1
            },
            {
                "title": "UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark",
                "abstract": "Localizing unusual activities, such as human errors or surveillance incidents, in videos holds practical significance. However, current video understanding models struggle with localizing these unusual events likely because of their insufficient representation in models' pretraining datasets. To explore foundation models' capability in localizing unusual activity, we introduce UAL-Bench, a comprehensive benchmark for unusual activity localization, featuring three video datasets: UAG-OOPS, UAG-SSBD, UAG-FunQA, and an instruction-tune dataset: OOPS-UAG-Instruct, to improve model capabilities. UAL-Bench evaluates three approaches: Video-Language Models (Vid-LLMs), instruction-tuned Vid-LLMs, and a novel integration of Vision-Language Models and Large Language Models (VLM-LLM). Our results show the VLM-LLM approach excels in localizing short-span unusual events and predicting their onset (start time) more accurately than Vid-LLMs. We also propose a new metric, R@1, TD<= p, to address limitations in existing evaluation methods. Our findings highlight the challenges posed by long-duration videos, particularly in autism diagnosis scenarios, and the need for further advancements in localization techniques. Our work not only provides a benchmark for unusual activity localization but also outlines the key challenges for existing foundation models, suggesting future research directions on this important task.",
                "authors": "Hasnat Md. Abdullah, Tian Liu, Kangda Wei, Shu Kong, Ruihong Huang",
                "citations": 1
            },
            {
                "title": "MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and Context-focused Prompt Tuning",
                "abstract": "Multiple instance learning (MIL) has become a standard paradigm for weakly supervised classification of whole slide images (WSI). However, this paradigm relies on the use of a large number of labelled WSIs for training. The lack of training data and the presence of rare diseases present significant challenges for these methods. Prompt tuning combined with the pre-trained Vision-Language models (VLMs) is an effective solution to the Few-shot Weakly Supervised WSI classification (FSWC) tasks. Nevertheless, applying prompt tuning methods designed for natural images to WSIs presents three significant challenges: 1) These methods fail to fully leverage the prior knowledge from the VLM's text modality; 2) They overlook the essential multi-scale and contextual information in WSIs, leading to suboptimal results; and 3) They lack exploration of instance aggregation methods. To address these problems, we propose a Multi-Scale and Context-focused Prompt Tuning (MSCPT) method for FSWC tasks. Specifically, MSCPT employs the frozen large language model to generate pathological visual language prior knowledge at multi-scale, guiding hierarchical prompt tuning. Additionally, we design a graph prompt tuning module to learn essential contextual information within WSI, and finally, a non-parametric cross-guided instance aggregation module has been introduced to get the WSI-level features. Based on two VLMs, extensive experiments and visualizations on three datasets demonstrated the powerful performance of our MSCPT.",
                "authors": "Minghao Han, Linhao Qu, Dingkang Yang, Xukun Zhang, Xiaoying Wang, Lihua Zhang",
                "citations": 1
            },
            {
                "title": "SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining",
                "abstract": "Vision-language models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets. However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text. This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images. This mismatch, particularly when non-co-occurring elements are masked, undermines the training of conventional VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby hindering the model's ability to accurately align fine-grained visual and textual features. Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text. This synchronization is accomplished by harnessing cross-attentional features obtained from a momentum model, ensuring a precise alignment between the two modalities. Additionally, we enhance grouped batch sampling with semi-hard negatives, effectively mitigating false negative issues in Image-Text Matching and Image-Text Contrastive learning objectives within fashion datasets. Our experiments demonstrate the effectiveness of the proposed approach, outper-forming existing methods in three downstream tasks.",
                "authors": "Chull Hwan Song, Taebaek Hwang, Jooyoung Yoon, Shunghyun Choi, Yeong Hyeon Gu",
                "citations": 1
            },
            {
                "title": "Training a Vision Language Model as Smartphone Assistant",
                "abstract": "Addressing the challenge of a digital assistant capable of executing a wide array of user tasks, our research focuses on the realm of instruction-based mobile device control. We leverage recent advancements in large language models (LLMs) and present a visual language model (VLM) that can fulfill diverse tasks on mobile devices. Our model functions by interacting solely with the user interface (UI). It uses the visual input from the device screen and mimics human-like interactions, encompassing gestures such as tapping and swiping. This generality in the input and output space allows our agent to interact with any application on the device. Unlike previous methods, our model operates not only on a single screen image but on vision-language sentences created from sequences of past screenshots along with corresponding actions. Evaluating our method on the challenging Android in the Wild benchmark demonstrates its promising efficacy and potential.",
                "authors": "Nicolai Dorka, Janusz Marecki, Ammar Anwar",
                "citations": 1
            },
            {
                "title": "SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework with Interactive Vision-Language Models",
                "abstract": "With the continuous advancement of vision language models (VLMs) technology, remarkable research achievements have emerged in the dermatology field, the fourth most prevalent human disease category. However, despite these advancements, VLM still faces\"hallucination\"in dermatological diagnosis, and due to the inherent complexity of dermatological conditions, existing tools offer relatively limited support for user comprehension. We propose SkinGEN, a diagnosis-to-generation framework that leverages the stable diffusion (SD) method to generate reference demonstrations from diagnosis results provided by VLM, thereby enhancing the visual explainability for users. Through extensive experiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for skin condition image generation. We conduct a user study with 32 participants evaluating both the system performance and explainability. Results demonstrate that SkinGEN significantly improves users' comprehension of VLM predictions and fosters increased trust in the diagnostic process. This work paves the way for more transparent and user-centric VLM applications in dermatology and beyond.",
                "authors": "Bo Lin, Yingjing Xu, Xuanwen Bao, Zhou Zhao, Zuyong Zhang, Zhouyang Wang, Jie Zhang, Shuiguang Deng, Jianwei Yin",
                "citations": 1
            },
            {
                "title": "MATE: Meet At The Embedding - Connecting Images with Long Texts",
                "abstract": "While advancements in Vision Language Models (VLMs) have significantly improved the alignment of visual and textual data, these models primarily focus on aligning images with short descriptive captions. This focus limits their ability to handle complex text interactions, particularly with longer texts such as lengthy captions or documents, which have not been extensively explored yet. In this paper, we introduce Meet At The Embedding (MATE), a novel approach that combines the capabilities of VLMs with Large Language Models (LLMs) to overcome this challenge without the need for additional image-long text pairs. Specifically, we replace the text encoder of the VLM with a pretrained LLM-based encoder that excels in understanding long texts. To bridge the gap between VLM and LLM, MATE incorporates a projection module that is trained in a multi-stage manner. It starts by aligning the embeddings from the VLM text encoder with those from the LLM using extensive text pairs. This module is then employed to seamlessly align image embeddings closely with LLM embeddings. We propose two new cross-modal retrieval benchmarks to assess the task of connecting images with long texts (lengthy captions / documents). Extensive experimental results demonstrate that MATE effectively connects images with long texts, uncovering diverse semantic relationships.",
                "authors": "Young Kyun Jang, Junmo Kang, Yong Jae Lee, Donghyun Kim",
                "citations": 1
            },
            {
                "title": "SARO: Space-Aware Robot System for Terrain Crossing via Vision-Language Model",
                "abstract": "The application of vision-language models (VLMs) has achieved impressive success in various robotics tasks. However, there are few explorations for these foundation models used in quadruped robot navigation through terrains in 3D environments. In this work, we introduce SARO (Space Aware Robot System for Terrain Crossing), an innovative system composed of a high-level reasoning module, a closed-loop sub-task execution module, and a low-level control policy. It enables the robot to navigate across 3D terrains and reach the goal position. For high-level reasoning and execution, we propose a novel algorithmic system taking advantage of a VLM, with a design of task decomposition and a closed-loop sub-task execution mechanism. For low-level locomotion control, we utilize the Probability Annealing Selection (PAS) method to effectively train a control policy by reinforcement learning. Numerous experiments show that our whole system can accurately and robustly navigate across several 3D terrains, and its generalization ability ensures the applications in diverse indoor and outdoor scenarios and terrains. Project page: https://saro-vlm.github.io/",
                "authors": "Shaoting Zhu, Derun Li, Linzhan Mou, Yong Liu, Ningyi Xu, Hang Zhao",
                "citations": 1
            },
            {
                "title": "BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks",
                "abstract": "Despite their superb multimodal capabilities, Vision-Language Models (VLMs) have been shown to be vulnerable to jailbreak attacks, which are inference-time attacks that induce the model to output harmful responses with tricky prompts. It is thus essential to defend VLMs against potential jailbreaks for their trustworthy deployment in real-world applications. In this work, we focus on black-box defense for VLMs against jailbreak attacks. Existing black-box defense methods are either unimodal or bimodal. Unimodal methods enhance either the vision or language module of the VLM, while bimodal methods robustify the model through text-image representation realignment. However, these methods suffer from two limitations: 1) they fail to fully exploit the cross-modal information, or 2) they degrade the model performance on benign inputs. To address these limitations, we propose a novel blue-team method BlueSuffix that defends the black-box target VLM against jailbreak attacks without compromising its performance. BlueSuffix includes three key components: 1) a visual purifier against jailbreak images, 2) a textual purifier against jailbreak texts, and 3) a blue-team suffix generator fine-tuned via reinforcement learning for enhancing cross-modal robustness. We empirically show on three VLMs (LLaVA, MiniGPT-4, and Gemini) and two safety benchmarks (MM-SafetyBench and RedTeam-2K) that BlueSuffix outperforms the baseline defenses by a significant margin. Our BlueSuffix opens up a promising direction for defending VLMs against jailbreak attacks.",
                "authors": "Yunhan Zhao, Xiang Zheng, Lin Luo, Yige Li, Xingjun Ma, Yu-Gang Jiang",
                "citations": 1
            },
            {
                "title": "Just KIDDIN: Knowledge Infusion and Distillation for Detection of INdecent Memes",
                "abstract": "Toxicity identification in online multimodal environments remains a challenging task due to the complexity of contextual connections across modalities (e.g., textual and visual). In this paper, we propose a novel framework that integrates Knowledge Distillation (KD) from Large Visual Language Models (LVLMs) and knowledge infusion to enhance the performance of toxicity detection in hateful memes. Our approach extracts sub-knowledge graphs from ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused within a compact VLM framework. The relational context between toxic phrases in captions and memes, as well as visual concepts in memes enhance the model's reasoning capabilities. Experimental results from our study on two hate speech benchmark datasets demonstrate superior performance over the state-of-the-art baselines across AU-ROC, F1, and Recall with improvements of 1.1%, 7%, and 35%, respectively. Given the contextual complexity of the toxicity detection task, our approach showcases the significance of learning from both explicit (i.e. KG) as well as implicit (i.e. LVLMs) contextual cues incorporated through a hybrid neurosymbolic approach. This is crucial for real-world applications where accurate and scalable recognition of toxic content is critical for creating safer online environments.",
                "authors": "Rahul Garg, Trilok Padhi, Hemang Jain, Ugur Kursuncu, P. Kumaraguru",
                "citations": 1
            },
            {
                "title": "Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks",
                "abstract": "Vision-language-action (VLA) models represent a promising direction for developing general-purpose robotic systems, demonstrating the ability to combine visual understanding, language comprehension, and action generation. However, systematic evaluation of these models across diverse robotic tasks remains limited. In this work, we present a comprehensive evaluation framework and benchmark suite for assessing VLA models. We profile three state-of-the-art VLM and VLAs - GPT-4o, OpenVLA, and JAT - across 20 diverse datasets from the Open-X-Embodiment collection, evaluating their performance on various manipulation tasks. Our analysis reveals several key insights: 1. current VLA models show significant variation in performance across different tasks and robot platforms, with GPT-4o demonstrating the most consistent performance through sophisticated prompt engineering, 2. all models struggle with complex manipulation tasks requiring multi-step planning, and 3. model performance is notably sensitive to action space characteristics and environmental factors. We release our evaluation framework and findings to facilitate systematic assessment of future VLA models and identify critical areas for improvement in the development of general purpose robotic systems.",
                "authors": "P. Guruprasad, Harshvardhan Digvijay Sikka, Jaewoo Song, Yangyue Wang, Paul Pu Liang",
                "citations": 1
            },
            {
                "title": "Exquisitor at the Lifelog Search Challenge 2024: Blending Conversational Search with User Relevance Feedback",
                "abstract": "The past decade has seen a rapid expansion of personal and interpersonal multimedia collections. These collections offer a wealth of information about individuals, including their interests, health, and significant life events. While automated techniques can assist in structuring and organizing these collections, they often have limitations in helping users effectively navigate and find relevant items within such large datasets. The Lifelog Search Challenge (LSC) provides a valuable benchmark for evaluating interactive retrieval systems designed for personal multimedia collections. Exquisitor utilizes a large-scale user relevance feedback (URF) approach for searching through large collections. To address challenges in highly descriptive retrieval tasks where the relevance feedback model may fail to identify essential elements, we have enhanced Exquisitor with conversational search capabilities powered by a Vision Language Model (VLM) and refined the features underlying the URF model. Furthermore, Exquisitor has been updated with a streamlined user interface that enables seamless switching between conversational search and URF modes.",
                "authors": "O. Khan, Ujjwal Sharma, Hongyi Zhu, S. Rudinac, Björn þór Jónsson",
                "citations": 1
            },
            {
                "title": "VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data",
                "abstract": "Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data. To address the issue, we introduce VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, we present an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, our framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised.",
                "authors": "Xuefeng Du, Reshmi Ghosh, Robert Sim, Ahmed Salem, Vitor Carvalho, Emily Lawton, Yixuan Li, Jack W. Stokes",
                "citations": 1
            },
            {
                "title": "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA",
                "abstract": "Visual question answering (VQA) tasks, often performed by visual language model (VLM), face challenges with long-tail knowledge. Recent retrieval-augmented VQA (RA-VQA) systems address this by retrieving and integrating external knowledge sources. However, these systems still suffer from redundant visual information irrelevant to the question during retrieval. To address these issues, in this paper, we propose LLM-RA, a novel method leveraging the reasoning capability of a large language model (LLM) to identify key visual entities, thus minimizing the impact of irrelevant information in the query of retriever. Furthermore, key visual entities are independently encoded for multimodal joint retrieval, preventing cross-entity interference. Experimental results demonstrate that our method outperforms other strong RA-VQA systems. In two knowledge-intensive VQA benchmarks, our method achieves the new state-of-the-art performance among those with similar scale of parameters and even performs comparably to models with 1-2 orders larger parameters.",
                "authors": "Pu Jian, Donglei Yu, Jiajun Zhang",
                "citations": 1
            },
            {
                "title": "TGS: Trajectory Generation and Selection using Vision Language Models in Mapless Outdoor Environments",
                "abstract": "We present a multi-modal trajectory generation and selection algorithm for real-world mapless outdoor navigation in human-centered environments. Such environments contain rich features like crosswalks, grass, and curbs, which are easily interpretable by humans, but not by mobile robots. We aim to compute suitable trajectories that (1) satisfy the environment-specific traversability constraints and (2) generate human-like paths while navigating on crosswalks, sidewalks, etc. Our formulation uses a Conditional Variational Autoencoder (CVAE) generative model enhanced with traversability constraints to generate multiple candidate trajectories for global navigation. We develop a visual prompting approach and leverage the Visual Language Model's (VLM) zero-shot ability of semantic understanding and logical reasoning to choose the best trajectory given the contextual information about the task. We evaluate our method in various outdoor scenes with wheeled robots and compare the performance with other global navigation algorithms. In practice, we observe an average improvement of 22.07% in satisfying traversability constraints and 30.53% in terms of human-like navigation in four different outdoor navigation scenarios.",
                "authors": "Daeun Song, Jing Liang, Xuesu Xiao, Dinesh Manocha",
                "citations": 1
            },
            {
                "title": "Guiding Long-Horizon Task and Motion Planning with Vision Language Models",
                "abstract": "Vision-Language Models (VLM) can generate plausible high-level plans when prompted with a goal, the context, an image of the scene, and any planning constraints. However, there is no guarantee that the predicted actions are geometrically and kinematically feasible for a particular robot embodiment. As a result, many prerequisite steps such as opening drawers to access objects are often omitted in their plans. Robot task and motion planners can generate motion trajectories that respect the geometric feasibility of actions and insert physically necessary actions, but do not scale to everyday problems that require common-sense knowledge and involve large state spaces comprised of many variables. We propose VLM-TAMP, a hierarchical planning algorithm that leverages a VLM to generate goth semantically-meaningful and horizon-reducing intermediate subgoals that guide a task and motion planner. When a subgoal or action cannot be refined, the VLM is queried again for replanning. We evaluate VLM- TAMP on kitchen tasks where a robot must accomplish cooking goals that require performing 30-50 actions in sequence and interacting with up to 21 objects. VLM-TAMP substantially outperforms baselines that rigidly and independently execute VLM-generated action sequences, both in terms of success rates (50 to 100% versus 0%) and average task completion percentage (72 to 100% versus 15 to 45%). See project site https://zt-yang.github.io/vlm-tamp-robot/ for more information.",
                "authors": "Zhutian Yang, Caelan Reed Garrett, Dieter Fox, Tom'as Lozano-P'erez, L. Kaelbling",
                "citations": 1
            },
            {
                "title": "Training-free Diffusion Model Alignment with Sampling Demons",
                "abstract": "Aligning diffusion models with user preferences has been a key challenge. Existing methods for aligning diffusion models either require retraining or are limited to differentiable reward functions. To address these limitations, we propose a stochastic optimization approach, dubbed Demon, to guide the denoising process at inference time without backpropagation through reward functions or model retraining. Our approach works by controlling noise distribution in denoising steps to concentrate density on regions corresponding to high rewards through stochastic optimization. We provide comprehensive theoretical and empirical evidence to support and validate our approach, including experiments that use non-differentiable sources of rewards such as Visual-Language Model (VLM) APIs and human judgements. To the best of our knowledge, the proposed approach is the first inference-time, backpropagation-free preference alignment method for diffusion models. Our method can be easily integrated with existing diffusion models without further training. Our experiments show that the proposed approach significantly improves the average aesthetics scores for text-to-image generation.",
                "authors": "Po-Hung Yeh, Kuang-Huei Lee, Jun-Cheng Chen",
                "citations": 1
            },
            {
                "title": "DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models",
                "abstract": "Large Language Models (LLMs) have demonstrated significant reasoning capabilities in robotic systems. However, their deployment in multi-robot systems remains fragmented and struggles to handle complex task dependencies and parallel execution. This study introduces the DART-LLM (Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models) system, designed to address these challenges. DART-LLM utilizes LLMs to parse natural language instructions, decomposing them into multiple subtasks with dependencies to establish complex task sequences, thereby enhancing efficient coordination and parallel execution in multi-robot systems. The system includes the QA LLM module, Breakdown Function modules, Actuation module, and a Vision-Language Model (VLM)-based object detection module, enabling task decomposition and execution from natural language instructions to robotic actions. Experimental results demonstrate that DART-LLM excels in handling long-horizon tasks and collaborative tasks with complex dependencies. Even when using smaller models like Llama 3.1 8B, the system achieves good performance, highlighting DART-LLM's robustness in terms of model size. Please refer to the project website \\url{https://wyd0817.github.io/project-dart-llm/} for videos and code.",
                "authors": "Yongdong Wang, Runze Xiao, Junichi Kasahara, Ryosuke Yajima, Keiji Nagatani, Atsushi Yamashita, Hajime Asama",
                "citations": 1
            },
            {
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration",
                "abstract": "Vision-Language Models (VLMs) have demonstrated impressive performance across a versatile set of tasks. A key challenge in accelerating VLMs is storing and accessing the large Key-Value (KV) cache that encodes long visual contexts, such as images or videos. While existing KV cache compression methods are effective for Large Language Models (LLMs), directly migrating them to VLMs yields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache, a novel KV cache compression recipe tailored for accelerating VLM inference. In this paper, we first investigate the unique sparsity pattern of VLM attention by distinguishing visual and text tokens in prefill and decoding phases. Based on these observations, we introduce a layer-adaptive sparsity-aware cache budget allocation method that effectively distributes the limited cache budget across different layers, further reducing KV cache size without compromising accuracy. Additionally, we develop a modality-aware token scoring policy to better evaluate the token importance. Empirical results on multiple benchmark datasets demonstrate that retaining only 10% of KV cache achieves accuracy comparable to that with full cache. In a speed benchmark, our method accelerates end-to-end latency of generating 100 tokens by up to 2.33x and speeds up decoding by up to 7.08x, while reducing the memory footprint of KV cache in GPU by 90%.",
                "authors": "Dezhan Tu, Danylo Vashchilenko, Yuzhe Lu, Panpan Xu",
                "citations": 1
            },
            {
                "title": "On AI-Inspired UI-Design",
                "abstract": "Graphical User Interface (or simply UI) is a primary mean of interaction between users and their device. In this paper, we discuss three major complementary approaches on how to use Artificial Intelligence (AI) to support app designers create better, more diverse, and creative UI of mobile apps. First, designers can prompt a Large Language Model (LLM) like GPT to directly generate and adjust one or multiple UIs. Second, a Vision-Language Model (VLM) enables designers to effectively search a large screenshot dataset, e.g. from apps published in app stores. The third approach is to train a Diffusion Model (DM) specifically designed to generate app UIs as inspirational images. We discuss how AI should be used, in general, to inspire and assist creative app design rather than automating it.",
                "authors": "Jialiang Wei, A. Courbis, Thomas Lambolais, Gérard Dray, Walid Maalej",
                "citations": 1
            },
            {
                "title": "ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling",
                "abstract": "Recognizing and disentangling visual attributes from objects is a foundation to many computer vision applications. While large vision language representations like CLIP had largely resolved the task of zero-shot object recognition, zero-shot visual attribute recognition remains a challenge because CLIP's contrastively-learned vision-language representation cannot effectively capture object-attribute dependencies. In this paper, we target this weakness and propose a sentence generation-based retrieval formulation for attribute recognition that is novel in 1) explicitly modeling a to-be-measured and retrieved object-attribute relation as a conditional probability graph, which converts the recognition problem into a dependency-sensitive language-modeling problem, and 2) applying a large pretrained Vision-Language Model (VLM) on this reformulation and naturally distilling its knowledge of image-object-attribute relations to use towards attribute recognition. Specifically, for each attribute to be recognized on an image, we measure the visual-conditioned probability of generating a short sentence encoding the attribute's relation to objects on the image. Unlike contrastive retrieval, which measures likelihood by globally aligning elements of the sentence to the image, generative retrieval is sensitive to the order and dependency of objects and attributes in the sentence. We demonstrate through experiments that generative retrieval consistently outperforms contrastive retrieval on two visual reasoning datasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual Genome Attribute Ranking (VGARank).",
                "authors": "William Y. Zhu, Keren Ye, Junjie Ke, Jiahui Yu, Leonidas J. Guibas, P. Milanfar, Feng Yang",
                "citations": 1
            },
            {
                "title": "Leveraging Large Language Models in Human-Robot Interaction: A Critical Analysis of Potential and Pitfalls",
                "abstract": "The emergence of large language models (LLM) and, consequently, vision language models (VLM) has ignited new imaginations among robotics researchers. At this point, the range of applications to which LLM and VLM can be applied in human-robot interaction (HRI), particularly socially assistive robots (SARs), is unchartered territory. However, LLM and VLM present unprecedented opportunities and challenges for SAR integration. We aim to illuminate the opportunities and challenges when roboticists deploy LLM and VLM in SARs. First, we conducted a meta-study of more than 250 papers exploring 1) major robots in HRI research and 2) significant applications of SARs, emphasizing education, healthcare, and entertainment while addressing 3) societal norms and issues like trust, bias, and ethics that the robot developers must address. Then, we identified 4) critical components of a robot that LLM or VLM can replace while addressing the 5) benefits of integrating LLM into robot designs and the 6) risks involved. Finally, we outline a pathway for the responsible and effective adoption of LLM or VLM into SARs, and we close our discussion by offering caution regarding this deployment.",
                "authors": "Jesse Atuhurra",
                "citations": 1
            },
            {
                "title": "Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model",
                "abstract": "We present a knowledge augmentation strategy for assessing the diagnostic groups and gait impairment from monocular gait videos. Based on a large-scale pre-trained Vision Language Model (VLM), our model learns and improves visual, textual, and numerical representations of patient gait videos, through a collective learning across three distinct modalities: gait videos, class-specific descriptions, and numerical gait parameters. Our specific contributions are two-fold: First, we adopt a knowledge-aware prompt tuning strategy to utilize the class-specific medical description in guiding the text prompt learning. Second, we integrate the paired gait parameters in the form of numerical texts to enhance the numeracy of the textual representation. Results demonstrate that our model not only significantly outperforms state-of-the-art methods in video-based classification tasks but also adeptly decodes the learned class-specific text features into natural language descriptions using the vocabulary of quantitative gait parameters. The code and the model will be made available at our project page: https://lisqzqng.github.io/GaitAnalysisVLM/.",
                "authors": "Diwei Wang, Kun Yuan, Candice Müller, Frédéric Blanc, N. Padoy, Hyewon Seo",
                "citations": 1
            },
            {
                "title": "BendVLM: Test-Time Debiasing of Vision-Language Embeddings",
                "abstract": "Vision-language model (VLM) embeddings have been shown to encode biases present in their training data, such as societal biases that prescribe negative characteristics to members of various racial and gender identities. VLMs are being quickly adopted for a variety of tasks ranging from few-shot classification to text-guided image generation, making debiasing VLM embeddings crucial. Debiasing approaches that fine-tune the VLM often suffer from catastrophic forgetting. On the other hand, fine-tuning-free methods typically utilize a\"one-size-fits-all\"approach that assumes that correlation with the spurious attribute can be explained using a single linear direction across all possible inputs. In this work, we propose Bend-VLM, a nonlinear, fine-tuning-free approach for VLM embedding debiasing that tailors the debiasing operation to each unique input. This allows for a more flexible debiasing approach. Additionally, we do not require knowledge of the set of inputs a priori to inference time, making our method more appropriate for online, open-set tasks such as retrieval and text guided image generation.",
                "authors": "Walter Gerych, Haoran Zhang, Kimia Hamidieh, Eileen Pan, Maanas Sharma, Tom Hartvigsen, Marzyeh Ghassemi",
                "citations": 1
            },
            {
                "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
                "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \\textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~\\footnote{Our code are available at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.",
                "authors": "Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, Li Bing",
                "citations": 0
            },
            {
                "title": "Supervision-free Vision-Language Alignment",
                "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation of these image-text pairs is both time-consuming and computationally expensive. To address this challenge, we introduce SVP (Supervision-free Visual Projection), a novel framework that enhances vision-language alignment without relying on curated data or preference annotation. SVP leverages self-captioning and a pre-trained grounding model as a feedback mechanism to elicit latent information in VLMs. We evaluate our approach across six key areas: captioning, referring, visual question answering, multitasking, hallucination control, and object recall. Results demonstrate significant improvements, including a 14% average improvement in captioning tasks, up to 12% increase in object recall, and substantial reduction in hallucination rates. Notably, a small VLM using SVP achieves hallucination reductions comparable to a model five times larger, while a VLM with initially poor referring capabilities more than doubles its performance, approaching parity with a model twice its size.",
                "authors": "Giorgio Giannone, Ruoteng Li, Qianli Feng, Evgeny Perevodchikov, Rui Chen, Aleix M. Martínez",
                "citations": 0
            },
            {
                "title": "Spatiotemporal economic risk of national road networks to episodic coastal flooding and sea level rise",
                "abstract": null,
                "authors": "R. Paulik, John Powell, A. Wild, Conrad Zorn, L. Wotherspoon",
                "citations": 0
            },
            {
                "title": "MSTS: A Multimodal Safety Test Suite for Vision-Language Models",
                "abstract": "Vision-language models (VLMs), which process image and text inputs, are increasingly integrated into chat assistants and other consumer AI applications. Without proper safeguards, however, VLMs may give harmful advice (e.g. how to self-harm) or encourage unsafe behaviours (e.g. to consume drugs). Despite these clear hazards, little work so far has evaluated VLM safety and the novel risks created by multimodal inputs. To address this gap, we introduce MSTS, a Multimodal Safety Test Suite for VLMs. MSTS comprises 400 test prompts across 40 fine-grained hazard categories. Each test prompt consists of a text and an image that only in combination reveal their full unsafe meaning. With MSTS, we find clear safety issues in several open VLMs. We also find some VLMs to be safe by accident, meaning that they are safe because they fail to understand even simple test prompts. We translate MSTS into ten languages, showing non-English prompts to increase the rate of unsafe model responses. We also show models to be safer when tested with text only rather than multimodal prompts. Finally, we explore the automation of VLM safety assessments, finding even the best safety classifiers to be lacking.",
                "authors": "Paul Röttger, Giuseppe Attanasio, Felix Friedrich, Janis Goldzycher, Alicia Parrish, Rishabh Bhardwaj, C. D. Bonaventura, Roman Eng, Gaia El Khoury Geagea, Sujata Goswami, Jieun Han, Dirk Hovy, Seogyeong Jeong, Paloma Jeretivc, F. Plaza-del-Arco, Donya Rooein, P. Schramowski, Anastassia Shaitarova, Xudong Shen, Richard Willats, Andrea Zugarini, Bertie Vidgen",
                "citations": 0
            },
            {
                "title": "MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders",
                "abstract": "Visual encoders are fundamental components in vision-language models (VLMs), each showcasing unique strengths derived from various pre-trained visual foundation models. To leverage the various capabilities of these encoders, recent studies incorporate multiple encoders within a single VLM, leading to a considerable increase in computational cost. In this paper, we present Mixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a novel framework that distills the unique proficiencies of multiple vision encoders into a single, efficient encoder model. Specifically, to mitigate conflicts and retain the unique characteristics of each teacher encoder, we employ low-rank adaptation (LoRA) and mixture-of-experts (MoEs) to selectively activate specialized knowledge based on input features, enhancing both adaptability and efficiency. To regularize the KD process and enhance performance, we propose an attention-based distillation strategy that adaptively weighs the different visual encoders and emphasizes valuable visual tokens, reducing the burden of replicating comprehensive but distinct features from multiple teachers. Comprehensive experiments on popular VLMs, such as LLaVA and LLaVA-NeXT, validate the effectiveness of our method. The code will be released.",
                "authors": "Jiajun Cao, Yuan Zhang, Tao Huang, Ming Lu, Qizhe Zhang, Ruichuan An, Ningning Ma, Shanghang Zhang",
                "citations": 0
            },
            {
                "title": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP Evaluation Benchmark",
                "abstract": "The proliferation of Vision-Language Models (VLMs) in the past several years calls for rigorous and comprehensive evaluation methods and benchmarks. This work analyzes existing VLM evaluation techniques, including automated metrics, AI-based assessments, and human evaluations across diverse tasks. We first introduce Robin - a novel suite of VLMs that we built by combining Large Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use Robin to identify shortcomings of current evaluation approaches across scales. Next, to overcome the identified limitations, we introduce CHIRP - a new long form response benchmark we developed for more robust and complete VLM evaluation. We provide open access to the Robin training code, model suite, and CHIRP benchmark to promote reproducibility and advance VLM research.",
                "authors": "Alexis Roger, Prateek Humane, Daniel Z Kaplan, Kshitij Gupta, Qi Sun, George Adamopoulos, Jonathan Siu Chi Lim, Quentin Anthony, Edwin Fennell, Irina Rish",
                "citations": 0
            },
            {
                "title": "Exploring the Capabilities of Vision-Language Models to Detect Visual Bugs in HTML5Applications",
                "abstract": "The HyperText Markup Language 5 (HTML5)is useful for creating visual-centric web applications. However, unlike traditional web applications, HTML5applications render objects onto thebitmap without representing them in the Document Object Model (DOM). Mismatches between the expected and actual visual output of thebitmap are termed visual bugs. Due to the visual-centric nature ofapplications, visual bugs are important to detect because such bugs can render aapplication useless. As we showed in prior work, Asset-Based graphics can provide the ground truth for a visual test oracle. However, manyapplications procedurally generate their graphics. In this paper, we investigate how to detect visual bugs inapplications that use Procedural graphics as well. In particular, we explore the potential of Vision-Language Models (VLMs) to automatically detect visual bugs. Instead of defining an exact visual test oracle, information about the application's expected functionality (the context) can be provided with the screenshot as input to the VLM. To evaluate this approach, we constructed a dataset containing 80 bug-injected screenshots across four visual bug types (Layout, Rendering, Appearance, and State) plus 20 bug-free screenshots from 20applications. We ran experiments with a state-of-the-art VLM using several combinations of text and image context to describe each application's expected functionality. Our results show that by providing the application README(s), a description of visual bug types, and a bug-free screenshot as context, VLMs can be leveraged to detect visual bugs with up to 100% per-application accuracy.",
                "authors": "Finlay Macklon, C. Bezemer",
                "citations": 0
            },
            {
                "title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models",
                "abstract": "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability - fine-grained motion comprehension - remains under-explored in current benchmarks. To address this gap, we propose MotionBench, a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models. MotionBench evaluates models' motion-level perception through six primary categories of motion-oriented question types and includes data collected from diverse sources, ensuring a broad representation of real-world video content. Experimental results reveal that existing VLMs perform poorly in understanding fine-grained motions. To enhance VLM's ability to perceive fine-grained motion within a limited sequence length of LLM, we conduct extensive experiments reviewing VLM architectures optimized for video feature compression and propose a novel and efficient Through-Encoder (TE) Fusion method. Experiments show that higher frame rate inputs and TE Fusion yield improvements in motion understanding, yet there is still substantial room for enhancement. Our benchmark aims to guide and motivate the development of more capable video understanding models, emphasizing the importance of fine-grained motion comprehension. Project page: https://motion-bench.github.io .",
                "authors": "Wenyi Hong, Yean Cheng, Zhuoyi Yang, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, Jie Tang",
                "citations": 0
            },
            {
                "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
                "abstract": "The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation. Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints. In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM fine-tuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation.",
                "authors": "Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, Hao Dong",
                "citations": 0
            },
            {
                "title": "MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation",
                "abstract": "Vision-language models (VLMs) are highly effective but often underperform on specialized tasks; for example, Llava-1.5 struggles with chart and diagram understanding due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MM-Gen, a scalable method that generates task-specific, high-quality synthetic text for candidate images by leveraging stronger models. MM-Gen employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-Gen leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-Gen achieves up to 1.6x better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.",
                "authors": "Siddharth Joshi, Besmira Nushi, Vidhisha Balachandran, Varun Chandrasekaran, Vibhav Vineet, Neel Joshi, Baharan Mirzasoleiman",
                "citations": 0
            },
            {
                "title": "Bi‐LORA: A Vision‐Language Approach for Synthetic Image Detection",
                "abstract": "Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the high challenge in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero‐shot nature of vision‐language models (VLMs). We introduce an innovative method called Bi‐LORA that leverages VLMs, combined with low‐rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model‐generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting‐edge VLM, notably bootstrapping language image pre‐training (BLIP)2. Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion‐generated images from unknown diffusion‐based generative models during training, showcasing robustness to noise, and demonstrating generalisation capabilities to GANs. The experiments show that Bi‐LORA outperforms state of the art models in cross‐generator tasks because it leverages multi‐modal learning, open‐world visual knowledge, and benefits from robust, high‐level semantic understanding. By combining visual and textual knowledge, it can handle variations in the data distribution (such as those caused by different generators) and maintain strong performance across different domains. Its ability to transfer knowledge, robustly extract features and perform zero‐shot learning also contributes to its generalisation capabilities, making it more adaptable to new generators. The experimental results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou‐Keita/VLM‐DETECT.",
                "authors": "Mamadou Keita, W. Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, David Camacho, Abdenour Hadid",
                "citations": 0
            },
            {
                "title": "CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries",
                "abstract": "Vision-language models (VLMs) have advanced human-AI interaction but struggle with cultural understanding, often misinterpreting symbols, gestures, and artifacts due to biases in predominantly Western-centric training data. In this paper, we construct CultureVerse, a large-scale multimodal benchmark covering 19, 682 cultural concepts, 188 countries/regions, 15 cultural concepts, and 3 question types, with the aim of characterizing and improving VLMs' multicultural understanding capabilities. Then, we propose CultureVLM, a series of VLMs fine-tuned on our dataset to achieve significant performance improvement in cultural understanding. Our evaluation of 16 models reveals significant disparities, with a stronger performance in Western concepts and weaker results in African and Asian contexts. Fine-tuning on our CultureVerse enhances cultural perception, demonstrating cross-cultural, cross-continent, and cross-dataset generalization without sacrificing performance on models' general VLM benchmarks. We further present insights on cultural generalization and forgetting. We hope that this work could lay the foundation for more equitable and culturally aware multimodal AI systems.",
                "authors": "Shudong Liu, Yiqiao Jin, Cheng Li, Derek F. Wong, Qingsong Wen, Lichao Sun, Haipeng Chen, Xing Xie, Jindong Wang",
                "citations": 0
            },
            {
                "title": "From InSAR‐Derived Subsidence to Relative Sea‐Level Rise—A Call for Rigor",
                "abstract": "Coastal subsidence, the gradual sinking of coastal land, considerably exacerbates the impacts of climate change‐driven sea‐level rise (SLR). While global sea levels rise, land subsidence often increases relative SLR locally. Thiéblemont et al. (2024, https://doi.org/10.1029/2024ef004523) reached a remarkable milestone by providing a continental‐scale estimate of vertical land motion (VLM) across European coastal zones by utilizing European Ground Motion Service (EGMS) data, obtained from Interferometric Synthetic Aperture Radar (InSAR) data from Sentinel‐1 satellites. Their findings reveal widespread coastal subsidence, with nearly half of the coastal floodplains, including major cities and ports, subsiding at rates exceeding 1 mm/yr, thereby exacerbating relative SLR. The study emphasizes the critical role of InSAR‐data calibration, indicating that the EGMS geodetic reference frame significantly influences VLM estimates. This study highlights the need for a robust InSAR‐data processing framework to accurately interpret VLM and its relationship to relative SLR. The processing pipeline should ensure internal consistency of SAR data and rigorously assess output accuracy, considering also post‐processing effects. Correct interpretation of results is essential as InSAR satellites measure reflector movement, which may not always align with land surface movement, particularly in urban areas. Ignoring these discrepancies can lead to underestimation of subsidence rates. While InSAR data offers valuable research opportunities, it poses risks of oversimplification and misinterpretation, especially when linked to sea‐level change. We call for standardized processing workflows and cross‐disciplinary collaboration, essential for accurate VLM interpretations, particularly in coastal cities and river deltas, to ultimately enhance the reliability of relative SLR projections and inform effective coastal management strategies.",
                "authors": "P. Minderhoud, M. Shirzaei, P. Teatini",
                "citations": 0
            },
            {
                "title": "Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives",
                "abstract": "Recent advancements in Vision-Language Models (VLMs) have sparked interest in their use for autonomous driving, particularly in generating interpretable driving decisions through natural language. However, the assumption that VLMs inherently provide visually grounded, reliable, and interpretable explanations for driving remains largely unexamined. To address this gap, we introduce DriveBench, a benchmark dataset designed to evaluate VLM reliability across 17 settings (clean, corrupted, and text-only inputs), encompassing 19,200 frames, 20,498 question-answer pairs, three question types, four mainstream driving tasks, and a total of 12 popular VLMs. Our findings reveal that VLMs often generate plausible responses derived from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs. This behavior, concealed by dataset imbalances and insufficient evaluation metrics, poses significant risks in safety-critical scenarios like autonomous driving. We further observe that VLMs struggle with multi-modal reasoning and display heightened sensitivity to input corruptions, leading to inconsistencies in performance. To address these challenges, we propose refined evaluation metrics that prioritize robust visual grounding and multi-modal understanding. Additionally, we highlight the potential of leveraging VLMs' awareness of corruptions to enhance their reliability, offering a roadmap for developing more trustworthy and interpretable decision-making systems in real-world autonomous driving contexts. The benchmark toolkit is publicly accessible.",
                "authors": "Shaoyuan Xie, Lingdong Kong, Yuhao Dong, Chonghao Sima, Wenwei Zhang, Qi Alfred Chen, Ziwei Liu, Liang Pan",
                "citations": 0
            },
            {
                "title": "Eve: Efficient Multimodal Vision Language Models with Elastic Visual Experts",
                "abstract": "Multimodal vision language models (VLMs) have made significant progress with the support of continuously increasing model sizes and data volumes. Running VLMs on edge devices has become a challenge for their widespread application. There are several efficient VLM efforts, but they often sacrifice linguistic capabilities to enhance multimodal abilities, or require extensive training. To address this quandary,we introduce the innovative framework of Efficient Vision Language Models with Elastic Visual Experts (Eve). By strategically incorporating adaptable visual expertise at multiple stages of training, Eve strikes a balance between preserving linguistic abilities and augmenting multimodal capabilities. This balanced approach results in a versatile model with only 1.8B parameters that delivers significant improvements in both multimodal and linguistic tasks. Notably, in configurations below 3B parameters, Eve distinctly outperforms in language benchmarks and achieves state-of-the-art results 68.87% in VLM Benchmarks. Additionally, its multimodal accuracy outstrips that of the larger 7B LLaVA-1.5 model. Our code is available at https://github.com/rangmiao/Eve.",
                "authors": "Miao Rang, Zhenni Bi, Chuanjian Liu, Yehui Tang, Kai Han, Yunhe Wang",
                "citations": 0
            },
            {
                "title": "BDNF expression mediates verbal learning and memory in women in a cohort enriched with risk for Alzheimer's disease",
                "abstract": "Abstract INTRODUCTION This study examined whether sex differences in verbal learning and memory (VLM) are mediated by plasma brain‐derived neurotrophic factor (BDNF) expression. METHODS In a sample of n = 201 participants (63.81 ± 6.04 years, 66.2% female, 65.7% family history of Alzheimer's disease [AD], 38% apolipoprotein E [APOE] ε4+) from the Wisconsin Registry for Alzheimer's Prevention, VLM was measured using trials 3 through 5 and delayed recall from the Rey Auditory Verbal Learning Test. Plasma BDNF was measured using a Human BDNF Quantikine Immunoassay. Mediation analysis used bootstrapping, and stratified mediation models tested the conditional dependence of APOE ε4 carriage. RESULTS BDNF partially mediated the sex–VLM relationship (β = −0.07; 95% confidence interval [CI]: −0.18, −0.01). Female APOE ε4 carriers had higher VLM scores (β = −0.53; p = 0.03), while female non‐carriers had both higher BDNF levels (β = −0.68; p < 0.01) and VLM scores (β = −1.06; p < 0.01); BDNF was again a significant mediator (β = −0.18; 95% CI: −0.37, −0.05). DISCUSSION This study found that circulating BDNF mediates higher verbal memory scores in females—particularly in APOE ε4 non‐carriers. Highlights Sex differences in verbal learning and memory (VLM) were mediated by plasma brain‐derived neurotrophic factor (BDNF) levels. Women exhibited higher VLM scores and plasma BDNF levels compared to men. The protective effect of BDNF in women was attenuated by apolipoprotein E ε4 carriage. Findings suggest sex‐specific mechanisms against verbal memory decline in aging.",
                "authors": "Kyle J. Edmunds, Alyssa A Pandos, Isabella Hoang, Gabriella M. Mamlouk, Alice Motovylyak, Sarah R. Lose, Sanjay Asthana, Matthew Stremlau, Sterling C. Johnson, Henriette van Praag, O. Okonkwo",
                "citations": 0
            },
            {
                "title": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models",
                "abstract": "In recent years, 2D Vision-Language Models (VLMs) have made significant strides in image-text understanding tasks. However, their performance in 3D spatial comprehension, which is critical for embodied intelligence, remains limited. Recent advances have leveraged 3D point clouds and multi-view images as inputs, yielding promising results. However, we propose exploring a purely vision-based solution inspired by human perception, which merely relies on visual cues for 3D spatial understanding. This paper empirically investigates the limitations of VLMs in 3D spatial knowledge, revealing that their primary shortcoming lies in the lack of global-local correspondence between the scene and individual frames. To address this, we introduce GPT4Scene, a novel visual prompting paradigm in VLM training and inference that helps build the global-local relationship, significantly improving the 3D spatial understanding of indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV) image from the video and marks consistent object IDs across both frames and the BEV image. The model then inputs the concatenated BEV image and video frames with markers. In zero-shot evaluations, GPT4Scene improves performance over closed-source VLMs like GPT-4o. Additionally, we prepare a processed video dataset consisting of 165K text annotation to fine-tune open-source VLMs, achieving state-of-the-art performance on all 3D understanding tasks. Surprisingly, after training with the GPT4Scene paradigm, VLMs consistently improve during inference, even without visual prompting and BEV image as explicit correspondence. It demonstrates that the proposed paradigm helps VLMs develop an intrinsic ability to understand 3D scenes, which paves the way for a noninvasive approach to extending pre-trained VLMs for 3D scene understanding.",
                "authors": "Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, Hengshuang Zhao",
                "citations": 0
            },
            {
                "title": "UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation",
                "abstract": "The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach.",
                "authors": "Oleg Sautenkov, Yasheerah Yaqoot, Artem Lykov, Muhammad Ahsan Mustafa, Grik Tadevosyan, Aibek Akhmetkazy, Miguel Altamirano Cabrera, Mikhail Martynov, Sausar Karaf, D. Tsetserukou",
                "citations": 0
            },
            {
                "title": "Descriptron: Testing Artificial Intelligence for Automating Taxonomic Species Descriptions with a User-friendly Software Package",
                "abstract": "Recent advances in Computer Vision, Convolutional Neural Networks (CNNs), Vision Transformers (ViTs) and Large Language Models (LLMs) suggest that it may be possible to approach mimicking the ability to decode knowledge about morphology and taxonomy to describe species in an automated way. Here we test for the first time a current state-of-the-art Vision Language Model (VLM) to approximate taxonomic species descriptions in an automated manner. The test utilizes a new graphical user interface, Descriptron, that collects data about biological images and transmits this highly specialized knowledge to a VLM to decode the taxonomic knowledge encoded in labeled biological images with text. Our results indicate that current state-of-the-art VLM (GPT-4o) can produce automated species descriptions that with error correction approximate taxonomists’ ability to describe morphological features of species and organize them in taxonomic keys. However, the results are not without significant errors and the VLM requires the input of the taxonomists knowledge to prevent widespread hallucinations by the VLM. We find here that the taxonomist is clearly needed to both teach and closely supervise the VLM. However, the time saved by utilizing Descriptron is also significant. Taxonomists remain essential for teaching and closely supervising the VLM. The time saved by utilizing Descriptron are nevertheless already very significant. The Descriptron program and supporting example prompt files are free to use under an Apache2 License available from: https://github.com/alexrvandam/Descriptron.",
                "authors": "Alex R. Van Dam, L. Š. Serbina",
                "citations": 0
            },
            {
                "title": "ImageRef-VL: Enabling Contextual Image Referencing in Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in understanding multimodal inputs and have been widely integrated into Retrieval-Augmented Generation (RAG) based conversational systems. While current VLM-powered chatbots can provide textual source references in their responses, they exhibit significant limitations in referencing contextually relevant images during conversations. In this paper, we introduce Contextual Image Reference -- the ability to appropriately reference relevant images from retrieval documents based on conversation context -- and systematically investigate VLMs' capability in this aspect. We conduct the first evaluation for contextual image referencing, comprising a dedicated testing dataset and evaluation metrics. Furthermore, we propose ImageRef-VL, a method that significantly enhances open-source VLMs' image referencing capabilities through instruction fine-tuning on a large-scale, manually curated multimodal conversation dataset. Experimental results demonstrate that ImageRef-VL not only outperforms proprietary models but also achieves an 88% performance improvement over state-of-the-art open-source VLMs in contextual image referencing tasks. Our code is available at https://github.com/bytedance/ImageRef-VL.",
                "authors": "Jingwei Yi, Junhao Yin, Ju Xu, Peng Bao, Yongliang Wang, Wei Fan, Hao Wang",
                "citations": 0
            },
            {
                "title": "Visual Large Language Models for Generalized and Specialized Applications",
                "abstract": "Visual-language models (VLM) have emerged as a powerful tool for learning a unified embedding space for vision and language. Inspired by large language models, which have demonstrated strong reasoning and multi-task capabilities, visual large language models (VLLMs) are gaining increasing attention for building general-purpose VLMs. Despite the significant progress made in VLLMs, the related literature remains limited, particularly from a comprehensive application perspective, encompassing generalized and specialized applications across vision (image, video, depth), action, and language modalities. In this survey, we focus on the diverse applications of VLLMs, examining their using scenarios, identifying ethics consideration and challenges, and discussing future directions for their development. By synthesizing these contents, we aim to provide a comprehensive guide that will pave the way for future innovations and broader applications of VLLMs. The paper list repository is available: https://github.com/JackYFL/awesome-VLLMs.",
                "authors": "Yifan Li, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, Yu Kong",
                "citations": 0
            },
            {
                "title": "Visuo-Tactile Zero-Shot Object Recognition with Vision-Language Model",
                "abstract": "Tactile perception is vital, especially when distinguishing visually similar objects. We propose an approach to incorporate tactile data into a Vision-Language Model (VLM) for visuo-tactile zero-shot object recognition. Our approach leverages the zero-shot capability of VLMs to infer tactile properties from the names of tactilely similar objects. The proposed method translates tactile data into a textual description solely by annotating object names for each tactile sequence during training, making it adaptable to various contexts with low training costs. The proposed method was evaluated on the FoodReplica and Cube datasets, demonstrating its effectiveness in recognizing objects that are difficult to distinguish by vision alone.",
                "authors": "Shiori Ueda, Atsushi Hashimoto, Masashi Hamaya, Kazutoshi Tanaka, Hideo Saito",
                "citations": 0
            },
            {
                "title": "LOBSTAR: Language Model-based Obstruction Detection for Augmented Reality",
                "abstract": "In Augmented Reality (AR), improper virtual content placement can obstruct real-world elements, causing confusion and degrading the experience. To address this, we present LOBSTAR (Language model-based OBSTruction detection for Augmented Reality), the first system leveraging a vision language model (VLM) to detect key objects and prevent obstructions in AR. We evaluated LOBSTAR using both real-world and virtual-scene images and developed a mobile app for AR content obstruction detection. Our results demonstrate that LOBSTAR effectively understands scenes and detects obstructive content with well-designed VLM prompts, achieving up to 96% accuracy and a detection latency of 580ms on a mobile app.",
                "authors": "Yanming Xiu, T. Scargill, Maria Gorlatova",
                "citations": 0
            },
            {
                "title": "Space-LLaVA: a Vision-Language Model Adapted to Extraterrestrial Applications",
                "abstract": "Foundation Models (FMs), e.g., large language models, possess attributes of intelligence which offer promise to endow a robot with the contextual understanding necessary to navigate complex, unstructured tasks in the wild. We see three core challenges in the future of space robotics that motivate building an FM for the space robotics community: 1) Scalability of ground-in-the-loop operations; 2) Generalizing prior knowledge to novel environments; and 3) Multi-modality in tasks and sensor data. As a first-step towards a space foundation model, we programmatically augment three extraterrestrial databases with fine-grained language annotations inspired by the sensory reasoning necessary to e.g., identify a site of scientific interest on Mars, building a synthetic dataset of visual-question-answer and visual instruction-following tuples. We fine-tune a pre-trained LLaVA 13B checkpoint on our augmented dataset to adapt a Vision-Language Model (VLM) to the visual semantic features in an extraterrestrial environment, demonstrating FMs as a tool for specialization and enhancing a VLM's zero-shot performance on unseen task types in comparison to state-of-the-art VLMs. Ablation studies show that fine-tuning the language backbone and vision-language adapter in concert is key to facilitate adaption while a small percentage, e.g., 20%, of the pre-training data can be used to safeguard against catastrophic forgetting.",
                "authors": "Matthew Foutter, Daniele Gammelli, J. Kruger, Ethan Foss, Praneet Bhoj, T. Guffanti, Simone D'Amico, Marco Pavone",
                "citations": 0
            },
            {
                "title": "Revisiting Vision-Language Features Adaptation and Inconsistency for Social Media Popularity Prediction",
                "abstract": "Social media popularity (SMP) prediction is a complex task involving multi-modal data integration. While pre-trained vision-language models (VLMs) like CLIP have been widely adopted for this task, their effectiveness in capturing the unique characteristics of social media content remains unexplored. This paper critically examines the applicability of CLIP-based features in SMP prediction, focusing on the overlooked phenomenon of semantic inconsistency between images and text in social media posts. Through extensive analysis, we demonstrate that this inconsistency increases with post popularity, challenging the conventional use of VLM features. We provide a comprehensive investigation of semantic inconsistency across different popularity intervals and analyze the impact of VLM feature adaptation on SMP tasks. Our experiments reveal that incorporating inconsistency measures and adapted text features significantly improves model performance, achieving an SRC of 0.729 and an MAE of 1.227. These findings not only enhance SMP prediction accuracy but also provide crucial insights for developing more targeted approaches in social media analysis.",
                "authors": "Chih-Chung Hsu, Chia-Ming Lee, Yu-Fan Lin, Yi-Shiuan Chou, Chih-Yu Jian, Chin-Han Tsai",
                "citations": 0
            },
            {
                "title": "ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time",
                "abstract": "Vision Language Models (VLMs) have become essential backbones for multimodal intelligence, yet significant safety challenges limit their real-world application. While textual inputs are often effectively safeguarded, adversarial visual inputs can easily bypass VLM defense mechanisms. Existing defense methods are either resource-intensive, requiring substantial data and compute, or fail to simultaneously ensure safety and usefulness in responses. To address these limitations, we propose a novel two-phase inference-time alignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual contents and output responses to establish a robust safety awareness in multimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep levels by conditioning the VLMs' generative distribution with an interference prefix and performing sentence-level best-of-N to search the most harmless and helpful generation paths. Extensive experiments show that ETA outperforms baseline methods in terms of harmlessness, helpfulness, and efficiency, reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6% win-ties in GPT-4 helpfulness evaluation. The code is publicly available at https://github.com/DripNowhy/ETA.",
                "authors": "Yi Ding, Bolian Li, Ruqi Zhang",
                "citations": 0
            },
            {
                "title": "PaliGemma 2: A Family of Versatile VLMs for Transfer",
                "abstract": "PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.",
                "authors": "A. Steiner, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, Siyang Qin, R. Ingle, Emanuele Bugliarello, Sahar Kazemzadeh, Thomas Mesnard, Ibrahim M. Alabdulmohsin, Lucas Beyer, Xiao-Qi Zhai",
                "citations": 0
            },
            {
                "title": "Robot Navigation Using Physically Grounded Vision-Language Models in Outdoor Environments",
                "abstract": "We present a novel autonomous robot navigation algorithm for outdoor environments that is capable of handling diverse terrain traversability conditions. Our approach, VLM-GroNav, uses vision-language models (VLMs) and integrates them with physical grounding that is used to assess intrinsic terrain properties such as deformability and slipperiness. We use proprioceptive-based sensing, which provides direct measurements of these physical properties, and enhances the overall semantic understanding of the terrains. Our formulation uses in-context learning to ground the VLM's semantic understanding with proprioceptive data to allow dynamic updates of traversability estimates based on the robot's real-time physical interactions with the environment. We use the updated traversability estimations to inform both the local and global planners for real-time trajectory replanning. We validate our method on a legged robot (Ghost Vision 60) and a wheeled robot (Clearpath Husky), in diverse real-world outdoor environments with different deformable and slippery terrains. In practice, we observe significant improvements over state-of-the-art methods by up to 50% increase in navigation success rate.",
                "authors": "Mohamed Bashir Elnoor, Kasun Weerakoon, Gershom Seneviratne, Ruiqi Xian, Tianrui Guan, M. K. M. Jaffar, Vignesh Rajagopal, Dinesh Manocha",
                "citations": 0
            },
            {
                "title": "IDEATOR: Jailbreaking Large Vision-Language Models Using Themselves",
                "abstract": "As large Vision-Language Models (VLMs) grow in prominence, ensuring their safe deployment has become critical. Recent studies have explored VLM robustness against jailbreak attacks--techniques that exploit model vulnerabilities to elicit harmful outputs. However, the limited availability of diverse multi-modal data has led current approaches to rely heavily on adversarial or manually crafted images derived from harmful text datasets, which may lack effectiveness and diversity across different contexts. In this paper, we propose a novel jailbreak method named IDEATOR, which autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is based on the insight that VLMs themselves could serve as powerful red team models for generating multimodal jailbreak prompts. Specifically, IDEATOR uses a VLM to create targeted jailbreak texts and pairs them with jailbreak images generated by a state-of-the-art diffusion model. Our extensive experiments demonstrate IDEATOR's high effectiveness and transferability. Notably, it achieves a 94% success rate in jailbreaking MiniGPT-4 with an average of only 5.34 queries, and high success rates of 82%, 88%, and 75% when transferred to LLaVA, InstructBLIP, and Meta's Chameleon, respectively. IDEATOR uncovers specific vulnerabilities in VLMs under black-box conditions, underscoring the need for improved safety mechanisms.",
                "authors": "Ruofan Wang, Bo Wang, Xiaosen Wang, Xingjun Ma, Yu-Gang Jiang",
                "citations": 0
            },
            {
                "title": "WiseAD: Knowledge Augmented End-to-End Autonomous Driving with Vision-Language Model",
                "abstract": "The emergence of general human knowledge and impressive logical reasoning capacity in rapidly progressed vision-language models (VLMs) have driven increasing interest in applying VLMs to high-level autonomous driving tasks, such as scene understanding and decision-making. However, an in-depth study on the relationship between knowledge proficiency, especially essential driving expertise, and closed-loop autonomous driving performance requires further exploration. In this paper, we investigate the effects of the depth and breadth of fundamental driving knowledge on closed-loop trajectory planning and introduce WiseAD, a specialized VLM tailored for end-to-end autonomous driving capable of driving reasoning, action justification, object recognition, risk analysis, driving suggestions, and trajectory planning across diverse scenarios. We employ joint training on driving knowledge and planning datasets, enabling the model to perform knowledge-aligned trajectory planning accordingly. Extensive experiments indicate that as the diversity of driving knowledge extends, critical accidents are notably reduced, contributing 11.9% and 12.4% improvements in the driving score and route completion on the Carla closed-loop evaluations, achieving state-of-the-art performance. Moreover, WiseAD also demonstrates remarkable performance in knowledge evaluations on both in-domain and out-of-domain datasets.",
                "authors": "Songyan Zhang, Wenhui Huang, Zihui Gao, Hao Chen, Chen Lv",
                "citations": 0
            },
            {
                "title": "Diagnostics-LLaVA",
                "abstract": "The recent advancements in the area of Large language models (LLMs) has opened horizons for conversational assistant-based intelligent models capable of interpreting images, and providing textual response, also known as Visual language models (VLMs). These models can assist equipment operators and maintenance technicians in the complex Prognostics and Health Management (PHM) tasks such as diagnostics of faults, root cause analysis and repair recommendation. Significant open source contributions in the area of VLMs have been made. However, models trained on general data fail to perform well in complex tasks in specialized domains such as diagnostics and repair of industrial equipment. Therefore, in this paper we discuss our work on development of Diagnostics-LLaVA, a VLM suitable for interpreting images of specific industrial equipment and provide better response than existing open source models in PHM tasks such as fault diagnostics and repair recommendation. We introduce Diagnostics-LLaVA based on the architecture of LLaVA, and created one instance of the Diagnostics-LLaVA for the automotive repair domain, referred to as Automotive-LLaVA. We demonstrate that our proposed Automotive-LLaVA model performs better than the state-of-the-art open source visual language models such as mPlugOWL and LLaVA in both qualitative and quantitative experiments.",
                "authors": "Aman Kumar, Mahbubul Alam, Ahmed K. Farahat, Maheshjabu Somineni, C. Gupta",
                "citations": 0
            },
            {
                "title": "HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers",
                "abstract": "Large Vision-Language-Action (VLA) models, leveraging powerful pre trained Vision-Language Models (VLMs) backends, have shown promise in robotic control due to their impressive generalization ability. However, the success comes at a cost. Their reliance on VLM backends with billions of parameters leads to high computational costs and inference latency, limiting the testing scenarios to mainly quasi-static tasks and hindering performance in dynamic tasks requiring rapid interactions. To address these limitations, this paper proposes HiRT, a Hierarchical Robot Transformer framework that enables flexible frequency and performance trade-off. HiRT keeps VLMs running at low frequencies to capture temporarily invariant features while enabling real-time interaction through a high-frequency vision-based policy guided by the slowly updated features. Experiment results in both simulation and real-world settings demonstrate significant improvements over baseline methods. Empirically, in static tasks, we double the control frequency and achieve comparable success rates. Additionally, on novel real-world dynamic ma nipulation tasks which are challenging for previous VLA models, HiRT improves the success rate from 48% to 75%.",
                "authors": "Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, Jianyu Chen",
                "citations": 0
            },
            {
                "title": "What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation",
                "abstract": "Vision-Language Models (VLMs) have gained community-spanning prominence due to their ability to integrate visual and textual inputs to perform complex tasks. Despite their success, the internal decision-making processes of these models remain opaque, posing challenges in high-stakes applications. To address this, we introduce NOTICE, the first Noise-free Text-Image Corruption and Evaluation pipeline for mechanistic interpretability in VLMs. NOTICE incorporates a Semantic Minimal Pairs (SMP) framework for image corruption and Symmetric Token Replacement (STR) for text. This approach enables semantically meaningful causal mediation analysis for both modalities, providing a robust method for analyzing multimodal integration within models like BLIP. Our experiments on the SVO-Probes, MIT-States, and Facial Expression Recognition datasets reveal crucial insights into VLM decision-making, identifying the significant role of middle-layer cross-attention heads. Further, we uncover a set of ``universal cross-attention heads'' that consistently contribute across tasks and modalities, each performing distinct functions such as implicit image segmentation, object inhibition, and outlier inhibition. This work paves the way for more transparent and interpretable multimodal systems.",
                "authors": "Michal Golovanevsky, William Rudman, Vedant Palit, Ritambhara Singh, Carsten Eickhoff",
                "citations": 0
            },
            {
                "title": "Frequency Is What You Need: Word-frequency Masking Benefits Vision-Language Model Pre-training",
                "abstract": "Vision Language Models (VLMs) can be trained more efficiently if training sets can be reduced in size. Recent work has shown the benefits of masking text during VLM training using a variety of approaches: truncation, random masking, block masking and syntax masking. In this paper, we show that the best masking strategy changes over training epochs and that, given sufficient training epochs, word frequency information is what you need to achieve the best performance. Experiments on a large range of data sets demonstrate the advantages of our approach, called Contrastive Language-Image Pre-training with word Frequency Masking (CLIPF). The benefits are particularly evident as the number of input tokens decreases. We analyze the impact of CLIPF vs. other masking approaches on word frequency balance and discuss the apparently critical contribution of CLIPF in maintaining word frequency balance across POS categories.",
                "authors": "Mingliang Liang, Martha Larson",
                "citations": 0
            },
            {
                "title": "TOXOCARIASIS VISCERAL AND OCULAR LARVA MIGRANS: IS IT STILL A NEGLECTED ZOONOTIC DISEASE?",
                "abstract": "Zoonotic toxocariasis (visceral larva migrans or VLM & ocular larva migrans or OLM) refers to human infection caused by helminths that are not natural human parasites. Toxocariasis is an underestimated geohelminthic infection may be worldwide. Toxocariasis occurs as a result of human infection with the dog ascarid larvae, Toxocara canis or less commonly, the cat ascarid larvae, T. cati . VLM is mainly a disease of young children, especially those with exposure to playgrounds and sandboxes contaminated by dog or cat feces. While common globally, prevalence in both animals and people is highest in developing countries. In developed countries, more infections are detected among persons in lower socioeconomic strata. TVLM & OLM clinical presentations, although most infections are asymptomatic, yet in VLM, which occurs mostly in preschool children, larvae invade multiple tissues (mainly liver, lung, skeletal muscle, or heart) causing nonspecific symptoms as fever, myalgia, weight loss, cough, rashes, hepatosplenomegaly accompanied by hypereosinophilia. Migration to CNS (neurotoxocariasis) is uncommon and can cause eosinophilic meningoencephalitis and epilepsy. Death can occur with severe cardiac, pulmonary, or neurologic involvement.",
                "authors": "Tosson A. Morsy, Y. M. A. Alqurashi, H. Ozbak",
                "citations": 0
            },
            {
                "title": "SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation",
                "abstract": "Automating garment manipulation poses a significant challenge for assistive robotics due to the diverse and deformable nature of garments. Traditional approaches typically require separate models for each garment type, which limits scalability and adaptability. In contrast, this paper presents a unified approach using vision-language models (VLMs) to improve keypoint prediction across various garment categories. By interpreting both visual and semantic information, our model enables robots to manage different garment states with a single model. We created a large-scale synthetic dataset using advanced simulation techniques, allowing scalable training without extensive real-world data. Experimental results indicate that the VLM-based method significantly enhances keypoint detection accuracy and task success rates, providing a more flexible and general solution for robotic garment manipulation. In addition, this research also underscores the potential of VLMs to unify various garment manipulation tasks within a single framework, paving the way for broader applications in home automation and assistive robotics for future.",
                "authors": "Xin Li, Siyuan Huang, Qiaojun Yu, Zhengkai Jiang, Ce Hao, Yimeng Zhu, Hongsheng Li, Peng Gao, Cewu Lu",
                "citations": 0
            },
            {
                "title": "VCA: Video Curious Agent for Long Video Understanding",
                "abstract": "Long video understanding poses unique challenges due to their temporal complexity and low information density. Recent works address this task by sampling numerous frames or incorporating auxiliary tools using LLMs, both of which result in high computational costs. In this work, we introduce a curiosity-driven video agent with self-exploration capability, dubbed as VCA. Built upon VLMs, VCA autonomously navigates video segments and efficiently builds a comprehensive understanding of complex video sequences. Instead of directly sampling frames, VCA employs a tree-search structure to explore video segments and collect frames. Rather than relying on external feedback or reward, VCA leverages VLM's self-generated intrinsic reward to guide its exploration, enabling it to capture the most crucial information for reasoning. Experimental results on multiple long video benchmarks demonstrate our approach's superior effectiveness and efficiency.",
                "authors": "Zeyuan Yang, Delin Chen, Xueyang Yu, Maohao Shen, Chuang Gan",
                "citations": 0
            },
            {
                "title": "MI-VisionShot: Few-shot adaptation of vision-language models for slide-level classification of histopathological images",
                "abstract": "Vision-language supervision has made remarkable strides in learning visual representations from textual guidance. In digital pathology, vision-language models (VLM), pre-trained on curated datasets of histological image-captions, have been adapted to downstream tasks, such as region of interest classification. Zero-shot transfer for slide-level prediction has been formulated by MI-Zero, but it exhibits high variability depending on the textual prompts. Inspired by prototypical learning, we propose MI-VisionShot, a training-free adaptation method on top of VLMs to predict slide-level labels in few-shot learning scenarios. Our framework takes advantage of the excellent representation learning of VLM to create prototype-based classifiers under a multiple-instance setting by retrieving the most discriminative patches within each slide. Experimentation through different settings shows the ability of MI-VisionShot to surpass zero-shot transfer with lower variability, even in low-shot scenarios. Code coming soon at thttps://github.com/cvblab/MIVisionShot.",
                "authors": "Pablo Meseguer Esbri, Rocío del Amor, Valery Naranjo",
                "citations": 0
            },
            {
                "title": "Why do LLaVA Vision-Language Models Reply to Images in English?",
                "abstract": "We uncover a surprising multilingual bias occurring in a popular class of multimodal vision-language models (VLMs). Including an image in the query to a LLaVA-style VLM significantly increases the likelihood of the model returning an English response, regardless of the language of the query. This paper investigates the causes of this loss with a two-pronged approach that combines extensive ablation of the design space with a mechanistic analysis of the models' internal representations of image and text inputs. Both approaches indicate that the issue stems in the language modelling component of the LLaVA model. Statistically, we find that switching the language backbone for a bilingual language model has the strongest effect on reducing this error. Mechanistically, we provide compelling evidence that visual inputs are not mapped to a similar space as text ones, and that intervening on intermediary attention layers can reduce this bias. Our findings provide important insights to researchers and engineers seeking to understand the crossover between multimodal and multilingual spaces, and contribute to the goal of developing capable and inclusive VLMs for non-English contexts.",
                "authors": "Musashi Hinck, Carolin Holtermann, M. L. Olson, Florian Schneider, Sungduk Yu, Anahita Bhiwandiwalla, Anne Lauscher, Shao-yen Tseng, Vasudev Lal",
                "citations": 0
            },
            {
                "title": "VLMine: Long-Tail Data Mining with Vision Language Models",
                "abstract": "Ensuring robust performance on long-tail examples is an important problem for many real-world applications of machine learning, such as autonomous driving. This work focuses on the problem of identifying rare examples within a corpus of unlabeled data. We propose a simple and scalable data mining approach that leverages the knowledge contained within a large vision language model (VLM). Our approach utilizes a VLM to summarize the content of an image into a set of keywords, and we identify rare examples based on keyword frequency. We find that the VLM offers a distinct signal for identifying long-tail examples when compared to conventional methods based on model uncertainty. Therefore, we propose a simple and general approach for integrating signals from multiple mining algorithms. We evaluate the proposed method on two diverse tasks: 2D image classification, in which inter-class variation is the primary source of data diversity, and on 3D object detection, where intra-class variation is the main concern. Furthermore, through the detection task, we demonstrate that the knowledge extracted from 2D images is transferable to the 3D domain. Our experiments consistently show large improvements (between 10\\% and 50\\%) over the baseline techniques on several representative benchmarks: ImageNet-LT, Places-LT, and the Waymo Open Dataset.",
                "authors": "Mao Ye, Gregory P. Meyer, Zaiwei Zhang, Dennis Park, Siva Karthik Mustikovela, Yuning Chai, Eric M. Wolff",
                "citations": 0
            },
            {
                "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model",
                "abstract": "Current vision-language models (VLMs) show exceptional abilities across diverse tasks including visual question answering. To enhance user experience in practical applications, recent studies investigate VLM personalization to understand user-provided concepts. However, existing studies mainly focus on single-concept personalization, neglecting the existence and interplay of multiple concepts, which limits the real-world applicability of personalized VLMs. In this paper, we propose the first multi-concept personalization method named MC-LLaVA along with a high-quality multi-concept personalization dataset. Specifically, MC-LLaVA uses a joint training strategy incorporating multiple concepts in a single training step, allowing VLMs to perform accurately in multi-concept personalization. To reduce the cost of joint training, MC-LLaVA leverages visual token information for concept token initialization, yielding improved concept representation and accelerating joint training. To advance multi-concept personalization research, we further contribute a high-quality dataset. We carefully collect images from various movies that contain multiple characters and manually generate the multi-concept question-answer samples. Our dataset features diverse movie types and question-answer types. We conduct comprehensive qualitative and quantitative experiments to demonstrate that MC-LLaVA can achieve impressive multi-concept personalized responses, paving the way for VLMs to become better user-specific assistants. The code and dataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA.",
                "authors": "Ruichuan An, Sihan Yang, Ming Lu, Kai Zeng, Yulin Luo, Ying Chen, Jiajun Cao, Hao Liang, Qi She, Shanghang Zhang, Wentao Zhang",
                "citations": 0
            },
            {
                "title": "RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model",
                "abstract": "Minigolf is an exemplary real-world game for examining embodied intelligence, requiring challenging spatial and kinodynamic understanding to putt the ball. Additionally, reflective reasoning is required if the feasibility of a challenge is not ensured. We introduce RoboGolf, a VLM-based framework that combines dual-camera perception with closed-loop action refinement, augmented by a reflective equilibrium loop. The core of both loops is powered by finetuned VLMs. We analyze the capabilities of the framework in an offline inference setting, relying on an extensive set of recorded trajectories. Exemplary demonstrations of the analyzed problem domain are available at https://jity16.github.io/RoboGolf/",
                "authors": "Hantao Zhou, Tianying Ji, Jianwei Zhang, Fuchun Sun, Huazhe Xu",
                "citations": 0
            },
            {
                "title": "Learning to Ground VLMs without Forgetting",
                "abstract": "Spatial awareness is key to enable embodied multimodal AI systems. Yet, without vast amounts of spatial supervision, current Visual Language Models (VLMs) struggle at this task. In this paper, we introduce LynX, a framework that equips pretrained VLMs with visual grounding ability without forgetting their existing image and language understanding skills. To this end, we propose a Dual Mixture of Experts module that modifies only the decoder layer of the language model, using one frozen Mixture of Experts (MoE) pre-trained on image and language understanding and another learnable MoE for new grounding capabilities. This allows the VLM to retain previously learned knowledge and skills, while acquiring what is missing. To train the model effectively, we generate a high-quality synthetic dataset we call SCouT, which mimics human reasoning in visual grounding. This dataset provides rich supervision signals, describing a step-by-step multimodal reasoning process, thereby simplifying the task of visual grounding. We evaluate LynX on several object detection and visual grounding datasets, demonstrating strong performance in object detection, zero-shot localization and grounded reasoning while maintaining its original image and language understanding capabilities on seven standard benchmark datasets.",
                "authors": "Aritra Bhowmik, Mohammad Mahdi Derakhshani, Dennis C. Koelma, Martin R. Oswald, Yuki Asano, Cees G. M. Snoek",
                "citations": 0
            },
            {
                "title": "Prompt-guided and multimodal landscape scenicness assessments with vision-language models",
                "abstract": "Recent advances in deep learning and Vision-Language Models (VLM) have enabled efficient transfer to downstream tasks even when limited labelled training data is available, as well as for text to be directly compared to image content. These properties of VLMs enable new opportunities for the annotation and analysis of images. We test the potential of VLMs for landscape scenicness prediction, i.e., the aesthetic quality of a landscape, using zero- and few-shot methods. We experiment with few-shot learning by fine-tuning a single linear layer on a pre-trained VLM representation. We find that a model fitted to just a few hundred samples performs favourably compared to a model trained on hundreds of thousands of examples in a fully supervised way. We also explore the zero-shot prediction potential of contrastive prompting using positive and negative landscape aesthetic concepts. Our results show that this method outperforms a linear probe with few-shot learning when using a small number of samples to tune the prompt configuration. We introduce Landscape Prompt Ensembling (LPE), which is an annotation method for acquiring landscape scenicness ratings through rated text descriptions without needing an image dataset during annotation. We demonstrate that LPE can provide landscape scenicness assessments that are concordant with a dataset of image ratings. The success of zero- and few-shot methods combined with their ability to use text-based annotations highlights the potential for VLMs to provide efficient landscape scenicness assessments with greater flexibility.",
                "authors": "Alex Levering, Diego Marcos, Nathan Jacobs, D. Tuia",
                "citations": 0
            },
            {
                "title": "Synthetic Vision: Training Vision-Language Models to Understand Physics",
                "abstract": "Physical reasoning, which involves the interpretation, understanding, and prediction of object behavior in dynamic environments, remains a significant challenge for current Vision-Language Models (VLMs). In this work, we propose two methods to enhance VLMs' physical reasoning capabilities using simulated data. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs generated from simulations relevant to physical reasoning tasks. Second, we introduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to create scene descriptions enriched with physical properties and processes. During physical reasoning tasks, these PCBs can be leveraged as context to assist a Large Language Model (LLM) to improve its performance. We evaluate both of our approaches using multiple benchmarks, including a new stability detection QA dataset called Falling Tower, which includes both simulated and real-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM can significantly outperform larger state-of-the-art foundational models. We also show that integrating PCBs boosts the performance of foundational LLMs on physical reasoning tasks. Using the real-world scenes from the Falling Tower dataset, we also validate the robustness of both approaches in Sim2Real transfer. Our results highlight the utility that simulated data can have in the creation of learning systems capable of advanced physical reasoning.",
                "authors": "Vahid Balazadeh Meresht, Mohammadmehdi Ataei, Hyunmin Cheong, A. Khasahmadi, Rahul G. Krishnan",
                "citations": 0
            },
            {
                "title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding",
                "abstract": "The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones but face the challenge of inferior performance. Most existing monolithic VLMs require tuning pre-trained LLMs to acquire vision abilities, which may degrade their language capabilities. To address this dilemma, this paper presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs have been shown capable of interpreting images, when image embeddings are aligned with text embeddings. The challenge for current monolithic VLMs actually lies in the lack of a holistic embedding module for both vision and language inputs. Therefore, HoVLE introduces a holistic embedding module that converts visual and textual inputs into a shared space, allowing LLMs to process images in the same way as texts. Furthermore, a multi-stage training strategy is carefully designed to empower the holistic embedding module. It is first trained to distill visual features from a pre-trained vision encoder and text embeddings from the LLM, enabling large-scale training with unpaired random images and text tokens. The whole model further undergoes next-token prediction on multi-modal data to align the embeddings. Finally, an instruction-tuning stage is incorporated. Our experiments show that HoVLE achieves performance close to leading compositional models on various benchmarks, outperforming previous monolithic models by a large margin. Model available at https://huggingface.co/OpenGVLab/HoVLE.",
                "authors": "Chenxin Tao, Shiqian Su, Xizhou Zhu, Chenyu Zhang, Zhe Chen, Jiawen Liu, Wenhai Wang, Lewei Lu, Gao Huang, Yu Qiao, Jifeng Dai",
                "citations": 0
            },
            {
                "title": "DARE: Diverse Visual Question Answering with Robustness Evaluation",
                "abstract": "Vision Language Models (VLMs) extend remarkable capabilities of text-only large language models and vision-only models, and are able to learn from and process multi-modal vision-text input. While modern VLMs perform well on a number of standard image classification and image-text matching tasks, they still struggle with a number of crucial vision-language (VL) reasoning abilities such as counting and spatial reasoning. Moreover, while they might be very brittle to small variations in instructions and/or evaluation protocols, existing benchmarks fail to evaluate their robustness (or rather the lack of it). In order to couple challenging VL scenarios with comprehensive robustness evaluation, we introduce DARE, Diverse Visual Question Answering with Robustness Evaluation, a carefully created and curated multiple-choice VQA benchmark. DARE evaluates VLM performance on five diverse categories and includes four robustness-oriented evaluations based on the variations of: prompts, the subsets of answer options, the output format and the number of correct answers. Among a spectrum of other findings, we report that state-of-the-art VLMs still struggle with questions in most categories and are unable to consistently deliver their peak performance across the tested robustness evaluations. The worst case performance across the subsets of options is up to 34% below the performance in the standard case. The robustness of the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the closed-source models such as GPT-4 and Gemini, but even the latter remain very brittle to different variations.",
                "authors": "Hannah Sterz, Jonas Pfeiffer, Ivan Vuli'c",
                "citations": 0
            },
            {
                "title": "Evaluating Vision-Language Models in Visual Comprehension for Autonomous Driving",
                "abstract": "Visual cues provide essential information for autonomous driving, while misunderstandings of the visual cues may degrade decision-making quality and put passengers at risk. However, the advent of large language models (LLMs) and vision-language models (VLMs) presents a potential solution to these challenges. These models, trained on vast datasets, demonstrate exceptional capability in understanding context and processing complex information, offering prospects for overcoming perceptual challenges and enhancing autonomous driving systems. This paper investigates the comprehension of visual cues by two large-scale language models, ChatGPT and Bard, through an evaluation involving image-based questions from Tokyo and Beijing driving-license tests. We hope our assessment of the visual capabilities of VLMs in autonomous driving contexts can offer a foundation for future exploration in VLM integration into autonomous driving systems.",
                "authors": "Shanmin Zhou, Jialong Li, Takuto Yamauchi, Jinyu Cai, Kenji Tei",
                "citations": 0
            },
            {
                "title": "What does Kiki look like? Cross-modal associations between speech sounds and visual shapes in vision-and-language models",
                "abstract": "Humans have clear cross-modal preferences when matching certain novel words to visual shapes. Evidence suggests that these preferences play a prominent role in our linguistic processing, language learning, and the origins of signal-meaning mappings. With the rise of multimodal models in AI, such as vision-and-language (VLM) models, it becomes increasingly important to uncover the kinds of visio-linguistic associations these models encode and whether they align with human representations. Informed by experiments with humans, we probe and compare four VLMs for a well-known human cross-modal preference, the bouba-kiki effect. We do not find conclusive evidence for this effect but suggest that results may depend on features of the models, such as architecture design, model size, and training details. Our findings inform discussions on the origins of the bouba-kiki effect in human cognition and future developments of VLMs that align well with human cross-modal associations.",
                "authors": "Tessa Verhoef, Kiana Shahrasbi, T. Kouwenhoven",
                "citations": 0
            },
            {
                "title": "VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models",
                "abstract": "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B.",
                "authors": "Jeongho Ju, Daeyoung Kim, SunYoung Park, Youngjune Kim",
                "citations": 0
            },
            {
                "title": "Category‐instance distillation based on visual‐language models for rehearsal‐free class incremental learning",
                "abstract": "Recently, visual‐language models (VLMs) have displayed potent capabilities in the field of computer vision. Their emerging trend as the backbone of visual tasks necessitates studying class incremental learning (CIL) issues within the VLM architecture. However, the pre‐training data for many VLMs is proprietary, and during the incremental phase, old task data may also raise privacy issues. Moreover, replay‐based methods can introduce new problems like class imbalance, the selection of data for replay and a trade‐off between replay cost and performance. Therefore, the authors choose the more challenging rehearsal‐free settings. In this paper, the authors study class‐incremental tasks based on the large pre‐trained vision‐language models like CLIP model. Initially, at the category level, the authors combine traditional optimisation and distillation techniques, utilising both pre‐trained models and models trained in previous incremental stages to jointly guide the training of the new model. This paradigm effectively balances the stability and plasticity of the new model, mitigating the issue of catastrophic forgetting. Moreover, utilising the VLM infrastructure, the authors redefine the relationship between instances. This allows us to glean fine‐grained instance relational information from the a priori knowledge provided during pre‐training. The authors supplement this approach with an entropy‐balancing method that allows the model to adaptively distribute optimisation weights across training samples. The authors’ experimental results validate that their method, within the framework of VLMs, outperforms traditional CIL methods.",
                "authors": "Weilong Jin, Zilei Wang, Y. Zhang",
                "citations": 0
            },
            {
                "title": "Visual Adversarial Attack on Vision-Language Models for Autonomous Driving",
                "abstract": "Vision-language models (VLMs) have significantly advanced autonomous driving (AD) by enhancing reasoning capabilities. However, these models remain highly vulnerable to adversarial attacks. While existing research has primarily focused on general VLM attacks, the development of attacks tailored to the safety-critical AD context has been largely overlooked. In this paper, we take the first step toward designing adversarial attacks specifically targeting VLMs in AD, exposing the substantial risks these attacks pose within this critical domain. We identify two unique challenges for effective adversarial attacks on AD VLMs: the variability of textual instructions and the time-series nature of visual scenarios. To this end, we propose ADvLM, the first visual adversarial attack framework specifically designed for VLMs in AD. Our framework introduces Semantic-Invariant Induction, which uses a large language model to create a diverse prompt library of textual instructions with consistent semantic content, guided by semantic entropy. Building on this, we introduce Scenario-Associated Enhancement, an approach where attention mechanisms select key frames and perspectives within driving scenarios to optimize adversarial perturbations that generalize across the entire scenario. Extensive experiments on several AD VLMs over multiple benchmarks show that ADvLM achieves state-of-the-art attack effectiveness. Moreover, real-world attack studies further validate its applicability and potential in practice.",
                "authors": "Tianyuan Zhang, Lu Wang, Xinwei Zhang, Yitong Zhang, Boyi Jia, Siyuan Liang, Shengshan Hu, Qiang Fu, Aishan Liu, Xianglong Liu",
                "citations": 0
            },
            {
                "title": "Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping",
                "abstract": "Sketches are a natural and accessible medium for UI designers to conceptualize early-stage ideas. However, existing research on UI/UX automation often requires high-fidelity inputs like Figma designs or detailed screenshots, limiting accessibility and impeding efficient design iteration. To bridge this gap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art Vision Language Models (VLMs) on automating the conversion of rudimentary sketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code supports interactive agent evaluation that mimics real-world design workflows, where a VLM-based agent iteratively refines its generations by communicating with a simulated user, either passively receiving feedback instructions or proactively asking clarification questions. We comprehensively analyze ten commercial and open-source models, showing that Sketch2Code is challenging for existing VLMs; even the most capable models struggle to accurately interpret sketches and formulate effective questions that lead to steady improvement. Nevertheless, a user study with UI/UX experts reveals a significant preference for proactive question-asking over passive feedback reception, highlighting the need to develop more effective paradigms for multi-turn conversational agents.",
                "authors": "Ryan Li, Yanzhe Zhang, Diyi Yang",
                "citations": 0
            },
            {
                "title": "Training Visual Language Models with Object Detection: Grounded Change Descriptions in Satellite Images",
                "abstract": "Recently, generalist Vision Language Models (VLMs) have shown exceptional progress in tasks previously dominated by specialized computer vision models. This becomes more prevalent when visual grounding capabilities, such as the ability to reason over input text and image to generate bounding boxes around objects, are required. However, how these capabilities transfer to specialized domains such as remote sensing remains understudied, despite the recent increase in specialized models for Earth observation. In this work, we evaluate how grounding visual entities – by generating bounding-box coordinates – affects VLM performance in satellite imagery. To this end, we create two instruction-following tasks sourced from the xBD dataset, describing changes due to natural disasters observed in satellite images. We fine-tune several instances of MiniGPTv2, an open-source VLM with grounding capabilities, and evaluate their performance under the \"grounded\" vs. \"not grounded\" settings. We find that generating bounding boxes to refer to visual entities increases performance in tasks related to objects in the image, but only when the number of entities in the image is limited.",
                "authors": "João Luis Prado, Syrielle Montariol, J. Castillo-Navarro, D. Tuia, Antoine Bosselut",
                "citations": 0
            },
            {
                "title": "LOBG:Less Overfitting for Better Generalization in Vision-Language Model",
                "abstract": "Existing prompt learning methods in Vision-Language Models (VLM) have effectively enhanced the transfer capability of VLM to downstream tasks, but they suffer from a significant decline in generalization due to severe overfitting. To address this issue, we propose a framework named LOBG for vision-language models. Specifically, we use CLIP to filter out fine-grained foreground information that might cause overfitting, thereby guiding prompts with basic visual concepts. To further mitigate overfitting, we devel oped a structural topology preservation (STP) loss at the feature level, which endows the feature space with overall plasticity, allowing effective reshaping of the feature space during optimization. Additionally, we employed hierarchical logit distilation (HLD) at the output level to constrain outputs, complementing STP at the output end. Extensive experimental results demonstrate that our method significantly improves generalization capability and alleviates overfitting compared to state-of-the-art approaches.",
                "authors": "Chenhao Ding, Xinyuan Gao, Songlin Dong, Yuhang He, Qiang Wang, Alex Kot, Yihong Gong",
                "citations": 0
            },
            {
                "title": "Enhancing Concept-Based Explanation with Vision-Language Models",
                "abstract": "Although concept-based approaches are widely used to explain a model’s behavior and assess the contributions of different concepts in decision-making, identifying relevant concepts can be challenging for non-experts. This paper introduces a novel method that simplifies concept selection by leveraging the capabilities of a state-of-the-art large Vision-Language Model (VLM). Our method employs a VLM to select textual concepts that describe the classes in the target dataset. We then transform these influential textual concepts into human-readable image concepts using a text-to-image model. This process allows us to explain the targeted network in a post-hoc manner. Further, we use directional derivatives and concept activation vectors to quantify the importance of the generated concepts. We evaluate our method on a neonatal pain classification task, analyzing the sensitivity of the model’s output for the generated concepts. The results demonstrate that the VLM not only generates coherent and meaningful concepts that are easily understandable by non-experts but also achieves performance comparable to that of natural image concepts without the need for additional annotation costs.",
                "authors": "Md Imran Hossain, Ghada Zamzmi, Peter R. Mouton, Yu Sun, Dmitry B. Goldgof",
                "citations": 0
            },
            {
                "title": "CLIP-Clique: Graph-Based Correspondence Matching Augmented by Vision Language Models for Object-Based Global Localization",
                "abstract": "This letter proposes a method of global localization on a map with semantic object landmarks. One of the most promising approaches for localization on object maps is to use semantic graph matching using landmark descriptors calculated from the distribution of surrounding objects. These descriptors are vulnerable to misclassification and partial observations. Moreover, many existing methods rely on inlier extraction using RANSAC, which is stochastic and sensitive to a high outlier rate. To address the former issue, we augment the correspondence matching using Vision Language Models (VLMs). Landmark discriminability is improved by VLM embeddings, which are independent of surrounding objects. In addition, inliers are estimated deterministically using a graph-theoretic approach. We also incorporate pose calculation using the weighted least squares considering correspondence similarity and observation completeness to improve the robustness. We confirmed improvements in matching and pose estimation accuracy through experiments on ScanNet and TUM datasets.",
                "authors": "Shigemichi Matsuzaki, Kazuhito Tanaka, Kazuhiro Shintani",
                "citations": 0
            },
            {
                "title": "Incorporating Feature Pyramid Tokenization and Open Vocabulary Semantic Segmentation",
                "abstract": "The visual understanding are often approached from 3 granular levels: image, patch and pixel. Visual Tokenization, trained by self-supervised reconstructive learning, compresses visual data by codebook in patch-level with marginal information loss, but the visual tokens does not have semantic meaning. Open Vocabulary semantic segmentation benefits from the evolving Vision-Language models (VLMs) with strong image zero-shot capability, but transferring image-level to pixel-level understanding remains an imminent challenge. In this paper, we treat segmentation as tokenizing pixels and study a united perceptual and semantic token compression for all granular understanding and consequently facilitate open vocabulary semantic segmentation. Referring to the cognitive process of pretrained VLM where the low-level features are progressively composed to high-level semantics, we propose Feature Pyramid Tokenization (PAT) to cluster and represent multi-resolution feature by learnable codebooks and then decode them by joint learning pixel reconstruction and semantic segmentation. We design loosely coupled pixel and semantic learning branches. The pixel branch simulates bottom-up composition and top-down visualization of codebook tokens, while the semantic branch collectively fuse hierarchical codebooks as auxiliary segmentation guidance. Our experiments show that PAT enhances the semantic intuition of VLM feature pyramid, improves performance over the baseline segmentation model and achieves competitive performance on open vocabulary semantic segmentation benchmark. Our model is parameter-efficient for VLM integration and flexible for the independent tokenization. We hope to give inspiration not only on improving segmentation but also on semantic visual token utilization.",
                "authors": "Jianyu Zhang, Li Zhang, Shi-Bei Li",
                "citations": 0
            },
            {
                "title": "Optimized GNSS Cal/Val Site Selection for Expanding InSAR Viability in Areas With Low Phase Coherence: A Case Study for Southern Louisiana",
                "abstract": "Interferometric synthetic aperture radar (InSAR) techniques can be used to derive spatially dense “relative” measurements of vertical land motion (VLM), whereas global navigation satellite system (GNSS) provides point-based “absolute” measurements of VLM. The combination of GNSS and InSAR observations can yield spatially dense VLM measurements in an absolute reference frame. In addition, GNSS observations can be used to correct atmospheric noise in InSAR deformation measurements and serve as a complementary measure to isolate deep and shallow subsidence components. Given the increasing spatial and temporal coverage available from InSAR satellites, there is a need to establish calibration/validation networks that enable the use of InSAR for measuring VLM in coherence-challenged areas such as many low-lying coastal lands. In this study, we provide a method for the selection of sites for new GNSS installations such that the resulting GNSS network can better serve as tie points and validation for InSAR in areas where low coherence prevents high-fidelity phase unwrapping. Our method is applied in a case study for expanding the existing GNSS network in southern Louisiana, using abandoned oil well sites as potential sites. Considering practical limitations, distribution among various land classes, and following National Geodetic Survey guidelines, our proposed GNSS network consists of 61 (45 existing + 16 new) stations spread over a 50 000 km2 area of southern Louisiana.",
                "authors": "B. Varugu, Cathleen E. Jones, Ke Wang, Jingyi Chen, Randy L. Osborne, G. Voyiadjis",
                "citations": 0
            },
            {
                "title": "OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images",
                "abstract": "Recent open-world 3D representation learning methods using Vision-Language Models (VLMs) to align 3D data with image-text information have shown superior 3D zero-shot performance. However, CAD-rendered images for this alignment often lack realism and texture variation, compromising alignment robustness. More-over, the volume discrepancy between 3D and 2D pretraining datasets highlights the need for effective strategies to transfer the representational abilities of VLMs to 3D learning. In this paper, we present OpenDlign, a novel open-world 3D model using depth-aligned images generated from a diffusion model for robust multimodal alignment. These images exhibit greater texture diversity than CAD renderings due to the stochastic nature of the diffusion model. By refining the depth map projection pipeline and designing depth-specific prompts, OpenDlign leverages rich knowledge in pre-trained VLM for 3D representation learning with stream-lined fine-tuning. Our experiments show that OpenDlign achieves high zero-shot and few-shot performance on diverse 3D tasks, despite only fine-tuning 6 million parameters on a limited ShapeNet dataset. In zero-shot classification, OpenDlign surpasses previous models by 8.0% on ModelNet40 and 16.4% on OmniObject3D. Additionally, using depth-aligned images for multimodal alignment consistently enhances the performance of other state-of-the-art models.",
                "authors": "Ye Mao, Junpeng Jing, K. Mikolajczyk",
                "citations": 0
            },
            {
                "title": "ChatGarment: Garment Estimation, Generation and Editing via Large Language Models",
                "abstract": "We introduce ChatGarment, a novel approach that leverages large vision-language models (VLMs) to automate the estimation, generation, and editing of 3D garments from images or text descriptions. Unlike previous methods that struggle in real-world scenarios or lack interactive editing capabilities, ChatGarment can estimate sewing patterns from in-the-wild images or sketches, generate them from text descriptions, and edit garments based on user instructions, all within an interactive dialogue. These sewing patterns can then be draped into 3D garments, which are easily animatable and simulatable. This is achieved by finetuning a VLM to directly generate a JSON file that includes both textual descriptions of garment types and styles, as well as continuous numerical attributes. This JSON file is then used to create sewing patterns through a programming parametric model. To support this, we refine the existing programming model, GarmentCode, by expanding its garment type coverage and simplifying its structure for efficient VLM fine-tuning. Additionally, we construct a large-scale dataset of image-to-sewing-pattern and text-to-sewing-pattern pairs through an automated data pipeline. Extensive evaluations demonstrate ChatGarment's ability to accurately reconstruct, generate, and edit garments from multimodal inputs, highlighting its potential to revolutionize workflows in fashion and gaming applications. Code and data will be available at https://chatgarment.github.io/.",
                "authors": "Siyuan Bian, Chenghao Xu, Yuliang Xiu, Artur Grigorev, Zhen Liu, Cewu Lu, Michael J. Black, Yao Feng",
                "citations": 0
            },
            {
                "title": "High-resolution open-vocabulary object 6D pose estimation",
                "abstract": "The generalisation to unseen objects in the 6D pose estimation task is very challenging. While Vision-Language Models (VLMs) enable using natural language descriptions to support 6D pose estimation of unseen objects, these solutions underperform compared to model-based methods. In this work we present Horyon, an open-vocabulary VLM-based architecture that addresses relative pose estimation between two scenes of an unseen object, described by a textual prompt only. We use the textual prompt to identify the unseen object in the scenes and then obtain high-resolution multi-scale features. These features are used to extract cross-scene matches for registration. We evaluate our model on a benchmark with a large variety of unseen objects across four datasets, namely REAL275, Toyota-Light, Linemod, and YCB-Video. Our method achieves state-of-the-art performance on all datasets, outperforming by 12.6 in Average Recall the previous best-performing approach.",
                "authors": "Jaime Corsetti, Davide Boscaini, Francesco Giuliari, Changjae Oh, Andrea Cavallaro, Fabio Poiesi",
                "citations": 0
            },
            {
                "title": "Lightweight Neural App Control",
                "abstract": "This paper introduces a novel mobile phone control architecture, termed ``app agents\", for efficient interactions and controls across various Android apps. The proposed Lightweight Multi-modal App Control (LiMAC) takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, within LiMAC, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.",
                "authors": "Filippos Christianos, Georgios Papoudakis, Thomas Coste, Jianye Hao, Jun Wang, Kun Shao",
                "citations": 0
            },
            {
                "title": "Explaining Chest X-ray Pathology Models using Textual Concepts",
                "abstract": "Deep learning models have revolutionized medical imaging and diagnostics, yet their opaque nature poses challenges for clinical adoption and trust. Amongst approaches to improve model interpretability, concept-based explanations aim to provide concise and human-understandable explanations of any arbitrary classifier. However, such methods usually require a large amount of manually collected data with concept annotation, which is often scarce in the medical domain. In this paper, we propose Conceptual Counterfactual Explanations for Chest X-ray (CoCoX), which leverages the joint embedding space of an existing vision-language model (VLM) to explain black-box classifier outcomes without the need for annotated datasets. Specifically, we utilize textual concepts derived from chest radiography reports and a pre-trained chest radiography-based VLM to explain three common cardiothoracic pathologies. We demonstrate that the explanations generated by our method are semantically meaningful and faithful to underlying pathologies.",
                "authors": "Vijay Sadashivaiah, M. Kalra, P. Yan, James A. Hendler",
                "citations": 0
            },
            {
                "title": "Vortex Lattice Method investigation of Tip-leakage flow",
                "abstract": "\n The use of a Vortex Lattice Method (VLM) is investigated on a tip-leakage flow single blade configuration (NACA 0012) for a wide range of tip gaps. The aim is to estimate the circulation of the tip-leakage vortex (TLV) detaching from the blade. The evaluation of that circulation without adding viscous effects to the potential method is comparable with the experimental measurements for the largest gaps but requires a diffusive model for the smaller gaps. A new diffusive model is constructed and integrated to the VLM, yielding a very good match with the experiment on the circulation for the whole tip gap range. This result indicates that for thin blades, the development of the TLV can be described in two stages: (i) a potential formation, (ii) a viscous diffusion leading to a decay once the vortex is detached from the blade.",
                "authors": "Christophe Montsarrat, Jérôme Boudet",
                "citations": 0
            },
            {
                "title": "DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for Semi-Supervised Medical Image Segmentation",
                "abstract": "Semi-supervised medical image segmentation (SSMIS) uses consistency learning to regularize model training, which alleviates the burden of pixel-wise manual annotations. However, it often suffers from error supervision from low-quality pseudo labels. Vision-Language Model (VLM) has great potential to enhance pseudo labels by introducing text prompt guided multimodal supervision information. It nevertheless faces the cross-modal problem: the obtained messages tend to correspond to multiple targets. To address aforementioned problems, we propose a Dual Semantic Similarity-Supervised VLM (DuSSS) for SSMIS. Specifically, 1) a Dual Contrastive Learning (DCL) is designed to improve cross-modal semantic consistency by capturing intrinsic representations within each modality and semantic correlations across modalities. 2) To encourage the learning of multiple semantic correspondences, a Semantic Similarity-Supervision strategy (SSS) is proposed and injected into each contrastive learning process in DCL, supervising semantic similarity via the distribution-based uncertainty levels. Furthermore, a novel VLM-based SSMIS network is designed to compensate for the quality deficiencies of pseudo-labels. It utilizes the pretrained VLM to generate text prompt guided supervision information, refining the pseudo label for better consistency regularization. Experimental results demonstrate that our DuSSS achieves outstanding performance with Dice of 82.52%, 74.61% and 78.03% on three public datasets (QaTa-COV19, BM-Seg and MoNuSeg).",
                "authors": "Qingtao Pan, Wenhao Qiao, Jingjiao Lou, Bing Ji, Shuo Li",
                "citations": 0
            },
            {
                "title": "Progressive Multi-modal Conditional Prompt Tuning",
                "abstract": "Pre-trained vision-language models (VLMs) have shown remarkable generalization capabilities via prompting, which leverages VLMs as knowledge bases to extract information beneficial for downstream tasks. However, existing methods primarily employ uni-modal prompting, which only engages a uni-modal branch, failing to simultaneously adjust vision-language (V-L) features. Additionally, the one-pass forward pipeline in VLM encoding struggles to align V-L features that have a huge gap. Confronting these challenges, we propose a novel method, Progressive Multi-modal conditional Prompt Tuning (ProMPT). ProMPT exploits a recurrent structure, optimizing and aligning V-L features by iteratively utilizing image and current encoding information. It comprises an initialization and a multi-modal iterative evolution (MIE) module. Initialization is responsible for encoding images and text using a VLM, followed by a feature filter that selects text features similar to image. MIE then facilitates multi-modal prompting through class-conditional vision prompting, instance-conditional text prompting, and feature filtering. In each MIE iteration, vision prompts are obtained from filtered text features via a vision generator, promoting image features to focus more on target object during vision prompting. The encoded image features are fed into a text generator to produce text prompts that are more robust to class shifts. Thus, V-L features are progressively aligned, enabling advance from coarse to exact prediction. Extensive experiments are conducted in three settings to evaluate the efficacy of ProMPT. The results indicate that ProMPT outperforms existing methods on average across all settings, demonstrating its superior generalization and robustness. Code is available at https://github.com/qiuxiaoyu9954/ProMPT.",
                "authors": "Xiaoyu Qiu, Hao Feng, Yuechen Wang, Wen-gang Zhou, Houqiang Li",
                "citations": 0
            },
            {
                "title": "Fine-Tuning Vision-Language Model for Automated Engineering Drawing Information Extraction",
                "abstract": "Geometric Dimensioning and Tolerancing (GD&T) plays a critical role in manufacturing by defining acceptable variations in part features to ensure component quality and functionality. However, extracting GD&T information from 2D engineering drawings is a time-consuming and labor-intensive task, often relying on manual efforts or semi-automated tools. To address these challenges, this study proposes an automated and computationally efficient GD&T extraction method by fine-tuning Florence-2, an open-source vision-language model (VLM). The model is trained on a dataset of 400 drawings with ground truth annotations provided by domain experts. For comparison, two state-of-the-art closed-source VLMs, GPT-4o and Claude-3.5-Sonnet, are evaluated on the same dataset. All models are assessed using precision, recall, F1-score, and hallucination metrics. Due to the computational cost and impracticality of fine-tuning large closed-source VLMs for domain-specific tasks, GPT-4o and Claude-3.5-Sonnet are evaluated in a zero-shot setting. In contrast, Florence-2, a smaller model with 0.23 billion parameters, is optimized through full-parameter fine-tuning across three distinct experiments, each utilizing datasets augmented to different levels. The results show that Florence-2 achieves a 29.95% increase in precision, a 37.75% increase in recall, a 52.40% improvement in F1-score, and a 43.15% reduction in hallucination rate compared to the best-performing closed-source model. These findings highlight the effectiveness of fine-tuning smaller, open-source VLMs like Florence-2, offering a practical and efficient solution for automated GD&T extraction to support downstream manufacturing tasks.",
                "authors": "Muhammad Tayyab Khan, Lequn Chen, Ye Han Ng, Wenhe Feng, Nicholas Yew Jin Tan, Seung Ki Moon",
                "citations": 0
            },
            {
                "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
                "abstract": "Detecting temporal changes in geographical landscapes is critical for applications like environmental monitoring and urban planning. While remote sensing data is abundant, existing vision-language models (VLMs) often fail to capture temporal dynamics effectively. This paper addresses these limitations by introducing an annotated dataset of video frame pairs to track evolving geographical patterns over time. Using fine-tuning techniques like Low-Rank Adaptation (LoRA), quantized LoRA (QLoRA), and model pruning on models such as Video-LLaVA and LLaVA-NeXT-Video, we significantly enhance VLM performance in processing remote sensing temporal changes. Results show significant improvements, with the best performance achieving a BERT score of 0.864 and ROUGE-1 score of 0.576, demonstrating superior accuracy in describing land-use transformations.",
                "authors": "Hosam Elgendy, Ahmed Sharshar, Ahmed Aboeitta, Yasser Ashraf, Mohsen Guizani",
                "citations": 0
            },
            {
                "title": "Decreased inhibition of ventral brainstem liver-related neurons in high-fat diet fed mice",
                "abstract": "Pre-sympathetic liver-related neurons in the ventrolateral and ventromedial medulla (VLM/VMM) participate in the regulation of hepatic carbohydrate and lipid metabolism. Increased sympathetic activity observed in diet-induced obese mice likely contributes to excessive hepatic glucose production, and thus to the development, and/or progression of type 2 diabetes mellitus. At the cellular level, change in neuronal activity and/or synaptic balance is one of the underlying mechanisms that can lead to increased sympathetic outflow. In this study, we tested the hypothesis that high-fat diet (HFD) diminishes glycinergic inhibition of liver-related neurons in the VLM/VMM. We crossed heterozygous glycine transporter 2 Cre (GlyT2Cre) mice with floxed channelrhodopsin 2 (ChR2-EYFP) mice to generate GlyT2-ChR2 expressing mice (GlyT2ChR2/EYFP). Liver-related neurons were identified in the VLM/VMM with a retrograde viral tracer and patch-clamp recordings were conducted. Whole-cell recordings from pre-sympathetic, liver-related neurons revealed that spontaneous inhibitory postsynaptic currents (sIPSCs) are mediated by GABA and glycine. Our data showed that increasing the activity of inhibitory inputs leads to increased glycine release. Next, light stimulation was used to trigger glycine release in GlyT2ChR2/EYFP mice. Light stimulation of ChR2-expressing glycinergic fibers triggered evoked IPSCs (eIPSCs) generated by co-release of GABA and glycine with identical time courses for the two neurotransmitters. Intriguingly, in HFD-fed mice we found that the amplitude of light evoked IPSCs was significantly decreased. These data suggest that liver-related neurons in the VLM/VMM are co-inhibited by GABA and glycine and in HFD-fed mice inhibition of liver-related neurons is reduced. This reduced inhibition of ventral brainstem neurons may contribute to increased sympathetic activity in diet-induced obesity. Supported by: NIDDK122842 and Tulane Brain Institute Marko Spark Innovation Research Fund. This is the full abstract presented at the American Physiology Summit 2024 meeting and is only available in HTML format. There are no additional versions or additional content available for this abstract. Physiology was not involved in the peer review process.",
                "authors": "Hong Gao, A. Zsombok, A. Derbenev",
                "citations": 0
            },
            {
                "title": "Affordance Perception by a Knowledge-Guided Vision-Language Model with Efficient Error Correction",
                "abstract": "Mobile robot platforms will increasingly be tasked with activities that involve grasping and manipulating objects in open world environments. Affordance understanding provides a robot with means to realise its goals and execute its tasks, e.g. to achieve autonomous navigation in unknown buildings where it has to find doors and ways to open these. In order to get actionable suggestions, robots need to be able to distinguish subtle differences between objects, as they may result in different action sequences: doorknobs require grasp and twist, while handlebars require grasp and push. In this paper, we improve affordance perception for a robot in an open-world setting. Our contribution is threefold: (1) We provide an affordance representation with precise, actionable affordances; (2) We connect this knowledge base to a foundational vision-language models (VLM) and prompt the VLM for a wider variety of new and unseen objects; (3) We apply a human-in-the-loop for corrections on the output of the VLM. The mix of affordance representation, image detection and a human-in-the-loop is effective for a robot to search for objects to achieve its goals. We have demonstrated this in a scenario of finding various doors and the many different ways to open them.",
                "authors": "G. Burghouts, M. Schaaphok, M. V. Bekkum, W. Meijer, Fieke Hillerström, Jelle van Mil",
                "citations": 0
            },
            {
                "title": "HPE-CogVLM: Advancing Vision Language Models with a Head Pose Grounding Task",
                "abstract": "Head pose estimation (HPE) requires a sophisticated understanding of 3D spatial relationships to generate precise yaw, pitch, and roll angles. Previous HPE models, primarily CNN-based, rely on cropped close-up human head images as inputs and often lack robustness in real-world scenario. Vision Language Models (VLMs) can analyze entire images while focusing on specific objects through their attention mechanisms. In this paper, we propose a novel framework to improve the HPE accuracy by leveraging the object detection grounding capability of a VLM, referred to as CogVLM. We empirically find that directly LoRA fine-tuning of this VLM for the HPE task fails to achieve desirable HPE accuracy, while some model merging methods can improve accuracy but frequently produce blended invalid response formats, struggling to handle both object detection and HPE tasks simultaneously. To integrate HPE capability into CogVLM effectively, we develop a novel LoRA layer-based model merging method. This merging approach applies a high cosine similarity threshold and a winner-takes-all layer selection strategy, aligning attention to the HPE task while preserving original object detection knowledge. It successfully resolves issues with blended invalid response formats and improves accuracy. Results show that our HPE-CogVLM achieves a 31.5\\% reduction in Mean Absolute Error over the current state-of-the-art CNN model, 6DRepNet, in cross-dataset evaluation. Furthermore, HPE-CogVLM outperforms both directly LoRA fine-tuned and task arithmetic-based merged VLMs across all HPE metrics.",
                "authors": "Yu Tian, Tianqi Shao, Tsukasa Demizu, Xuyang Wu, Hsin-Tai Wu",
                "citations": 0
            },
            {
                "title": "Modelling Multimodal Integration in Human Concept Processing with Vision-and-Language Models",
                "abstract": "Representations from deep neural networks (DNNs) have proven remarkably predictive of neural activity involved in both visual and linguistic processing. Despite these successes, most studies to date concern unimodal DNNs, encoding either visual or textual input but not both. Yet, there is growing evidence that human meaning representations integrate linguistic and sensory-motor information. Here we investigate whether the integration of multimodal information operated by current vision-and-language DNN models (VLMs) leads to representations that are more aligned with human brain activity than those obtained by language-only and vision-only DNNs. We focus on fMRI responses recorded while participants read concept words in the context of either a full sentence or an accompanying picture. Our results reveal that VLM representations correlate more strongly than language- and vision-only DNNs with activations in brain areas functionally related to language processing. A comparison between different types of visuo-linguistic architectures shows that recent generative VLMs tend to be less brain-aligned than previous architectures with lower performance on downstream applications. Moreover, through an additional analysis comparing brain vs. behavioural alignment across multiple VLMs, we show that -- with one remarkable exception -- representations that strongly align with behavioural judgments do not correlate highly with brain responses. This indicates that brain similarity does not go hand in hand with behavioural similarity, and vice versa.",
                "authors": "A. Bavaresco, Marianne de Heer Kloots, Sandro Pezzelle, R. Fern'andez",
                "citations": 0
            },
            {
                "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents",
                "abstract": "Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play crucial roles in real-world multi-modality documents. In this paper, we introduce VisRAG, which tackles this issue by establishing a vision-language model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the document to obtain text, the document is directly embedded using a VLM as an image and then retrieved to enhance the generation of a VLM. Compared to traditional text-based RAG, VisRAG maximizes the retention and utilization of the data information in the original documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in VisRAG and explore a variety of generation methods. Experiments demonstrate that VisRAG outperforms traditional RAG in both the retrieval and generation stages, achieving a 25--39\\% end-to-end performance gain over traditional text-based RAG pipeline. Further analysis reveals that VisRAG is effective in utilizing training data and demonstrates strong generalization capability, positioning it as a promising solution for RAG on multi-modality documents. Our code and data are available at https://github.com/openbmb/visrag .",
                "authors": "Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun",
                "citations": 0
            },
            {
                "title": "Refining Skewed Perceptions in Vision-Language Models through Visual Representations",
                "abstract": "Large vision-language models (VLMs), such as CLIP, have become foundational, demonstrating remarkable success across a variety of downstream tasks. Despite their advantages, these models, akin to other foundational systems, inherit biases from the disproportionate distribution of real-world data, leading to misconceptions about the actual environment. Prevalent datasets like ImageNet are often riddled with non-causal, spurious correlations that can diminish VLM performance in scenarios where these contextual elements are absent. This study presents an investigation into how a simple linear probe can effectively distill task-specific core features from CLIP's embedding for downstream applications. Our analysis reveals that the CLIP text representations are often tainted by spurious correlations, inherited in the biased pre-training dataset. Empirical evidence suggests that relying on visual representations from CLIP, as opposed to text embedding, is more practical to refine the skewed perceptions in VLMs, emphasizing the superior utility of visual representations in overcoming embedded biases. Our codes will be available here.",
                "authors": "Haocheng Dai, Sarang Joshi",
                "citations": 0
            },
            {
                "title": "On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control: System Design and Real-World Validation",
                "abstract": "Personalized driving refers to an autonomous vehicle's ability to adapt its driving behavior or control strategies to match individual users' preferences and driving styles while maintaining safety and comfort standards. However, existing works either fail to capture every individual preference precisely or become computationally inefficient as the user base expands. Vision-Language Models (VLMs) offer promising solutions to this front through their natural language understanding and scene reasoning capabilities. In this work, we propose a lightweight yet effective on-board VLM framework that provides low-latency personalized driving performance while maintaining strong reasoning capabilities. Our solution incorporates a Retrieval-Augmented Generation (RAG)-based memory module that enables continuous learning of individual driving preferences through human feedback. Through comprehensive real-world vehicle deployment and experiments, our system has demonstrated the ability to provide safe, comfortable, and personalized driving experiences across various scenarios and significantly reduce takeover rates by up to 76.9%. To the best of our knowledge, this work represents the first end-to-end VLM-based motion control system in real-world autonomous vehicles.",
                "authors": "Can Cui, Zichong Yang, Yupeng Zhou, Juntong Peng, Sung-Yeon Park, Cong Zhang, Yunsheng Ma, Xu Cao, Wenqian Ye, Yiheng Feng, Jitesh Panchal, Lingxi Li, Yaobin Chen, Ziran Wang",
                "citations": 0
            },
            {
                "title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos",
                "abstract": "Large language models (LLMs) excel at retrieving information from lengthy text, but their vision-language counterparts (VLMs) face difficulties with hour-long videos, especially for temporal grounding. Specifically, these VLMs are constrained by frame limitations, often losing essential temporal details needed for accurate event localization in extended video content. We propose ReVisionLLM, a recursive vision-language model designed to locate events in hour-long videos. Inspired by human search strategies, our model initially targets broad segments of interest, progressively revising its focus to pinpoint exact temporal boundaries. Our model can seamlessly handle videos of vastly different lengths, from minutes to hours. We also introduce a hierarchical training strategy that starts with short clips to capture distinct events and progressively extends to longer videos. To our knowledge, ReVisionLLM is the first VLM capable of temporal grounding in hour-long videos, outperforming previous state-of-the-art methods across multiple datasets by a significant margin (+2.6% R1@0.1 on MAD). The code is available at https://github.com/Tanveer81/ReVisionLLM.",
                "authors": "Tanveer Hannan, Md Mohaiminul Islam, Jindong Gu, Thomas Seidl, Gedas Bertasius",
                "citations": 0
            },
            {
                "title": "Improved GUI Grounding via Iterative Narrowing",
                "abstract": "Graphical User Interface (GUI) grounding plays a crucial role in enhancing the capabilities of Vision-Language Model (VLM) agents. While general VLMs, such as GPT-4V, demonstrate strong performance across various tasks, their proficiency in GUI grounding remains suboptimal. Recent studies have focused on fine-tuning these models specifically for zero-shot GUI grounding, yielding significant improvements over baseline performance. We introduce a visual prompting framework that employs an iterative narrowing mechanism to further improve the performance of both general and fine-tuned models in GUI grounding. For evaluation, we tested our method on a comprehensive benchmark comprising various UI platforms and provided the code to reproduce our results.",
                "authors": "Anthony Nguyen",
                "citations": 0
            },
            {
                "title": "F$^3$OCUS -- Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics",
                "abstract": "Effective training of large Vision-Language Models (VLMs) on resource-constrained client devices in Federated Learning (FL) requires the usage of parameter-efficient fine-tuning (PEFT) strategies. To this end, we demonstrate the impact of two factors \\textit{viz.}, client-specific layer importance score that selects the most important VLM layers for fine-tuning and inter-client layer diversity score that encourages diverse layer selection across clients for optimal VLM layer selection. We first theoretically motivate and leverage the principal eigenvalue magnitude of layerwise Neural Tangent Kernels and show its effectiveness as client-specific layer importance score. Next, we propose a novel layer updating strategy dubbed F$^3$OCUS that jointly optimizes the layer importance and diversity factors by employing a data-free, multi-objective, meta-heuristic optimization on the server. We explore 5 different meta-heuristic algorithms and compare their effectiveness for selecting model layers and adapter layers towards PEFT-FL. Furthermore, we release a new MedVQA-FL dataset involving overall 707,962 VQA triplets and 9 modality-specific clients and utilize it to train and evaluate our method. Overall, we conduct more than 10,000 client-level experiments on 6 Vision-Language FL task settings involving 58 medical image datasets and 4 different VLM architectures of varying sizes to demonstrate the effectiveness of the proposed method.",
                "authors": "Pramit Saha, Felix Wagner, Divyanshu Mishra, Can Peng, Anshul Thakur, David Clifton, K. Kamnitsas, J. A. Noble",
                "citations": 0
            },
            {
                "title": "OpenDlign: Open-World Point Cloud Understanding with Depth-Aligned Images",
                "abstract": "Recent open-world 3D representation learning methods using Vision-Language Models (VLMs) to align 3D point cloud with image-text information have shown superior 3D zero-shot performance. However, CAD-rendered images for this alignment often lack realism and texture variation, compromising alignment robustness. Moreover, the volume discrepancy between 3D and 2D pretraining datasets highlights the need for effective strategies to transfer the representational abilities of VLMs to 3D learning. In this paper, we present OpenDlign, a novel open-world 3D model using depth-aligned images generated from a diffusion model for robust multimodal alignment. These images exhibit greater texture diversity than CAD renderings due to the stochastic nature of the diffusion model. By refining the depth map projection pipeline and designing depth-specific prompts, OpenDlign leverages rich knowledge in pre-trained VLM for 3D representation learning with streamlined fine-tuning. Our experiments show that OpenDlign achieves high zero-shot and few-shot performance on diverse 3D tasks, despite only fine-tuning 6 million parameters on a limited ShapeNet dataset. In zero-shot classification, OpenDlign surpasses previous models by 8.0% on ModelNet40 and 16.4% on OmniObject3D. Additionally, using depth-aligned images for multimodal alignment consistently enhances the performance of other state-of-the-art models.",
                "authors": "Ye Mao, Junpeng Jing, K. Mikolajczyk",
                "citations": 0
            },
            {
                "title": "Environmental awareness in machines: a case study of automated debris removal using Generative Artificial Intelligence and Vision Language Models",
                "abstract": "Water channels play a crucial role in stormwater management, but the build-up of debris in their grilles can lead to flooding, endangering humans and animals, properties, and critical infrastructure nearby. While automated mechanical grab systems are necessary for efficient debris removal, their deployment in outdoor environments has been non-existent due to safety concerns. Here we report the successful use of Generative Artificial Intelligence (GenAI) and a Vision Language Model (VLM) to endow an automated mechanical grab with “awareness”, which allows it to differentiate between non-living and living objects, deciding whether to initiate or abort grabbing actions. The existing approaches such as YOLOv7 only achieve a sensitivity of 86.94% (95% CI: 83.44% to 89.93%) in detecting humans and specified animals. They systematically miss crouching workers and animals facing away from the cameras. Grounding DINO (VLM) can achieve a sensitivity of 100% (95% CI: 99.17% to 100.00%) and a specificity of 85.37% (95% CI: 77.86% to 91.09%). Together with BLIP-2 (GenAI), it acquires “awareness”, allowing it to detect animals beyond those specified. This opens up possibilities for the application of GenAI/VLM in automation sectors where human-machine mingling occurs, such as manufacturing, logistics, and construction. This innovation can potentially improve the safety and efficiency in these domains.",
                "authors": "Jolly P. C. Chan, Heiton M. H. Ho, T. K. Wong, Lawrence Y L Ho, Jackie Cheung, Samson Tai",
                "citations": 0
            },
            {
                "title": "Open-Vocabulary Object Detectors: Robustness Challenges under Distribution Shifts",
                "abstract": "The challenge of Out-Of-Distribution (OOD) robustness remains a critical hurdle towards deploying deep vision models. Vision-Language Models (VLMs) have recently achieved groundbreaking results. VLM-based open-vocabulary object detection extends the capabilities of traditional object detection frameworks, enabling the recognition and classification of objects beyond predefined categories. Investigating OOD robustness in recent open-vocabulary object detection is essential to increase the trustworthiness of these models. This study presents a comprehensive robustness evaluation of the zero-shot capabilities of three recent open-vocabulary (OV) foundation object detection models: OWL-ViT, YOLO World, and Grounding DINO. Experiments carried out on the robustness benchmarks COCO-O, COCO-DC, and COCO-C encompassing distribution shifts due to information loss, corruption, adversarial attacks, and geometrical deformation, highlighting the challenges of the model's robustness to foster the research for achieving robustness. Project page: https://prakashchhipa.github.io/projects/ovod_robustness",
                "authors": "Prakash Chandra Chhipa, Kanjar De, Meenakshi Subhash Chippa, Rajkumar Saini, Marcus Liwicki",
                "citations": 0
            },
            {
                "title": "HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction",
                "abstract": "How can we predict future interaction trajectories of human hands in a scene given high-level colloquial task specifications in the form of natural language? In this paper, we extend the classic hand trajectory prediction task to two tasks involving explicit or implicit language queries. Our proposed tasks require extensive understanding of human daily activities and reasoning abilities about what should be happening next given cues from the current scene. We also develop new benchmarks to evaluate the proposed two tasks, Vanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We enable solving these tasks by integrating high-level world knowledge and reasoning capabilities of Vision-Language Models (VLMs) with the auto-regressive nature of low-level ego-centric hand trajectories. Our model, HandsOnVLM is a novel VLM that can generate textual responses and produce future hand trajectories through natural-language conversations. Our experiments show that HandsOnVLM outperforms existing task-specific methods and other VLM baselines on proposed tasks, and demonstrates its ability to effectively utilize world knowledge for reasoning about low-level human hand trajectories based on the provided context. Our website contains code and detailed video results https://www.chenbao.tech/handsonvlm/",
                "authors": "Chen Bao, Jiarui Xu, Xiaolong Wang, Abhinav Gupta, Homanga Bharadhwaj",
                "citations": 0
            },
            {
                "title": "Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving",
                "abstract": "End-to-end autonomous driving demonstrates strong planning capabilities with large-scale data but still struggles in complex, rare scenarios due to limited commonsense. In contrast, Large Vision-Language Models (LVLMs) excel in scene understanding and reasoning. The path forward lies in merging the strengths of both approaches. Previous methods using LVLMs to predict trajectories or control signals yield suboptimal results, as LVLMs are not well-suited for precise numerical predictions. This paper presents Senna, an autonomous driving system combining an LVLM (Senna-VLM) with an end-to-end model (Senna-E2E). Senna decouples high-level planning from low-level trajectory prediction. Senna-VLM generates planning decisions in natural language, while Senna-E2E predicts precise trajectories. Senna-VLM utilizes a multi-image encoding approach and multi-view prompts for efficient scene understanding. Besides, we introduce planning-oriented QAs alongside a three-stage training strategy, which enhances Senna-VLM's planning performance while preserving commonsense. Extensive experiments on two datasets show that Senna achieves state-of-the-art planning performance. Notably, with pre-training on a large-scale dataset DriveX and fine-tuning on nuScenes, Senna significantly reduces average planning error by 27.12% and collision rate by 33.33% over model without pre-training. We believe Senna's cross-scenario generalization and transferability are essential for achieving fully autonomous driving. Code and models will be released at https://github.com/hustvl/Senna.",
                "authors": "Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin, Qian Zhang, Chang Huang, Wenyu Liu, Xinggang Wang",
                "citations": 0
            },
            {
                "title": "Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays",
                "abstract": "Difference visual question answering (diff-VQA) is a challenging task that requires answering complex questions based on differences between a pair of images. This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice. However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model's performance using a pretrained vision-language model (VLM). Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task. The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data. The longitudinal data consist of pairs of X-ray images, along with question-answer sets and radiologist's reports that describe the changes in lung abnormalities and diseases over time. Our experimental results show that the PLURAL model outperforms state-of-the-art methods not only in diff-VQA for longitudinal X-rays but also in conventional VQA for a single X-ray image. Through extensive experiments, we demonstrate the effectiveness of the proposed VLM architecture and pretraining method in improving the model's performance.",
                "authors": "Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin",
                "citations": 0
            },
            {
                "title": "Training-Free Feature Reconstruction with Sparse Optimization for Vision-Language Models",
                "abstract": "In this paper, we address the challenge of adapting vision-language models (VLMs) to few-shot image recognition in a training-free manner. We observe that existing methods are not able to effectively characterize the semantic relationship between support and query samples in a training-free setting. We recognize that, in the semantic feature space, the feature of the query image is a linear and sparse combination of support image features since support-query pairs are from the class and share the same small set of distinctive visual attributes. Motivated by this interesting observation, we pro-pose a novel method called T raining-free Fe a ture Re C onstruction with Sparse o ptimization ( TaCo ), which formulates the few-shot image recognition task as a feature reconstruction and sparse optimization problem. Specifically, we exploit the VLM to encode the query and support images into features. We utilize sparse optimization to reconstruct the query feature from the corresponding support features. The feature reconstruction error is then used to define the reconstruction similarity. Coupled with the text-image similarity provided by the VLM, our reconstruction similarity analysis accurately characterizes the relationship between support and query images. This results in significantly improved performance in few-shot image recognition. Our extensive experimental results on few-shot recognition demonstrate that our method outperforms existing",
                "authors": "Yi Zhang, Ke Yu, Angelica I. Avilés-Rivero, Jiyuan Jia, Yushun Tang, Zhihai He",
                "citations": 0
            },
            {
                "title": "Adapting Vision-Language Models via Learning to Inject Knowledge",
                "abstract": "Pre-trained vision-language models (VLM) such as CLIP, have demonstrated impressive zero-shot performance on various vision tasks. Trained on millions or even billions of image-text pairs, the text encoder has memorized a substantial amount of appearance knowledge. Such knowledge in VLM is usually leveraged by learning specific task-oriented prompts, which may limit its performance in unseen tasks. This paper proposes a new knowledge injection framework to pursue a generalizable adaption of VLM to downstream vision tasks. Instead of learning task-specific prompts, we extract task-agnostic knowledge features, and insert them into features of input images or texts. The fused features hence gain better discriminative capability and robustness to intra-category variances. Those knowledge features are generated by inputting learnable prompt sentences into text encoder of VLM, and extracting its multi-layer features. A new knowledge injection module (KIM) is proposed to refine text features or visual features using knowledge features. This knowledge injection framework enables both modalities to benefit from the rich knowledge memorized in the text encoder. Experiments show that our method outperforms recently proposed methods under few-shot learning, base-to-new classes generalization, cross-dataset transfer, and domain generalization settings. For instance, it outperforms CoOp by 4.5% under the few-shot learning setting, and CoCoOp by 4.4% under the base-to-new classes generalization setting. Our code will be released.",
                "authors": "Shiyu Xuan, Ming Yang, Shiliang Zhang",
                "citations": 0
            },
            {
                "title": "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models",
                "abstract": "Vision-language models (VLMs) have gained widespread adoption in both industry and academia. In this study, we propose a unified framework for systematically evaluating gender, race, and age biases in VLMs with respect to professions. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. Additionally, we propose an automated pipeline to generate high-quality synthetic datasets that intentionally conceal gender, race, and age information across different professional domains, both in generated text and images. The dataset includes action-based descriptions of each profession and serves as a benchmark for evaluating societal biases in vision-language models (VLMs). In our comparative analysis of widely used VLMs, we have identified that varying input-output modalities lead to discernible differences in bias magnitudes and directions. Additionally, we find that VLM models exhibit distinct biases across different bias attributes we investigated. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.",
                "authors": "Ashutosh Sathe, Prachi Jain, Sunayana Sitaram",
                "citations": 0
            },
            {
                "title": "The Solution for CVPR2024 Foundational Few-Shot Object Detection Challenge",
                "abstract": "This report introduces an enhanced method for the Foundational Few-Shot Object Detection (FSOD) task, leveraging the vision-language model (VLM) for object detection. However, on specific datasets, VLM may encounter the problem where the detected targets are misaligned with the target concepts of interest. This misalignment hinders the zero-shot performance of VLM and the application of fine-tuning methods based on pseudo-labels. To address this issue, we propose the VLM+ framework, which integrates the multimodal large language model (MM-LLM). Specifically, we use MM-LLM to generate a series of referential expressions for each category. Based on the VLM predictions and the given annotations, we select the best referential expression for each category by matching the maximum IoU. Subsequently, we use these referential expressions to generate pseudo-labels for all images in the training set and then combine them with the original labeled data to fine-tune the VLM. Additionally, we employ iterative pseudo-label generation and optimization to further enhance the performance of the VLM. Our approach achieve 32.56 mAP in the final test.",
                "authors": "Hongpeng Pan, Shifeng Yi, Shouwei Yang, Lei Qi, Bing Hu, Yi Xu, Yang Yang",
                "citations": 0
            },
            {
                "title": "Verbal Learning and Memory Deficits across Neurological and Neuropsychiatric Disorders: Insights from an ENIGMA Mega Analysis",
                "abstract": "Deficits in memory performance have been linked to a wide range of neurological and neuropsychiatric conditions. While many studies have assessed the memory impacts of individual conditions, this study considers a broader perspective by evaluating how memory recall is differentially associated with nine common neuropsychiatric conditions using data drawn from 55 international studies, aggregating 15,883 unique participants aged 15–90. The effects of dementia, mild cognitive impairment, Parkinson’s disease, traumatic brain injury, stroke, depression, attention-deficit/hyperactivity disorder (ADHD), schizophrenia, and bipolar disorder on immediate, short-, and long-delay verbal learning and memory (VLM) scores were estimated relative to matched healthy individuals. Random forest models identified age, years of education, and site as important VLM covariates. A Bayesian harmonization approach was used to isolate and remove site effects. Regression estimated the adjusted association of each clinical group with VLM scores. Memory deficits were strongly associated with dementia and schizophrenia (p < 0.001), while neither depression nor ADHD showed consistent associations with VLM scores (p > 0.05). Differences associated with clinical conditions were larger for longer delayed recall duration items. By comparing VLM across clinical conditions, this study provides a foundation for enhanced diagnostic precision and offers new insights into disease management of comorbid disorders.",
                "authors": "Eamonn Kennedy, S. W. Liebel, H. Lindsey, Shashank Vadlamani, Pui‐wa Lei, M. Adamson, M. Alda, S. Alonso-Lana, Tim J Anderson, Celso Arango, R. F. Asarnow, Mihai Avram, R. Ayesa-Arriola, T. Babikian, N. Banaj, L. Bird, S. Borgwardt, A. Brodtmann, Katharina Brosch, Karen Caeyenberghs, V. Calhoun, N. Chiaravalloti, David X Cifu, B. Crespo-Facorro, J. Dalrymple-Alford, Kristen Dams‐O'Connor, U. Dannlowski, David Darby, N. Davenport, John DeLuca, C. Díaz-Caneja, S. Disner, E. Dobryakova, S. Ehrlich, C. Esopenko, Fabio Ferrarelli, Lea E. Frank, C. Franz, P. Fuentes-Claramonte, H. Genova, C. Giza, J. Goltermann, D. Grotegerd, M. Gruber, A. Gutiérrez-Zotes, Minji Ha, J. Haavik, C. Hinkin, K. Hoskinson, Daniela Hubl, Andrei Irimia, Andreas Jansen, Michael Kaess, Xiaojian Kang, K. Kenney, Barbora Keřková, M. Khlif, Minah Kim, J. Kindler, T. Kircher, K. Knížková, Knut K. Kolskår, Denise Krch, W. Kremen, T. Kuhn, Veena Kumari, J. Kwon, Roberto Langella, Sarah Laskowitz, Jungha Lee, J. Lengenfelder, Victoria Liou-Johnson, S. Lippa, M. Løvstad, A. Lundervold, Cassandra Marotta, Craig A. Marquardt, P. Mattos, Ahmad Mayeli, Carrie R. McDonald, S. Meinert, Tracy R Melzer, J. Merchán-Naranjo, Chantal Michel, R. Morey, B. Mwangi, D. Myall, I. Nenadić, Mary R. Newsome, Abraham Nunes, Terence J. O’Brien, V. Oertel, John Ollinger, Alexander Olsen, V. Ortiz García de la Foz, Mustafa Ozmen, H. Pardoe, Marise B. Parent, F. Piras, F. Piras, E. Pomarol-Clotet, J. Repple, Genevive Richard, Jonathan Rodriguez, Mabel Rodriguez, K. Rootes-Murdy, J. Rowland, N. Ryan, R. Salvador, Anne-Marthe Sanders, André Schmidt, J. Soares, Gianfranco Spalleta, Filip Španiel, S. Sponheim, A. Stasenko, F. Stein, B. Straube, A. Thames, F. Thomas-Odenthal, S. Thomopoulos, Erin B. Tone, Ivan Torres, M. Troyanskaya, J. A. Turner, Kristine M. Ulrichsen, Guillermo Umpierrez, D. Vecchio, E. Vilella, L. Vivash, William C Walker, E. Werden, L. Westlye, Krista Wild, A. Wroblewski, Mon-Ju Wu, Glenn R. Wylie, Lakshmi N. Yatham, G. Zunta-Soares, Paul M. Thompson, M. J. Pugh, David F Tate, Frank G Hillary, Elisabeth A. Wilde, Emily L. Dennis",
                "citations": 0
            },
            {
                "title": "Exploring The Efficacy of Multi-modal LLM In Endometrial Cancer Diagnosis",
                "abstract": "Endometrial Carcinoma (EC) remains a significant diagnostic challenge due to its heterogeneous histopathology and clinical presentation. Recent advances in Visual Language Models (VLMs) have shown promise in medical imaging. One of the emergent technologies involves to explore large language models (LLMs) abilities with multimodal data. In this research, we find how Integration of VLMs in medical imaging analysis offers substantial benefits, includes better diagnostic accuracy, enhanced clinical decision support. This study demonstrates the efficacy of a multi-modal VLM framework for automated detection and classification of Endometrial Cancer stages, achieving 92% accuracy. Our approach integrates Magnetic resonance image (MRI) of endometrial Cancer and clinical text data to enhance diagnostic decision-making.",
                "authors": "B. S. Sumitha, Mohammed Tajuddin",
                "citations": 0
            },
            {
                "title": "LARE: Latent Augmentation using Regional Embedding with Vision-Language Model",
                "abstract": "In recent years, considerable research has been conducted on vision-language models that handle both image and text data; these models are being applied to diverse downstream tasks, such as\"image-related chat,\"\"image recognition by instruction,\"and\"answering visual questions.\"Vision-language models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), are also high-performance image classifiers that are being developed into domain adaptation methods that can utilize language information to extend into unseen domains. However, because these VLMs embed images as a single point in a unified embedding space, there is room for improvement in the classification accuracy. Therefore, in this study, we proposed the Latent Augmentation using Regional Embedding (LARE), which embeds the image as a region in the unified embedding space learned by the VLM. By sampling the augmented image embeddings from within this latent region, LARE enables data augmentation to various unseen domains, not just to specific unseen domains. LARE achieves robust image classification for domains in and out using augmented image embeddings to fine-tune VLMs. We demonstrate that LARE outperforms previous fine-tuning models in terms of image classification accuracy on three benchmarks. We also demonstrate that LARE is a more robust and general model that is valid under multiple conditions, such as unseen domains, small amounts of data, and imbalanced data.",
                "authors": "Kosuke Sakurai, Tatsuya Ishii, Ryotaro Shimizu, Linxin Song, Masayuki Goto",
                "citations": 0
            },
            {
                "title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset",
                "abstract": "Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning, we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms. Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms.",
                "authors": "Yingzi Ma, Jiong Wang, Fei Wang, Siyuan Ma, Jiazhao Li, Xiujun Li, Furong Huang, Lichao Sun, Bo Li, Yejin Choi, Muhao Chen, Chaowei Xiao",
                "citations": 0
            },
            {
                "title": "Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models",
                "abstract": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its transparency, Multiple-Choice Question Answering (MCQA) is widely used to measure the performance of these models through accuracy. However, existing MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to selection bias, when models disproportionately favor certain answer options based on positional patterns observed during training. In this work, we conduct a comprehensive empirical analysis of several VLM architectures across major datasets designed to assess complex video-focused reasoning. We identify where the bias is most pronounced and demonstrate to what extent model responses reflect genuine understanding of video content and related questions, as opposed to reliance on arbitrary patterns or superficial cues, such as answer position. By decomposing the MCQA task and adapting fairness bias metrics to VLMs, we introduce a post-processing calibration technique BOLD to balance this bias. Our results show that reducing selection bias improves not only debiasing metrics but also overall model performance, including Accuracy and F1 Mean score. Our method, by suppressing\"blind guessing\", offers a more cost- and time-effective approach to mitigating selection bias compared to existing techniques. This study represents the first focused investigation of selection bias in video-to-text LLM-powered models.",
                "authors": "Olga Loginova, Oleksandr Bezrukov, Alexey Kravets",
                "citations": 0
            },
            {
                "title": "Semi-Supervised Implicit Augmentation for Data-Scarce VQA",
                "abstract": ": Vision-language models (VLMs) have demonstrated increasing potency in solving complex vision-language tasks in the recent past. Visual question answering (VQA) is one of the primary downstream tasks for assessing the capability of VLMs, as it helps in gauging the multimodal understanding of a VLM in answering open-ended questions. The vast contextual information learned during the pretraining stage in VLMs can be utilised effectively to finetune the VQA model for specific datasets. In particular, special types of VQA datasets, such as OK-VQA, A-OKVQA (outside knowledge-based), and ArtVQA (domain-specific), have a relatively smaller number of images and corresponding question-answer annotations in the training set. Such datasets can be categorised as data-scarce. This hinders the effective learning of VLMs due to the low information availability. We introduce SemIAug ( Sem i-Supervised I mplicit Aug mentation), a model and dataset agnostic strategy specially designed to address the challenges faced by limited data availability in the domain-specific VQA datasets. SemIAug uses the annotated image-question data present within the chosen dataset and augments it with meaningful new image-question associations. We show that SemIAug improves the VQA performance on data-scarce datasets without the need for additional data or labels.",
                "authors": "Kuan-Chuan Peng, Abhishek Aich, Ziyan Wu, Bhargav Dodla, Kartik Hegde, A. N. Rajagopalan",
                "citations": 0
            },
            {
                "title": "Seeing Syntax: Uncovering Syntactic Learning Limitations in Vision-Language Models",
                "abstract": "Vision-language models (VLMs), serve as foundation models for multi-modal applications such as image captioning and text-to-image generation. Recent studies have highlighted limitations in VLM text encoders, particularly in areas like compositionality and semantic understanding, though the underlying reasons for these limitations remain unclear. In this work, we aim to address this gap by analyzing the syntactic information, one of the fundamental linguistic properties, encoded by the text encoders of VLMs. We perform a thorough analysis comparing VLMs with different objective functions, parameter size and training data size, and with uni-modal language models (ULMs) in their ability to encode syntactic knowledge. Our findings suggest that ULM text encoders acquire syntactic information more effectively than those in VLMs. The syntactic information learned by VLM text encoders is shaped primarily by the pre-training objective, which plays a more crucial role than other factors such as model architecture, model size, or the volume of pre-training data. Models exhibit different layer-wise trends where CLIP performance dropped across layers while for other models, middle layers are rich in encoding syntactic knowledge.",
                "authors": "Sri Harsha Dumpala, David Arps, Sageev Oore, Laura Kallmeyer, Hassan Sajjad",
                "citations": 0
            },
            {
                "title": "Molecular organization of adrenergic and noradrenergic neurons in the mouse ventrolateral medulla",
                "abstract": "Catecholaminergic neurons in the ventrolateral medulla (VLM) regulate many important aspects of physiology, including the sympathetic control of blood pressure, glucose homeostasis, immune function, and the neuroendocrine responses to inflammation and stress. Previous studies have parsed catecholaminergic VLM neurons into distinct sub-populations based on neurochemical content, connectivity, and distinct response to the activation of autonomic reflexes. However, there is no consensus for the molecular organization of catecholaminergic neurons. In this study, we used high-throughput single nuclei RNA sequencing combined with anatomically- and genetically-targeted cell-sorting of spinally-projecting and catecholaminergic neurons to systematically characterize C1 and A1 neurons in the mouse VLM. Our results, based on an analysis of 894 catecholaminergic neurons derived from a dataset of 114,805 single-nuclei transcriptomes, indicate that the VLM contains 7 molecularly distinct subpopulations of catecholaminergic neurons, including 5 adrenergic subtypes and 2 noradrenergic subtypes identified by the expression of enzymes involved in the production of catecholamine ( Th, Dbh) and the vesicular transporters for glutamate ( Slc6a17) and noradrenaline ( Slc6a2). Neurons with projections to the spinal cord were present in 4 of 5 adrenergic cell clusters, and 1 noradrenergic cell cluster. We histologically verified two novel markers of the adrenergic ( Hk2) and noradrenergic ( Eya1) neurons using RNA fluorescent in situ hybridization. Hk2 expression was found primarily in Dbh+ neurons in the rostral VLM, including both bulbospinal and non-bulbospinal neurons, as well as majority of Pnmt+ neurons in the rostral VLM and C2/C3 regions. Eya1 co-localized with Dbh and Slc6a2 consistently in the caudal VLM, aligning with the location of the A1 neurons, whereas Eya1 signal was diffuse in the rostral VLM. Collectively, this shows that Hk2 expression is enriched in adrenergic neurons as judged by conventional criteria irrespective of the cell's location in the medulla, whereas Eya1 is enriched in brainstem noradrenergic neurons and provides a marker for A1 neurons. These data show molecularly distinct adrenergic and noradrenergic neurons are spatially intermixed in the VLM, and provides novel gene markers for distinct subtypes of catecholaminergic neurons that can be used to dissect the function in future studies. This work was supported by National Institute of Health (NIH) R01 HL148004 to SBGA; and R01 HL153916, Pathway to Stop Diabetes Initiator Award 1-18-INI-14, and UVA 3Cavaliers grant to JNC. This is the full abstract presented at the American Physiology Summit 2024 meeting and is only available in HTML format. There are no additional versions or additional content available for this abstract. Physiology was not involved in the peer review process.",
                "authors": "Daniel Stornetta, Dana Schwalbe, Ruei-Jen Abraham-Fan, George Souza, Maira Jalil, Maisie E. Crook, John Campbell",
                "citations": 0
            },
            {
                "title": "A model-based approach for transforming InSAR-derived vertical land motion from a local to a global reference frame",
                "abstract": "Vertical land motion (VLM) observations obtained from Interferometric Synthetic Aperture Radar (InSAR) have transformed our understanding of crustal deformation processes over the past 3 decades. However, these observations are often related to a local reference frame, posing challenges for studies that require large-scale observations within a global reference frame, such as assessments of relative sea level rise and associated hazards. Here, we present a novel approach that enables transforming InSAR-derived VLM at any location worldwide to a global (e.g., International Terrestrial Reference Frame) reference frame without a direct need for GNSS (Global Navigation Satellite System) measurements. To this end, we employ a coarse resolution model of global VLM obtained by interpolating rates of all available GNSS stations over the global land areas. Our rationale is that the high-resolution InSAR-derived VLM data do not capture the long-wavelength signals present in the global VLM model. Therefore, we employ a set of 2D polynomial models to evaluate the difference between InSAR-derived VLM and the global model and then add it back to the InSAR-derived VLM. We examined the validity of our rationale using normalized power spectrum analysis and tested the effect of polynomial order on the accuracy of transformed VLM and the overall success of our approach using two datasets from Los Angeles and New York City. This approach improves the usability of InSAR-derived VLM in geophysical applications, including monitoring regional land subsidence.",
                "authors": "Mahmoud Reshadati, M. Shirzaei",
                "citations": 0
            },
            {
                "title": "Vision Language Models Can Parse Floor Plan Maps",
                "abstract": "Vision language models (VLMs) can simultaneously reason about images and texts to tackle many tasks, from visual question answering to image captioning. This paper focuses on map parsing, a novel task that is unexplored within the VLM context and particularly useful to mobile robots. Map parsing requires understanding not only the labels but also the geometric configurations of a map, i.e., what areas are like and how they are connected. To evaluate the performance of VLMs on map parsing, we prompt VLMs with floorplan maps to generate task plans for complex indoor navigation. Our results demonstrate the remarkable capability of VLMs in map parsing, with a success rate of 0.96 in tasks requiring a sequence of nine navigation actions, e.g., approaching and going through doors. Other than intuitive observations, e.g., VLMs do better in smaller maps and simpler navigation tasks, there was a very interesting observation that its performance drops in large open areas. We provide practical suggestions to address such challenges as validated by our experimental results. Webpage: https://shorturl.at/OUkEY",
                "authors": "David DeFazio, Hrudayangam Mehta, Jeremy Blackburn, Shiqi Zhang",
                "citations": 0
            },
            {
                "title": "Vertical land motion is underestimated in sea-level projections from the Oka estuary, northern Spain",
                "abstract": null,
                "authors": "Tanghua Li, Ane García-Artola, T. Shaw, Dongju Peng, Jennifer Walker, A. Cearreta, Benjamin P. Horton",
                "citations": 0
            },
            {
                "title": "Open-World Task and Motion Planning via Vision-Language Model Inferred Constraints",
                "abstract": "Foundation models trained on internet-scale data, such as Vision-Language Models (VLMs), excel at performing tasks involving common sense, such as visual question answering. Despite their impressive capabilities, these models cannot currently be directly applied to challenging robot manipulation problems that require complex and precise continuous reasoning. Task and Motion Planning (TAMP) systems can control high-dimensional continuous systems over long horizons through combining traditional primitive robot operations. However, these systems require detailed model of how the robot can impact its environment, preventing them from directly interpreting and addressing novel human objectives, for example, an arbitrary natural language goal. We propose deploying VLMs within TAMP systems by having them generate discrete and continuous language-parameterized constraints that enable TAMP to reason about open-world concepts. Specifically, we propose algorithms for VLM partial planning that constrain a TAMP system's discrete temporal search and VLM continuous constraints interpretation to augment the traditional manipulation constraints that TAMP systems seek to satisfy. We demonstrate our approach on two robot embodiments, including a real world robot, across several manipulation tasks, where the desired objectives are conveyed solely through language.",
                "authors": "Nishanth Kumar, Fabio Ramos, Dieter Fox, Caelan Reed Garrett",
                "citations": 0
            },
            {
                "title": "Virtual-Stator Loss Model for Synchronous Generators With Fractional-Slot Winding",
                "abstract": "In synchronous generators, the end-region field can cause additional losses at clamping structures and stator end core. It can be a computational challenge to solve the end-region field of synchronous generators featuring fractional-slot winding. With a minor modification of the number of stator slots, the winding scheme can be changed to the integral slot and, consequently, a virtual stator is derived. The model size of a three-dimensional (3D) Virtual-stator Loss Model (VLM) is much smaller than the Real-stator Loss Model (RLM), and the computational burden is substantially alleviated. This paper delves into the theory underpinning VLM and its practical implementation. From the perspective of two-dimensional (2D) analysis, 2D VLM demonstrates close results in field analysis and loss computation when compared to 2D RLM, indicating the potential of the 3D VLM in accurately computing end-region losses. The 3D VLM is used to compute the end-region losses at different operating points. It concludes that the 3D VLM can keep the essential nature of the loss-producing mechanism while dramatically mitigating the computation hurdles. Lab tests at a 100 kVA salient-pole synchronous generator prove that the investigation approach employed in the paper is trustworthy.",
                "authors": "Zhaoqiang Zhang, A. Nysveen, Børge Johannes Fagermyr, R. Nilssen, H. Ehya, Satish Belkhode",
                "citations": 0
            },
            {
                "title": "Effects of combined use of ribociclib with PARP1 inhibitor on cell kinetics in breast cancer.",
                "abstract": "In the present study, antiproliferative and anticancer effects of Valamor (VLM), which contains the active component ribociclib, and DPQ, a poly(ADP-ribose) polymerase 1 inhibitor, alone and in combination were evaluated in the MCF-7 and MDA-MB-231 breast cancer cell lines in vitro. VLM was applied at concentrations of 40, 80 and 160 µg/ml, and DPQ was used at concentrations of 3, 6 and 9 µg/ml. The proliferation rate, cell index obtained from the real-time cell analysis system, mitosis activity, bromodeoxyuridine cell proliferation and caspase activity parameters were determined. In conclusion, the results obtained from cell kinetics parameters demonstrated the anticancer and antiproliferative effects of the combination of VLM and DPQ on breast cancer cells.",
                "authors": "Ercan Pulat, M. Topcul",
                "citations": 0
            },
            {
                "title": "Beyond End-to-End VLMs: Leveraging Intermediate Text Representations for Superior Flowchart Understanding",
                "abstract": "Flowcharts are typically presented as images, driving the trend of using vision-language models (VLMs) for end-to-end flowchart understanding. However, two key challenges arise: (i) Limited controllability--users have minimal influence over the downstream task, as they can only modify input images, while the training of VLMs is often out of reach for most researchers. (ii) Lack of explainability--it is difficult to trace VLM errors to specific causes, such as failures in visual encoding or reasoning. We propose TextFlow, addressing aforementioned issues with two stages: (i) Vision Textualizer--which generates textual representations from flowchart images; and (ii) Textual Reasoner--which performs question-answering based on the text representations. TextFlow offers three key advantages: (i) users can select the type of text representations (e.g., Graphviz, Mermaid, PlantUML), or further convert them into executable graph object to call tools, enhancing performance and controllability; (ii) it improves explainability by helping to attribute errors more clearly to visual or textual processing components; and (iii) it promotes the modularization of the solution, such as allowing advanced LLMs to be used in the Reasoner stage when VLMs underperform in end-to-end fashion. Experiments on the FlowVQA and FlowLearn benchmarks demonstrate TextFlow's state-of-the-art performance as well as its robustness. All code is publicly available.",
                "authors": "J. Ye, Ankan Dash, Wenpeng Yin, Guiling Wang",
                "citations": 0
            },
            {
                "title": "Joint Classification of Hyperspectral Image and LiDAR Data Based on Spectral Prompt Tuning",
                "abstract": "The pretrained vision-language models (VLMs) have achieved outstanding performance in various visual tasks, primarily due to the knowledge they have acquired from massive image-text pairs. This enables VLMs to generalize to a wide range of downstream tasks. This article presents the first attempt to adapt VLMs for the joint classification task of hyperspectral image (HSI) and LiDAR data, aiming to leverage the well-learned VLMs to extract more generalizable features from diverse remote sensing image sources. Initially, using a patch encoder (PE), low-dimensional patches of HSI and LiDAR data are transformed into high-dimensional latent feature representations, meeting the dimensional requirements of VLMs for visual input data. Unlike traditional classifiers that rely on discrete class labels, VLM-based classification methods depend on continuous vectors, which can be derived from textual templates with class names, i.e., prompts. The classification performance of VLM-based methods heavily relies on these prompts, but prompt engineering not only demands extensive expert knowledge but also is extremely time-consuming. To address this, prompt tuning (PT) methods are introduced to enhance the generalizability of VLMs by adding spectral-based prompts to the vision encoder and incorporating randomly initialized, learnable text prompts (TPs) into the text encoder. Finally, through a novel class-discriminative loss function, the distance between text features of different classes is increased, thereby enhancing the model’s discriminative ability. Experimental results on the Houston 2013, Trento, and MUUFL datasets demonstrate that the proposed method can achieve competitive classification accuracy with a limited number of labeled pixels.",
                "authors": "Yi Kong, Yuhu Cheng, Yang Chen, Xuesong Wang",
                "citations": 0
            },
            {
                "title": "PolySmart @ TRECVid 2024 Video-To-Text",
                "abstract": "In this paper, we present our methods and results for the Video-To-Text (VTT) task at TRECVid 2024, exploring the capabilities of Vision-Language Models (VLMs) like LLaVA and LLaVA-NeXT-Video in generating natural language descriptions for video content. We investigate the impact of fine-tuning VLMs on VTT datasets to enhance description accuracy, contextual relevance, and linguistic consistency. Our analysis reveals that fine-tuning substantially improves the model's ability to produce more detailed and domain-aligned text, bridging the gap between generic VLM tasks and the specialized needs of VTT. Experimental results demonstrate that our fine-tuned model outperforms baseline VLMs across various evaluation metrics, underscoring the importance of domain-specific tuning for complex VTT tasks.",
                "authors": "Jiaxin Wu, Wengyu Zhang, Xiao Wei, Qing Li",
                "citations": 0
            },
            {
                "title": "AdvDreamer Unveils: Are Vision-Language Models Truly Ready for Real-World 3D Variations?",
                "abstract": "Vision Language Models (VLMs) have exhibited remarkable generalization capabilities, yet their robustness in dynamic real-world scenarios remains largely unexplored. To systematically evaluate VLMs' robustness to real-world 3D variations, we propose AdvDreamer, the first framework that generates physically reproducible adversarial 3D transformation (Adv-3DT) samples from single-view images. AdvDreamer integrates advanced generative techniques with two key innovations and aims to characterize the worst-case distributions of 3D variations from natural images. To ensure adversarial effectiveness and method generality, we introduce an Inverse Semantic Probability Objective that executes adversarial optimization on fundamental vision-text alignment spaces, which can be generalizable across different VLM architectures and downstream tasks. To mitigate the distribution discrepancy between generated and real-world samples while maintaining physical reproducibility, we design a Naturalness Reward Model that provides regularization feedback during adversarial optimization, preventing convergence towards hallucinated and unnatural elements. Leveraging AdvDreamer, we establish MM3DTBench, the first VQA dataset for benchmarking VLMs' 3D variations robustness. Extensive evaluations on representative VLMs with diverse architectures highlight that 3D variations in the real world may pose severe threats to model performance across various tasks.",
                "authors": "Shouwei Ruan, Hanqing Liu, Yao Huang, Xiaoqi Wang, Cai Kang, Hang Su, Yinpeng Dong, Xingxing Wei",
                "citations": 0
            },
            {
                "title": "MarvelOVD: Marrying Object Recognition and Vision-Language Models for Robust Open-Vocabulary Object Detection",
                "abstract": "Learning from pseudo-labels that generated with VLMs~(Vision Language Models) has been shown as a promising solution to assist open vocabulary detection (OVD) in recent studies. However, due to the domain gap between VLM and vision-detection tasks, pseudo-labels produced by the VLMs are prone to be noisy, while the training design of the detector further amplifies the bias. In this work, we investigate the root cause of VLMs' biased prediction under the OVD context. Our observations lead to a simple yet effective paradigm, coded MarvelOVD, that generates significantly better training targets and optimizes the learning procedure in an online manner by marrying the capability of the detector with the vision-language model. Our key insight is that the detector itself can act as a strong auxiliary guidance to accommodate VLM's inability of understanding both the ``background'' and the context of a proposal within the image. Based on it, we greatly purify the noisy pseudo-labels via Online Mining and propose Adaptive Reweighting to effectively suppress the biased training boxes that are not well aligned with the target object. In addition, we also identify a neglected ``base-novel-conflict'' problem and introduce stratified label assignments to prevent it. Extensive experiments on COCO and LVIS datasets demonstrate that our method outperforms the other state-of-the-arts by significant margins. Codes are available at https://github.com/wkfdb/MarvelOVD",
                "authors": "Kuo Wang, Lechao Cheng, Weikai Chen, Pingping Zhang, Liang Lin, Fan Zhou, Guanbin Li",
                "citations": 0
            },
            {
                "title": "From Goal-Conditioned to Language-Conditioned Agents via Vision-Language Models",
                "abstract": "Vision-language models (VLMs) have tremendous potential for grounding language, and thus enabling language-conditioned agents (LCAs) to perform diverse tasks specified with text. This has motivated the study of LCAs based on reinforcement learning (RL) with rewards given by rendering images of an environment and evaluating those images with VLMs. If single-task RL is employed, such approaches are limited by the cost and time required to train a policy for each new task. Multi-task RL (MTRL) is a natural alternative, but requires a carefully designed corpus of training tasks and does not always generalize reliably to new tasks. Therefore, this paper introduces a novel decomposition of the problem of building an LCA: first find an environment configuration that has a high VLM score for text describing a task; then use a (pretrained) goal-conditioned policy to reach that configuration. We also explore several enhancements to the speed and quality of VLM-based LCAs, notably, the use of distilled models, and the evaluation of configurations from multiple viewpoints to resolve the ambiguities inherent in a single 2D view. We demonstrate our approach on the Humanoid environment, showing that it results in LCAs that outperform MTRL baselines in zero-shot generalization, without requiring any textual task descriptions or other forms of environment-specific annotation during training. Videos and an interactive demo can be found at https://europe.naverlabs.com/text2control",
                "authors": "Théo Cachet, Christopher R. Dance, Olivier Sigaud",
                "citations": 0
            },
            {
                "title": "GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language Models for Complex Lighting Environments",
                "abstract": "This paper introduces GAgent: an Gripping Agent designed for open-world environments that provides advanced cognitive abilities via VLM agents and flexible grasping abilities with variable stiffness soft grippers. GAgent comprises three primary components - Prompt Engineer module, Visual-Language Model (VLM) core and Workflow module. These three modules enhance gripper success rates by recognizing objects and materials and accurately estimating grasp area even under challenging lighting conditions. As part of creativity, researchers also created a bionic hybrid soft gripper with variable stiffness capable of gripping heavy loads while still gently engaging objects. This intelligent agent, featuring VLM-based cognitive processing with bionic design, shows promise as it could potentially benefit UAVs in various scenarios.",
                "authors": "Zhuowei Li, Miao Zhang, Xiaotian Lin, Meng Yin, Shuai Lu, Xueqian Wang",
                "citations": 0
            },
            {
                "title": "Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment",
                "abstract": "Benefiting from the powerful capabilities of Large Language Models (LLMs), pre-trained visual encoder models connected to LLMs form Vision Language Models (VLMs). However, recent research shows that the visual modality in VLMs is highly vulnerable, allowing attackers to bypass safety alignment in LLMs through visually transmitted content, launching harmful attacks. To address this challenge, we propose a progressive concept-based alignment strategy, PSA-VLM, which incorporates safety modules as concept bottlenecks to enhance visual modality safety alignment. By aligning model predictions with specific safety concepts, we improve defenses against risky images, enhancing explain-ability and controllability while minimally impacting general performance. Our method is obtained through two-stage training. The low computational cost of the first stage brings",
                "authors": "Zhendong Liu, Yuanbi Nie, Yingshui Tan, Xiangyu Yue, Qiushi Cui, Chong-Jun Wang, Xiaoyong Zhu, Bo Zheng",
                "citations": 0
            },
            {
                "title": "VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation",
                "abstract": "We propose the VLR-Bench, a visual question answering (VQA) benchmark for evaluating vision language models (VLMs) based on retrieval augmented generation (RAG). Unlike existing evaluation datasets for external knowledge-based VQA, the proposed VLR-Bench includes five input passages. This allows testing of the ability to determine which passage is useful for answering a given query, a capability lacking in previous research. In this context, we constructed a dataset of 32,000 automatically generated instruction-following examples, which we denote as VLR-IF. This dataset is specifically designed to enhance the RAG capabilities of VLMs by enabling them to learn how to generate appropriate answers based on input passages. We evaluated the validity of the proposed benchmark and training data and verified its performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3 model. The proposed VLR-Bench and VLR-IF datasets are publicly available online.",
                "authors": "HyeonSeok Lim, Dongjae Shin, Seohyun Song, Inho Won, Minjun Kim, Junghun Yuk, Haneol Jang, KyungTae Lim",
                "citations": 0
            },
            {
                "title": "Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models",
                "abstract": "The task of few-shot image classification and segmentation (FS-CS) involves classifying and segmenting target objects in a query image, given only a few examples of the target classes. We introduce the Vision-Instructed Segmentation and Evaluation (VISE) method that transforms the FS-CS problem into the Visual Question Answering (VQA) problem, utilising Vision-Language Models (VLMs), and addresses it in a training-free manner. By enabling a VLM to interact with off-the-shelf vision models as tools, the proposed method is capable of classifying and segmenting target objects using only image-level labels. Specifically, chain-of-thought prompting and in-context learning guide the VLM to answer multiple-choice questions like a human; vision models such as YOLO and Segment Anything Model (SAM) assist the VLM in completing the task. The modular framework of the proposed method makes it easily extendable. Our approach achieves state-of-the-art performance on the Pascal-5i and COCO-20i datasets.",
                "authors": "Tian Meng, Yang Tao, Ruilin Lyu, Wuliang Yin",
                "citations": 0
            },
            {
                "title": "ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination",
                "abstract": "Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.",
                "authors": "Xinxin Zhao, Wenzhe Cai, Likun Tang, Teng Wang",
                "citations": 0
            },
            {
                "title": "Remote Sensing Temporal Vision-Language Models: A Comprehensive Survey",
                "abstract": "Temporal image analysis in remote sensing has traditionally centered on change detection, which identifies regions of change between images captured at different times. However, change detection remains limited by its focus on visual-level interpretation, often lacking contextual or descriptive information. The rise of Vision-Language Models (VLMs) has introduced a new dimension to remote sensing temporal image analysis by integrating visual information with natural language, creating an avenue for advanced interpretation of temporal image changes. Remote Sensing Temporal VLMs (RSTVLMs) allow for dynamic interactions, generating descriptive captions, answering questions, and providing a richer semantic understanding of temporal images. This temporal vision-language capability is particularly valuable for complex remote sensing applications, where higher-level insights are crucial. This paper comprehensively reviews the progress of RSTVLM research, with a focus on the latest VLM applications for temporal image analysis. We categorize and discuss core methodologies, datasets, and metrics, highlight recent advances in temporal vision-language tasks, and outline key challenges and future directions for research in this emerging field. This survey fills a critical gap in the literature by providing an integrated overview of RSTVLM, offering a foundation for further advancements in remote sensing temporal image understanding. We will keep tracing related works at \\url{https://github.com/Chen-Yang-Liu/Awesome-RS-Temporal-VLM}",
                "authors": "Chenyang Liu, Jiafan Zhang, Keyan Chen, Man Wang, Zhengxia Zou, Z. Shi",
                "citations": 0
            },
            {
                "title": "NL-Eye: Abductive NLI for Images",
                "abstract": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050 images) spanning diverse reasoning categories: physical, functional, logical, emotional, cultural, and social. The data curation process involved two steps - writing textual descriptions and generating images using text-to-image models, both requiring substantial human involvement to ensure high-quality and challenging scenes. Our experiments show that VLMs struggle significantly on NL-Eye, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality. This demonstrates a deficiency in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a crucial step toward developing VLMs capable of robust multimodal reasoning for real-world applications, including accident-prevention bots and generated video verification.",
                "authors": "Mor Ventura, Michael Toker, Nitay Calderon, Zorik Gekhman, Yonatan Bitton, Roi Reichart",
                "citations": 0
            },
            {
                "title": "TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models",
                "abstract": "Multi-head self-attention (MHSA) is a key component of Transformers, a widely popular architecture in both language and vision. Multiple heads intuitively enable different parallel processes over the same input. Yet, they also obscure the attribution of each input patch to the output of a model. We propose a novel 1-head Transformer Attention Bottleneck (TAB) layer, inserted after the traditional MHSA architecture, to serve as an attention bottleneck for interpretability and intervention. Unlike standard self-attention, TAB constrains the total attention over all patches to $\\in [0, 1]$. That is, when the total attention is 0, no visual information is propagated further into the network and the vision-language model (VLM) would default to a generic, image-independent response. To demonstrate the advantages of TAB, we train VLMs with TAB to perform image difference captioning. Over three datasets, our models perform similarly to baseline VLMs in captioning but the bottleneck is superior in localizing changes and in identifying when no changes occur. TAB is the first architecture to enable users to intervene by editing attention, which often produces expected outputs by VLMs.",
                "authors": "Pooyan Rahmanzadehgervi, Hung Huy Nguyen, Rosanne Liu, Long Mai, Anh Totti Nguyen",
                "citations": 0
            },
            {
                "title": "Improving Medical Diagnostics with Vision-Language Models: Convex Hull-Based Uncertainty Analysis",
                "abstract": "In recent years, vision-language models (VLMs) have been applied to various fields, including healthcare, education, finance, and manufacturing, with remarkable performance. However, concerns remain regarding VLMs' consistency and uncertainty, particularly in critical applications such as healthcare, which demand a high level of trust and reliability. This paper proposes a novel approach to evaluate uncertainty in VLMs' responses using a convex hull approach on a healthcare application for Visual Question Answering (VQA). LLM-CXR model is selected as the medical VLM utilized to generate responses for a given prompt at different temperature settings, i.e., 0.001, 0.25, 0.50, 0.75, and 1.00. According to the results, the LLM-CXR VLM shows a high uncertainty at higher temperature settings. Experimental outcomes emphasize the importance of uncertainty in VLMs' responses, especially in healthcare applications.",
                "authors": "Ferhat Ozgur Catak, M. Kuzlu, Taylor Patrick",
                "citations": 0
            },
            {
                "title": "Multimodal Fact-Checking with Vision Language Models: A Probing Classifier based Solution with Embedding Strategies",
                "abstract": "This study evaluates the effectiveness of Vision Language Models (VLMs) in representing and utilizing multimodal content for fact-checking. To be more specific, we investigate whether incorporating multimodal content improves performance compared to text-only models and how well VLMs utilize text and image information to enhance misinformation detection. Furthermore we propose a probing classifier based solution using VLMs. Our approach extracts embeddings from the last hidden layer of selected VLMs and inputs them into a neural probing classifier for multi-class veracity classification. Through a series of experiments on two fact-checking datasets, we demonstrate that while multimodality can enhance performance, fusing separate embeddings from text and image encoders yielded superior results compared to using VLM embeddings. Furthermore, the proposed neural classifier significantly outperformed KNN and SVM baselines in leveraging extracted embeddings, highlighting its effectiveness for multimodal fact-checking.",
                "authors": "R. Çekinel, Pinar Senkul, Çağrı Çöltekin",
                "citations": 0
            },
            {
                "title": "Open Vocabulary Multi-Label Video Classification",
                "abstract": "Pre-trained vision-language models (VLMs) have enabled significant progress in open vocabulary computer vision tasks such as image classification, object detection and image segmentation. Some recent works have focused on extending VLMs to open vocabulary single label action classification in videos. However, previous methods fall short in holistic video understanding which requires the ability to simultaneously recognize multiple actions and entities e.g., objects in the video in an open vocabulary setting. We formulate this problem as open vocabulary multilabel video classification and propose a method to adapt a pre-trained VLM such as CLIP to solve this task. We leverage large language models (LLMs) to provide semantic guidance to the VLM about class labels to improve its open vocabulary performance with two key contributions. First, we propose an end-to-end trainable architecture that learns to prompt an LLM to generate soft attributes for the CLIP text-encoder to enable it to recognize novel classes. Second, we integrate a temporal modeling module into CLIP's vision encoder to effectively model the spatio-temporal dynamics of video concepts as well as propose a novel regularized finetuning technique to ensure strong open vocabulary classification performance in the video domain. Our extensive experimentation showcases the efficacy of our approach on multiple benchmark datasets.",
                "authors": "Rohit Gupta, Mamshad Nayeem Rizve, Jayakrishnan Unnikrishnan, Ashish Tawari, Son Tran, Mubarak Shah, Benjamin Yao, Trishul M. Chilimbi",
                "citations": 0
            },
            {
                "title": "Open-vocabulary Temporal Action Localization using VLMs",
                "abstract": "—Video action localization aims to find timings of a specific action from a long video. Although existing learning-based approaches have been successful, those require annotating videos that come with a considerable labor cost. This paper proposes a learning-free, open-vocabulary approach based on emerging vision-language models (VLM). The challenge stems from the fact that VLMs are neither designed to process long videos nor tailored for finding actions. We overcome these problems by extending an iterative visual prompting technique. Specifically, we sample video frames into a concatenated image with frame index labels, making a VLM guess a frame that is considered to be closest to the start/end of the action. Iterating this process by narrowing a sampling time window results in finding a specific frame of start and end of an action. We demonstrate that this sampling technique yields reasonable results, illustrating a practical extension of VLMs for understanding videos. The code will be available shortly.",
                "authors": "Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi",
                "citations": 0
            },
            {
                "title": "Do Vision-Language Models Understand Compound Nouns?",
                "abstract": "Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., *lab coat*) as well as they understand nouns (e.g., *lab*)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs’ limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25% on Compun. Code and benchmark are available.",
                "authors": "Sonal Kumar, Sreyan Ghosh, S. Sakshi, Utkarsh Tyagi, Dinesh Manocha",
                "citations": 0
            },
            {
                "title": "Q-GroundCAM: Quantifying Grounding in Vision Language Models via GradCAM",
                "abstract": "Vision and Language Models (VLMs) continue to demonstrate remarkable zero-shot (ZS) performance across various tasks. However, many probing studies have revealed that even the best-performing VLMs struggle to capture aspects of compositional scene understanding, lacking the ability to properly ground and localize linguistic phrases in images. Recent VLM advancements include scaling up both model and dataset sizes, additional training objectives and levels of supervision, and variations in the model architectures. To characterize the grounding ability of VLMs, such as phrase grounding, referring expressions comprehension, and relationship understanding, Pointing Game has been used as an evaluation metric for datasets with bounding box annotations. In this paper, we introduce a novel suite of quantitative metrics that utilize GradCAM activations to rigorously evaluate the grounding capabilities of pre-trained VLMs like CLIP, BLIP, and ALBEF. These metrics offer an explainable and quantifiable approach for a more detailed comparison of the zero-shot capabilities of VLMs and enable measuring models' grounding uncertainty. This characterization reveals interesting tradeoffs between the size of the model, the dataset size, and their performance.",
                "authors": "Navid Rajabi, J. Kosecka",
                "citations": 0
            },
            {
                "title": "Read Like a Radiologist: Efficient Vision-Language Model for 3D Medical Imaging Interpretation",
                "abstract": "Recent medical vision-language models (VLMs) have shown promise in 2D medical image interpretation. However extending them to 3D medical imaging has been challenging due to computational complexities and data scarcity. Although a few recent VLMs specified for 3D medical imaging have emerged, all are limited to learning volumetric representation of a 3D medical image as a set of sub-volumetric features. Such process introduces overly correlated representations along the z-axis that neglect slice-specific clinical details, particularly for 3D medical images where adjacent slices have low redundancy. To address this limitation, we introduce MS-VLM that mimic radiologists' workflow in 3D medical image interpretation. Specifically, radiologists analyze 3D medical images by examining individual slices sequentially and synthesizing information across slices and views. Likewise, MS-VLM leverages self-supervised 2D transformer encoders to learn a volumetric representation that capture inter-slice dependencies from a sequence of slice-specific features. Unbound by sub-volumetric patchification, MS-VLM is capable of obtaining useful volumetric representations from 3D medical images with any slice length and from multiple images acquired from different planes and phases. We evaluate MS-VLM on publicly available chest CT dataset CT-RATE and in-house rectal MRI dataset. In both scenarios, MS-VLM surpasses existing methods in radiology report generation, producing more coherent and clinically relevant reports. These findings highlight the potential of MS-VLM to advance 3D medical image interpretation and improve the robustness of medical VLMs.",
                "authors": "Changsun Lee, Sangjoon Park, Cheong-Il Shin, Woo Hee Choi, HyunWook Park, Jeonghyeon Lee, Jong Chul Ye",
                "citations": 0
            },
            {
                "title": "Learning Manipulation Skills through Robot Chain-of-Thought with Sparse Failure Guidance",
                "abstract": "Defining reward functions for skill learning has been a long-standing challenge in robotics. Recently, vision-language models (VLMs) have shown promise in defining reward signals for teaching robots manipulation skills. However, existing works often provide reward guidance that is too coarse, leading to inefficient learning processes. In this paper, we address this issue by implementing more fine-grained reward guidance. We decompose tasks into simpler sub-tasks, using this decomposition to offer more informative reward guidance with VLMs. We also propose a VLM-based self imitation learning process to speed up learning. Empirical evidence demonstrates that our algorithm consistently outperforms baselines such as CLIP, LIV, and RoboCLIP. Specifically, our algorithm achieves a $5.4 \\times$ higher average success rate compared to the best baseline, RoboCLIP, across a series of manipulation tasks.",
                "authors": "Kaifeng Zhang, Zhao-Heng Yin, Weirui Ye, Yang Gao",
                "citations": 0
            },
            {
                "title": "Knowledge-grounded Adaptation Strategy for Vision-language Models: Building Unique Case-set for Screening Mammograms for Residents Training",
                "abstract": "A visual-language model (VLM) pre-trained on natural images and text pairs poses a significant barrier when applied to medical contexts due to domain shift. Yet, adapting or fine-tuning these VLMs for medical use presents considerable hurdles, including domain misalignment, limited access to extensive datasets, and high-class imbalances. Hence, there is a pressing need for strategies to effectively adapt these VLMs to the medical domain, as such adaptations would prove immensely valuable in healthcare applications. In this study, we propose a framework designed to adeptly tailor VLMs to the medical domain, employing selective sampling and hard-negative mining techniques for enhanced performance in retrieval tasks. We validate the efficacy of our proposed approach by implementing it across two distinct VLMs: the in-domain VLM (MedCLIP) and out-of-domain VLMs (ALBEF). We assess the performance of these models both in their original off-the-shelf state and after undergoing our proposed training strategies, using two extensive datasets containing mammograms and their corresponding reports. Our evaluation spans zero-shot, few-shot, and supervised scenarios. Through our approach, we observe a notable enhancement in Recall@K performance for the image-text retrieval task.",
                "authors": "Aisha Urooj Khan, John Garrett, Tyler Bradshaw, Lonie R. Salkowski, Jiwoong Jeong, Amara Tariq, Imon Banerjee",
                "citations": 0
            },
            {
                "title": "DenseVLM: A Retrieval and Decoupled Alignment Framework for Open-Vocabulary Dense Prediction",
                "abstract": "Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated impressive zero-shot recognition capability, but still underperform in dense prediction tasks. Self-distillation recently is emerging as a promising approach for fine-tuning VLMs to better adapt to local regions without requiring extensive annotations. However, previous state-of-the-art approaches often suffer from significant `foreground bias', where models tend to wrongly identify background regions as foreground objects. To alleviate this issue, we propose DenseVLM, a framework designed to learn unbiased region-language alignment from powerful pre-trained VLM representations. By leveraging the pre-trained VLM to retrieve categories for unlabeled regions, DenseVLM effectively decouples the interference between foreground and background region features, ensuring that each region is accurately aligned with its corresponding category. We show that DenseVLM can be seamlessly integrated into open-vocabulary object detection and image segmentation tasks, leading to notable performance improvements. Furthermore, it exhibits promising zero-shot scalability when training on more extensive and diverse datasets.",
                "authors": "Yunheng Li, Yuxuan Li, Quansheng Zeng, Wenhai Wang, Qibin Hou, Ming-Ming Cheng",
                "citations": 0
            },
            {
                "title": "LifeGraph 4 - Lifelog Retrieval using Multimodal Knowledge Graphs and Vision-Language Models",
                "abstract": "In the scope of the 7th Lifelog Search Challenge (LSC'24), we present the 4th iteration of LifeGraph, a multimodal knowledge-graph approach with data augmentations using Vision-Language Models (VLM). We extend the LifeGraph model presented in former LSC challenges by event-based clustering using temporal and spatial relations as well as information extracted from descriptions of Lifelog image captions produced by VLMs.",
                "authors": "Luca Rossetto, Athina Kyriakou, Svenja Lange, Florian Ruosch, Ruijie Wang, Kathrin Wardatzky, Abraham Bernstein",
                "citations": 0
            },
            {
                "title": "Evaluating Vision-Language Models as Evaluators in Path Planning",
                "abstract": "Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still contribute to the planning framework as a helpful plan evaluator? In this work, we generalize this question to consider LLMs augmented with visual understanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a novel benchmark evaluating VLMs as plan evaluators in complex path-planning scenarios. Succeeding in the benchmark requires a VLM to be able to abstract traits of optimal paths from the scenario description, demonstrate precise low-level perception on each path, and integrate this information to decide the better path. Our analysis of state-of-the-art VLMs reveals that these models face significant challenges on the benchmark. We observe that the VLMs can precisely abstract given scenarios to identify the desired traits and exhibit mixed performance in integrating the provided information. Yet, their vision component presents a critical bottleneck, with models struggling to perceive low-level details about a path. Our experimental results show that this issue cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific discriminative adaptation of these vision encoders is needed for these VLMs to become effective path evaluators.",
                "authors": "Mohamed Aghzal, Xiang Yue, E. Plaku, Ziyu Yao",
                "citations": 0
            },
            {
                "title": "ACE: Action Concept Enhancement of Video-Language Models in Procedural Videos",
                "abstract": "Vision-language models (VLMs) are capable of recognizing unseen actions. However, existing VLMs lack intrinsic understanding of procedural action concepts. Hence, they overfit to fixed labels and are not invariant to unseen action synonyms. To address this, we propose a simple fine-tuning technique, Action Concept Enhancement (ACE), to improve the robustness and concept understanding of VLMs in procedural action classification. ACE continually incorporates augmented action synonyms and negatives in an auxiliary classification loss by stochastically replacing fixed labels during training. This creates new combinations of action labels over the course of fine-tuning and prevents overfitting to fixed action representations. We show the enhanced concept understanding of our VLM, by visualizing the alignment of encoded embeddings of unseen action synonyms in the embedding space. Our experiments on the ATA, IKEA and GTEA datasets demonstrate the efficacy of ACE in domains of cooking and assembly leading to significant improvements in zero-shot action classification while maintaining competitive performance on seen actions.",
                "authors": "Reza Ghoddoosian, Nakul Agarwal, Isht Dwivedi, Behzad Darisuh",
                "citations": 0
            },
            {
                "title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models",
                "abstract": "The recent surge in high-quality visual instruction tuning samples from closed-source vision-language models (VLMs) such as GPT-4V has accelerated the release of open-source VLMs across various model sizes. However, scaling VLMs to improve performance using larger models brings significant computational challenges, especially for deployment on resource-constrained devices like mobile platforms and robots. To address this, we propose VLsI: Verbalized Layers-to-Interactions, a new VLM family in 2B and 7B model sizes, which prioritizes efficiency without compromising accuracy. VLsI leverages a unique, layer-wise distillation process, introducing intermediate\"verbalizers\"that map features from each layer to natural language space, allowing smaller VLMs to flexibly align with the reasoning processes of larger VLMs. This approach mitigates the training instability often encountered in output imitation and goes beyond typical final-layer tuning by aligning the small VLMs' layer-wise progression with that of the large ones. We validate VLsI across ten challenging vision-language benchmarks, achieving notable performance gains (11.0% for 2B and 17.4% for 7B) over GPT-4V without the need for model scaling, merging, or architectural changes.",
                "authors": "Byung-Kwan Lee, Ryo Hachiuma, Yu-Chiang Frank Wang, Yonghyun Ro, Yueh-Hua Wu",
                "citations": 0
            },
            {
                "title": "Do Pre-trained Vision-Language Models Encode Object States?",
                "abstract": "For a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for example how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts. We curate an object state recognition dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and generative objectives. We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states. Through extensive experiments, we identify three areas for improvements for VLMs to better encode object states, namely the quality of object localization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states. Data and code are released.",
                "authors": "Kaleb Newman, Shijie Wang, Yuan Zang, David Heffren, Chen Sun",
                "citations": 0
            },
            {
                "title": "A novel interpretation of Nesterov's acceleration via variable step-size linear multistep methods",
                "abstract": "Nesterov's acceleration in continuous optimization can be understood in a novel way when Nesterov's accelerated gradient (NAG) method is considered as a linear multistep (LM) method for gradient flow. Although the NAG method for strongly convex functions (NAG-sc) has been fully discussed, the NAG method for $L$-smooth convex functions (NAG-c) has not. To fill this gap, we show that the existing NAG-c method can be interpreted as a variable step size LM (VLM) for the gradient flow. Surprisingly, the VLM allows linearly increasing step sizes, which explains the acceleration in the convex case. Here, we introduce a novel technique for analyzing the absolute stability of VLMs. Subsequently, we prove that NAG-c is optimal in a certain natural class of VLMs. Finally, we construct a new broader class of VLMs by optimizing the parameters in the VLM for ill-conditioned problems. According to numerical experiments, the proposed method outperforms the NAG-c method in ill-conditioned cases. These results imply that the numerical analysis perspective of the NAG is a promising working environment, and considering a broader class of VLMs could further reveal novel methods.",
                "authors": "Ryota Nozawa, Shun Sato, Takayasu Matsuo",
                "citations": 0
            },
            {
                "title": "Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations",
                "abstract": "Vision-language models (VLMs) like CLIP have been adapted for Multi-Label Recognition (MLR) with partial annotations by leveraging prompt-learning, where positive and negative prompts are learned for each class to associate their embeddings with class presence or absence in the shared vision-text feature space. While this approach improves MLR performance by relying on VLM priors, we hypothesize that learning negative prompts may be suboptimal, as the datasets used to train VLMs lack image-caption pairs explicitly focusing on class absence. To analyze the impact of positive and negative prompt learning on MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is learned with VLM guidance while the other is replaced by an embedding vector learned directly in the shared feature space without relying on the text encoder. Through empirical analysis, we observe that negative prompts degrade MLR performance, and learning only positive prompts, combined with learned negative embeddings (PositiveCoOp), outperforms dual prompt learning approaches. Moreover, we quantify the performance benefits that prompt-learning offers over a simple vision-features-only baseline, observing that the baseline displays strong performance comparable to dual prompt learning approach (DualCoOp), when the proportion of missing labels is low, while requiring half the training compute and 16 times fewer parameters",
                "authors": "Samyak Rawlekar, Shubhang Bhatnagar, Narendra Ahuja",
                "citations": 0
            },
            {
                "title": "EdgeCloudAI: Edge-Cloud Distributed Video Analytics",
                "abstract": "Recent advances in Visual Language Models (VLMs) have significantly enhanced video analytics. VLMs capture complex visual and textual connections. While Convolutional Neural Networks (CNNs) excel in spatial pattern recognition, VLMs provide a global context, making them ideal for tasks like complex incidents and anomaly detection. However, VLMs are much more computationally intensive, posing challenges for large-scale and real-time applications. This paper introduces EdgeCloudAI, a scalable system integrating VLMs and CNNs through edge-cloud computing. Edge-CloudAI performs initial video processing (e.g., CNN) on edge devices and offloads deeper analysis (e.g., VLM) to the cloud, optimizing resource use and reducing latency. We have deployed EdgeCloudAI on the NSF COSMOS testbed in NYC. In this demo, we will demonstrate EdgeCloudAI’s performance in detecting user-defined incidents in real-time.",
                "authors": "Mahshid Ghasemi, Z. Kostić, Javad Ghaderi, Gil Zussman",
                "citations": 0
            },
            {
                "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
                "abstract": "Recent advances in Vision-Language Models (VLMs) and the scarcity of high-quality multi-modal alignment data have inspired numerous researches on synthetic VLM data generation. The conventional norm in VLM data construction uses a mixture of specialists in caption and OCR, or stronger VLM APIs and expensive human annotation.In this paper, we present World to Code (W2C), a meticulously curated multi-modal data construction pipeline that organizes the final generation output into a Python code format. The pipeline leverages the VLM itself to extract cross-modal information via different prompts and filter the generated outputs again via a consistency filtering strategy. Experiments have demonstrated the high quality of W2C by improving various existing visual question answering and visual grounding benchmarks across different VLMs. Further analysis also demonstrates that the new code parsing ability of VLMs presents better cross-modal equivalence than the commonly used detail caption ability. Our code is available at https://github.com/foundation-multimodal-models/World2Code.",
                "authors": "Jiacong Wang, Bohong Wu, Haiyong Jiang, Xun Zhou, Xin Xiao, Haoyuan Guo, Jun Xiao",
                "citations": 0
            },
            {
                "title": "More Distinctively Black and Feminine Faces Lead to Increased Stereotyping in Vision-Language Models",
                "abstract": "Vision Language Models (VLMs), exemplified by GPT-4V, adeptly integrate text and vision modalities. This integration enhances Large Language Models' ability to mimic human perception, allowing them to process image inputs. Despite VLMs' advanced capabilities, however, there is a concern that VLMs inherit biases of both modalities in ways that make biases more pervasive and difficult to mitigate. Our study explores how VLMs perpetuate homogeneity bias and trait associations with regards to race and gender. When prompted to write stories based on images of human faces, GPT-4V describes subordinate racial and gender groups with greater homogeneity than dominant groups and relies on distinct, yet generally positive, stereotypes. Importantly, VLM stereotyping is driven by visual cues rather than group membership alone such that faces that are rated as more prototypically Black and feminine are subject to greater stereotyping. These findings suggest that VLMs may associate subtle visual cues related to racial and gender groups with stereotypes in ways that could be challenging to mitigate. We explore the underlying reasons behind this behavior and discuss its implications and emphasize the importance of addressing these biases as VLMs come to mirror human perception.",
                "authors": "Messi H.J. Lee, Jacob M. Montgomery, Calvin K Lai",
                "citations": 0
            },
            {
                "title": "Ventrolateral medullary compression by vascular contact in primary hemifacial spasm: a radiological analysis.",
                "abstract": null,
                "authors": "D. Anudeep, K. Karthik, V. Holla, N. Kamble, Ravi Yadav, P. Pal, R. Mahale",
                "citations": 0
            },
            {
                "title": "Is Your Text-to-Image Model Robust to Caption Noise?",
                "abstract": "In text-to-image (T2I) generation, a prevalent training technique involves utilizing Vision Language Models (VLMs) for image re-captioning. Even though VLMs are known to exhibit hallucination, generating descriptive content that deviates from the visual reality, the ramifications of such caption hallucinations on T2I generation performance remain under-explored. Through our empirical investigation, we first establish a comprehensive dataset comprising VLM-generated captions, and then systematically analyze how caption hallucination influences generation outcomes. Our findings reveal that (1) the disparities in caption quality persistently impact model outputs during fine-tuning. (2) VLMs confidence scores serve as reliable indicators for detecting and characterizing noise-related patterns in the data distribution. (3) even subtle variations in caption fidelity have significant effects on the quality of learned representations. These findings collectively emphasize the profound impact of caption quality on model performance and highlight the need for more sophisticated robust training algorithm in T2I. In response to these observations, we propose a approach leveraging VLM confidence score to mitigate caption noise, thereby enhancing the robustness of T2I models against hallucination in caption.",
                "authors": "Weicheng Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang",
                "citations": 0
            },
            {
                "title": "ViSTa Dataset: Do vision-language models understand sequential tasks?",
                "abstract": "Using vision-language models (VLMs) as reward models in reinforcement learning holds promise for reducing costs and improving safety. So far, VLM reward models have only been used for goal-oriented tasks, where the agent must reach a particular final outcome. We explore VLMs' potential to supervise tasks that cannot be scored by the final state alone. To this end, we introduce ViSTa, a dataset for evaluating Vision-based understanding of Sequential Tasks. ViSTa comprises over 4,000 videos with step-by-step descriptions in virtual home, Minecraft, and real-world environments. Its novel hierarchical structure -- basic single-step tasks composed into more and more complex sequential tasks -- allows a fine-grained understanding of how well VLMs can judge tasks with varying complexity. To illustrate this, we use ViSTa to evaluate state-of-the-art VLMs, including CLIP, ViCLIP, and GPT-4o. We find that, while they are all good at object recognition, they fail to understand sequential tasks, with only GPT-4o achieving non-trivial performance.",
                "authors": "Evvzen Wybitul, Evan Ryan Gunter, Mikhail Seleznyov, David Lindner",
                "citations": 0
            },
            {
                "title": "Patrol Agent: An Autonomous UAV Framework for Urban Patrol Using on Board Vision Language Model and on Cloud Large Language Model",
                "abstract": "Unmanned Aerial Vehicles (UAVs) used for urban patrols typically require human control or supervision. To enhance the automation of UAV s in this context, we propose the Patrol Agent, which is able to patrol, identify and track a target in a fixed area autonomously without any human intervention. The Patrol Agent employs Vision Language Model (VLM) for accurate visual information, object detection model for rough detection about the target, and Large Language Model (LLM) deployed on cloud for analysis and action-deciding. During patrols, the agent uses a lightweight VLM to generate captions of the scenes it observes. These captions are then sent to the LLM on cloud for further analysis which provides responses regarding the danger level of the scene, appropriate actions to take, and the detailed reasons behind these actions. When the agent identifies and tracks a target, it activates the VLM only when the object detection model detects an object corresponding to the target. This approach conserves computing resources and enhances onboard operational speed. The proposed agent can identify and track targets without requiring fine-tuning data or human intervention. It outperforms Visual Question Answering (VQA) models in patrol and uses fewer computing resources compared to agents that solely rely on VLM for tracking.",
                "authors": "Zihao Yuan, Fangfang Xie, Tingwei Ji",
                "citations": 0
            },
            {
                "title": "CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven Visual Language Models",
                "abstract": "The obesity phenomenon, known as the heavy issue, is a leading cause of preventable chronic diseases worldwide. Traditional calorie estimation tools often rely on specific data formats or complex pipelines, limiting their practicality in real-world scenarios. Recently, vision-language models (VLMs) have excelled in understanding real-world contexts and enabling conversational interactions, making them ideal for downstream tasks such as ingredient analysis. However, applying VLMs to calorie estimation requires domain-specific data and alignment strategies. To this end, we curated CalData, a 330K image-text pair dataset tailored for ingredient recognition and calorie estimation, combining a large-scale recipe dataset with detailed nutritional instructions for robust vision-language training. Built upon this dataset, we present CaLoRAify, a novel VLM framework aligning ingredient recognition and calorie estimation via training with visual-text pairs. During inference, users only need a single monocular food image to estimate calories while retaining the flexibility of agent-based conversational interaction. With Low-rank Adaptation (LoRA) and Retrieve-augmented Generation (RAG) techniques, our system enhances the performance of foundational VLMs in the vertical domain of calorie estimation. Our code and data are fully open-sourced at https://github.com/KennyYao2001/16824-CaLORAify.",
                "authors": "Dongyu Yao, Keling Yao, Junhong Zhou, Yinghao Zhang",
                "citations": 0
            },
            {
                "title": "Adapting Vision-Language Model with Fine-grained Semantics for Open-Vocabulary Segmentation",
                "abstract": "Despite extensive research, open-vocabulary segmentation methods still struggle to generalize across diverse domains. To reduce the computational cost of adapting Vision-Language Models (VLMs) while preserving their pre-trained knowledge, most methods freeze the VLMs for mask classification and train only the mask generator. However, our comprehensive analysis reveals a surprising insight: open-vocabulary segmentation is primarily bottlenecked by mask classification, not mask generation. This discovery prompts us to rethink the existing paradigm and explore an alternative approach. Instead of freezing the VLM, we propose to freeze the pre-trained mask generator and focus on optimizing the mask classifier. Building on the observation that VLMs pre-trained on global-pooled image-text features often fail to capture fine-grained semantics necessary for effective mask classification, we propose a novel Fine-grained Semantic Adaptation (FISA) method to address this limitation. FISA enhances the extracted visual features with fine-grained semantic awareness by explicitly integrating this crucial semantic information early in the visual encoding process. As our method strategically optimizes only a small portion of the VLM's parameters, it enjoys the efficiency of adapting to new data distributions while largely preserving the valuable VLM pre-trained knowledge. Extensive ablation studies confirm the superiority of our approach. Notably, FISA achieves new state-of-the-art results across multiple representative benchmarks, improving performance by up to +1.0 PQ and +3.0 mIoU and reduces training costs by nearly 5x compared to previous best methods. Our code and data will be made public.",
                "authors": "Yong Xien Chng, Xuchong Qiu, Yizeng Han, Kai Ding, Wan Ding, Gao Huang",
                "citations": 0
            },
            {
                "title": "Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts",
                "abstract": "The recent progress in Vision-Language Models (VLMs) has broadened the scope of multimodal applications. However, evaluations often remain limited to functional tasks, neglecting abstract dimensions such as personality traits and human values. To address this gap, we introduce Value-Spectrum, a novel Visual Question Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's value dimensions that capture core values guiding people's preferences and actions. We designed a VLM agent pipeline to simulate video browsing and constructed a vector database comprising over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels. These videos span multiple months and cover diverse topics, including family, health, hobbies, society, technology, etc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs handle value-oriented content. Beyond identifying VLMs' intrinsic preferences, we also explored the ability of VLM agents to adopt specific personas when explicitly prompted, revealing insights into the adaptability of the model in role-playing scenarios. These findings highlight the potential of Value-Spectrum as a comprehensive evaluation set for tracking VLM alignments in value-based tasks and abilities to simulate diverse personas.",
                "authors": "Jingxuan Li, Yuning Yang, Shengqi Yang, Linfan Zhang, Ying Nian Wu",
                "citations": 0
            },
            {
                "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
                "abstract": "The scaling of large language models to encode all the world's knowledge in model parameters is unsustainable and has exacerbated resource barriers. Retrieval-Augmented Generation (RAG) presents a potential solution, yet its application to vision-language models (VLMs) is under explored. Existing methods focus on models designed for single tasks. Furthermore, they're limited by the need for resource intensive pre training, additional parameter requirements, unaddressed modality prioritization and lack of clear benefit over non-retrieval baselines. This paper introduces RAVEN, a multitask retrieval augmented VLM framework that enhances base VLMs through efficient, task specific fine-tuning. By integrating retrieval augmented samples without the need for additional retrieval-specific parameters, we show that the model acquires retrieval properties that are effective across multiple tasks. Our results and extensive ablations across retrieved modalities for the image captioning and VQA tasks indicate significant performance improvements compared to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a +3\\% accuracy on specific VQA question types. This underscores the efficacy of applying RAG approaches to VLMs, marking a stride toward more efficient and accessible multimodal learning.",
                "authors": "Varun Nagaraj Rao, Siddharth Choudhary, Aditya Deshpande, R. Satzoda, Srikar Appalaraju",
                "citations": 0
            },
            {
                "title": "Multi-Method Modeling and Simulation of a Vertical Lift Module With an Integrated Buffer System Using Anylogic",
                "abstract": "As industry trend continues to accelerate sellers to begin transitioning the sale of their products exclusively through e-commerce platforms, companies must remain vigilant and recognize the requirement for their products to be safely stored and quickly retrieved. This research presents a comprehensive model and simulation study of a Vertical Lift Module (VLM) with an integrated shuttle-based storage and retrieval system (SBS/RS) or buffer system. This work evaluates a proposed solution to the ever-increasing emergent storage and retrieval challenges faced by warehouses worldwide. The VLM system was modeled using AnyLogic software to evaluate system capacity, travel distance, velocity profiles, and other user-defined operational constraints. The VLM performance is modeled under various conditions and compared to the performance of a traditional stand-alone VLM in terms of throughput and cycle-time to identify potential VLM-Buffer system integration drawbacks or limitations.",
                "authors": "Noe Tavira, Abhimanyu Sharotry, Jesus A. Jimenez, Jakob Marolt, T. Lerher",
                "citations": 0
            },
            {
                "title": "Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding",
                "abstract": "Vision-Language Models (VLM) can support clinicians by analyzing medical images and engaging in natural language interactions to assist in diagnostic and treatment tasks. However, VLMs often exhibit\"hallucinogenic\"behavior, generating textual outputs not grounded in contextual multimodal information. This challenge is particularly pronounced in the medical domain, where we do not only require VLM outputs to be accurate in single interactions but also to be consistent with clinical reasoning and diagnostic pathways throughout multi-turn conversations. For this purpose, we propose a new alignment algorithm that uses symbolic representations of clinical reasoning to ground VLMs in medical knowledge. These representations are utilized to (i) generate GPT-4-guided visual instruction tuning data at scale, simulating clinician-VLM conversations with demonstrations of clinical reasoning, and (ii) create an automatic reward function that evaluates the clinical validity of VLM generations throughout clinician-VLM interactions. Our algorithm eliminates the need for human involvement in training data generation or reward model construction, reducing costs compared to standard reinforcement learning with human feedback (RLHF). We apply our alignment algorithm to develop Dr-LLaVA, a conversational VLM finetuned for analyzing bone marrow pathology slides, demonstrating strong performance in multi-turn medical conversations.",
                "authors": "Shenghuan Sun, Gregory M. Goldgof, Alexander Schubert, Zhiqing Sun, Thomas Hartvigsen, A. Butte, Ahmed Alaa",
                "citations": 0
            },
            {
                "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension",
                "abstract": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in various open-vocabulary tasks, yet their zero-shot performance lags behind task-specific finetuned models, particularly in complex tasks like Referring Expression Comprehension (REC). Fine-tuning usually requires 'white-box' access to the model's architecture and weights, which is not always feasible due to proprietary or privacy concerns. In this work, we propose LLM-wrapper, a method for 'black-box' adaptation of VLMs for the REC task using Large Language Models (LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved with a light fine-tuning, to select the most relevant bounding box matching the referring expression, from candidates generated by a zero-shot black-box VLM. Our approach offers several advantages: it enables the adaptation of closed-source models without needing access to their internal workings, it is versatile as it works with any VLM, it transfers to new VLMs, and it allows for the adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on multiple datasets using different VLMs and LLMs, demonstrating significant performance improvements and highlighting the versatility of our method. While LLM-wrapper is not meant to directly compete with standard white-box fine-tuning, it offers a practical and effective alternative for black-box VLM adaptation. The code will be open-sourced.",
                "authors": "Amaia Cardiel, Éloi Zablocki, Elias Ramzi, Oriane Sim'eoni, Matthieu Cord",
                "citations": 0
            },
            {
                "title": "AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding",
                "abstract": "AI personal assistants deployed via robots or wearables require embodied understanding to collaborate with humans effectively. However, current Vision-Language Models (VLMs) primarily focus on third-person view videos, neglecting the richness of egocentric perceptual experience. To address this gap, we propose three key contributions. First, we introduce the Egocentric Video Understanding Dataset (EVUD) for training VLMs on video captioning and question answering tasks specific to egocentric videos. Second, we present AlanaVLM, a 7B parameter VLM trained using parameter-efficient methods on EVUD. Finally, we evaluate AlanaVLM's capabilities on OpenEQA, a challenging benchmark for embodied video question answering. Our model achieves state-of-the-art performance, outperforming open-source models including strong Socratic models using GPT-4 as a planner by 3.6%. Additionally, we outperform Claude 3 and Gemini Pro Vision 1.0 and showcase competitive results compared to Gemini Pro 1.5 and GPT-4V, even surpassing the latter in spatial reasoning. This research paves the way for building efficient VLMs that can be deployed in robots or wearables, leveraging embodied video understanding to collaborate seamlessly with humans in everyday tasks, contributing to the next generation of Embodied AI.",
                "authors": "Alessandro Suglia, Claudio Greco, Katie Baker, Jose L. Part, Ioannis Papaionnou, Arash Eshghi, Ioannis Konstas, Oliver Lemon",
                "citations": 0
            },
            {
                "title": "The potential promise and pitfalls of point-of-care viral load monitoring to expedite HIV treatment decision-making in rural Uganda: a qualitative study",
                "abstract": null,
                "authors": "Joseph G Rosen, W. Ddaaki, N. Nakyanjo, L. Chang, Anh Van Vo, Tongying Zhao, G. Nakigozi, Fred Nalugoda, G. Kigozi, J. Kagaayi, Thomas C. Quinn, M. K. Grabowski, S. Reynolds, Caitlin E Kennedy, R. Galiwango",
                "citations": 0
            },
            {
                "title": "Understanding Graphical Perception in Data Visualization through Zero-shot Prompting of Vision-Language Models",
                "abstract": "Vision Language Models (VLMs) have been successful at many chart comprehension tasks that require attending to both the images of charts and their accompanying textual descriptions. However, it is not well established how VLM performance profiles map to human-like behaviors. If VLMs can be shown to have human-like chart comprehension abilities, they can then be applied to a broader range of tasks, such as designing and evaluating visualizations for human readers. This paper lays the foundations for such applications by evaluating the accuracy of zero-shot prompting of VLMs on graphical perception tasks with established human performance profiles. Our findings reveal that VLMs perform similarly to humans under specific task and style combinations, suggesting that they have the potential to be used for modeling human performance. Additionally, variations to the input stimuli show that VLM accuracy is sensitive to stylistic changes such as fill color and chart contiguity, even when the underlying data and data mappings are the same.",
                "authors": "Grace Guo, Jenna Jiayi Kang, Raj Sanjay Shah, Hanspeter Pfister, Sashank Varma",
                "citations": 0
            },
            {
                "title": "How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?",
                "abstract": "Large foundation models have revolutionized the field, yet challenges remain in optimizing multi-modal models for specialized visual tasks. We propose a novel, generalizable methodology to identify preferred image distributions for black-box Vision-Language Models (VLMs) by measuring output consistency across varied input prompts. Applying this to different rendering types of 3D objects, we demonstrate its efficacy across various domains requiring precise interpretation of complex structures, with a focus on Computer-Aided Design (CAD) as an exemplar field. We further refine VLM outputs using in-context learning with human feedback, significantly enhancing explanation quality. To address the lack of benchmarks in specialized domains, we introduce CAD-VQA, a new dataset for evaluating VLMs on CAD-related visual question answering tasks. Our evaluation of state-of-the-art VLMs on CAD-VQA establishes baseline performance levels, providing a framework for advancing VLM capabilities in complex visual reasoning tasks across various fields requiring expert-level visual interpretation. We release the dataset and evaluation codes at \\url{https://github.com/asgsaeid/cad_vqa}.",
                "authors": "Saeid Asgari Taghanaki, Joseph Lambourne, Alana Mongkhounsavath",
                "citations": 0
            },
            {
                "title": "Molecular organization of autonomic, respiratory, and spinally-projecting neurons in the mouse ventrolateral medulla.",
                "abstract": "The ventrolateral medulla (VLM) is a crucial region in the brain for visceral and somatic control, serving as a significant source of synaptic input to the spinal cord. Experimental studies have shown that gene expression in individual VLM neurons is predictive of their function. However, the molecular and cellular organization of the VLM has remained uncertain. This study aimed to create a comprehensive dataset of VLM cells using single-cell RNA sequencing in male and female mice. The dataset was enriched with targeted sequencing of spinally-projecting and adrenergic/noradrenergic VLM neurons. Based on differentially expressed genes, the resulting dataset of 114,805 VLM cells identifies 23 subtypes of neurons, excluding those in the inferior olive, and 5 subtypes of astrocytes. Spinally-projecting neurons were found to be abundant in 7 subtypes of neurons, which were validated through in-situ hybridization. These subtypes included adrenergic/noradrenergic neurons, serotonergic neurons, and neurons expressing gene markers associated with pre-motor neurons in the ventromedial medulla. Further analysis of adrenergic/noradrenergic neurons and serotonergic neurons identified 9 and 6 subtypes, respectively, within each class of monoaminergic neurons. Marker genes that identify the neural network responsible for breathing were concentrated in 2 subtypes of neurons, delineated from each other by markers for excitatory and inhibitory neurons. These datasets are available for public download and for analysis with a user-friendly interface. Collectively, this study provides a fine-scale molecular identification of cells in the VLM, forming the foundation for a better understanding of the VLM's role in vital functions and motor control.Significance statement The ventrolateral medulla (VLM) is an anatomically complex region of the brain that plays a crucial role in regulating vital functions, including autonomic and respiratory control, sleep-wake behaviors, cranial motor functions, and locomotion. This study comprehensively classifies VLM cell types and neuronal subtypes based on their molecular and anatomical features, by leveraging single-nuclei RNA sequencing, RNA fluorescence in situ hybridization, and axonal tract tracing. We present a dataset comprising 114,805 single-nuclei transcriptomes that identifies and validates the precise molecular characteristics of neurons involved in autonomic and motor systems functions. This publicly-available dataset offers new opportunities for comprehensive experimental studies to dissect the central organization of vital homeostatic functions and body movement.",
                "authors": "Dana C Schwalbe, Daniel S. Stornetta, Ruei-Jen Abraham-Fan, George Souza, Maira Jalil, Maisie E. Crook, John N. Campbell, Stephen B. G. Abbott",
                "citations": 0
            },
            {
                "title": "RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models for Multi-label Chest X-ray Classification",
                "abstract": "In this paper we introduce RE-tune, a novel approach for fine-tuning pre-trained Multimodal Biomedical Vision-Language models (VLMs) in Incremental Learning scenarios for multi-label chest disease diagnosis. RE-tune freezes the backbones and only trains simple adaptors on top of the Image and Text encoders of the VLM. By engineering positive and negative text prompts for diseases, we leverage the ability of Large Language Models to steer the training trajectory. We evaluate RE-tune in three realistic incremental learning scenarios: class-incremental, label-incremental, and data-incremental. Our results demonstrate that Biomedical VLMs are natural continual learners and prevent catastrophic forgetting. RE-tune not only achieves accurate multi-label classification results, but also prioritizes patient privacy and it distinguishes itself through exceptional computational efficiency, rendering it highly suitable for broad adoption in real-world healthcare settings.",
                "authors": "Marco Mistretta, Andrew D. Bagdanov",
                "citations": 0
            },
            {
                "title": "Advancements in Visual Language Models for Remote Sensing: Datasets, Capabilities, and Enhancement Techniques",
                "abstract": "Recently, the remarkable success of ChatGPT has sparked a renewed wave of interest in artificial intelligence (AI), and the advancements in visual language models (VLMs) have pushed this enthusiasm to new heights. Differring from previous AI approaches that generally formulated different tasks as discriminative models, VLMs frame tasks as generative models and align language with visual information, enabling the handling of more challenging problems. The remote sensing (RS) field, a highly practical domain, has also embraced this new trend and introduced several VLM-based RS methods that have demonstrated promising performance and enormous potential. In this paper, we first review the fundamental theories related to VLM, then summarize the datasets constructed for VLMs in remote sensing and the various tasks they addressed. Finally, we categorize the improvement methods into three main parts according to the core components of VLMs and provide a detailed introduction and comparison of these methods. A project associated with this review has been created at https://github.com/taolijie11111/VLMs-in-RS-review.",
                "authors": "Lijie Tao, Haokui Zhang, Haizhao Jing, Yu Liu, Kelu Yao, Chao Li, Xizhe Xue",
                "citations": 0
            },
            {
                "title": "Harnessing LLMs for VQA: A Prompted Benchmark with Animate/Inanimate Keywords",
                "abstract": "In the field of NLP, Large Language Models (LLMs) have recently achieved significant advancements, leading to the development of various benchmarks for their evaluation. Along-side NLP, Vision Language Models (VLMs) have also VLM have also significantly progressed, similar to LLMs. However, benchmarks for VLMs are still relatively underdeveloped compared to those for NLP, and their construction is often costly. In this work, we propose an automatically generated benchmark for evaluating VLMs based on LLMs and conduct a visual question answering task to assess this benchmark. The benchmark includes multiple-choice questions that not only distinguish between animate and inanimate objects but also generate these distinctions automatically, along with entity and object information within images. We evaluate the performance of open VLM using the generated multiple-choice questions, demonstrating the model's capabilities and the significance of the automatically generated benchmark. Finally, we discuss the necessity and future directions for benchmark research in this area.",
                "authors": "Chanwoo Lee, Hyunjeong Lee, Minsang Kim, Hyun Kim, Haneol Jang, Cheoneum Park",
                "citations": 0
            },
            {
                "title": "Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model",
                "abstract": "To develop high-performing Visual Language Models (VLMs), it is essential to prepare multimodal resources, such as image-text pairs, interleaved data, and instruction data. While multimodal resources for English are abundant, there is a significant lack of corresponding resources for non-English languages, such as Japanese. To address this problem, we take Japanese as a non-English language and propose a method for rapidly creating Japanese multimodal datasets from scratch. We collect Japanese image-text pairs and interleaved data from web archives and generate Japanese instruction data directly from images using an existing VLM. Our experimental results show that a VLM trained on these native datasets outperforms those relying on machine-translated content.",
                "authors": "Keito Sasagawa, Koki Maeda, Issa Sugiura, Shuhei Kurita, Naoaki Okazaki, Daisuke Kawahara",
                "citations": 0
            },
            {
                "title": "OpenDAS: Open-Vocabulary Domain Adaptation for 2D and 3D Segmentation",
                "abstract": "Recently, Vision-Language Models (VLMs) have advanced segmentation techniques by shifting from the traditional segmentation of a closed-set of predefined object classes to open-vocabulary segmentation (OVS), allowing users to segment novel classes and concepts unseen during training of the segmentation model. However, this flexibility comes with a trade-off: fully-supervised closed-set methods still outperform OVS methods on base classes, that is on classes on which they have been explicitly trained. This is due to the lack of pixel-aligned training masks for VLMs (which are trained on image-caption pairs), and the absence of domain-specific knowledge, such as autonomous driving. Therefore, we propose the task of open-vocabulary domain adaptation to infuse domain-specific knowledge into VLMs while preserving their open-vocabulary nature. By doing so, we achieve improved performance in base and novel classes. Existing VLM adaptation methods improve performance on base (training) queries, but fail to fully preserve the open-set capabilities of VLMs on novel queries. To address this shortcoming, we combine parameter-efficient prompt tuning with a triplet-loss-based training strategy that uses auxiliary negative queries. Notably, our approach is the only parameter-efficient method that consistently surpasses the original VLM on novel classes. Our adapted VLMs can seamlessly be integrated into existing OVS pipelines, e.g., improving OVSeg by +6.0% mIoU on ADE20K for open-vocabulary 2D segmentation, and OpenMask3D by +4.1% AP on ScanNet++ Offices for open-vocabulary 3D instance segmentation without other changes. The project page is available at https://open-das.github.io/.",
                "authors": "Gonca Yilmaz, Songyou Peng, Marc Pollefeys, Francis Engelmann, Hermann Blum",
                "citations": 0
            },
            {
                "title": "Multi-level supervised vision language model based steel surface defect detection",
                "abstract": "The shape of defects on steel surfaces is highly variable and training samples are limited, making it a significant challenge to transfer a high-performance pretrained vision language model to steel surface defect detection. Therefore, a Multi-level Supervised Vision Language Model based Steel Surface Defect Detection method MLS-VLM is proposed in this paper. MLS-VLM delves deeply into the extraction of profound features from limited samples with three levels of training: supervised contrast training from labeled areas and the entire image, as well as self-supervised contrast learning from Region Proposals. MLS-VLM can be rapidly transferred to two-stage object detector. Experimental results demonstrate that, compared to traditional object detection methods, MLS-VLM achieves 5.68~8.37 mAP improvement on three benchmark object detectors.",
                "authors": "Ying Tan, Jing Shan, Jiaying Wang",
                "citations": 0
            },
            {
                "title": "CluMo: Cluster-based Modality Fusion Prompt for Continual Learning in Visual Question Answering",
                "abstract": "Large vision-language models (VLMs) have shown significant performance boost in various application domains. However, adopting them to deal with several sequentially encountered tasks has been challenging because finetuning a VLM on a task normally leads to reducing its generalization power and the capacity of learning new tasks as well as causing catastrophic forgetting on previously learned tasks. Enabling using VLMs in multimodal continual learning (CL) settings can help to address such scenarios. To improve generalization capacity and prevent catastrophic forgetting, we propose a novel prompt-based CL method for VLMs, namely $\\textbf{Clu}$ster-based $\\textbf{Mo}$dality Fusion Prompt (\\textbf{CluMo}). We design a novel \\textbf{Key-Key-Prompt} pair, where each prompt is associated with a visual prompt key and a textual prompt key. We adopt a two-stage training strategy. During the first stage, the single-modal keys are trained via $K$-means clustering algorithm to help select the best semantically matched prompt. During the second stage, the prompt keys are frozen, the selected prompt is attached to the input for training the VLM in the CL scenario. Experiments on two benchmarks demonstrate that our method achieves SOTA performance.",
                "authors": "Yuliang Cai, Mohammad Rostami",
                "citations": 0
            },
            {
                "title": "V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding",
                "abstract": "Vision-Language Models (VLMs) have shown promising capabilities in handling various multimodal tasks, yet they struggle in long-context scenarios, particularly in tasks involving videos, high-resolution images, or lengthy image-text documents. In our work, we first conduct an empirical analysis of the long-context capabilities of VLMs using our augmented long-context multimodal datasets. Our findings reveal that directly applying the positional encoding mechanism used for textual tokens to visual tokens is suboptimal, and VLM performance degrades sharply when the position encoding exceeds the model's context window. To address this, we propose Variable Visual Position Encoding (V2PE), a novel positional encoding approach that employs variable and smaller increments for visual tokens, enabling more efficient management of long multimodal sequences. Our experiments demonstrate the effectiveness of V2PE to enhances VLMs' ability to effectively understand and reason over long multimodal contexts. We further integrate V2PE with our augmented long-context multimodal datasets to fine-tune the open-source VLM, InternVL2. The fine-tuned model achieves strong performance on both standard and long-context multimodal tasks. Notably, when the sequence length of the training dataset is increased to 256K tokens, the model is capable of processing multimodal sequences up to 1M tokens, highlighting its potential for real-world long-context applications.",
                "authors": "Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, Xizhou Zhu",
                "citations": 0
            },
            {
                "title": "End-to-End Navigation with Vision Language Models: Transforming Spatial Reasoning into Question-Answering",
                "abstract": "We present VLMnav, an embodied framework to transform a Vision-Language Model (VLM) into an end-to-end navigation policy. In contrast to prior work, we do not rely on a separation between perception, planning, and control; instead, we use a VLM to directly select actions in one step. Surprisingly, we find that a VLM can be used as an end-to-end policy zero-shot, i.e., without any fine-tuning or exposure to navigation data. This makes our approach open-ended and generalizable to any downstream navigation task. We run an extensive study to evaluate the performance of our approach in comparison to baseline prompting methods. In addition, we perform a design analysis to understand the most impactful design decisions. Visual examples and code for our project can be found at https://jirl-upenn.github.io/VLMnav/",
                "authors": "Dylan Goetting, Himanshu Gaurav Singh, Antonio Loquercio",
                "citations": 0
            },
            {
                "title": "Sharingan: Extract User Action Sequence from Desktop Recordings",
                "abstract": "Video recordings of user activities, particularly desktop recordings, offer a rich source of data for understanding user behaviors and automating processes. However, despite advancements in Vision-Language Models (VLMs) and their increasing use in video analysis, extracting user actions from desktop recordings remains an underexplored area. This paper addresses this gap by proposing two novel VLM-based methods for user action extraction: the Direct Frame-Based Approach (DF), which inputs sampled frames directly into VLMs, and the Differential Frame-Based Approach (DiffF), which incorporates explicit frame differences detected via computer vision techniques. We evaluate these methods using a basic self-curated dataset and an advanced benchmark adapted from prior work. Our results show that the DF approach achieves an accuracy of 70% to 80% in identifying user actions, with the extracted action sequences being re-playable though Robotic Process Automation. We find that while VLMs show potential, incorporating explicit UI changes can degrade performance, making the DF approach more reliable. This work represents the first application of VLMs for extracting user action sequences from desktop recordings, contributing new methods, benchmarks, and insights for future research.",
                "authors": "Yanting Chen, Yi Ren, Xiaoting Qin, Jue Zhang, Kehong Yuan, Lu Han, Qingwei Lin, Dongmei Zhang, S. Rajmohan, Qi Zhang",
                "citations": 0
            },
            {
                "title": "Retention Score: Quantifying Jailbreak Risks for Vision Language Models",
                "abstract": "The emergence of Vision-Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to enhance multi-modal machine learning capabilities. However, this progress has also made VLMs vulnerable to sophisticated adversarial attacks, raising concerns about their reliability. The objective of this paper is to assess the resilience of VLMs against jailbreak attacks that can compromise model safety compliance and result in harmful outputs. To evaluate a VLM's ability to maintain its robustness against adversarial input perturbations, we propose a novel metric called the \\textbf{Retention Score}. Retention Score is a multi-modal evaluation metric that includes Retention-I and Retention-T scores for quantifying jailbreak risks in visual and textual components of VLMs. Our process involves generating synthetic image-text pairs using a conditional diffusion model. These pairs are then predicted for toxicity score by a VLM alongside a toxicity judgment classifier. By calculating the margin in toxicity scores, we can quantify the robustness of the VLM in an attack-agnostic manner. Our work has four main contributions. First, we prove that Retention Score can serve as a certified robustness metric. Second, we demonstrate that most VLMs with visual components are less robust against jailbreak attacks than the corresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find that the security settings in Google Gemini significantly affect the score and robustness. Moreover, the robustness of GPT4V is similar to the medium settings of Gemini. Finally, our approach offers a time-efficient alternative to existing adversarial attack methods and provides consistent model robustness rankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA.",
                "authors": "Zaitang Li, Pin-Yu Chen, Tsung-Yi Ho",
                "citations": 0
            },
            {
                "title": "The vastus medialis oblique compensates in current patellar dislocation patients with the increased femoral anteversion",
                "abstract": null,
                "authors": "C. Dong, Zhenhui Huo, Y. Niu, Hui-jun Kang, Fei Wang",
                "citations": 0
            },
            {
                "title": "Tuning Vision-Language Models with Candidate Labels by Prompt Alignment",
                "abstract": "Vision-language models (VLMs) can learn high-quality representations from a large-scale training dataset of image-text pairs. Prompt learning is a popular approach to fine-tuning VLM to adapt them to downstream tasks. Despite the satisfying performance, a major limitation of prompt learning is the demand for labelled data. In real-world scenarios, we may only obtain candidate labels (where the true label is included) instead of the true labels due to data privacy or sensitivity issues. In this paper, we provide the first study on prompt learning with candidate labels for VLMs. We empirically demonstrate that prompt learning is more advantageous than other fine-tuning methods, for handling candidate labels. Nonetheless, its performance drops when the label ambiguity increases. In order to improve its robustness, we propose a simple yet effective framework that better leverages the prior knowledge of VLMs to guide the learning process with candidate labels. Specifically, our framework disambiguates candidate labels by aligning the model output with the mixed class posterior jointly predicted by both the learnable and the handcrafted prompt. Besides, our framework can be equipped with various off-the-shelf training objectives for learning with candidate labels to further improve their performance. Extensive experiments demonstrate the effectiveness of our proposed framework.",
                "authors": "Zhifang Zhang, Beibei Li",
                "citations": 0
            },
            {
                "title": "Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks",
                "abstract": "Vision language models (VLMs) like CLIP show stellar zero-shot capability on classification benchmarks. However, selecting the VLM with the highest performance on the unlabeled downstream task is non-trivial. Existing VLM selection methods focus on the class-name-only setting, relying on a supervised large-scale dataset and large language models, which may not be accessible or feasible during deployment. This paper introduces the problem of \\textbf{unsupervised vision-language model selection}, where only unsupervised downstream datasets are available, with no additional information provided. To solve this problem, we propose a method termed Visual-tExtual Graph Alignment (VEGA), to select VLMs without any annotations by measuring the alignment of the VLM between the two modalities on the downstream task. VEGA is motivated by the pretraining paradigm of VLMs, which aligns features with the same semantics from the visual and textual modalities, thereby mapping both modalities into a shared representation space. Specifically, we first construct two graphs on the vision and textual features, respectively. VEGA is then defined as the overall similarity between the visual and textual graphs at both node and edge levels. Extensive experiments across three different benchmarks, covering a variety of application scenarios and downstream datasets, demonstrate that VEGA consistently provides reliable and accurate estimates of VLMs' performance on unlabeled downstream tasks.",
                "authors": "Yuhe Ding, Bo Jiang, Aihua Zheng, Qin Xu, Jian Liang",
                "citations": 0
            },
            {
                "title": "Dual-Branch Knowledge Enhancement Network with Vision-Language Model for Human-Object Interaction Detection",
                "abstract": "Human-Object Interaction (HOI) detection aims to localize human-object pairs and comprehend their interactions. Recently, pre-trained Vision-Language Models (VLM) have shown their great recognition ability in HOI detection task. However, these VLM based methods are struggle to transfer knowledge to achieve desired performance. To this end, we propose a Dual-Branch Knowledge Enhancement Network with VLM (DBKEN-VLM) within the two-stage paradigm to enhance the effectiveness of VLM. Specifically, we propose a semantic mining decoder to supplement contextual and action-related semantic information into our model. It forms a dual-branch knowledge enhancement network with spatial guided decoder. Furthermore, we propose a two-level fusion strategy for the dualbranch network to facilitate better knowledge transfer of VLM. One is feature-level fusion, producing more instructive interaction features; another is decision-level fusion, further enhancing the capability of VLM for HOI detection. The proposed method achieves competitive performance compared to recent methods on two benchmark datasets, HICO-DET and V-COCO.",
                "authors": "Guangpu Zhou, Dehui Kong, Jinghua Li, Dongpan Chen, Zhuowei Bai, Baocai Yin",
                "citations": 0
            },
            {
                "title": "Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models",
                "abstract": "Vision language models (VLMs) demonstrate impressive capabilities in visual question answering and image captioning, acting as a crucial link between visual and language models. However, existing open-source VLMs heavily rely on pretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness across diverse domains, it still exhibits non-negligible image understanding errors. These errors propagate to the VLM responses, resulting in sub-optimal performance. In our work, we propose an efficient and robust method for updating vision encoders within VLMs. Our approach selectively and locally updates encoders, leading to substantial performance improvements on data where previous mistakes occurred, while maintaining overall robustness. Furthermore, we demonstrate the effectiveness of our method during continual few-shot updates. Theoretical grounding, generality, and computational efficiency characterize our approach.",
                "authors": "A. Panos, Rahaf Aljundi, Daniel Olmeda Reino, Richard E Turner",
                "citations": 0
            },
            {
                "title": "Practical Techniques for Vision-Language Segmentation Model in Remote Sensing",
                "abstract": "Abstract. Traditional semantic segmentation models often struggle with poor generalizability in zero-shot scenarios such as recognizing attributes unseen in the training labels. On the other hands, language-vision models (VLMs) have shown promise in improving performance on zero-shot tasks by leveraging semantic information from textual inputs and fusing this information with visual features. However, existing VLM-based methods do not perform as effectively on remote sensing data due to the lack of such data in their training datasets. In this paper, we introduce a two-stage fine-tuning approach for a VLM-based segmentation model using a large remote sensing image-caption dataset, which we created using an existing image-caption model. Additionally, we propose a modified decoder and a visual prompt technique using a saliency map to enhance segmentation results. Through these methods, we achieve superior segmentation performance on remote sensing data, demonstrating the effectiveness of our approach.\n",
                "authors": "Yuting Lin, Kumiko Suzuki, Shinichiro Sogo",
                "citations": 0
            },
            {
                "title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
                "abstract": "Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.",
                "authors": "Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, S. Mondal, Aman Chadha",
                "citations": 142
            },
            {
                "title": "ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models",
                "abstract": "Large vision-language models (LVLMs) have shown premise in a broad range of vision-language tasks with their strong reasoning and generalization capabilities. However, they require considerable computational resources for training and deployment. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To this end, we propose a comprehensive pipeline for generating a synthetic dataset. The key idea is to leverage strong proprietary models to generate (i) fine-grained image annotations for vision-language alignment and (ii) complex reasoning visual question-answering pairs for visual instruction fine-tuning, yielding 1.3M samples in total. We train a series of lite VLMs on the synthetic dataset and experimental results demonstrate the effectiveness of the proposed scheme, where they achieve competitive performance on 17 benchmarks among 4B LVLMs, and even perform on par with 7B/13B-scale models on various benchmarks. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. We name our dataset \\textit{ALLaVA}, and open-source it to research community for developing better resource-efficient LVLMs for wider usage.",
                "authors": "Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, Benyou Wang",
                "citations": 94
            },
            {
                "title": "PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning",
                "abstract": "Vision-language pre-training has significantly elevated performance across a wide range of image-language applications. Yet, the pre-training process for video-related tasks demands exceptionally large computational and data resources, which hinders the progress of video-language models. This paper investigates a straight-forward, highly efficient, and resource-light approach to adapting an existing image-language pre-trained model for dense video understanding. Our preliminary experiments reveal that directly fine-tuning pre-trained image-language models with multiple frames as inputs on video datasets leads to performance saturation or even a drop. Our further investigation reveals that it is largely attributed to the bias of learned high-norm visual features. Motivated by this finding, we propose a simple but effective pooling strategy to smooth the feature distribution along the temporal dimension and thus reduce the dominant impacts from the extreme features. The new model is termed Pooling LLaVA, or PLLaVA in short. PLLaVA achieves new state-of-the-art performance on modern benchmark datasets for both video question-answer and captioning tasks. Notably, on the recent popular VideoChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average of five evaluated dimensions, exceeding the previous SOTA results from GPT4V (IG-VLM) by 9%. On the latest multi-choice benchmark MVBench, PLLaVA achieves 58.1% accuracy on average across 20 sub-tasks, 14.5% higher than GPT4V (IG-VLM). Code is available at https://pllava.github.io/",
                "authors": "Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, Jiashi Feng",
                "citations": 84
            },
            {
                "title": "Evolutionary Optimization of Model Merging Recipes",
                "abstract": "We present a novel application of evolutionary algorithms to automate the creation of powerful foundation models. While model merging has emerged as a promising approach for LLM development due to its cost-effectiveness, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.",
                "authors": "Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha",
                "citations": 61
            },
            {
                "title": "Evolutionary Optimization of Model Merging Recipes",
                "abstract": "We present a novel application of evolutionary algorithms to automate the creation of powerful foundation models. While model merging has emerged as a promising approach for LLM development due to its cost-effectiveness, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.",
                "authors": "Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha",
                "citations": 61
            },
            {
                "title": "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
                "abstract": "Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at https://os-world.github.io.",
                "authors": "Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu",
                "citations": 54
            },
            {
                "title": "OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics",
                "abstract": "Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot's performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. Videos of our experiments and code are available on our website: https://ok-robot.github.io",
                "authors": "Peiqi Liu, Yaswanth Orru, Chris Paxton, N. Shafiullah, Lerrel Pinto",
                "citations": 42
            },
            {
                "title": "CogVLM2: Visual Language Models for Image and Video Understanding",
                "abstract": "Beginning with VisualGLM and CogVLM, we are continuously exploring VLMs in pursuit of enhanced vision-language fusion, efficient higher-resolution architecture, and broader modalities and applications. Here we propose the CogVLM2 family, a new generation of visual language models for image and video understanding including CogVLM2, CogVLM2-Video and GLM-4V. As an image understanding model, CogVLM2 inherits the visual expert architecture with improved training recipes in both pre-training and post-training stages, supporting input resolution up to $1344 \\times 1344$ pixels. As a video understanding model, CogVLM2-Video integrates multi-frame input with timestamps and proposes automated temporal grounding data construction. Notably, CogVLM2 family has achieved state-of-the-art results on benchmarks like MMBench, MM-Vet, TextVQA, MVBench and VCGBench. All models are open-sourced in https://github.com/THUDM/CogVLM2 and https://github.com/THUDM/GLM-4, contributing to the advancement of the field.",
                "authors": "Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, Lei Zhao, Zhuoyi Yang, Xiaotao Gu, Xiaohan Zhang, Guanyu Feng, Da Yin, Zihan Wang, Ji Qi, Xixuan Song, Peng Zhang, De-Feng Liu, Bin Xu, Juanzi Li, Yu-Chen Dong, Jie Tang",
                "citations": 37
            },
            {
                "title": "Yell At Your Robot: Improving On-the-Fly from Language Corrections",
                "abstract": "Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations. However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail. Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback? In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. We show that even fine-grained corrections, such as small movements (\"move a bit to the left\"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions. This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback. Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation. Videos and code are available at https://yay-robot.github.io/.",
                "authors": "Lucy Xiaoyang Shi, Zheyuan Hu, Tony Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn",
                "citations": 35
            },
            {
                "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
                "abstract": "Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such\"in-the-wild\"data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.",
                "authors": "Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, K. Gopalakrishnan, Karol Hausman, Brian Ichter, A. Irpan, Nikhil J. Joshi, Ryan C. Julian, Sean Kirmani, Isabel Leal, T. Lee, Sergey Levine, Yao Lu, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag R. Sanketi, P. Sermanet, Q. Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu",
                "citations": 33
            },
            {
                "title": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
                "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87 times and 8.56 times compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications. Our code is available at https://github.com/KuofengGao/Verbose_Images.",
                "authors": "Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip Torr, Zhifeng Li, Wei Liu",
                "citations": 26
            },
            {
                "title": "CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models",
                "abstract": "Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object’s grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: copa-2024.github.io",
                "authors": "Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, Yang Gao",
                "citations": 26
            },
            {
                "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
                "abstract": "Today’s most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like",
                "authors": "Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Christopher Callison-Burch, Andrew Head, Rose Hendrix, F. Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Christopher Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jennifer Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Marie Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hanna Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi",
                "citations": 27
            },
            {
                "title": "Real-World Robot Applications of Foundation Models: A Review",
                "abstract": "Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.",
                "authors": "Kento Kawaharazuka, T. Matsushima, Andrew Gambardella, Jiaxian Guo, Chris Paxton, Andy Zeng",
                "citations": 29
            },
            {
                "title": "RegionGPT: Towards Region Understanding Vision Language Model",
                "abstract": "Vision language models (VLMs) have experienced rapid advancements through the integration of large language models (LLMs) with image-text pairs, yet they struggle with detailed regional visual understanding due to limited spatial awareness of the vision encoder, and the use of coarse-grained training data that lacks detailed, region-specific captions. To address this, we introduce RegionGPT (short as RGPT), a novel framework designed for complex region-level captioning and understanding. RGPT enhances the spatial awareness of regional representation with simple yet effective modifications to existing visual encoders in VLMs. We further improve performance on tasks requiring a specific output scope by integrating task-guided instruction prompts during both training and inference phases, while maintaining the model's versatility for general-purpose tasks. Additionally, we develop an automated region caption data generation pipeline, enriching the training set with detailed region-level captions. We demonstrate that a universal RGPT model can be effectively applied and significantly enhancing performance across a range of region-level tasks, including but not limited to complex region descriptions, reasoning, object classification, and referring expressions comprehension. Code will be released at the project page.",
                "authors": "Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, Sifei Liu",
                "citations": 25
            },
            {
                "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
                "abstract": "Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long video understanding by incorporating two additional stages, i.e., long context extension and long video supervised fine-tuning. However, training on long video is computationally and memory intensive. We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing. LongVILA efficiently extends the number of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in 6,000-frame (more than 1 million tokens) video needle-in-a-haystack. LongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g. 65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and tensor parallelism. Moreover, it seamlessly integrates with Hugging Face Transformers.",
                "authors": "Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han",
                "citations": 22
            },
            {
                "title": "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model",
                "abstract": "Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs' spatial perception and reasoning capabilities. SpatialRGPT advances VLMs' spatial understanding through two key innovations: (1) a data curation pipeline that enables effective learning of regional representation from 3D scene graphs, and (2) a flexible plugin module for integrating depth information into the visual encoder of existing VLMs. During inference, when provided with user-specified region proposals, SpatialRGPT can accurately perceive their relative directions and distances. Additionally, we propose SpatialRGBT-Bench, a benchmark with ground-truth 3D annotations encompassing indoor, outdoor, and simulated environments, for evaluating 3D spatial cognition in VLMs. Our results demonstrate that SpatialRGPT significantly enhances performance in spatial reasoning tasks, both with and without local region prompts. The model also exhibits strong generalization capabilities, effectively reasoning about complex spatial relations and functioning as a region-aware dense reward annotator for robotic tasks. Code, dataset, and benchmark are released at https://www.anjiecheng.me/SpatialRGPT",
                "authors": "An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, Sifei Liu",
                "citations": 22
            },
            {
                "title": "VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation",
                "abstract": "VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework.",
                "authors": "Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, Yao Lu",
                "citations": 20
            },
            {
                "title": "Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training",
                "abstract": "Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a\"visual prompt\", where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts. CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model's prior). CRG achieves substantial improvements in a wide variety of VL tasks: When region annotations are provided, CRG increases absolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse region-based tasks such as recognition, math, and object relationship reasoning. We also show CRG's applicability to spatial reasoning, with 10% improvement on What'sUp, as well as to compositional generalization -- improving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe -- and to image-text alignment for generated images, where we improve by up to 8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG allows us to re-rank proposed regions in referring expression comprehension and phrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an average gain of 3.2% in accuracy. Our analysis explores alternative masking strategies for CRG, quantifies CRG's probability shift, and evaluates the role of region guidance strength, empirically validating CRG's design choices.",
                "authors": "David Wan, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal",
                "citations": 20
            },
            {
                "title": "The Neglected Tails in Vision-Language Models",
                "abstract": "Vision-language models (VLMs) excel in zero-shot recognition but their performance varies greatly across different visual concepts. For example, although CLIP achieves impressive accuracy on ImageNet (60-80%), its performance drops below 10% for more than ten concepts like night snake, presumably due to their limited presence in the pretraining data. However, measuring the frequency of concepts in VLMs' large-scale datasets is challenging. We address this by using large language models (LLMs) to count the number of pretraining texts that con-tain synonyms of these concepts. Our analysis confirms that popular datasets, such as LAION, exhibit a long-tailed concept distribution, yielding biased performance in VLMs. We also find that downstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and text-to-image models (e.g., Stable Diffusion), often fail to recognize or generate images of rare concepts identified by our method. To mit-igate the imbalanced performance of zero-shot VLMs, we propose REtrieval-Augmented Learning (REAL). First, in-stead of prompting VLMs using the original class names, REAL uses their most frequent synonyms found in pretraining texts. This simple change already outperforms costly human-engineered and LLM-enriched prompts over nine benchmark datasets. Second, REAL trains a linear classifier on a small yet balanced set of pretraining data re-trieved using concept synonyms. REAL surpasses the previous zero-shot SOTA, using 400× less storage and 10,000× less training time!",
                "authors": "Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, Shu Kong",
                "citations": 20
            },
            {
                "title": "Toward Generalist Anomaly Detection via In-Context Residual Learning with Few-Shot Sample Prompts",
                "abstract": "This paper explores the problem of Generalist Anomaly Detection (GAD), aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data. Some recent studies have showed that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets, but their methods rely heavily on handcrafted text prompts about defects, making them difficult to generalize to anomalies in other applications, e.g., medical image anomalies or semantic anomalies in natural images. In this work, we propose to train a GAD model with few-shot normal images as sample prompts for AD on diverse datasets on the fly. To this end, we introduce a novel approach that learns an in-context residual learning model for GAD, termed InCTRL. It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and few-shot normal sample prompts. Regardless of the datasets, per definition of anomaly, larger residuals are expected for anomalies than normal samples, thereby enabling InCTRL to generalize across different domains without further training. Comprehensive experiments on nine AD datasets are performed to establish a GAD benchmark that encapsulate the detection of industrial defect anomalies, medical anomalies, and semantic anomalies in both one-vs-all and multi-class setting, on which InCTRL is the best performer and significantly outperforms state-of-the-art competing methods. Code is available at https://github.com/mala-lab/InCTRL.",
                "authors": "Jiawen Zhu, Guansong Pang",
                "citations": 20
            },
            {
                "title": "Scaling Laws for Data Filtering—Data Curation Cannot be Compute Agnostic",
                "abstract": "Vision-language models (VLMs) are trained for thousands of GPU hours on carefully selected subsets of massive web scrapes. For instance, the LAION public dataset retained only about 10% of the total crawled data. In recent times, data curation has gained prominence with several works developing strategies to retain ‘high-quality’ subsets of ‘raw’ scraped data. However, these strategies are typically developed agnostic to the available compute for training. In this paper, we demonstrate that making filtering decisions independent of training compute is often suboptimal—well-curated data rapidly loses its utility when repeated, eventually decreasing below the utility of ‘unseen’ but ‘lower-quality’ data. While past research in neural scaling laws has considered web data to be homogenous, real data is not. Our work bridges this important gap in the literature by developing scaling laws that characterize the differing ‘utility’ of various data subsets, and accounting for how this diminishes for a data point at its ‘nth’ repetition. Our key message is that data curation can not be agnostic of the total compute a model will be trained for. Even without ever jointly training on multiple data buckets, our scaling laws enable us to estimate model performance under this dynamic trade-off between quality and repetition. This allows us to curate the best possible pool for achieving top performance on Datacomp at various compute budgets, carving out a pareto-frontier for data curation.",
                "authors": "Sachin Goyal, Pratyush Maini, Zachary Chase Lipton, Aditi Raghunathan, J. Kolter",
                "citations": 22
            },
            {
                "title": "VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models",
                "abstract": "Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.",
                "authors": "Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao",
                "citations": 22
            },
            {
                "title": "Enhancing Video-Language Representations With Structural Spatio-Temporal Alignment",
                "abstract": "While pre-training large-scale video-language models (VLMs) has shown remarkable potential for various downstream video-language tasks, existing VLMs can still suffer from certain commonly seen limitations, e.g., coarse-grained cross-modal aligning, under-modeling of temporal dynamics, detached video-language view. In this work, we target enhancing VLMs with a fine-grained structural spatio-temporal alignment learning method (namely Finsta). First of all, we represent the input texts and videos with fine-grained scene graph (SG) structures, both of which are further unified into a holistic SG (HSG) for bridging two modalities. Then, an SG-based framework is built, where the textual SG (TSG) is encoded with a graph Transformer, while the video dynamic SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for spatial and temporal feature propagation. A spatial-temporal Gaussian differential graph Transformer is further devised to strengthen the sense of the changes in objects across spatial and temporal dimensions. Next, based on the fine-grained structural features of TSG and DSG, we perform object-centered spatial alignment and predicate-centered temporal alignment respectively, enhancing the video-language grounding in both the spatiality and temporality. We design our method as a plug&play system, which can be integrated into existing well-trained VLMs for further representation augmentation, without training from scratch or relying on SG annotations in downstream applications. On 6 representative VL modeling tasks over 12 datasets in both standard and long-form video scenarios, Finsta consistently improves the existing 13 strong-performing VLMs persistently, and refreshes the current state-of-the-art end task performance significantly in both the fine-tuning and zero-shot settings.",
                "authors": "Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan",
                "citations": 20
            },
            {
                "title": "Red Teaming Visual Language Models",
                "abstract": "VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.",
                "authors": "Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu",
                "citations": 20
            },
            {
                "title": "Embodied Understanding of Driving Scenarios",
                "abstract": "Embodied scene understanding serves as the cornerstone for autonomous agents to perceive, interpret, and respond to open driving scenarios. Such understanding is typically founded upon Vision-Language Models (VLMs). Nevertheless, existing VLMs are restricted to the 2D domain, devoid of spatial awareness and long-horizon extrapolation proficiencies. We revisit the key aspects of autonomous driving and formulate appropriate rubrics. Hereby, we introduce the Embodied Language Model (ELM), a comprehensive framework tailored for agents' understanding of driving scenes with large spatial and temporal spans. ELM incorporates space-aware pre-training to endow the agent with robust spatial localization capabilities. Besides, the model employs time-aware token selection to accurately inquire about temporal cues. We instantiate ELM on the reformulated multi-faced benchmark, and it surpasses previous state-of-the-art approaches in all aspects. All code, data, and models will be publicly shared.",
                "authors": "Yunsong Zhou, Linyan Huang, Qingwen Bu, Jia Zeng, Tianyu Li, Hang Qiu, Hongzi Zhu, Minyi Guo, Yu Qiao, Hongyang Li",
                "citations": 19
            },
            {
                "title": "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models",
                "abstract": "Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on prompts to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different prompts are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across prompts. Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks. Our source code is available at \\url{https://github.com/Haochen-Luo/CroPA}.",
                "authors": "Haochen Luo, Jindong Gu, Fengyuan Liu, Philip Torr",
                "citations": 16
            },
            {
                "title": "MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?",
                "abstract": "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly proliferating, they often encounter challenges such as hallucination, bias, and the production of unsafe, low-quality output. To effectively address these issues, it is crucial to align these models with desired behaviors based on feedback from a multimodal judge. Despite their significance, current multimodal judges frequently undergo inadequate evaluation of their capabilities and limitations, potentially leading to misalignment and unsafe fine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, we evaluate a large variety of multimodal judges including smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in average. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. Notably, human evaluations on end-to-end fine-tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench. All data, code, models are available at https://huggingface.co/MJ-Bench.",
                "authors": "Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, Canyu Chen, Qinghao Ye, Zhihong Zhu, Yuqing Zhang, Jiawei Zhou, Zhuokai Zhao, Rafael Rafailov, Chelsea Finn, Huaxiu Yao",
                "citations": 15
            },
            {
                "title": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning",
                "abstract": "Video summarization aims to create short, accurate, and cohesive summaries of longer videos. Despite the existence of various video summarization datasets, a notable limitation is their limited amount of source videos, which hampers the effective training of advanced large vision-language models (VLMs). Additionally, most existing datasets are created for video-to-video summarization, overlooking the contemporary need for multimodal video content summarization. Recent efforts have been made to expand from unimodal to multimodal video summarization, categorizing the task into three sub-tasks based on the summary's modality: video-to-video (V2V), video-to-text (V2T), and a combination of video and text summarization (V2VT). However, the textual summaries in previous multimodal datasets are inadequate. To address these issues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset featuring 30,000 diverse videos sourced from YouTube, with lengths ranging from 40 to 940 seconds and an average summarization ratio of 16.39%. Each video summary in Instruct-V2Xum is paired with a textual summary that references specific frame indexes, facilitating the generation of aligned video and textual summaries. In addition, we propose a new video summarization framework named V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the first framework that unifies different video summarization tasks into one large language model's (LLM) text decoder and achieves task-controllable video summarization with temporal prompts and task instructions. Experiments show that V2Xum-LLaMA outperforms strong baseline models on multiple video summarization tasks. Furthermore, we propose an enhanced evaluation metric for V2V and V2VT summarization tasks.",
                "authors": "Hang Hua, Yunlong Tang, Chenliang Xu, Jiebo Luo",
                "citations": 15
            },
            {
                "title": "Vision language models are blind",
                "abstract": "While large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini 1.5 Pro, are powering various image-text applications and scoring high on many vision-understanding benchmarks, we find that they are surprisingly still struggling with low-level vision tasks that are easy to humans. Specifically, on BlindTest, our suite of 7 very simple tasks such as identifying (a) whether two circles overlap; (b) whether two lines intersect; (c) which letter is being circled in a word; and (d) counting circles in an Olympic-like logo, four state-of-the-art VLMs are only 58.57% accurate on average. Claude 3.5 Sonnet performs the best at 74.94% accuracy, but this is still far from the human expected accuracy of 100%. Across different image resolutions and line widths, VLMs consistently struggle with tasks that require precise spatial information and recognizing geometric primitives that overlap or are close together. Code and data are available at: https://vlmsareblind.github.io",
                "authors": "Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, A. Nguyen",
                "citations": 15
            },
            {
                "title": "JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models",
                "abstract": "The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignment. This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms. Our study categorizes jailbreaks into seven distinct types and elaborates on defense strategies that address these vulnerabilities. Through this comprehensive examination, we identify research gaps and propose directions for future studies to enhance the security frameworks of LLMs and VLMs. Our findings underscore the necessity for a unified perspective that integrates both jailbreak strategies and defensive solutions to foster a robust, secure, and reliable environment for the next generation of language models. More details can be found on our website: \\url{https://chonghan-chen.com/llm-jailbreak-zoo-survey/}.",
                "authors": "Haibo Jin, Leyang Hu, Xinuo Li, Peiyan Zhang, Chonghan Chen, Jun Zhuang, Haohan Wang",
                "citations": 15
            },
            {
                "title": "GUICourse: From General Vision Language Models to Versatile GUI Agents",
                "abstract": "Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse, a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at https://github.com/yiye3/GUICourse.",
                "authors": "Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Gui-Fang Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, Maosong Sun",
                "citations": 14
            },
            {
                "title": "ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference",
                "abstract": "Despite the success of large-scale pretrained Vision-Language Models (VLMs) especially CLIP in various open-vocabulary tasks, their application to semantic segmentation remains challenging, producing noisy segmentation maps with mis-segmented regions. In this paper, we carefully re-investigate the architecture of CLIP, and identify residual connections as the primary source of noise that degrades segmentation quality. With a comparative analysis of statistical properties in the residual connection and the attention output across different pretrained models, we discover that CLIP's image-text contrastive training paradigm emphasizes global features at the expense of local discriminability, leading to noisy segmentation results. In response, we propose ClearCLIP, a novel approach that decomposes CLIP's representations to enhance open-vocabulary semantic segmentation. We introduce three simple modifications to the final layer: removing the residual connection, implementing the self-self attention, and discarding the feed-forward network. ClearCLIP consistently generates clearer and more accurate segmentation maps and outperforms existing approaches across multiple benchmarks, affirming the significance of our discoveries.",
                "authors": "Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, Wayne Zhang",
                "citations": 12
            },
            {
                "title": "CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-Spoofing",
                "abstract": "Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the model's performance on unseen domains. Existing methods either rely on domain labels to align domain-invariant feature spaces, or disentangle generalizable features from the whole sample, which inevitably lead to the distortion of semantic feature structures and achieve limited generalization. In this work, we make use of large-scale VLMs like CLIP and leverage the textual feature to dynamically adjust the classifier's weights for exploring generalizable visual features. Specifically, we propose a novel Class Free Prompt Learning (CFPL) paradigm for DG FAS, which utilizes two lightweight transformers, namely Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different semantic prompts conditioned on content and style features by using a set of learnable query vectors, respectively. Thus, the generalizable prompt can be learned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is introduced to ensure CQF learns visual representation that is most informative of the content description. (2) A Diversified Style Prompt (DSP) technology is proposed to diversify the learning of style prompts by mixing feature statistics between instance-specific styles. Finally, the learned text features modulate visual features to generalization through the designed Prompt Modulation (PM). Extensive experiments show that the CFPL is effective and outperforms the state-of-the-art methods on several cross-domain datasets.",
                "authors": "Ajian Liu, Shuai Xue, Jianwen Gan, Jun Wan, Yanyan Liang, Jiankang Deng, Sergio Escalera, Zhen Lei",
                "citations": 14
            },
            {
                "title": "CoLLaVO: Crayon Large Language and Vision mOdel",
                "abstract": "The remarkable success of Large Language Models (LLMs) and instruction tuning drives the evolution of Vision Language Models (VLMs) towards a versatile general-purpose model. Yet, it remains unexplored whether current VLMs genuinely possess quality object-level image understanding capabilities determined from 'what objects are in the image?' or 'which object corresponds to a specified bounding box?'. Our findings reveal that the image understanding capabilities of current VLMs are strongly correlated with their zero-shot performance on vision language (VL) tasks. This suggests that prioritizing basic image understanding is crucial for VLMs to excel at VL tasks. To enhance object-level image understanding, we propose Crayon Large Language and Vision mOdel (CoLLaVO), which incorporates instruction tuning with Crayon Prompt as a new visual prompt tuning scheme based on panoptic color maps. Furthermore, we present a learning strategy of Dual QLoRA to preserve object-level image understanding without forgetting it during visual instruction tuning, thereby achieving a significant leap in numerous VL benchmarks in a zero-shot setting.",
                "authors": "Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yonghyun Ro",
                "citations": 14
            },
            {
                "title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
                "abstract": "Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning -- a fundamental component of human cognition -- remains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.",
                "authors": "Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Neel Joshi",
                "citations": 14
            },
            {
                "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
                "abstract": "There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.",
                "authors": "Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong",
                "citations": 14
            },
            {
                "title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models",
                "abstract": "Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness. Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments.",
                "authors": "Yuxuan Kuang, Hai Lin, Meng Jiang",
                "citations": 12
            },
            {
                "title": "On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities",
                "abstract": "In this paper, we highlight the critical issues of robustness 001 and safety associated with integrating large language models 002 (LLMs) and vision-language models (VLMs) into robotics 003 applications. Recent works have focused on using LLMs and 004 VLMs to improve the performance of robotics tasks, such 005 as manipulation, navigation, etc. However, such integration 006 can introduce significant vulnerabilities, in terms of their 007 susceptibility to adversarial attacks due to the language mod-008 els, potentially leading to catastrophic consequences. By 009 examining recent works at the interface of LLMs/VLMs and 010 robotics, we show that it is easy to manipulate or misguide the 011 robot’s actions, leading to safety hazards. We define and pro-012 vide examples of several plausible adversarial attacks, and 013 conduct experiments on three prominent robot frameworks 014 integrated with a language model, including KnowNo [40], 015 VIMA [21], and Instruct2Act [20], to assess their susceptibil-016 ity to these attacks. Our empirical findings reveal a striking 017 vulnerability of LLM/VLM-robot integrated systems: simple 018 adversarial attacks can significantly undermine the effective-019 ness of LLM/VLM-robot integrated systems. Specifically, our 020 data demonstrate an average performance deterioration of 021 21.2% under prompt attacks and a more alarming 30.2% un-022 der perception attacks. These results underscore the critical 023 need for robust countermeasures to ensure the safe and reli-024 able deployment of the advanced LLM/VLM-based robotic 025 systems. 026",
                "authors": "Xiyang Wu, Ruiqi Xian, Tianrui Guan, Jing Liang, Souradip Chakraborty, Fuxiao Liu, Brian M. Sadler, Dinesh Manocha, A. S. Bedi",
                "citations": 13
            },
            {
                "title": "Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases",
                "abstract": "We present a system for generating indoor scenes in response to text prompts. The prompts are not limited to a fixed vocabulary of scene descriptions, and the objects in generated scenes are not restricted to a fixed set of object categories -- we call this setting indoor scene generation. Unlike most prior work on indoor scene generation, our system does not require a large training dataset of existing 3D scenes. Instead, it leverages the world knowledge encoded in pre-trained large language models (LLMs) to synthesize programs in a domain-specific layout language that describe objects and spatial relations between them. Executing such a program produces a specification of a constraint satisfaction problem, which the system solves using a gradient-based optimization scheme to produce object positions and orientations. To produce object geometry, the system retrieves 3D meshes from a database. Unlike prior work which uses databases of category-annotated, mutually-aligned meshes, we develop a pipeline using vision-language models (VLMs) to retrieve meshes from massive databases of un-annotated, inconsistently-aligned meshes. Experimental evaluations show that our system outperforms generative models trained on 3D data for traditional, closed-universe scene generation tasks; it also outperforms a recent LLM-based layout generation method on open-universe scene generation.",
                "authors": "Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. K. Jones, Qiuhong Anna Wei, Kailiang Fu, Daniel Ritchie",
                "citations": 12
            },
            {
                "title": "VLM2Scene: Self-Supervised Image-Text-LiDAR Learning with Foundation Models for Autonomous Driving Scene Understanding",
                "abstract": "Vision and language foundation models (VLMs) have showcased impressive capabilities in 2D scene understanding. However, their latent potential in elevating the understanding of 3D autonomous driving scenes remains untapped. In this paper, we propose VLM2Scene, which exploits the potential of VLMs to enhance 3D self-supervised representation learning through our proposed image-text-LiDAR contrastive learning strategy. Specifically, in the realm of autonomous driving scenes, the inherent sparsity of LiDAR point clouds poses a notable challenge for point-level contrastive learning methods. This method often grapples with limitations tied to a restricted receptive field and the presence of noisy points. To tackle this challenge, our approach emphasizes region-level learning, leveraging regional masks without semantics derived from the vision foundation model. This approach capitalizes on valuable contextual information to enhance the learning of point cloud representations. First, we introduce Region Caption Prompts to generate fine-grained language descriptions for the corresponding regions, utilizing the language foundation model. These region prompts then facilitate the establishment of positive and negative text-point pairs within the contrastive loss framework. Second, we propose a Region Semantic Concordance Regularization, which involves a semantic-filtered region learning and a region semantic assignment strategy. The former aims to filter the false negative samples based on the semantic distance, and the latter mitigates potential inaccuracies in pixel semantics, thereby enhancing overall semantic consistency. Extensive experiments on representative autonomous driving datasets demonstrate that our self-supervised method significantly outperforms other counterparts. Codes are available at https://github.com/gbliao/VLM2Scene.",
                "authors": "Guibiao Liao, Jiankun Li, Xiaoqing Ye",
                "citations": 12
            },
            {
                "title": "Large Language Model for Table Processing: A Survey",
                "abstract": null,
                "authors": "Weizheng Lu, Jiaming Zhang, Jing Zhang, Yueguo Chen",
                "citations": 13
            },
            {
                "title": "Towards Multimodal In-Context Learning for Vision & Language Models",
                "abstract": "State-of-the-art Vision-Language Models (VLMs) ground the vision and the language modality primarily via projecting the vision tokens from the encoder to language-like tokens, which are directly fed to the Large Language Model (LLM) decoder. While these models have shown unprecedented performance in many downstream zero-shot tasks (eg image captioning, question answers, etc), still little emphasis has been put on transferring one of the core LLM capability of In-Context Learning (ICL). ICL is the ability of a model to reason about a downstream task with a few examples demonstrations embedded in the prompt. In this work, through extensive evaluations, we find that the state-of-the-art VLMs somewhat lack the ability to follow ICL instructions. In particular, we discover that even models that underwent large-scale mixed modality pre-training and were implicitly guided to make use of interleaved image and text information (intended to consume helpful context from multiple images) under-perform when prompted with few-shot demonstrations (in an ICL way), likely due to their lack of direct ICL instruction tuning. To enhance the ICL abilities of the present VLM, we propose a simple yet surprisingly effective multi-turn curriculum-based learning methodology with effective data mixes, leading up to a significant 21.03% (and 11.3% on average) ICL performance boost over the strongest VLM baselines and a variety of ICL benchmarks. Furthermore, we also contribute new benchmarks for ICL evaluation in VLMs and discuss their advantages over the prior art.",
                "authors": "Sivan Doveh, Shaked Perek, M. J. Mirza, Amit Alfassy, Assaf Arbelle, S. Ullman, Leonid Karlinsky",
                "citations": 12
            },
            {
                "title": "Empowering Unsupervised Domain Adaptation with Large-scale Pre-trained Vision-Language Models",
                "abstract": "Unsupervised Domain Adaptation (UDA) aims to leverage the labeled source domain to solve the tasks on the unlabeled target domain. Traditional UDA methods face the challenge of the tradeoff between domain alignment and semantic class discriminability, especially when a large domain gap exists between the source and target domains. The efforts of applying large-scale pre-training to bridge the domain gaps remain limited. In this work, we propose that Vision-Language Models (VLMs) can empower UDA tasks due to their training pattern with language alignment and their large-scale pre-trained datasets. For example, CLIP and GLIP have shown promising zero-shot generalization in classification and detection tasks. However, directly fine-tuning these VLMs into downstream tasks may be computationally expensive and not scalable if we have multiple domains that need to be adapted. Therefore, in this work, we first study an efficient adaption of VLMs to preserve the original knowledge while maximizing its flexibility for learning new knowledge. Then, we design a domain-aware pseudo-labeling scheme tailored to VLMs for domain disentanglement. We show the superiority of the proposed methods in four UDA-classification and two UDA-detection benchmarks, with a significant improvement (+9.9%) on DomainNet.",
                "authors": "Zhengfeng Lai, Haoping Bai, Haotian Zhang, Xianzhi Du, Jiulong Shan, Yinfei Yang, Chen-Nee Chuah, Meng Cao",
                "citations": 12
            },
            {
                "title": "TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection",
                "abstract": "Task-oriented object detection aims to find objects suitable for accomplishing specific tasks. As a challenging task, it requires simultaneous visual data processing and reasoning under ambiguous semantics. Recent solutions are mainly all-in-one models. However, the object detection backbones are pre-trained without text supervision. Thus, to incorporate task requirements, their intricate models undergo extensive learning on a highly imbalanced and scarce dataset, resulting in capped performance, laborious training, and poor generalizability. In contrast, we propose TaskCLIP, a more natural two-stage design composed of general object detection and task-guided object selection. Particularly for the latter, we resort to the recently successful large Vision-Language Models (VLMs) as our backbone, which provides rich semantic knowledge and a uniform embedding space for images and texts. Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of object images and their visual attributes, which are mainly adjective phrases. To this end, we design a transformer-based aligner after the pre-trained VLMs to re-calibrate both embeddings. Finally, we employ a trainable score function to post-process the VLM matching results for object selection. Experimental results demonstrate that our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by 3.5% and only requires a single NVIDIA RTX 4090 for both training and inference.",
                "authors": "Hanning Chen, Wenjun Huang, Yang Ni, Sanggeon Yun, Fei Wen, Hugo Latapie, Mohsen Imani",
                "citations": 13
            },
            {
                "title": "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models",
                "abstract": "Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains. Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models. In this paper, we introduce an unsupervised domain prompt distillation framework, which aims to transfer the knowledge of a larger teacher model to a lightweight target model through prompt-driven imitation using unlabeled domain images. Specifically, our framework consists of two distinct stages. In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels. After pretraining, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder. In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits. Further, we align the logits of both the teacher and student models via KL divergence, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable prompts. The proposed prompt distillation process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain. Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference. To our best knowledge, we are the first to (1) perform unsupervised domain-specific prompt-driven knowledge distillation for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student. Extensive experiments on 11 datasets demonstrate the effectiveness of our method. Code is publicly available at https://github.com/zhengli97/PromptKD.",
                "authors": "Zheng Li, Xiang Li, Xinyi Fu, Xing Zhang, Weiqiang Wang, Shuo Chen, Jian Yang",
                "citations": 13
            },
            {
                "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-Trained Vision-Language Models",
                "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness ($\\epsilon=4/255$) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
                "authors": "Lin Li, Haoyan Guan, Jianing Qiu, Michael W. Spratling",
                "citations": 10
            },
            {
                "title": "PromptFix: You Prompt and We Fix the Photo",
                "abstract": "Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks. The dataset and code are available at https://www.yongshengyu.com/PromptFix-Page.",
                "authors": "Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, Jiebo Luo",
                "citations": 10
            },
            {
                "title": "Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs",
                "abstract": "An elusive goal in navigation research is to build an intelligent agent that can understand multimodal instructions including natural language and image, and perform useful navigation. To achieve this, we study a widely useful category of navigation tasks we call Multimodal Instruction Navigation with demonstration Tours (MINT), in which the environment prior is provided through a previously recorded demonstration video. Recent advances in Vision Language Models (VLMs) have shown a promising path in achieving this goal as it demonstrates capabilities in perceiving and reasoning about multimodal inputs. However, VLMs are typically trained to predict textual output and it is an open research question about how to best utilize them in navigation. To solve MINT, we present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation policy that combines the environment understanding and common sense reasoning power of long-context VLMs and a robust low-level navigation policy based on topological graphs. The high-level policy consists of a long-context VLM that takes the demonstration tour video and the multimodal user instruction as input to find the goal frame in the tour video. Next, a low-level policy uses the goal frame and an offline constructed topological graph to generate robot actions at every timestep. We evaluated Mobility VLA in a 836m^2 real world environment and show that Mobility VLA has a high end-to-end success rates on previously unsolved multimodal instructions such as\"Where should I return this?\"while holding a plastic bin. A video demonstrating Mobility VLA can be found here: https://youtu.be/-Tof__Q8_5s",
                "authors": "Hao-Tien Lewis Chiang, Zhuo Xu, Zipeng Fu, M. Jacob, Tingnan Zhang, T. Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, Fei Xia, Jasmine Hsu, Jonathan Hoech, Pete Florence, Sean Kirmani, Sumeet Singh, Vikas Sindhwani, Carolina Parada, Chelsea Finn, Peng Xu, Sergey Levine, Jie Tan",
                "citations": 10
            },
            {
                "title": "On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study",
                "abstract": "Recently, large language models (LLMs) have taken the spotlight in natural language processing. Further, integrating LLMs with vision enables the users to explore emergent abilities with multimodal data. Visual language models (VLMs), such as LLaVA, Flamingo, or CLIP, have demonstrated impressive performance on various visio-linguistic tasks. Consequently, there are enormous applications of large models that could be potentially used in the biomedical imaging field. Along that direction, there is a lack of related work to show the ability of large models to diagnose the diseases. In this work, we study the zero-shot and few-shot robustness of VLMs on the medical imaging analysis tasks. Our comprehensive experiments demonstrate the effectiveness of VLMs in analyzing biomedical images such as brain MRIs, microscopic images of blood cells, and chest X- rays. While VLMs can not outperform classic vision models like CNN or ResNet, it is worth noting that VLMs can serve as chat assistants to provide pre-diagnosis before making decisions without the need for retraining or finetuning stages.",
                "authors": "Minh-Hao Van, Prateek Verma, Xintao Wu",
                "citations": 11
            },
            {
                "title": "Low-Rank Few-Shot Adaptation of Vision-Language Models",
                "abstract": "Recent progress in the few-shot adaptation of VisionLanguage Models (VLMs) has further pushed their generalization capabilities, at the expense of just a few labeled samples within the target downstream task. However, this promising, already quite abundant few-shot literature has focused principally on prompt learning and, to a lesser extent, on adapters, overlooking the recent advances in Parameter-Efficient Fine-Tuning (PEFT). Furthermore, existing few-shot learning methods for VLMs often rely on heavy training procedures and/or carefully chosen, taskspecific hyper-parameters, which might impede their applicability. In response, we introduce Low-Rank Adaptation (LoRA) in few-shot learning for VLMs, and show its potential on 11 datasets, in comparison to current state-of-the-art prompt- and adapter-based approaches. Surprisingly, our simple CLIP-LoRA method exhibits substantial improvements, while reducing the training times and keeping the same hyper-parameters in all the target tasks, i.e., across all the datasets and numbers of shots. Certainly, our surprising results do not dismiss the potential of promptlearning and adapter-based research. However, we believe that our strong baseline could be used to evaluate progress in these emergent subjects in few-shot VLMs.",
                "authors": "Maxime Zanella, Ismail Ben Ayed",
                "citations": 10
            },
            {
                "title": "\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors",
                "abstract": "Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement. The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic.",
                "authors": "L. Guan, Yifan Zhou, Denis Liu, Yantian Zha, H. B. Amor, Subbarao Kambhampati",
                "citations": 11
            },
            {
                "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model",
                "abstract": "The emergence of Vision Language Models (VLMs) has brought unprecedented advances in understanding multimodal information. The combination of textual and visual semantics in VLMs is highly complex and diverse, making the safety alignment of these models challenging. Furthermore, due to the limited study on the safety alignment of VLMs, there is a lack of large-scale, high-quality datasets. To address these limitations, we propose a Safety Preference Alignment dataset for Vision Language Models named SPA-VL. In terms of breadth, SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and contains 100,788 samples of the quadruple (question, image, chosen response, rejected response). In terms of depth, the responses are collected from 12 open- (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure diversity. The experimental results indicate that models trained with alignment techniques on the SPA-VL dataset exhibit substantial improvements in harmlessness and helpfulness while maintaining core capabilities. SPA-VL, as a large-scale, high-quality, and diverse dataset, represents a significant milestone in ensuring that VLMs achieve both harmlessness and helpfulness. We have made our code https://github.com/EchoseChen/SPA-VL-RLHF and SPA-VL dataset url https://huggingface.co/datasets/sqrti/SPA-VL publicly available.",
                "authors": "Yongting Zhang, Luyao Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhen-fei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, Feng Zhao, Tao Gui, Jing Shao",
                "citations": 11
            },
            {
                "title": "Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions",
                "abstract": "The zero-shot performance of existing vision-language models (VLMs) such as CLIP [29] is limited by the availability of large-scale, aligned image and text datasets in specific domains. In this work, we leverage two complementary sources of information-descriptions of categories generated by large language models (LLMs) and abundant, fine-grained image classification datasets-to improve the zero-shot classification performance of VLMs across fine-grained domains. On the technical side, we develop methods to train VLMs with this “bag-level” image-text super-vision. We find that simply using these attributes at test-time does not improve performance, but our training strategy, for example, on the iNaturalist [41] dataset, leads to an average improvement of 4-5% in zero-shot classification accuracy for novel categories of birds [42] and flow-ers [23]. Similar improvements are observed in domains where a subset of the categories was used to fine-tune the model. By prompting LLMs in various ways, we generate descriptions that capture visual appearance, habitat, and geographic regions and pair them with existing attributes such as the taxonomic structure of the categories. We systematically evaluate their ability to improve zero-shot categorization in natural domains. Our findings suggest that geographic priors can be just as effective and are complementary to visual appearance. Our method also outperforms prior work on prompt-based tuning of VLMs. We release the benchmark, consisting of 14 datasets at https://github.com/cvl-umass/AdaptCLIPZS, which will contribute to future research in zero-shot recognition.",
                "authors": "Oindrila Saha, Grant Van Horn, Subhransu Maji",
                "citations": 11
            },
            {
                "title": "VoCo-LLaMA: Towards Vision Compression with Large Language Models",
                "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in various multi-modal tasks, but they are often bottlenecked by the limited context window and high computational cost of processing high-resolution image inputs and videos. Vision compression can alleviate this problem by reducing the vision token count. Previous approaches compress vision tokens with external modules and force LLMs to understand the compressed ones, leading to visual information loss. However, the LLMs' understanding paradigm of vision tokens is not fully utilised in the compression learning process. We propose VoCo-LLaMA, the first approach to compress vision tokens using LLMs. By introducing Vision Compression tokens during the vision instruction tuning phase and leveraging attention distillation, our method distill how LLMs comprehend vision tokens into their processing of VoCo tokens. VoCo-LLaMA facilitates effective vision compression and improves the computational efficiency during the inference stage. Specifically, our method achieves minimal performance loss with a compression ratio of 576$\\times$, resulting in up to 94.8$\\%$ fewer FLOPs and 69.6$\\%$ acceleration in inference time. Furthermore, through continuous training using time-series compressed token sequences of video frames, VoCo-LLaMA demonstrates the ability to understand temporal correlations, outperforming previous methods on popular video question-answering benchmarks. Our approach presents a promising way to unlock the full potential of VLMs' contextual window, enabling more scalable multi-modal applications. The project page, along with the associated code, can be accessed via $\\href{https://yxxxb.github.io/VoCo-LLaMA-page/}{\\text{this https URL}}$.",
                "authors": "Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, Yansong Tang",
                "citations": 10
            },
            {
                "title": "MouSi: Poly-Visual-Expert Vision-Language Models",
                "abstract": "Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations. For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient and manageable 64 or even down to 1. Experimental results demonstrate that VLMs with multiple experts exhibit consistently superior performance over isolated visual encoders and mark a significant performance boost as more experts are integrated. We have open-sourced the training code used in this report. All of these resources can be found on our project website.",
                "authors": "Xiaoran Fan, Tao Ji, Changhao Jiang, Shuo Li, Senjie Jin, Sirui Song, Junke Wang, Boyang Hong, Luyao Chen, Guodong Zheng, Ming Zhang, Caishuang Huang, Rui Zheng, Zhiheng Xi, Yuhao Zhou, Shihan Dou, Junjie Ye, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yunchun Jiang",
                "citations": 11
            },
            {
                "title": "Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models",
                "abstract": "This study addresses the Domain-Class Incremental Learning problem, a realistic but challenging continual learning scenario where both the domain distribution and target classes vary across tasks. To handle these diverse tasks, pre-trained Vision-Language Models (VLMs) are introduced for their strong generalizability. However, this incurs a new problem: the knowledge encoded in the pre-trained VLMs may be disturbed when adapting to new tasks, compromising their inherent zero-shot ability. Existing methods tackle it by tuning VLMs with knowledge distillation on extra datasets, which demands heavy computation overhead. To address this problem efficiently, we propose the Distribution-aware Interference-free Knowledge Integration (DIKI) framework, retaining pre-trained knowledge of VLMs from a perspective of avoiding information interference. Specifically, we design a fully residual mechanism to infuse newly learned knowledge into a frozen backbone, while introducing minimal adverse impacts on pre-trained knowledge. Besides, this residual property enables our distribution-aware integration calibration scheme, explicitly controlling the information implantation process for test data from unseen distributions. Experiments demonstrate that our DIKI surpasses the current state-of-the-art approach using only 0.86% of the trained parameters and requiring substantially less training time. Code is available at: https://github.com/lloongx/DIKI .",
                "authors": "Longxiang Tang, Zhuotao Tian, Kai Li, Chunming He, Hantao Zhou, Hengshuang Zhao, Xiu Li, Jiaya Jia",
                "citations": 10
            },
            {
                "title": "Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",
                "abstract": "Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual reasoning and responses for end-to-end autonomous driving safety tasks using traffic scene images and other data modalities. However, current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary. To address these previous issues, we develop EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which performs Visual Question Answering for autonomous driving. In comparison to previous approaches, EM-VLM4AD requires at least 10 times less memory and floating point operations, while also achieving higher CIDEr and ROUGE-L scores than the existing baseline on the DriveLM dataset. EM-VLM4AD also exhibits the ability to extract relevant information from traffic views related to prompts and can answer questions for various autonomous driving subtasks. We release our code to train and evaluate our model at https://github.com/akshaygopalkr/EM-VLM4AD.",
                "authors": "Akshay Gopalkrishnan, Ross Greer, Mohan M. Trivedi",
                "citations": 10
            },
            {
                "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
                "abstract": "The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.",
                "authors": "Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, Aman Chadha",
                "citations": 9
            },
            {
                "title": "Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction",
                "abstract": "Whole Slide Image (WSI) classification is often formu-lated as a Multiple Instance Learning (MIL) problem. Re-cently, Vision-Language Models (VLMs) have demonstrated remarkable performance in WSI classification. However, existing methods leverage coarse-grained pathogenetic de-scriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of mod-els on diverse downstream tasks. Additionally, processing high-resolution WSIs can be computationally expensive. In this paper, we propose a novel “Fine-grained Visual-Semantic Interaction” (FiVE) framework for WSI classi-fication. It is designed to enhance the model's general-izability by leveraging the interaction between localized visual patterns and fine-grained pathological semantics. Specifically, with meticulously designed queries, we start by utilizing a large language model to extract fine-grained pathological descriptions from various non-standardized raw reports. The output descriptions are then reconstructed into fine-grained labels used for training. By introducing a Task-specific Fine-grained Semantics (TFS) module, we enable prompts to capture crucial visual information in WSIs, which enhances representation learning and aug-ments generalization capabilities significantly. Further-more, given that pathological visual patterns are redun-dantly distributed across tissue slices, we sample a subset of visual instances during training. Our method demon-strates robust generalizability and strong transferability, dominantly outperforming the counterparts on the TCGA Lung Cancer dataset with at least 9.19% higher accu-racy in few-shot experiments. The code is available at: https://github.com/lslrius/WSI_FiVE.",
                "authors": "Hao Li, Ying Chen, Yifei Chen, Wenxian Yang, Bowen Ding, Yuchen Han, Liansheng Wang, Rongshan Yu",
                "citations": 9
            },
            {
                "title": "Cross-Modal Safety Alignment: Is textual unlearning all you need?",
                "abstract": "Recent studies reveal that integrating new modalities into Large Language Models (LLMs), such as Vision-Language Models (VLMs), creates a new attack surface that bypasses existing safety training techniques like Supervised Fine-tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF). While further SFT and RLHF-based safety training can be conducted in multi-modal settings, collecting multi-modal training datasets poses a significant challenge. Inspired by the structural design of recent multi-modal models, where, regardless of the combination of input modalities, all inputs are ultimately fused into the language space, we aim to explore whether unlearning solely in the textual domain can be effective for cross-modality safety alignment. Our evaluation across six datasets empirically demonstrates the transferability -- textual unlearning in VLMs significantly reduces the Attack Success Rate (ASR) to less than 8\\% and in some cases, even as low as nearly 2\\% for both text-based and vision-text-based attacks, alongside preserving the utility. Moreover, our experiments show that unlearning with a multi-modal dataset offers no potential benefits but incurs significantly increased computational demands, possibly up to 6 times higher.",
                "authors": "Trishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael B. Abu-Ghazaleh, M. S. Asif, Yue Dong, A. Roy-Chowdhury, Chengyu Song",
                "citations": 9
            },
            {
                "title": "Diagnostic accuracy of vision-language models on Japanese diagnostic radiology, nuclear medicine, and interventional radiology specialty board examinations",
                "abstract": "Purpose: The performance of vision-language models (VLMs) with image interpretation capabilities, such as GPT-4 omni (GPT-4o), GPT-4 vision (GPT-4V), and Claude-3, has not been compared and remains unexplored in specialized radiological fields, including nuclear medicine and interventional radiology. This study aimed to evaluate and compare the diagnostic accuracy of various VLMs, including GPT-4 + GPT-4V, GPT-4o, Claude-3 Sonnet, and Claude-3 Opus, using Japanese diagnostic radiology, nuclear medicine, and interventional radiology (JDR, JNM, and JIR, respectively) board certification tests. Methods: In total, 383 questions from the JDR test (358 images), 300 from the JNM test (92 images), and 322 from the JIR test (96 images) from 2019 to 2023 were consecutively collected. The accuracy rates of the GPT-4 + GPT-4V, GPT-4o, Claude-3 Sonnet, and Claude-3 Opus were calculated for all questions or questions with images. The accuracy rates of the VLMs were compared using McNemar's test. Results: GPT-4o demonstrated the highest accuracy rates across all evaluations with the JDR (all questions, 49%; questions with images, 48%), JNM (all questions, 64%; questions with images, 59%), and JIR tests (all questions, 43%; questions with images, 34%), followed by Claude-3 Opus with the JDR (all questions, 40%; questions with images, 38%), JNM (all questions, 51%; questions with images, 43%), and JIR tests (all questions, 40%; questions with images, 30%). For all questions, McNemar's test showed that GPT-4o significantly outperformed the other VLMs (all P < 0.007), except for Claude-3 Opus in the JIR test. For questions with images, GPT-4o outperformed the other VLMs in the JDR and JNM tests (all P < 0.001), except Claude-3 Opus in the JNM test. Conclusion: The GPT-4o had the highest success rates for questions with images and all questions from the JDR, JNM, and JIR board certification tests.",
                "authors": "Tatsushi Oura, Hiroyuki Tatekawa, Daisuke Horiuchi, Shunichi Matsushita, H. Takita, Natsuko Atsukawa, Yasuhito Mitsuyama, Atsushi Yoshida, Kazuki Murai, Rikako Tanaka, T. Shimono, Akira Yamamoto, Yukio Miki, D. Ueda",
                "citations": 9
            },
            {
                "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?",
                "abstract": "Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs.",
                "authors": "Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Bianca Lamm, M. J. Mirza, M. Keuper, Janis Keuper",
                "citations": 9
            },
            {
                "title": "CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection",
                "abstract": "The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, diffusion-based and commercial tools. Code and pre-trained models: https://github.com/sohailahmedkhan/CLIPping-the-Deception",
                "authors": "Sohail Ahmed Khan, Duc-Tien Dang-Nguyen",
                "citations": 8
            },
            {
                "title": "MMA: Multi-Modal Adapter for Vision-Language Models",
                "abstract": "Pretrained Vision-Language Models (VLMs) have served as excellent foundation models for transfer learning in diverse downstream tasks. However, tuning VLMs for few-shot generalization tasks faces a discrimination - generalization dilemma, i.e., general knowledge should be preserved and task-specific knowledge should be fine-tuned. How to precisely identify these two types of representations remains a challenge. In this paper, we propose a Multi-Modal Adapter (MMA) for VLMs to improve the alignment between representations from text and vision branches. MMA aggregates features from different branches into a shared feature space so that gradients can be communicated across branches. To determine how to incorporate MMA, we systematically analyze the discriminability and generalizability of features across diverse datasets in both the vision and language branches, and find that (1) higher lay-ers contain discriminable dataset-specific knowledge, while lower layers contain more generalizable knowledge, and (2) language features are more discriminable than visual features, and there are large semantic gaps between the features of the two modalities, especially in the lower layers. Therefore, we only incorporate MMA to a few higher lay-ers of transformers to achieve an optimal balance between discrimination and generalization. We evaluate the effectiveness of our approach on three tasks: generalization to novel classes, novel target datasets, and domain generalization. Compared to many state-of-the-art methods, our MMA achieves leading performance in all evaluations. Code is at https://github.com/ZjjConan/Multi-Modal-Adapter",
                "authors": "Lingxiao Yang, Ru-Yuan Zhang, Yanchen Wang, Xiaohua Xie",
                "citations": 8
            },
            {
                "title": "Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning",
                "abstract": "The advancement of large language models (LLMs) has significantly broadened the scope of applications in natural language processing, with multi-modal LLMs extending these capabilities to integrate and interpret visual data. However, existing benchmarks for visual language models (VLMs) predominantly focus on single-image inputs, neglecting the crucial aspect of multi-image understanding. In this paper, we introduce a Multi-Image Relational Benchmark MIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across multiple images. Our benchmark encompasses four categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. Through a comprehensive evaluation of a wide range of open-source and closed-source models, we demonstrate that while open-source VLMs were shown to approach the performance of GPT-4V in single-image tasks, a significant performance gap remains in multi-image reasoning tasks. Our findings also reveal that even the state-of-the-art GPT-4V model struggles with our benchmark, underscoring the need for further research and development in this area. We believe our contribution of MIRB could serve as a testbed for developing the next-generation multi-modal models.",
                "authors": "Bingchen Zhao, Yongshuo Zong, Letian Zhang, Timothy M. Hospedales",
                "citations": 8
            },
            {
                "title": "Is It Safe to Cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing",
                "abstract": "Safely navigating street intersections is a complex challenge for blind and low-vision individuals, as it requires a nuanced understanding of the surrounding context - a task heavily reliant on visual cues. Traditional methods for assisting in this decision-making process often fall short, lacking the ability to provide a comprehensive scene analysis and safety level. This paper introduces an innovative approach that leverages vision-language models (VLMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques. By generating a safety score and scene description in natural language, our method supports safe decision-making for blind and low-vision individuals. We collected crosswalk intersection data that contains multiview egocentric images captured by a quadruped robot and annotated the images with corresponding safety scores based on our predefined safety score categorization. Grounded on the visual knowledge, extracted from images and text prompts, we evaluate a VLM for safety score prediction and scene description. Our findings highlight the reasoning and safety score prediction capabilities of the VLM, activated by various prompts, as a pathway to developing a trustworthy system, crucial for applications requiring reliable decision-making support.",
                "authors": "Hochul Hwang, Sunjae Kwon, Yekyung Kim, Donghyun Kim",
                "citations": 8
            },
            {
                "title": "SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference",
                "abstract": "In vision-language models (VLMs), visual tokens usually consume a significant amount of computational overhead, despite their sparser information density compared to text tokens. To address this, most existing methods learn a network to prune redundant visual tokens and require additional training data. Differently, we propose an efficient training-free token optimization mechanism dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely, given that visual tokens complement text tokens in VLMs for linguistic reasoning, we select visual-relevant text tokens to rate the significance of vision tokens within the self-attention matrix extracted from the VLMs. Then we progressively prune irrelevant tokens. To maximize sparsity while retaining essential information, we introduce a rank-based strategy to adaptively determine the sparsification ratio for each layer, alongside a token recycling method that compresses pruned tokens into more compact representations. Experimental results show that our SparseVLM improves the efficiency of various VLMs across a range of image and video understanding tasks. In particular, LLaVA equipped with SparseVLM reduces 61% to 67% FLOPs with a compression ratio of 78% while maintaining 93% of the accuracy. Our code is available at https://github.com/Gumpest/SparseVLMs.",
                "authors": "Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis A Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang",
                "citations": 8
            },
            {
                "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
                "abstract": "Vision-Language Models (VLMs) have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks. A detailed analysis reveals that VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.",
                "authors": "Yizhe Zhang, Richard He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, J. Susskind, N. Jaitly",
                "citations": 8
            },
            {
                "title": "RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulaiton",
                "abstract": "Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a novel paradigm, aiming to enhance the model's ability to generalize to new objects and instructions. However, due to variations in camera specifications and mounting positions, existing methods exhibit significant performance disparities across different robotic platforms. To address this challenge, we propose RoboUniView in this paper, an innovative approach that decouples visual feature extraction from action learning. We first learn a unified view representation from multi-perspective views by pre-training on readily accessible data, and then derive actions from this unified view representation to control robotic manipulation. This unified view representation more accurately mirrors the physical world and is not constrained by the robotic platform's camera parameters. Thanks to this methodology, we achieve state-of-the-art performance on the demanding CALVIN benchmark, enhancing the success rate in the $D \\to D$ setting from 93.0% to 96.2%, and in the $ABC \\to D$ setting from 92.2% to 94.2%. Moreover, our model exhibits outstanding adaptability and flexibility: it maintains high performance under unseen camera parameters, can utilize multiple datasets with varying camera parameters, and is capable of joint cross-task learning across datasets. Code is provided for re-implementation. https://github.com/liufanfanlff/RoboUniview",
                "authors": "Fanfan Liu, Feng Yan, Liming Zheng, Chengjian Feng, Yiyang Huang, Lin Ma",
                "citations": 7
            },
            {
                "title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models",
                "abstract": "Recent advancements in Large Vision-Language Models (VLMs) have underscored their superiority in various multimodal tasks. However, the adversarial robustness of VLMs has not been fully explored. Existing methods mainly assess robustness through unimodal adversarial attacks that perturb images, while assuming inherent resilience against text-based attacks. Different from existing attacks, in this work we propose a more comprehensive strategy that jointly attacks both text and image modalities to exploit a broader spectrum of vulnerability within VLMs. Specifically, we propose a dual optimization objective aimed at guiding the model to generate affirmative responses with high toxicity. Our attack method begins by optimizing an adversarial image prefix from random noise to generate diverse harmful responses in the absence of text input, thus imbuing the image with toxic semantics. Subsequently, an adversarial text suffix is integrated and co-optimized with the adversarial image prefix to maximize the probability of eliciting affirmative responses to various harmful instructions. The discovered adversarial image prefix and text suffix are collectively denoted as a Universal Master Key (UMK). When integrated into various malicious queries, UMK can circumvent the alignment defenses of VLMs and lead to the generation of objectionable content, known as jailbreaks. The experimental results demonstrate that our universal attack strategy can effectively jailbreak MiniGPT-4 with a 96% success rate, highlighting the vulnerability of VLMs and the urgent need for new alignment strategies.",
                "authors": "Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, Yu-Gang Jiang",
                "citations": 7
            },
            {
                "title": "RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation",
                "abstract": "This work proposes a retrieve-and-transfer framework for zero-shot robotic manipulation, dubbed RAM, featuring generalizability across various objects, environments, and embodiments. Unlike existing approaches that learn manipulation from expensive in-domain demonstrations, RAM capitalizes on a retrieval-based affordance transfer paradigm to acquire versatile manipulation capabilities from abundant out-of-domain data. First, RAM extracts unified affordance at scale from diverse sources of demonstrations including robotic data, human-object interaction (HOI) data, and custom data to construct a comprehensive affordance memory. Then given a language instruction, RAM hierarchically retrieves the most similar demonstration from the affordance memory and transfers such out-of-domain 2D affordance to in-domain 3D executable affordance in a zero-shot and embodiment-agnostic manner. Extensive simulation and real-world evaluations demonstrate that our RAM consistently outperforms existing works in diverse daily tasks. Additionally, RAM shows significant potential for downstream applications such as automatic and efficient data collection, one-shot visual imitation, and LLM/VLM-integrated long-horizon manipulation. For more details, please check our website at https://yxkryptonite.github.io/RAM/.",
                "authors": "Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, Yue Wang",
                "citations": 7
            },
            {
                "title": "PALO: A Polyglot Large Multimodal Model for 5B People",
                "abstract": "In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called PALO. PALO offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of ~5B people (65% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines. We also propose the first multilingual multimodal benchmark for the forthcoming approaches to evaluate their vision-language reasoning capabilities across languages. Code: https://github.com/mbzuai-oryx/PALO.",
                "authors": "Muhammad Maaz, H. Rasheed, Abdelrahman M. Shaker, Salman H. Khan, Hisham Cholakkal, R. Anwer, Timothy Baldwin, M. Felsberg, F. Khan",
                "citations": 7
            },
            {
                "title": "Can LLMs' Tuning Methods Work in Medical Multimodal Domain?",
                "abstract": "While Large Language Models (LLMs) excel in world knowledge understanding, adapting them to specific subfields requires precise adjustments. Due to the model's vast scale, traditional global fine-tuning methods for large models can be computationally expensive and impact generalization. To address this challenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT) methods have emerged and achieved remarkable success in both LLMs and Large Vision-Language Models (LVLMs). In the medical domain, fine-tuning a medical Vision-Language Pretrained (VLP) model is essential for adapting it to specific tasks. Can the fine-tuning methods for large models be transferred to the medical field to enhance transfer learning efficiency? In this paper, we delve into the fine-tuning methods of LLMs and conduct extensive experiments to investigate the impact of fine-tuning methods for large models on the existing multimodal model in the medical domain from the training data level and the model structure level. We show the different impacts of fine-tuning methods for large models on medical VLMs and develop the most efficient ways to fine-tune medical VLP models. We hope this research can guide medical domain researchers in optimizing VLMs' training costs, fostering the broader application of VLMs in healthcare fields. The code and dataset have been released at https://github.com/TIMMY-CHAN/MILE.",
                "authors": "Jiawei Chen, Yue Jiang, Dingkang Yang, Mingcheng Li, Jinjie Wei, Ziyun Qian, Lihua Zhang",
                "citations": 7
            },
            {
                "title": "A Concept-Based Explainability Framework for Large Multimodal Models",
                "abstract": "Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as ``multi-modal concepts''. We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. Our code is publicly available at https://github.com/mshukor/xl-vlms",
                "authors": "Jayneel Parekh, Pegah Khayatan, Mustafa Shukor, A. Newson, Matthieu Cord",
                "citations": 7
            },
            {
                "title": "ViTamin: Designing Scalable Vision Models in the Vision-Language Era",
                "abstract": "Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).",
                "authors": "Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan L. Yuille, Liang-Chieh Chen",
                "citations": 7
            },
            {
                "title": "FINEMATCH: Aspect-based Fine-grained Image and Text Mismatch Detection and Correction",
                "abstract": "Recent progress in large-scale pre-training has led to the development of advanced vision-language models (VLMs) with remarkable proficiency in comprehending and generating multimodal content. Despite the impressive ability to perform complex reasoning for VLMs, current models often struggle to effectively and precisely capture the compositional information on both the image and text sides. To address this, we propose FineMatch, a new aspect-based fine-grained text and image matching benchmark, focusing on text and image mismatch detection and correction. This benchmark introduces a novel task for boosting and evaluating the VLMs' compositionality for aspect-based fine-grained text and image matching. In this task, models are required to identify mismatched aspect phrases within a caption, determine the aspect's class, and propose corrections for an image-text pair that may contain between 0 and 3 mismatches. To evaluate the models' performance on this new task, we propose a new evaluation metric named ITM-IoU for which our experiments show a high correlation to human evaluation. In addition, we also provide a comprehensive experimental analysis of existing mainstream VLMs, including fully supervised learning and in-context learning settings. We have found that models trained on FineMatch demonstrate enhanced proficiency in detecting fine-grained text and image mismatches. Moreover, models (e.g., GPT-4V, Gemini Pro Vision) with strong abilities to perform multimodal in-context learning are not as skilled at fine-grained compositional image and text matching analysis. With FineMatch, we are able to build a system for text-to-image generation hallucination detection and correction.",
                "authors": "Hang Hua, Jing Shi, Kushal Kafle, Simon Jenni, Daoan Zhang, John P. Collomosse, Scott Cohen, Jiebo Luo",
                "citations": 7
            },
            {
                "title": "MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries",
                "abstract": "In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient's medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries. This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care. Our dataset, code, and pre-trained models will be made publicly available.",
                "authors": "Akash Ghosh, Arkadeep Acharya, Prince Jha, Aniket Gaudgaul, Rajdeep Majumdar, Sriparna Saha, Aman Chadha, Raghav Jain, Setu Sinha, Shivani Agarwal",
                "citations": 7
            },
            {
                "title": "NEVLP: Noise-Robust Framework for Efficient Vision-Language Pre-training",
                "abstract": "The success of Vision Language Models (VLMs) on various vision-language tasks heavily relies on pre-training with large scale web-crawled datasets. However, the noisy and incomplete nature of web data makes dataset scale crucial for performance, rendering end-to-end training increasingly prohibitive. In this paper, we propose NEVLP, a noise-robust framework for efficient vision-language pre-training that requires less pre-training data. Specifically, we bridge the modality gap between a frozen image encoder and a large language model with a transformer and introduce two innovative learning strategies: noise-adaptive learning and concept-enhanced learning to mitigate the impact of noise. In noise-adaptive learning, we estimate the noise probability of each image-text pair based on the transformer's memorization effect and employ noise-adaptive regularization on image-text contrastive learning to condition cross-modal alignment. In concept-enhanced learning, we enrich incomplete text by incorporating visual concepts (objects in the image) to provide prior information about existing objects for image-text matching and image-grounded text generation, thereby mitigating text incompletion. Our framework effectively utilizes noisy web data and achieves state-of-the-art performance with less pre-training data across a wide range of vision-language tasks, including image-text retrieval, image captioning, and visual question answering.",
                "authors": "Yiyi Tao, Zhuoyue Wang, Hang Zhang, Lun Wang",
                "citations": 7
            },
            {
                "title": "Unified Physical-Digital Face Attack Detection",
                "abstract": "Face Recognition (FR) systems can suffer from physical (i.e., print photo) and digital (i.e., DeepFake) attacks. However, previous related work rarely considers both situations at the same time. This implies the deployment of multiple models and thus more computational burden. The main reasons for this lack of an integrated model are caused by two factors: (1) The lack of a dataset including both physical and digital attacks which the same ID covers the real face and all attack types; (2) Given the large intra-class variance between these two attacks, it is difficult to learn a compact feature space to detect both attacks simultaneously. To address these issues, we collect a Unified physical-digital Attack dataset, called UniAttackData. The dataset consists of 1,800 participations of 2 and 12 physical and digital attacks, respectively, resulting in a total of 28,706 videos. Then, we propose a Unified Attack Detection framework based on Vision-Language Models (VLMs), namely UniAttackDetection, which includes three main modules: the Teacher-Student Prompts (TSP) module, focused on acquiring unified and specific knowledge respectively; the Unified Knowledge Mining (UKM) module, designed to capture a comprehensive feature space; and the Sample-Level Prompt Interaction (SLPI) module, aimed at grasping sample-level semantics. These three modules seamlessly form a robust unified attack detection framework. Extensive experiments on UniAttackData and three other datasets demonstrate the superiority of our approach for unified face attack detection. Dataset link: https://sites.google.com/view/face-anti-spoofing-challenge/dataset-download/uniattackdatacvpr2024",
                "authors": "Hao Fang, Ajian Liu, Haocheng Yuan, Junze Zheng, Dingheng Zeng, Yanhong Liu, Jiankang Deng, Sergio Escalera, Xiaoming Liu, Jun Wan, Zhen Lei",
                "citations": 7
            },
            {
                "title": "Diffusion Feedback Helps CLIP See Better",
                "abstract": "Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the perception capabilities of multimodal large language models (MLLMs) built on CLIP. The main reason could be that the image-text pairs used to train CLIP are inherently biased, due to the lack of the distinctiveness of the text and the diversity of images. In this work, we present a simple post-training approach for CLIP models, which largely overcomes its visual shortcomings via a self-supervised diffusion process. We introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP. Specifically, DIVA leverages generative feedback from text-to-image diffusion models to optimize CLIP representations, with only images (without corresponding text). We demonstrate that DIVA improves CLIP's performance on the challenging MMVP-VLM benchmark which assesses fine-grained visual abilities to a large extent (e.g., 3-7%), and enhances the performance of MLLMs and vision models on multimodal understanding and segmentation tasks. Extensive evaluation on 29 image classification and retrieval benchmarks confirms that our framework preserves CLIP's strong zero-shot capabilities. The code is available at https://github.com/baaivision/DIVA.",
                "authors": "Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, Xinlong Wang",
                "citations": 7
            },
            {
                "title": "Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection",
                "abstract": "Open-vocabulary human-object interaction (HOI) detection, which is concerned with the problem of detecting novel HOIs guided by natural language, is crucial for understanding human-centric scenes. However, prior zero-shot HOI detectors often employ the same levels of feature maps to model HOIs with varying distances, leading to suboptimal performance in scenes containing human-object pairs with a wide range of distances. In addition, these detectors primarily rely on category names and over-look the rich contextual information that language can provide, which is essential for capturing open vocabulary concepts that are typically rare and not well-represented by category names alone. In this paper, we introduce a novel end-to-end open vocabulary HOI detection framework with conditional multi-level decoding and fine-grained semantic enhancement (CMD-SE), harnessing the potential of Visual-Language Models (VLMs). Specifically, we propose to model human-object pairs with different distances with different levels of feature maps by incorporating a soft constraint during the bipartite matching process. Furthermore, by leveraging large language models (LLMs) such as GPT models, we exploit their extensive world knowledge to generate descriptions of human body part states for various interactions. Then we integrate the generalizable and fine- grained semantics of human body parts to improve interaction recognition. Experimental results on two datasets, SWIG-HOI and HICO-DET, demonstrate that our proposed method achieves state-of-the-art results in open vocabulary HOI detection. The code and models are available at https://github.com/ltttpku/CMD-SE-release.",
                "authors": "Ting Lei, Shaofeng Yin, Yang Liu",
                "citations": 7
            },
            {
                "title": "Unveiling Encoder-Free Vision-Language Models",
                "abstract": "Existing vision-language models (VLMs) mostly rely on vision encoders to extract visual features followed by large language models (LLMs) for visual-language tasks. However, the vision encoders set a strong inductive bias in abstracting visual representation, e.g., resolution, aspect ratio, and semantic priors, which could impede the flexibility and efficiency of the VLMs. Training pure VLMs that accept the seamless vision and language inputs, i.e., without vision encoders, remains challenging and rarely explored. Empirical observations reveal that direct training without encoders results in slow convergence and large performance gaps. In this work, we bridge the gap between encoder-based and encoder-free models, and present a simple yet effective training recipe towards pure VLMs. Specifically, we unveil the key aspects of training encoder-free VLMs efficiently via thorough experiments: (1) Bridging vision-language representation inside one unified decoder; (2) Enhancing visual recognition capability via extra supervision. With these strategies, we launch EVE, an encoder-free vision-language model that can be trained and forwarded efficiently. Notably, solely utilizing 35M publicly accessible data, EVE can impressively rival the encoder-based VLMs of similar capacities across multiple vision-language benchmarks. It significantly outperforms the counterpart Fuyu-8B with mysterious training procedures and undisclosed training data. We believe that EVE provides a transparent and efficient route for developing a pure decoder-only architecture across modalities. Our code and models are publicly available at: https://github.com/baaivision/EVE.",
                "authors": "Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, Xinlong Wang",
                "citations": 6
            },
            {
                "title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples",
                "abstract": "Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a $\\textbf{vision-centric}$ design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.",
                "authors": "Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan",
                "citations": 6
            },
            {
                "title": "Learn \"No\" to Say \"Yes\" Better: Improving Vision-Language Models via Negations",
                "abstract": "Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This paper highlights the limitations of popular VLMs such as CLIP, at understanding the implications of negations, i.e., the effect of the word\"not\"in a given prompt. To enable evaluation of VLMs on fluent prompts with negations, we present CC-Neg, a dataset containing 228,246 images, true captions and their corresponding negated captions. Using CC-Neg along with modifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework, has an improved understanding of negations. This training paradigm improves CoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average gain in top-1 accuracy for zero-shot image classification across 8 datasets. Further, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks such as SugarCREPE by 4.4%, showcasing emergent compositional understanding of objects, relations, and attributes in text. Overall, our work addresses a crucial limitation of VLMs by introducing a dataset and framework that strengthens semantic associations between images and text, demonstrating improved large-scale foundation models with significantly reduced computational cost, promoting efficiency and accessibility.",
                "authors": "Jaisidh Singh, Ishaan Shrivastava, M. Vatsa, Richa Singh, Aparna Bharati",
                "citations": 6
            },
            {
                "title": "CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning",
                "abstract": "This paper explores the problem of continual learning (CL) of vision-language models (VLMs) in open domains, where the models need to perform continual updating and inference on a streaming of datasets from diverse seen and unseen domains with novel classes. Such a capability is crucial for various applications in open environments, e.g., AI assistants, autonomous driving systems, and robotics. Current CL studies mostly focus on closed-set scenarios in a single domain with known classes. Large pre-trained VLMs like CLIP have demonstrated superior zero-shot recognition ability, and a number of recent studies leverage this ability to mitigate catastrophic forgetting in CL, but they focus on closed-set CL in a single domain dataset. Open-domain CL of large VLMs is significantly more challenging due to 1) large class correlations and domain gaps across the datasets and 2) the forgetting of zero-shot knowledge in the pre-trained VLMs in addition to the knowledge learned from the newly adapted datasets. In this work we introduce a novel approach, termed CoLeCLIP, that learns an open-domain CL model based on CLIP. It addresses these challenges by a joint learning of a set of task prompts and a cross-domain class vocabulary. Extensive experiments on 11 domain datasets show that CoLeCLIP outperforms state-of-the-art methods for open-domain CL under both task- and class-incremental learning settings.",
                "authors": "Yukun Li, Guansong Pang, Wei Suo, Chenchen Jing, Yuling Xi, Lingqiao Liu, Hao Chen, Guoqiang Liang, Peng Wang",
                "citations": 6
            },
            {
                "title": "Defending Jailbreak Attack in VLMs via Cross-modality Information Detector",
                "abstract": "Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively understand vision information, achieving remarkable performance in many vision-centric tasks. Despite that, recent studies have shown that these models are susceptible to jailbreak attacks, which refer to an exploitative technique where malicious users can break the safety alignment of the target model and generate mis-leading and harmful answers. This potential threat is caused by both the inherent vulnerabilities of LLM and the larger attack scope introduced by vision input. To enhance the security of VLMs against jailbreak attacks, researchers have developed various defense techniques. However, these methods either require modifications to the model’s internal structure or demand significant computational resources during the inference phase. Multimodal information is a double-edged sword. While it increases the risk of attacks, it also provides additional data that can enhance safeguards. Inspired by this, we propose C ross-modality I nformation DE tecto R ( CIDER ), a plug-and-play jailbreaking detector designed to identify maliciously perturbed image inputs, utilizing the cross-modal similarity between harmful queries and adversarial images. This simple yet effective cross-modality information detector, CIDER , is independent of the target VLMs and requires less computation cost. Extensive experimental results demonstrate the effectiveness and efficiency of CIDER , as well as its transferability to both white-box and black-box VLMs.",
                "authors": "Yue Xu, Xiuyuan Qi, Zhan Qin, Wenjie Wang",
                "citations": 6
            },
            {
                "title": "TopViewRS: Vision-Language Models as Top-View Spatial Reasoners",
                "abstract": "Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of ‘non-human’ agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs in this setup remain unattested and underexplored. In this work, we study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.",
                "authors": "Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, Ivan Vuli'c",
                "citations": 6
            },
            {
                "title": "Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models",
                "abstract": "Vision-Large-Language-models(VLMs) have great application prospects in autonomous driving. Despite the ability of VLMs to comprehend and make decisions in complex scenarios, their integration into safety-critical autonomous driving systems poses serious security risks. In this paper, we propose BadVLMDriver, the first backdoor attack against VLMs for autonomous driving that can be launched in practice using physical objects. Unlike existing backdoor attacks against VLMs that rely on digital modifications, BadVLMDriver uses common physical items, such as a red balloon, to induce unsafe actions like sudden acceleration, highlighting a significant real-world threat to autonomous vehicle safety. To execute BadVLMDriver, we develop an automated pipeline utilizing natural language instructions to generate backdoor training samples with embedded malicious behaviors. This approach allows for flexible trigger and behavior selection, enhancing the stealth and practicality of the attack in diverse scenarios. We conduct extensive experiments to evaluate BadVLMDriver for two representative VLMs, five different trigger objects, and two types of malicious backdoor behaviors. BadVLMDriver achieves a 92% attack success rate in inducing a sudden acceleration when coming across a pedestrian holding a red balloon. Thus, BadVLMDriver not only demonstrates a critical security risk but also emphasizes the urgent need for developing robust defense mechanisms to protect against such vulnerabilities in autonomous driving technologies.",
                "authors": "Zhenyang Ni, Rui Ye, Yuxian Wei, Zhen Xiang, Yanfeng Wang, Siheng Chen",
                "citations": 6
            },
            {
                "title": "A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models",
                "abstract": "Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of popular vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.",
                "authors": "Ashutosh Sathe, Prachi Jain, Sunayana Sitaram",
                "citations": 6
            },
            {
                "title": "Too Many Frames, not all Useful: Efficient Strategies for Long-Form Video QA",
                "abstract": "Long-form videos that span across wide temporal intervals are highly information redundant and contain multiple distinct events or entities that are often loosely related. Therefore, when performing long-form video question answering (LVQA), all information necessary to generate a correct response can often be contained within a small subset of frames. Recent literature explore the use of large language models (LLMs) in LVQA benchmarks, achieving exceptional performance, while relying on vision language models (VLMs) to convert all visual content within videos into natural language. Such VLMs often independently caption a large number of frames uniformly sampled from long videos, which is not efficient and can mostly be redundant. Questioning these decision choices, we explore optimal strategies for key-frame selection that can significantly reduce these redundancies, namely Hierarchical Keyframe Selector. Our proposed framework, LVNet, achieves state-of-the-art performance at a comparable caption scale across three benchmark LVQA datasets: EgoSchema, NExT-QA, IntentQA. The code can be found at https://github.com/jongwoopark7978/LVNet",
                "authors": "Jong Sung Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryoo, Donghyun Kim, M. Ryoo",
                "citations": 6
            },
            {
                "title": "Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts",
                "abstract": "Current weakly supervised video anomaly detection (WSVAD) task aims to achieve frame-level anomalous event detection with only coarse video-level annotations available. Existing works typically involve extracting global features from full-resolution video frames and training frame-level classifiers to detect anomalies in the temporal dimension. However, most anomalous events tend to occur in localized spatial regions rather than the entire video frames, which implies existing frame-level feature based works may be misled by the dominant background information and lack the interpretation of the detected anomalies. To address this dilemma, this paper introduces a novel method called STPrompt that learns spatio-temporal prompt embeddings for weakly supervised video anomaly detection and localization (WSVADL) based on pre-trained vision-language models (VLMs). Our proposed method employs a two-stream network structure, with one stream focusing on the temporal dimension and the other primarily on the spatial dimension. By leveraging the learned knowledge from pre-trained VLMs and incorporating natural motion priors from raw videos, our model learns prompt embeddings that are aligned with spatio-temporal regions of videos (e.g., patches of individual frames) for identify specific local regions of anomalies, enabling accurate video anomaly detection while mitigating the influence of background information. Without relying on detailed spatio-temporal annotations or auxiliary object detection/tracking, our method achieves state-of-the-art performance on three public benchmarks for the WSVADL task.",
                "authors": "Peng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, Peng Wang, Yanning Zhang",
                "citations": 5
            },
            {
                "title": "RS-Agent: Automating Remote Sensing Tasks through Intelligent Agents",
                "abstract": "An increasing number of models have achieved great performance in remote sensing tasks with the recent development of Large Language Models (LLMs) and Visual Language Models (VLMs). However, these models are constrained to basic vision and language instruction-tuning tasks, facing challenges in complex remote sensing applications. Additionally, these models lack specialized expertise in professional domains. To address these limitations, we propose a LLM-driven remote sensing intelligent agent named RS-Agent. Firstly, RS-Agent is powered by a large language model (LLM) that acts as its\"Central Controller,\"enabling it to understand and respond to various problems intelligently. Secondly, our RS-Agent integrates many high-performance remote sensing image processing tools, facilitating multi-tool and multi-turn conversations. Thirdly, our RS-Agent can answer professional questions by leveraging robust knowledge documents. We conducted experiments using several datasets, e.g., RSSDIVCS, RSVQA, and DOTAv1. The experimental results demonstrate that our RS-Agent delivers outstanding performance in many tasks, i.e., scene classification, visual question answering, and object counting tasks.",
                "authors": "Wenjia Xu, Zijian Yu, Yixu Wang, Jiuniu Wang, Mugen Peng",
                "citations": 5
            },
            {
                "title": "A3VLM: Actionable Articulation-Aware Vision Language Model",
                "abstract": "Vision Language Models (VLMs) have received significant attention in recent years in the robotics community. VLMs are shown to be able to perform complex visual reasoning and scene understanding tasks, which makes them regarded as a potential universal solution for general robotics problems such as manipulation and navigation. However, previous VLMs for robotics such as RT-1, RT-2, and ManipLLM have focused on directly learning robot-centric actions. Such approaches require collecting a significant amount of robot interaction data, which is extremely costly in the real world. Thus, we propose A3VLM, an object-centric, actionable, articulation-aware vision language model. A3VLM focuses on the articulation structure and action affordances of objects. Its representation is robot-agnostic and can be translated into robot actions using simple action primitives. Extensive experiments in both simulation benchmarks and real-world settings demonstrate the effectiveness and stability of A3VLM. We release our code and other materials at https://github.com/changhaonan/A3VLM.",
                "authors": "Siyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Peng Gao, Abdeslam Boularias, Hongsheng Li",
                "citations": 5
            },
            {
                "title": "The Conversation is the Command: Interacting with Real-World Autonomous Robots Through Natural Language",
                "abstract": "In recent years, autonomous agents have surged in real-world environments such as our homes, offices, and public spaces. However, natural human-robot interaction remains a key challenge. In this paper, we introduce an approach that synergistically exploits the capabilities of large language models (LLMs) and multimodal vision-language models (VLMs) to enable humans to interact naturally with autonomous robots through conversational dialogue. We leveraged the LLMs to decode the high-level natural language instructions from humans and abstract them into precise robot actionable commands or queries. Further, we utilised the VLMs to provide a visual and semantic understanding of the robot's task environment. Our results with 99.13% command recognition accuracy and 97.96% commands execution success show that our approach can enhance human-robot interaction in real-world applications. The video demonstrations of this paper can be found at https://osf.io/wzyf6 and the code is available at our GitHub repository.",
                "authors": "Linus Nwankwo, Elmar Rueckert",
                "citations": 5
            },
            {
                "title": "Efficiency in Focus: LayerNorm as a Catalyst for Fine-tuning Medical Visual Language Pre-trained Models",
                "abstract": "In the realm of Medical Visual Language Models (Med-VLMs), the quest for universal efficient fine-tuning mechanisms remains paramount, especially given researchers in interdisciplinary fields are often extremely short of training resources, yet largely unexplored. Given the unique challenges in the medical domain, such as limited data scope and significant domain-specific requirements, evaluating and adapting Parameter-Efficient Fine-Tuning (PEFT) methods specifically for Med-VLMs is essential. Most of the current PEFT methods on Med-VLMs have yet to be comprehensively investigated but mainly focus on adding some components to the model's structure or input. However, fine-tuning intrinsic model components often yields better generality and consistency, and its impact on the ultimate performance of Med-VLMs has been widely overlooked and remains understudied. In this paper, we endeavour to explore an alternative to traditional PEFT methods, especially the impact of fine-tuning LayerNorm layers, FFNs and Attention layers on the Med-VLMs. Our comprehensive studies span both small-scale and large-scale Med-VLMs, evaluating their performance under various fine-tuning paradigms across tasks such as Medical Visual Question Answering and Medical Imaging Report Generation. The findings reveal unique insights into the effects of intrinsic parameter fine-tuning methods on fine-tuning Med-VLMs to downstream tasks and expose fine-tuning solely the LayerNorm layers not only surpasses the efficiency of traditional PEFT methods but also retains the model's accuracy and generalization capabilities across a spectrum of medical downstream tasks. The experiments show LayerNorm fine-tuning's superior adaptability and scalability, particularly in the context of large-scale Med-VLMs.",
                "authors": "Jiawei Chen, Dingkang Yang, Yue Jiang, Mingcheng Li, Jinjie Wei, Xiaolu Hou, Lihua Zhang",
                "citations": 5
            },
            {
                "title": "LRQ-Fact: LLM-Generated Relevant Questions for Multimodal Fact-Checking",
                "abstract": "Human fact-checkers have specialized domain knowledge that allows them to formulate precise questions to verify information accuracy. However, this expert-driven approach is labor-intensive and is not scalable, especially when dealing with complex multimodal misinformation. In this paper, we propose a fully-automated framework, LRQ-Fact, for multimodal fact-checking. Firstly, the framework leverages Vision-Language Models (VLMs) and Large Language Models (LLMs) to generate comprehensive questions and answers for probing multimodal content. Next, a rule-based decision-maker module evaluates both the original content and the generated questions and answers to assess the overall veracity. Extensive experiments on two benchmarks show that LRQ-Fact improves detection accuracy for multimodal misinformation. Moreover, we evaluate its generalizability across different model backbones, offering valuable insights for further refinement.",
                "authors": "Alimohammad Beigi, Bohan Jiang, Dawei Li, Tharindu Kumarage, Zhen Tan, Pouya Shaeri, Huan Liu",
                "citations": 5
            },
            {
                "title": "ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty",
                "abstract": "Compositionality is a critical capability in Text-to-Image (T2I) models, as it reflects their ability to understand and combine multiple concepts from text descriptions. Existing evaluations of compositional capability rely heavily on human-designed text prompts or fixed templates, limiting their diversity and complexity, and yielding low discriminative power. We propose ConceptMix, a scalable, controllable, and customizable benchmark which automatically evaluates compositional generation ability of T2I models. This is done in two stages. First, ConceptMix generates the text prompts: concretely, using categories of visual concepts (e.g., objects, colors, shapes, spatial relationships), it randomly samples an object and k-tuples of visual concepts, then uses GPT4-o to generate text prompts for image generation based on these sampled concepts. Second, ConceptMix evaluates the images generated in response to these prompts: concretely, it checks how many of the k concepts actually appeared in the image by generating one question per visual concept and using a strong VLM to answer them. Through administering ConceptMix to a diverse set of T2I models (proprietary as well as open ones) using increasing values of k, we show that our ConceptMix has higher discrimination power than earlier benchmarks. Specifically, ConceptMix reveals that the performance of several models, especially open models, drops dramatically with increased k. Importantly, it also provides insight into the lack of prompt diversity in widely-used training datasets. Additionally, we conduct extensive human studies to validate the design of ConceptMix and compare our automatic grading with human judgement. We hope it will guide future T2I model development.",
                "authors": "Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, Sanjeev Arora",
                "citations": 5
            },
            {
                "title": "OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments",
                "abstract": "Environment representations endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks. Open-vocabulary representation, powered by Visual-Language models (VLMs), possesses inherent advantages, including zero-shot learning and open-set cognition. However, existing open-vocabulary maps are primarily designed for small-scale environments, such as desktops or rooms, and are typically geared towards limited-area tasks involving robotic indoor navigation or in-place manipulation. They face challenges in direct generalization to outdoor environments characterized by numerous objects and complex tasks, owing to limitations in both understanding level and map structure. In this work, we propose OpenGraph, a novel open-vocabulary hierarchical graph representation designed for large-scale outdoor environments. OpenGraph initially extracts instances and their captions from visual images, enhancing textual reasoning by encoding captions. Subsequently, it achieves 3D incremental object-centric mapping with feature embedding by projecting images onto LiDAR point clouds. Finally, the environment is segmented based on lane graph connectivity to construct a hierarchical representation. Validation results from SemanticKITTI and real-world scene demonstrate that OpenGraph achieves high segmentation and query accuracy.",
                "authors": "Yinan Deng, Jiahui Wang, Jingyu Zhao, Xinyu Tian, Guangyan Chen, Yi Yang, Yufeng Yue",
                "citations": 5
            },
            {
                "title": "VoxAct-B: Voxel-Based Acting and Stabilizing Policy for Bimanual Manipulation",
                "abstract": "Bimanual manipulation is critical to many robotics applications. In contrast to single-arm manipulation, bimanual manipulation tasks are challenging due to higher-dimensional action spaces. Prior works leverage large amounts of data and primitive actions to address this problem, but may suffer from sample inefficiency and limited generalization across various tasks. To this end, we propose VoxAct-B, a language-conditioned, voxel-based method that leverages Vision Language Models (VLMs) to prioritize key regions within the scene and reconstruct a voxel grid. We provide this voxel grid to our bimanual manipulation policy to learn acting and stabilizing actions. This approach enables more efficient policy learning from voxels and is generalizable to different tasks. In simulation, we show that VoxAct-B outperforms strong baselines on fine-grained bimanual manipulation tasks. Furthermore, we demonstrate VoxAct-B on real-world $\\texttt{Open Drawer}$ and $\\texttt{Open Jar}$ tasks using two UR5s. Code, data, and videos are available at https://voxact-b.github.io.",
                "authors": "I-Chun Arthur Liu, Sicheng He, Daniel Seita, Gaurav Sukhatme",
                "citations": 5
            },
            {
                "title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments",
                "abstract": "High-resolution Vision-Language Models (VLMs) are widely used in multimodal tasks to enhance accuracy by preserving detailed image information. However, these models often generate an excessive number of visual tokens due to the need to encode multiple partitions of a high-resolution image input. Processing such a large number of visual tokens through multiple transformer networks poses significant computational challenges, particularly for resource-constrained commodity GPUs. To address this challenge, we propose High-Resolution Early Dropping (HiRED), a plug-and-play token-dropping method designed to operate within a fixed token budget. HiRED leverages the attention of CLS token in the vision transformer (ViT) to assess the visual content of the image partitions and allocate an optimal token budget for each partition accordingly. The most informative visual tokens from each partition within the allocated budget are then selected and passed to the subsequent Large Language Model (LLM). We showed that HiRED achieves superior accuracy and performance, compared to existing token-dropping methods. Empirically, HiRED-20% (i.e., a 20% token budget) on LLaVA-Next-7B achieves a 4.7x increase in token generation throughput, reduces response latency by 78%, and saves 14% of GPU memory for single inference on an NVIDIA TESLA P40 (24 GB). For larger batch sizes (e.g., 4), HiRED-20% prevents out-of-memory errors by cutting memory usage by 30%, while preserving throughput and latency benefits. Code - https://github.com/hasanar1f/HiRED",
                "authors": "Kazi Hasan Ibn Arif, JinYi Yoon, Dimitrios S. Nikolopoulos, Hans Vandierendonck, Deepu John, Bo Jin",
                "citations": 5
            },
            {
                "title": "GraspSplats: Efficient Manipulation with 3D Feature Splatting",
                "abstract": "The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.",
                "authors": "Mazeyu Ji, Ri-Zhao Qiu, Xueyan Zou, Xiaolong Wang",
                "citations": 5
            },
            {
                "title": "Recent Advances of Foundation Language Models-based Continual Learning: A Survey",
                "abstract": "Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing (NLP) and computer vision (CV). Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich commonsense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. Despite these capabilities, LMs still struggle with catastrophic forgetting, hindering their ability to learn continuously like humans. To address this, continual learning (CL) methodologies have been introduced, allowing LMs to adapt to new tasks while retaining learned knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking. In this paper, we delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models (PLMs), large language models (LLMs) and vision-language models (VLMs). We divide these studies into offline and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning.",
                "authors": "Yutao Yang, Jie Zhou, Xuanwen Ding, Tianyu Huai, Shunyu Liu, Qin Chen, Liang He, Yuan Xie",
                "citations": 5
            },
            {
                "title": "OmniCLIP: Adapting CLIP for Video Recognition with Spatial-Temporal Omni-Scale Feature Learning",
                "abstract": "Recent Vision-Language Models (VLMs) \\textit{e.g.} CLIP have made great progress in video recognition. Despite the improvement brought by the strong visual backbone in extracting spatial features, CLIP still falls short in capturing and integrating spatial-temporal features which is essential for video recognition. In this paper, we propose OmniCLIP, a framework that adapts CLIP for video recognition by focusing on learning comprehensive features encompassing spatial, temporal, and dynamic spatial-temporal scales, which we refer to as omni-scale features. This is achieved through the design of spatial-temporal blocks that include parallel temporal adapters (PTA), enabling efficient temporal modeling. Additionally, we introduce a self-prompt generator (SPG) module to capture dynamic object spatial features. The synergy between PTA and SPG allows OmniCLIP to discern varying spatial information across frames and assess object scales over time. We have conducted extensive experiments in supervised video recognition, few-shot video recognition, and zero-shot recognition tasks. The results demonstrate the effectiveness of our method, especially with OmniCLIP achieving a top-1 accuracy of 74.30\\% on HMDB51 in a 16-shot setting, surpassing the recent MotionPrompt approach even with full training data. The code is available at \\url{https://github.com/XiaoBuL/OmniCLIP}.",
                "authors": "Mushui Liu, Bozheng Li, Yunlong Yu",
                "citations": 4
            },
            {
                "title": "DEAL: Disentangle and Localize Concept-level Explanations for VLMs",
                "abstract": "Large pre-trained Vision-Language Models (VLMs) have become ubiquitous foundational components of other models and downstream tasks. Although powerful, our empirical results reveal that such models might not be able to identify fine-grained concepts. Specifically, the explanations of VLMs with respect to fine-grained concepts are entangled and mislocalized. To address this issue, we propose to DisEntAngle and Localize (DEAL) the concept-level explanations for VLMs without human annotations. The key idea is encouraging the concept-level explanations to be distinct while maintaining consistency with category-level explanations. We conduct extensive experiments and ablation studies on a wide range of benchmark datasets and vision-language models. Our empirical results demonstrate that the proposed method significantly improves the concept-level explanations of the model in terms of disentanglability and localizability. Surprisingly, the improved explainability alleviates the model's reliance on spurious correlations, which further benefits the prediction accuracy.",
                "authors": "Tang Li, Mengmeng Ma, Xi Peng",
                "citations": 4
            },
            {
                "title": "BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models",
                "abstract": "Vision language models (VLMs) perceive the world through a combination of a visual encoder and a large language model (LLM). The visual encoder, pre-trained on large-scale vision-text datasets, provides zero-shot generalization to visual data, and the LLM endows its high reasoning ability to VLMs. It leads VLMs to achieve high performance on wide benchmarks without fine-tuning, exhibiting zero or few-shot capability. However, recent studies show that VLMs are vulnerable to hallucination. This undesirable behavior degrades reliability and credibility, thereby making users unable to fully trust the output from VLMs. To enhance trustworthiness and better tackle the hallucination of VLMs, we curate a new evaluation dataset, called the BEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID). Unlike prior works that focus only on constructing questions and answers, the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes. This allows us to clearly assess whether VLMs correctly understand a given scene by observing the ability to perceive changes. We also visualize image-wise object relationship by virtue of our two-axis view: vision and text. Upon evaluating VLMs with our dataset, we observed that our metrics reveal different aspects of VLM hallucination that have not been reported before. Project page: \\url{https://beafbench.github.io/}",
                "authors": "Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, Tae-Hyun Oh",
                "citations": 4
            },
            {
                "title": "Retrieval-Augmented Open-Vocabulary Object Detection",
                "abstract": "Open-vocabulary object detection (OVD) has been stud-ied with Vision-Language Models (VLMs) to detect novel objects beyond the pre-trained categories. Previous ap-proaches improve the generalization ability to expand the knowledge of the detector, using ‘positive’ pseudo-labels with additional ‘class' names, e.g., sock, iPod, and alli-gator. To extend the previous methods in two aspects, we propose Retrieval-Augmented Losses and visual Features (RALF). Our method retrieves related ‘negative’ classes and augments loss functions. Also, visual features are aug-mented with ‘verbalized concepts' of classes, e.g., worn on the feet, handheld music player, and sharp teeth. Specif-ically, RALF consists of two modules: Retrieval Aug-mented Losses (RAL) and Retrieval-Augmented visual Fea-tures (RAF). RAL constitutes two losses reflecting the se-mantic similarity with negative vocabularies. In addition, RAF augments visual features with the verbalized con-cepts from a large language model (LLM). Our experiments demonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets. We achieve improvement up to 3.4 box APN50 on novel categories of the COCO dataset and 3.6 mask APr gains on the LVIS dataset. Code is available at https://github.com/mlvlab/RALF.",
                "authors": "Jooyeon Kim, Eulrang Cho, Sehyung Kim, Hyunwoo J. Kim",
                "citations": 4
            },
            {
                "title": "MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs",
                "abstract": "While Vision-Language Models (VLMs) hold promise for tasks requiring extensive collaboration, traditional multi-agent simulators have facilitated rich explorations of an interactive artificial society that reflects collective behavior. However, these existing simulators face significant limitations. Firstly, they struggle with handling large numbers of agents due to high resource demands. Secondly, they often assume agents possess perfect information and limitless capabilities, hindering the ecological validity of simulated social interactions. To bridge this gap, we propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing three key features: large-scale scalability, limited multimodal senses, and physical needs. Our simulator supports 64 or more agents. Agents have limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. Additionally, we further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior.The source code of MineLand and Alex is openly available at https://github.com/cocacola-lab/MineLand.",
                "authors": "Xianhao Yu, Jiaqi Fu, Renjia Deng, Wenjuan Han",
                "citations": 4
            },
            {
                "title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts",
                "abstract": "Do vision-language models (VLMs) pre-trained to caption an image of a\"durian\"learn visual concepts such as\"brown\"(color) and\"spiky\"(texture) at the same time? We aim to answer this question as visual concepts learned\"for free\"would enable wide applications such as neuro-symbolic reasoning or human-interpretable object classification. We assume that the visual concepts, if captured by pre-trained VLMs, can be extracted by their vision-language interface with text-based concept prompts. We observe that recent works prompting VLMs with concepts often differ in their strategies to define and evaluate the visual concepts, leading to conflicting conclusions. We propose a new concept definition strategy based on two observations: First, certain concept prompts include shortcuts that recognize correct concepts for wrong reasons; Second, multimodal information (e.g. visual discriminativeness, and textual knowledge) should be leveraged when selecting the concepts. Our proposed concept discovery and learning (CDL) framework is thus designed to identify a diverse list of generic visual concepts (e.g.\"spiky\"as opposed to\"spiky durian\"), which are ranked and selected based on visual and language mutual information. We carefully design quantitative and human evaluations of the discovered concepts on six diverse visual recognition datasets, which confirm that pre-trained VLMs do learn visual concepts that provide accurate and thorough descriptions for the recognized objects. All code and models are publicly released.",
                "authors": "Yuan Zang, Tian Yun, Hao Tan, Trung Bui, Chen Sun",
                "citations": 4
            },
            {
                "title": "SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations",
                "abstract": "Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly in object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We show that all the models which achieve better performance on compositionality datasets need not perform equally well on SUGARCREPE++, signifying that compositionality alone may not be sufficient for understanding semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community.",
                "authors": "Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, E. Milios, Sageev Oore, Hassan Sajjad",
                "citations": 4
            },
            {
                "title": "Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment",
                "abstract": "Existing image-text modality alignment in Vision Language Models (VLMs) treats each text token equally in an autoregressive manner. Despite being simple and effective, this method results in sub-optimal cross-modal alignment by over-emphasizing the text tokens that are less correlated with or even contradictory with the input images. In this paper, we advocate for assigning distinct contributions for each text token based on its visual correlation. Specifically, we present by contrasting image inputs, the difference in prediction logits on each text token provides strong guidance of visual correlation. We therefore introduce Contrastive ALignment (CAL), a simple yet effective re-weighting strategy that prioritizes training visually correlated tokens. Our experimental results demonstrate that CAL consistently improves different types of VLMs across different resolutions and model sizes on various benchmark datasets. Importantly, our method incurs minimal additional computational overhead, rendering it highly efficient compared to alternative data scaling strategies. Codes are available at https://github.com/foundation-multimodal-models/CAL.",
                "authors": "Xin Xiao, Bohong Wu, Jiacong Wang, Chunyuan Li, Xun Zhou, Haoyuan Guo",
                "citations": 4
            },
            {
                "title": "OmniCLIP: Adapting CLIP for Video Recognition with Spatial-Temporal Omni-Scale Feature Learning",
                "abstract": "Recent Vision-Language Models (VLMs) \\textit{e.g.} CLIP have made great progress in video recognition. Despite the improvement brought by the strong visual backbone in extracting spatial features, CLIP still falls short in capturing and integrating spatial-temporal features which is essential for video recognition. In this paper, we propose OmniCLIP, a framework that adapts CLIP for video recognition by focusing on learning comprehensive features encompassing spatial, temporal, and dynamic spatial-temporal scales, which we refer to as omni-scale features. This is achieved through the design of spatial-temporal blocks that include parallel temporal adapters (PTA), enabling efficient temporal modeling. Additionally, we introduce a self-prompt generator (SPG) module to capture dynamic object spatial features. The synergy between PTA and SPG allows OmniCLIP to discern varying spatial information across frames and assess object scales over time. We have conducted extensive experiments in supervised video recognition, few-shot video recognition, and zero-shot recognition tasks. The results demonstrate the effectiveness of our method, especially with OmniCLIP achieving a top-1 accuracy of 74.30\\% on HMDB51 in a 16-shot setting, surpassing the recent MotionPrompt approach even with full training data. The code is available at \\url{https://github.com/XiaoBuL/OmniCLIP}.",
                "authors": "Mushui Liu, Bozheng Li, Yunlong Yu",
                "citations": 4
            },
            {
                "title": "Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning",
                "abstract": "Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, we aim to develop a generic anomaly detection model applicable across multiple scenarios. To achieve this, we customize generic visual-language foundation models that possess extensive knowledge and robust reasoning abilities into anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers multi-modal prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling multi-modal anomaly detection and reasoning. Our preliminary studies demonstrate that combining visual and language prompts as conditions for customizing the models enhances anomaly detection performance. The customized models showcase the ability to detect anomalies across different data modalities such as images and point clouds. Qualitative case studies further highlight the anomaly detection and reasoning capabilities, particularly for multi-object scenes and temporal data. Our code is available at https://github.com/Xiaohao-Xu/Customizable-VLM.",
                "authors": "Xiaohao Xu, Yunkang Cao, Yongqi Chen, Weiming Shen, Xiaonan Huang",
                "citations": 4
            },
            {
                "title": "Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning",
                "abstract": "We propose a generalized method for boosting the generalization ability of pre-trained vision-language models (VLMs) while fine-tuning on downstream few-shot tasks. The idea is realized by exploiting out-of-distribution (OOD) detection to predict whether a sample belongs to a base distribution or a novel distribution and then using the score generated by a dedicated competition based scoring function to fuse the zero-shot and few-shot classifier. The fused classifier is dynamic, which will bias towards the zero-shot classifier if a sample is more likely from the distribution pre-trained on, leading to improved base-to-novel generalization ability. Our method is performed only in test stage, which is applicable to boost existing methods without time-consuming re-training. Extensive experiments show that even weak distribution detectors can still improve VLMs' generalization ability. Specifically, with the help of OOD detectors, the harmonic mean of CoOp and ProGrad increase by 2.6 and 1.5 percentage points over 11 recognition datasets in the base-to-novel setting.",
                "authors": "Kun Ding, Haojian Zhang, Qiang Yu, Ying Wang, Shiming Xiang, Chunhong Pan",
                "citations": 4
            },
            {
                "title": "Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model",
                "abstract": "Large Vision-Language Models (LVLMs) rely on vision encoders and Large Language Models (LLMs) to exhibit remarkable capabilities on various multi-modal tasks in the joint space of vision and language. However, typographic attacks, which disrupt Vision-Language Models (VLMs) such as Contrastive Language-Image Pretraining (CLIP), have also been expected to be a security threat to LVLMs. Firstly, we verify typographic attacks on current well-known commercial and open-source LVLMs and uncover the widespread existence of this threat. Secondly, to better assess this vulnerability, we propose the most comprehensive and largest-scale Typographic Dataset to date. The Typographic Dataset not only considers the evaluation of typographic attacks under various multi-modal tasks but also evaluates the effects of typographic attacks, influenced by texts generated with diverse factors. Based on the evaluation results, we investigate the causes why typographic attacks impacting VLMs and LVLMs, leading to three highly insightful discoveries. During the process of further validating the rationality of our discoveries, we can reduce the performance degradation caused by typographic attacks from 42.07\\% to 13.90\\%. Code and Dataset are available in \\href{https://github.com/ChaduCheng/TypoDeceptions}",
                "authors": "Hao Cheng, Erjia Xiao, Jindong Gu, Le Yang, Jinhao Duan, Jize Zhang, Jiahang Cao, Kaidi Xu, Renjing Xu",
                "citations": 4
            },
            {
                "title": "Learning to Learn Better Visual Prompts",
                "abstract": "Prompt tuning provides a low-cost way of adapting vision-language models (VLMs) for various downstream vision tasks without requiring updating the huge pre-trained parameters. Dispensing with the conventional manual crafting of prompts, the recent prompt tuning method of Context Optimization (CoOp) introduces adaptable vectors as text prompts. Nevertheless, several previous works point out that the CoOp-based approaches are easy to overfit to the base classes and hard to generalize to novel classes. In this paper, we reckon that the prompt tuning works well only in the base classes because of the limited capacity of the adaptable vectors. The scale of the pre-trained model is hundreds times the scale of the adaptable vector, thus the learned vector has a very limited ability to absorb the knowledge of novel classes. To minimize this excessive overfitting of textual knowledge on the base class, we view prompt tuning as learning to learn (LoL) and learn the prompt in the way of meta-learning, the training manner of dividing the base classes into many different subclasses could fully exert the limited capacity of prompt tuning and thus transfer it power to recognize the novel classes. To be specific, we initially perform fine-tuning on the base class based on the CoOp method for pre-trained CLIP. Subsequently, predicated on the fine-tuned CLIP model, we carry out further fine-tuning in an N-way K-shot manner from the perspective of meta-learning on the base classes. We finally apply the learned textual vector and VLM for unseen classes.Extensive experiments on benchmark datasets validate the efficacy of our meta-learning-informed prompt tuning, affirming its role as a robust optimization strategy for VLMs.",
                "authors": "Fengxiang Wang, Wanrong Huang, Shaowu Yang, Qi Fan, Long Lan",
                "citations": 4
            },
            {
                "title": "The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models",
                "abstract": "‘Scale the model, scale the data, scale the GPU farms’ is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored. This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks. In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe. Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased. Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models. Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples. We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward. All the meta-datasets curated in this endeavor and the code used are shared at: https://github.com/SepehrDehdashtian/the-dark-side-of-dataset-scaling. Content warning: This article contains racially dehumanising and offensive descriptions.",
                "authors": "Abeba Birhane, Sepehr Dehdashtian, Vinay Prabhu, Vishnu Naresh Boddeti",
                "citations": 4
            },
            {
                "title": "Affordance-Guided Reinforcement Learning via Visual Prompting",
                "abstract": "Robots equipped with reinforcement learning (RL) have the potential to learn a wide range of skills solely from a reward signal. However, obtaining a robust and dense reward signal for general manipulation tasks remains a challenge. Existing learning-based approaches require significant data, such as human demonstrations of success and failure, to learn task-specific reward functions. Recently, there is also a growing adoption of large multi-modal foundation models for robotics that can perform visual reasoning in physical contexts and generate coarse robot motions for manipulation tasks. Motivated by this range of capability, in this work, we present Keypoint-based Affordance Guidance for Improvements (KAGI), a method leveraging rewards shaped by vision-language models (VLMs) for autonomous RL. State-of-the-art VLMs have demonstrated impressive reasoning about affordances through keypoints in zero-shot, and we use these to define dense rewards that guide autonomous robotic learning. On real-world manipulation tasks specified by natural language descriptions, KAGI improves the sample efficiency of autonomous RL and enables successful task completion in 20K online fine-tuning steps. Additionally, we demonstrate the robustness of KAGI to reductions in the number of in-domain demonstrations used for pre-training, reaching similar performance in 35K online fine-tuning steps. Project website: https://sites.google.com/view/affordance-guided-rl",
                "authors": "Olivia Y. Lee, Annie Xie, Kuan Fang, Karl Pertsch, Chelsea Finn",
                "citations": 4
            },
            {
                "title": "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture",
                "abstract": "Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision–language Models (VLMs) and large language models (LLMs) on newly collected, unseen food images and corresponding questions. FoodieQA comprises three multiple-choice question-answering tasks where models need to answer questions based on multiple images, a single image, and text-only descriptions, respectively. While LLMs excel at text-based question answering, surpassing human accuracy, the open-sourced VLMs still fall short by 41% on multi-image and 21% on single-image VQA tasks, although closed-weights models perform closer to human levels (within 10%). Our findings highlight that understanding food and its cultural implications remains a challenging and under-explored direction.",
                "authors": "Wenyan Li, Xinyu Zhang, Jiaang Li, Qiwei Peng, Raphael Tang, Li Zhou, Weijia Zhang, Guimin Hu, Yifei Yuan, Anders Sogaard, Daniel Hershcovich, Desmond Elliott",
                "citations": 4
            },
            {
                "title": "GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs",
                "abstract": "The ability to understand and reason about spatial relationships between objects in images is an important component of visual reasoning. This skill rests on the ability to recognize and localize objects of interest and determine their spatial relation. Early vision and language models (VLMs) have been shown to struggle to recognize spatial relations. We extend the previously released What'sUp dataset and propose a novel comprehensive evaluation for spatial relationship understanding that highlights the strengths and weaknesses of 27 different models. In addition to the VLMs evaluated in What'sUp, our extensive evaluation encompasses 3 classes of Multimodal LLMs (MLLMs) that vary in their parameter sizes (ranging from 7B to 110B), training/instruction-tuning methods, and visual resolution to benchmark their performances and scrutinize the scaling laws in this task.",
                "authors": "Navid Rajabi, J. Kosecka",
                "citations": 4
            },
            {
                "title": "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis",
                "abstract": "We explore multi-step reasoning in vision-language models (VLMs). The problem is challenging, as reasoning data consisting of multiple steps of visual and language processing are barely available. To overcome the challenge, we first introduce a least-to-most visual reasoning paradigm, which interleaves steps of decomposing a question into sub-questions and invoking external tools for resolving sub-questions. Based on the paradigm, we further propose a novel data synthesis approach that can automatically create questions and multi-step reasoning paths for an image in a bottom-up manner. Our approach divides the complex synthesis task into a few simple sub-tasks, and (almost entirely) relies on open-sourced models to accomplish the sub-tasks. Therefore, the entire synthesis process is reproducible and cost-efficient, and the synthesized data is quality guaranteed. With the approach, we construct 50k visual reasoning examples. Then, we develop a visual reasoner through supervised fine-tuning, which is capable of generally enhancing the reasoning abilities of a wide range of existing VLMs in a plug-and-play fashion. Extensive experiments indicate that the visual reasoner can consistently and significantly improve four VLMs on four VQA benchmarks.",
                "authors": "Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan",
                "citations": 4
            },
            {
                "title": "LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models",
                "abstract": "Out-of-distribution (OOD) detection is crucial for model reliability, as it identifies samples from unknown classes and reduces errors due to unexpected inputs. Vision-Language Models (VLMs) such as CLIP are emerging as powerful tools for OOD detection by integrating multi-modal information. However, the practical application of such systems is challenged by manual prompt engineering, which demands domain expertise and is sensitive to linguistic nuances. In this paper, we introduce Label-driven Automated Prompt Tuning (LAPT), a novel approach to OOD detection that reduces the need for manual prompt engineering. We develop distribution-aware prompts with in-distribution (ID) class names and negative labels mined automatically. Training samples linked to these class labels are collected autonomously via image synthesis and retrieval methods, allowing for prompt learning without manual effort. We utilize a simple cross-entropy loss for prompt optimization, with cross-modal and cross-distribution mixing strategies to reduce image noise and explore the intermediate space between distributions, respectively. The LAPT framework operates autonomously, requiring only ID class names as input and eliminating the need for manual intervention. With extensive experiments, LAPT consistently outperforms manually crafted prompts, setting a new standard for OOD detection. Moreover, LAPT not only enhances the distinction between ID and OOD samples, but also improves the ID classification accuracy and strengthens the generalization robustness to covariate shifts, resulting in outstanding performance in challenging full-spectrum OOD detection tasks. Codes are available at \\url{https://github.com/YBZh/LAPT}.",
                "authors": "Yabin Zhang, Wen-Qing Zhu, Chenhang He, Lei Zhang",
                "citations": 4
            },
            {
                "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
                "abstract": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html",
                "authors": "Michael S. Ryoo, Honglu Zhou, Shrikant B. Kendre, Can Qin, Le Xue, Manli Shu, Silvio Savarese, Ran Xu, Caiming Xiong, Juan Carlos Niebles",
                "citations": 4
            },
            {
                "title": "Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models",
                "abstract": "Online user generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment. However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents. Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users. This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content. In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs. We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators. Our in-depth studies reveal a new understanding of this problem and the urgent need for automatically flagging illicit UGCG promotions. We additionally create a cutting-edge system, UGCG-Guard, designed to aid social media platforms in effectively identifying images used for illicit UGCG promotions. This system leverages recently introduced large vision-language models (VLMs) and employs a novel conditional prompting strategy for zero-shot domain adaptation, along with chain-of-thought (CoT) reasoning for contextual identification. UGCG-Guard achieves outstanding results, with an accuracy rate of 94% in detecting these images used for the illicit promotion of such games in real-world scenarios.",
                "authors": "Keyan Guo, Ayush Utkarsh, Wenbo Ding, Isabelle Ondracek, Ziming Zhao, Guo Freeman, Nishant Vishwamitra, Hongxin Hu",
                "citations": 4
            },
            {
                "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
                "abstract": "Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.",
                "authors": "Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, Yu Qiao",
                "citations": 3
            },
            {
                "title": "XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization",
                "abstract": "Utilizing potent representations of the large vision-language models (VLMs) to accomplish various downstream tasks has attracted increasing attention. Within this research field, soft prompt learning has become a representative approach for efficiently adapting VLMs such as CLIP, to tasks like image classification. However, most existing prompt learning methods learn text tokens that are unexplainable, which cannot satisfy the stringent interpretability requirements of Explainable Artificial Intelligence (XAI) in high-stakes scenarios like healthcare. To address this issue, we propose a novel explainable prompt learning framework that leverages medical knowledge by aligning the semantics of images, learnable prompts, and clinical concept-driven prompts at multiple granularities. Moreover, our framework addresses the lack of valuable concept annotations by eliciting knowledge from large language models and offers both visual and textual explanations for the prompts. Extensive experiments and explainability analyses conducted on various datasets, with and without concept labels, demonstrate that our method simultaneously achieves superior diagnostic performance, flexibility, and interpretability, shedding light on the effectiveness of foundation models in facilitating XAI. The code will be made publically available.",
                "authors": "Yequan Bie, Luyang Luo, Zhixuan Chen, Hao Chen",
                "citations": 3
            },
            {
                "title": "No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models",
                "abstract": "We study cultural and socioeconomic diversity in contrastive vision-language models (VLMs). Using a broad range of benchmark datasets and evaluation metrics, we bring to attention several important findings. First, the common filtering of training data to English image-text pairs disadvantages communities of lower socioeconomic status and negatively impacts cultural understanding. Notably, this performance gap is not captured by - and even at odds with - the currently popular evaluation metrics derived from the Western-centric ImageNet and COCO datasets. Second, pretraining with global, unfiltered data before fine-tuning on English content can improve cultural understanding without sacrificing performance on said popular benchmarks. Third, we introduce the task of geo-localization as a novel evaluation metric to assess cultural diversity in VLMs. Our work underscores the value of using diverse data to create more inclusive multimodal systems and lays the groundwork for developing VLMs that better represent global perspectives.",
                "authors": "Angeline Pouget, Lucas Beyer, Emanuele Bugliarello, Xiao Wang, A. Steiner, Xiao-Qi Zhai, Ibrahim M. Alabdulmohsin",
                "citations": 3
            },
            {
                "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
                "abstract": "The emergence of Vision Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to produce detailed text descriptions based on visual inputs, yet it introduces new security vulnerabilities. Unlike prior work that centered on single modalities or classification tasks, this study introduces TrojVLM, the first exploration of backdoor attacks aimed at VLMs engaged in complex image-to-text generation. Specifically, TrojVLM inserts predetermined target text into output text when encountering poisoned images. Moreover, a novel semantic preserving loss is proposed to ensure the semantic integrity of the original image content. Our evaluation on image captioning and visual question answering (VQA) tasks confirms the effectiveness of TrojVLM in maintaining original semantic content while triggering specific target text outputs. This study not only uncovers a critical security risk in VLMs and image-to-text generation but also sets a foundation for future research on securing multimodal models against such sophisticated threats.",
                "authors": "Weimin Lyu, Lu Pang, Teng Ma, Haibin Ling, Chao Chen",
                "citations": 3
            },
            {
                "title": "Safety Alignment for Vision Language Models",
                "abstract": "Benefiting from the powerful capabilities of Large Language Models (LLMs), pre-trained visual encoder models connected to an LLMs can realize Vision Language Models (VLMs). However, existing research shows that the visual modality of VLMs is vulnerable, with attackers easily bypassing LLMs' safety alignment through visual modality features to launch attacks. To address this issue, we enhance the existing VLMs' visual modality safety alignment by adding safety modules, including a safety projector, safety tokens, and a safety head, through a two-stage training process, effectively improving the model's defense against risky images. For example, building upon the LLaVA-v1.5 model, we achieve a safety score of 8.26, surpassing the GPT-4V on the Red Teaming Visual Language Models (RTVLM) benchmark. Our method boasts ease of use, high flexibility, and strong controllability, and it enhances safety while having minimal impact on the model's general performance. Moreover, our alignment strategy also uncovers some possible risky content within commonly used open-source multimodal datasets. Our code will be open sourced after the anonymous review.",
                "authors": "Zhendong Liu, Yuanbi Nie, Yingshui Tan, Xiangyu Yue, Qiushi Cui, Chong-Jun Wang, Xiaoyong Zhu, Bo Zheng",
                "citations": 3
            },
            {
                "title": "Revisiting the Adversarial Robustness of Vision Language Models: a Multimodal Perspective",
                "abstract": "Pretrained vision-language models (VLMs) like CLIP exhibit exceptional generalization across diverse downstream tasks. While recent studies reveal their vulnerability to adversarial attacks, research to date has primarily focused on enhancing the robustness of image encoders against image-based attacks, with defenses against text-based and multimodal attacks remaining largely unexplored. To this end, this work presents the first comprehensive study on improving the adversarial robustness of VLMs against attacks targeting image, text, and multimodal inputs. This is achieved by proposing multimodal contrastive adversarial training (MMCoA). Such an approach strengthens the robustness of both image and text encoders by aligning the clean text embeddings with adversarial image embeddings, and adversarial text embeddings with clean image embeddings. The robustness of the proposed MMCoA is examined against existing defense methods over image, text, and multimodal attacks on the CLIP model. Extensive experiments on 15 datasets across two tasks reveal the characteristics of different adversarial defense methods under distinct distribution shifts and dataset complexities across the three attack types. This paves the way for a unified framework of adversarial robustness against different modality attacks, opening up new possibilities for securing VLMs against multimodal attacks. The code is available at https://github.com/ElleZWQ/MMCoA.git.",
                "authors": "Wanqi Zhou, Shuanghao Bai, Qibin Zhao, Badong Chen",
                "citations": 3
            },
            {
                "title": "Label Propagation for Zero-shot Classification with Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) have demonstrated im-pressive performance on zero-shot classification, i.e. classi-fication when provided merely with a list of class names. In this paper, we tackle the case of zero-shot classification in the presence of unlabeled data. We leverage the graph structure of the unlabeled data and introduce ZLaP, a method based on label propagation (LP) that utilizes geodesic distances for classification. We tailor LP to graphs containing both text and image features and further pro-pose an efficient method for performing inductive infer-ence based on a dual solution and a sparsification step. We perform extensive experiments to evaluate the effectiveness of our method on 14 common datasets and show that ZLaP outperforms the latest related works. Code: https://github.com/vladan-stojnic/ZLaP",
                "authors": "Vladan Stojni'c, Yannis Kalantidis, Giorgos Tolias",
                "citations": 3
            },
            {
                "title": "D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions",
                "abstract": "Large vision language models (VLMs) have progressed incredibly from research to applicability for general-purpose use cases. LLaVA-Med, a pioneering large language and vision assistant for biomedicine, can perform multi-modal biomedical image and data analysis to provide a natural language interface for radiologists. While it is highly generalizable and works with multi-modal data, it is currently limited by well-known challenges that exist in the large language model space. Hallucinations and imprecision in responses can lead to misdiagnosis which currently hinder the clinical adaptability of VLMs. To create precise, user-friendly models in healthcare, we propose D-Rax -- a domain-specific, conversational, radiologic assistance tool that can be used to gain insights about a particular radiologic image. In this study, we enhance the conversational analysis of chest X-ray (CXR) images to support radiological reporting, offering comprehensive insights from medical imaging and aiding in the formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the LLaVA-Med architecture on our curated enhanced instruction-following data, comprising of images, instructions, as well as disease diagnosis and demographic predictions derived from MIMIC-CXR imaging data, CXR-related visual question answer (VQA) pairs, and predictive outcomes from multiple expert AI models. We observe statistically significant improvement in responses when evaluated for both open and close-ended conversations. Leveraging the power of state-of-the-art diagnostic models combined with VLMs, D-Rax empowers clinicians to interact with medical images using natural language, which could potentially streamline their decision-making process, enhance diagnostic accuracy, and conserve their time.",
                "authors": "Hareem Nisar, Syed Muhammad Anwar, Zhifan Jiang, Abhijeet Parida, Vishwesh Nath, Holger Roth, M. Linguraru",
                "citations": 3
            },
            {
                "title": "See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding",
                "abstract": "Vision-language models (VLMs) can respond to queries about images in many languages. However, beyond language, culture affects how we see things. For example, individuals from Western cultures focus more on the central figure in an image while individuals from Eastern cultures attend more to scene context. In this work, we present a novel investigation that demonstrates and localizes VLMs' Western bias in image understanding. We evaluate large VLMs across subjective and objective visual tasks with culturally diverse images and annotations. We find that VLMs perform better on the Western subset than the Eastern subset of each task. Controlled experimentation tracing the source of this bias highlights the importance of a diverse language mix in text-only pre-training for building equitable VLMs, even when inference is performed in English. Moreover, while prompting in the language of a target culture can lead to reductions in bias, it is not a substitute for building AI more representative of the world's languages.",
                "authors": "Amith Ananthram, Elias Stengel-Eskin, Carl Vondrick, Mohit Bansal, Kathleen McKeown",
                "citations": 3
            },
            {
                "title": "PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration",
                "abstract": "Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model to generate captions for these images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models.",
                "authors": "Yuxuan Sun, Yunlong Zhang, Yixuan Si, Chenglu Zhu, Zhongyi Shui, Kai Zhang, Jingxiong Li, Xingheng Lyu, Tao Lin, Lin Yang",
                "citations": 3
            },
            {
                "title": "Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?",
                "abstract": "Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes. In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic grounding by\"receiving\"feedback, without requiring in-domain data, fine-tuning, or modifications to the network architectures. We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal. We find that if prompted appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve grounding in internet-scale VLMs. Furthermore, VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, we find that this issue can be mitigated via a binary verification mechanism. Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs' grounding performance, showing grounding accuracy consistently improves using automated feedback across all models in all settings investigated. Overall, our iterative framework improves semantic grounding in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism. The project website is hosted at https://andrewliao11.github.io/vlms_feedback",
                "authors": "Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna",
                "citations": 3
            },
            {
                "title": "A Survey of Robot Intelligence with Large Language Models",
                "abstract": "Since the emergence of ChatGPT, research on large language models (LLMs) has actively progressed across various fields. LLMs, pre-trained on vast text datasets, have exhibited exceptional abilities in understanding natural language and planning tasks. These abilities of LLMs are promising in robotics. In general, traditional supervised learning-based robot intelligence systems have a significant lack of adaptability to dynamically changing environments. However, LLMs help a robot intelligence system to improve its generalization ability in dynamic and complex real-world environments. Indeed, findings from ongoing robotics studies indicate that LLMs can significantly improve robots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs), trained on extensive visual and linguistic data for the vision question answering (VQA) problem, excel at integrating computer vision with natural language processing. VLMs can comprehend visual contexts and execute actions through natural language. They also provide descriptions of scenes in natural language. Several studies have explored the enhancement of robot intelligence using multimodal data, including object recognition and description by VLMs, along with the execution of language-driven commands integrated with visual information. This review paper thoroughly investigates how foundation models such as LLMs and VLMs have been employed to boost robot intelligence. For clarity, the research areas are categorized into five topics: reward design in reinforcement learning, low-level control, high-level planning, manipulation, and scene understanding. This review also summarizes studies that show how foundation models, such as the Eureka model for automating reward function design in reinforcement learning, RT-2 for integrating visual data, language, and robot actions in vision-language-action models, and AutoRT for generating feasible tasks and executing robot behavior policies via LLMs, have improved robot intelligence.",
                "authors": "Hyeongyo Jeong, Haechan Lee, Changwon Kim, Sungtae Shin",
                "citations": 3
            },
            {
                "title": "HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning",
                "abstract": "Hallucination has been a major problem for large language models and remains a critical challenge when it comes to multimodality in which vision-language models (VLMs) have to deal with not just textual but also visual inputs. Despite rapid progress in VLMs, resources for evaluating and addressing multimodal hallucination are limited and mostly focused on evaluation. This work introduces HaloQuest, a novel visual question answering dataset that captures various aspects of multimodal hallucination such as false premises, insufficient contexts, and visual challenges. A novel idea from HaloQuest is to leverage synthetic images, apart from real ones, to enable dataset creation at scale. With over 7.7K examples spanning across a wide variety of categories, HaloQuest was designed to be both a challenging benchmark for VLMs and a fine-tuning dataset for advancing multimodal reasoning. Our experiments reveal that current models struggle with HaloQuest, with all open-source VLMs achieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest significantly reduces hallucination rates while preserving performance on standard reasoning tasks. Our results discover that benchmarking with generated images is highly correlated (r=0.97) with real images. Last but not least, we propose a novel Auto-Eval mechanism that is highly correlated with human raters (r=0.99) for evaluating VLMs. In sum, this work makes concrete strides towards understanding, evaluating, and mitigating hallucination in VLMs, serving as an important step towards more reliable multimodal AI systems in the future.",
                "authors": "Zhecan Wang, Garrett Bingham, Adams Yu, Quoc V. Le, Thang Luong, Golnaz Ghiasi",
                "citations": 3
            },
            {
                "title": "Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions",
                "abstract": "Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments. More videos can be found at https://ut-austin-rpl.github.io/Harmon/.",
                "authors": "Zhenyu Jiang, Yuqi Xie, Jinhan Li, Ye Yuan, Yifeng Zhu, Yuke Zhu",
                "citations": 3
            },
            {
                "title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "abstract": "Vision language models (VLMs) are an exciting emerging class of language models (LMs) that have merged classic LM capabilities with those of image processing systems. However, the ways that these capabilities combine are not always intuitive and warrant direct investigation. One understudied capability in VLMs is visual spatial planning -- the ability to comprehend the spatial arrangements of objects and devise action plans to achieve desired outcomes in visual scenes. In our study, we introduce VSP, a benchmark that 1) evaluates the spatial planning capability in these models in general, and 2) breaks down the visual planning task into finer-grained sub-tasks, including perception and reasoning, and measure the LMs capabilities in these sub-tasks. Our evaluation shows that both open-source and private VLMs fail to generate effective plans for even simple spatial planning tasks. Evaluations on the fine-grained analytical tasks further reveal fundamental deficiencies in the models' visual perception and bottlenecks in reasoning abilities, explaining their worse performance in the general spatial planning tasks. Our work illuminates future directions for improving VLMs' abilities in spatial planning. Our benchmark is publicly available at https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.",
                "authors": "Qiucheng Wu, Handong Zhao, Michael Stephen Saxon, T. Bui, William Yang Wang, Yang Zhang, Shiyu Chang",
                "citations": 3
            },
            {
                "title": "VHELM: A Holistic Evaluation of Vision Language Models",
                "abstract": "Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website (https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.",
                "authors": "Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, Percy Liang",
                "citations": 3
            },
            {
                "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
                "abstract": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.",
                "authors": "Vasily Kostumov, Bulat Nutfullin, Oleg Pilipenko, Eugene Ilyushin",
                "citations": 3
            },
            {
                "title": "Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models",
                "abstract": "Large-scale vision-language models (VLMs) have shown a strong zero-shot generalization capability on unseen-domain data. However, adapting pre-trained VLMs to a sequence of downstream tasks often leads to the forgetting of previously learned knowledge and a reduction in zero-shot classification performance. To tackle this problem, we propose a unique Selective Dual-Teacher Knowledge Transfer framework that leverages the most recent fine-tuned and the original pre-trained VLMs as dual teachers to preserve the previously learned knowledge and zero-shot capabilities, respectively. With only access to an unlabeled reference dataset, our proposed framework performs a selective knowledge distillation mechanism by measuring the feature discrepancy from the dual-teacher VLMs. Consequently, our selective dual-teacher knowledge distillation mitigates catastrophic forgetting of previously learned knowledge while preserving the zero-shot capabilities of pre-trained VLMs. Extensive experiments on benchmark datasets demonstrate that our framework is favorable against state-of-the-art continual learning approaches for preventing catastrophic forgetting and zero-shot degradation. Project page: https://chuyu.org/research/snd",
                "authors": "Yu-Chu Yu, Chi-Pin Huang, Jr-Jen Chen, Kai-Po Chang, Yung-Hsuan Lai, Fu-En Yang, Yu-Chiang Frank Wang",
                "citations": 3
            },
            {
                "title": "Private Attribute Inference from Images with Vision-Language Models",
                "abstract": "As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus. While LLM privacy research has primarily focused on the leakage of model training data, it has recently been shown that LLMs can make accurate privacy-infringing inferences from previously unseen texts. With the rise of vision-language models (VLMs), capable of understanding both images and text, a key question is whether this concern transfers to the previously unexplored domain of benign images posted online. To answer this question, we compile an image dataset with human-annotated labels of the image owner's personal attributes. In order to understand the privacy risks posed by VLMs beyond traditional human attribute recognition, our dataset consists of images where the inferable private attributes do not stem from direct depictions of humans. On this dataset, we evaluate 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger inferential adversaries, establishing an imperative for the development of adequate defenses.",
                "authors": "Batuhan Tömekçe, Mark Vero, Robin Staab, Martin T. Vechev",
                "citations": 3
            },
            {
                "title": "Toward Automatic Relevance Judgment using Vision-Language Models for Image-Text Retrieval Evaluation",
                "abstract": "Vision--Language Models (VLMs) have demonstrated success across diverse applications, yet their potential to assist in relevance judgments remains uncertain. This paper assesses the relevance estimation capabilities of VLMs, including CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc} retrieval task tailored for multimedia content creation in a zero-shot fashion. Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V, encompassing open-source and closed-source visual-instruction-tuned Large Language Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared to human relevance judgments, surpassing the CLIPScore metric. (2) While CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based retrieval systems. (3) GPT-4V's score distribution aligns more closely with human judgments than other models, achieving a Cohen's $\\kappa$ value of around 0.08, which outperforms CLIPScore at approximately -0.096. These findings underscore the potential of LLM-powered VLMs in enhancing relevance judgments.",
                "authors": "Jheng-Hong Yang, Jimmy Lin",
                "citations": 3
            },
            {
                "title": "A Design of Interface for Visual-Impaired People to Access Visual Information from Images Featuring Large Language Models and Visual Language Models",
                "abstract": "We propose a design of interface for visual-impaired People to access visual information from images utilizing Large Language Models(LLMs), Visual Language Models (VLMs), and Segment-Anything. We use Semantic-Segment-Anything to generate the segmentation of semantic objects in images. The segmentation includes two parts: a term set describing the semantic object, and segmented mask which represents the shape of the semantic object. We provide two methods for the visual-impaired user to access the information of the semantic object and its peripheral information in image. In one method, the LLM summarize the term set to create an description. In the other method, the image with the object masked is provided to Visual Language Models which is prompted to respond with a description. In both methods, the mask can be accessed with dot display after processed for the visual-impaired people to access, and the description is prompted to the user in synthesized voice.",
                "authors": "Zhe-Xin Zhang",
                "citations": 3
            },
            {
                "title": "Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models",
                "abstract": "Vision-Language Large Models (VLMs) recently become primary backbone of AI, due to the impressive performance. However, their expensive computation costs, i.e., throughput and delay, impede potentials in the real-world scenarios. To achieve acceleration for VLMs, most existing methods focus on the model perspective: pruning, distillation, quantization, but completely overlook the data-perspective redundancy. To fill the overlook, this paper pioneers the severity of data redundancy, and designs one plug-and-play Turbo module guided by information degree to prune inefficient tokens from visual or textual data. In pursuit of efficiency-performance trade-offs, information degree takes two crucial factors into consideration: mutual redundancy and semantic value. Concretely, the former evaluates data duplication between sequential tokens; while the latter evaluates each token by its contribution to the overall semantics. As a result, tokens with high information degree carry less redundancy and stronger semantics. For VLMs' calculation, Turbo works as a user-friendly plug-in that sorts data referring to information degree, utilizing only top-level ones to save costs. Its advantages are multifaceted, e.g., being generally compatible to various VLMs across understanding and generation, simple use without re-training and trivial engineering efforts. On multiple VLMs benchmarks, we fully experiment to demonstrate the good acceleration of Turbo, under negligible performance drop.",
                "authors": "Chen Ju, Haicheng Wang, Haozhe Cheng, Xu Chen, Zhonghua Zhai, Weilin Huang, Jinsong Lan, Shuai Xiao, Bo Zheng",
                "citations": 3
            },
            {
                "title": "VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models",
                "abstract": "Visual Language Models (VLMs) have rapidly progressed with the recent success of large language models. However, there have been few attempts to incorporate efficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In this study, we introduce VisualRWKV, the first application of a linear RNN model to multimodal learning tasks, leveraging the pre-trained RWKV language model. We propose a data-dependent recurrence and sandwich prompts to enhance our modeling capabilities, along with a 2D image scanning mechanism to enrich the processing of visual sequences. Extensive experiments demonstrate that VisualRWKV achieves competitive performance compared to Transformer-based models like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV has a speed advantage of 3.98 times and can save 54% of GPU memory when reaching an inference length of 24K tokens. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at the following GitHub repository: see https://github.com/howard-hou/VisualRWKV.",
                "authors": "Haowen Hou, Peigen Zeng, Fei Ma, F. Yu",
                "citations": 3
            },
            {
                "title": "AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation",
                "abstract": "Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks. However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes. To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space. AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module. We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT's effectiveness and adaptability across different VLMs, architectures, and scales.",
                "authors": "Yuhan Zhu, Yuyang Ji, Zhiyu Zhao, Gangshan Wu, Limin Wang",
                "citations": 3
            },
            {
                "title": "Wolf: Captioning Everything with a World Summarization Framework",
                "abstract": "We propose Wolf, a WOrLd summarization Framework for accurate video captioning. Wolf is an automated captioning framework that adopts a mixture-of-experts approach, leveraging complementary strengths of Vision Language Models (VLMs). By utilizing both image and video models, our framework captures different levels of information and summarizes them efficiently. Our approach can be applied to enhance video understanding, auto-labeling, and captioning. To evaluate caption quality, we introduce CapScore, an LLM-based metric to assess the similarity and quality of generated captions compared to the ground truth captions. We further build four human-annotated datasets in three domains: autonomous driving, general scenes, and robotics, to facilitate comprehensive comparisons. We show that Wolf achieves superior captioning performance compared to state-of-the-art approaches from the research community (VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For instance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise by 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally, we establish a benchmark for video captioning and introduce a leaderboard, aiming to accelerate advancements in video understanding, captioning, and data alignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.",
                "authors": "Boyi Li, Ligeng Zhu, Ran Tian, Shuhan Tan, Yuxiao Chen, Yao Lu, Yin Cui, Sushant Veer, Max Ehrlich, Jonah Philion, Xinshuo Weng, Fuzhao Xue, Andrew Tao, Mingqiang Liu, Sanja Fidler, B. Ivanovic, Trevor Darrell, Jitendra Malik, Song Han, Marco Pavone",
                "citations": 3
            },
            {
                "title": "OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields With Fine-Grained Understanding",
                "abstract": "In recent years, there has been a surge of interest in open-vocabulary 3D scene reconstruction facilitated by visual language models (VLMs), which showcase remarkable capabilities in open-set retrieval tasks. Although the semantic ambiguity of existing point-wise feature maps is alleviated by open-vocabulary mask segmenters for object-level understanding, effectively retaining fine-grained features within objects simultaneously remains challenging. To address these challenges, we introduce OpenObj, an innovative approach to build open-vocabulary object-level Neural Radiance Fields (NeRF) with fine-grained understanding. In essence, OpenObj establishes a robust framework for efficient and watertight scene modeling and comprehension at the object level. Specifically, we obtain cross-frame consistent instance-level masks for supervision through our two-stage mask clustering module. Moreover, by incorporating part-level features into the object NeRF models, OpenObj not only captures object-level instances but also preserves an understanding of their internal granularity. The results on multiple datasets demonstrate that OpenObj achieves superior performance in zero-shot segmentation and retrieval tasks. Additionally, OpenObj supports real-world robotics tasks at several levels, including global movement and local manipulation.",
                "authors": "Yinan Deng, Jiahui Wang, Jingyu Zhao, Jianyu Dou, Yi Yang, Yufeng Yue",
                "citations": 3
            },
            {
                "title": "Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification",
                "abstract": "Remote-sensing fine-grained ship classification (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pretrained vision-language models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To address these issues, we introduce a novel prompt tuning technique that employs a hierarchical, multigranularity prompt design. Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network. This strategy enhances the model’s generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features. Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes. Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques. The source code will be made publicly available.",
                "authors": "Long Lan, Fengxiang Wang, Xiangtao Zheng, Zengmao Wang, Xinwang Liu",
                "citations": 3
            },
            {
                "title": "Veagle: Advancements in Multimodal Representation Learning",
                "abstract": "Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by the successes and insights of previous works. Veagle leverages a dynamic mechanism to project encoded visual information directly into the language model. This dynamic approach allows for a more nuanced understanding of intricate details present in visual contexts. To validate the effectiveness of Veagle, we conduct comprehensive experiments on benchmark datasets, emphasizing tasks such as visual question answering and image understanding. Our results indicate a improvement of 5-6 \\% in performance, with Veagle outperforming existing models by a notable margin. The outcomes underscore the model's versatility and applicability beyond traditional benchmarks.",
                "authors": "Rajat Chawla, Arkajit Datta, Tushar Verma, Adarsh Jha, Anmol Gautam, Ayush Vatsal, Sukrit Chaterjee, NS Mukunda, Ishaan Bhola",
                "citations": 3
            },
            {
                "title": "Long Story Short: Story-level Video Understanding from 20K Short Films",
                "abstract": "Recent developments in vision-language models have significantly advanced video understanding. Existing datasets and tasks, however, have notable limitations. Most datasets are confined to short videos with limited events and narrow narratives. For example, datasets with instructional and egocentric videos often depict activities of one person in a single scene. Although existing movie datasets offer richer content, they are often limited to short-term tasks, lack publicly available videos, and frequently encounter data leakage issues given the use of subtitles and other information about commercial movies during LLM pretraining. To address the above limitations, we propose Short-Films 20K (SF20K), the largest publicly available movie dataset. SF20K is composed of 20,143 amateur films and offers long-term video tasks in the form of multiple-choice and open-ended question answering. Our extensive analysis of SF20K reveals minimal data leakage, emphasizes the need for long-term reasoning, and demonstrates the strong performance of recent VLMs. Finally, we show that instruction tuning on the SF20K-Train set substantially improves model performance, paving the way for future progress in long-term video understanding.",
                "authors": "Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, Ivan Laptev",
                "citations": 3
            },
            {
                "title": "Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA",
                "abstract": "In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other. Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA. Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions. Code is available at https://github.com/matthewdm0816/BridgeQA.",
                "authors": "Wentao Mo, Yang Liu",
                "citations": 3
            },
            {
                "title": "MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models",
                "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. Our proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4o's compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/",
                "authors": "Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao, Zhengyuan Yang, Hangfeng He, Chenliang Xu, Jiebo Luo",
                "citations": 3
            },
            {
                "title": "ReplanVLM: Replanning Robotic Tasks With Visual Language Models",
                "abstract": "Large language models (LLMs) have gained increasing popularity in robotic task planning due to their exceptional abilities in text analytics and generation, as well as their broad knowledge of the world. However, they fall short in decoding visual cues. LLMs have limited direct perception of the world, which leads to a deficient grasp of the current state of the world. By contrast, the emergence of visual language models (VLMs) fills this gap by integrating visual perception modules, which can enhance the autonomy of robotic task planning. Despite these advancements, VLMs still face challenges, such as the potential for task execution errors, even when provided with accurate instructions. To address such issues, this letter proposes a ReplanVLM framework for robotic task planning. In this study, we focus on error correction interventions. An internal error correction mechanism and an external error correction mechanism are presented to correct errors under corresponding phases. A replan strategy is developed to replan tasks or correct error codes when task execution fails. Experimental results on real robots and in simulation environments have demonstrated the superiority of the proposed framework, with higher success rates and robust error correction capabilities in open-world tasks.",
                "authors": "Aoran Mei, Guo-Niu Zhu, Huaxiang Zhang, Zhongxue Gan",
                "citations": 3
            },
            {
                "title": "Multi-Modal Attribute Prompting for Vision-Language Models",
                "abstract": "Pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong generalization ability to downstream tasks but struggle in few-shot scenarios. Existing prompting techniques primarily focus on global text and image representations, yet overlooking multi-modal attribute characteristics. This limitation hinders the model’s ability to perceive fine-grained visual details and restricts its generalization ability to a broader range of unseen classes. To address this issue, we propose a Multi-modal Attribute Prompting method (MAP) by jointly exploring textual attribute prompting, visual attribute prompting, and attribute-level alignment. The proposed MAP enjoys several merits. First, we introduce learnable visual attribute prompts enhanced by textual attribute semantics to adaptively capture visual attributes for images from unknown categories, boosting fine-grained visual perception capabilities for CLIP. Second, the proposed attribute-level alignment complements the global alignment to enhance the robustness of cross-modal alignment for open-vocabulary objects. To our knowledge, this is the first work to establish cross-modal attribute-level alignment for CLIP-based few-shot adaptation. Extensive experimental results on 11 datasets demonstrate that our method performs favorably against state-of-the-art approaches.",
                "authors": "Xin Liu, Jiamin Wu, Wenfei Yang, Xu Zhou, Tianzhu Zhang",
                "citations": 3
            },
            {
                "title": "MINDS. Hydrocarbons detected by JWST/MIRI in the inner disk of Sz28 consistent with a high C/O gas-phase chemistry",
                "abstract": "With the advent of JWST, we are acquiring unprecedented insights into the physical and chemical structure of the inner regions of planet-forming disks where terrestrial planet formation occurs. Very low-mass stars (VLMSs) are known to have a high occurrence of the terrestrial planets orbiting them. Exploring the chemical composition of the gas in these inner disk regions can help us better understand the connection between planet-forming disks and planets. sun We used the dust-fitting tool DuCK to determine the dust continuum and to place constraints on the dust composition and grain sizes. We used 0D slab models to identify and fit the molecular spectral features, which yielded estimates on the temperature, column density, and emitting area. To test our understanding of the chemistry in the disks around VLMSs, we employed the thermo-chemical disk model P RO D I M O and investigated the reservoirs of the detected hydrocarbons. We explored how the C/O ratio affects the inner disk chemistry. JWST reveals a plethora of hydrocarbons, including CH3 CH4 C2H2 CCH2 C2H6 C3H4 C4H2 and C6H6 which suggests a disk with a gaseous C/O\\,>\\,1. Additionally, we detect CO2 CO2 HCN and HC3N H2O and OH are absent from the spectrum. We do not detect polycyclic aromatic hydrocarbons. Photospheric stellar absorption lines of H2O and CO are identified. Notably, our radiation thermo-chemical disk models are able to produce these detected hydrocarbons in the surface layers of the disk when C/O\\,>\\,1. The presence of C C+ H, and H2 is crucial for the formation of hydrocarbons in the surface layers, and a C/O ratio larger than 1 ensures the surplus of C needed to drive this chemistry. Based on this, we predict a list of additional hydrocarbons that should also be detectable. Both amorphous and crystalline silicates (enstatite and forsterite) are present in the disk and we find grain sizes of 2 and 5\\,mu m. The disk around Sz28 is rich in hydrocarbons, and its inner regions have a high gaseous C/O ratio. In contrast, it is the first VLMS disk in the MINDS sample to show both distinctive dust features and a rich hydrocarbon chemistry. The presence of large grains indicates dust growth and evolution. Thermo-chemical disk models that employ an extended hydrocarbon chemical network together with C/O\\,>1 are able to explain the hydrocarbon species detected in the spectrum.",
                "authors": "J. Kanwar, I. Kamp, H. Jang, L. Waters, E. V. Dishoeck, V. Christiaens, A. M. Arabhavi, Thomas K. Henning, M. Gudel, P. Woitke, Olivier Absil, D. Barrado, A. C. O. Garatti, A. Glauser, F. Lahuis, S. Scheithauer, B. Vandenbussche, D. Gasman, S. Grant, N. Kurtovic, G. Perotti, B. Tabone, M. Temmink",
                "citations": 3
            },
            {
                "title": "ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs",
                "abstract": "Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM-only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.",
                "authors": "Irene Huang, Wei Lin, M. J. Mirza, Jacob A. Hansen, Sivan Doveh, V. Butoi, Roei Herzig, Assaf Arbelle, Hilde Kuhene, Trevor Darrel, Chuang Gan, Aude Oliva, Rogério Feris, Leonid Karlinsky",
                "citations": 3
            },
            {
                "title": "MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation",
                "abstract": "Online memes have emerged as powerful digital cultural artifacts in the age of social media, offering not only humor but also platforms for political discourse, social critique, and information dissemination. Their extensive reach and influence in shaping online communities' sentiments make them invaluable tools for campaigning and promoting ideologies. Despite the development of several meme generation tools, there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies. Addressing this, we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements. MemeCraft presents an end-to-end pipeline, transforming user prompts into compelling multimodal memes without manual intervention. Conscious of the misuse potential in creating divisive content, an intrinsic safety mechanism is embedded to curb hateful meme production. Our assessment, focusing on two UN Sustainable Development Goals-Climate Action and Gender Equality-shows MemeCraft's prowess in creating memes that are both funny and supportive of advocacy goals. This paper highlights how generative AI can promote social good and pioneers the use of LLMs and VLMs in meme generation.",
                "authors": "Han Wang, Roy Ka-Wei Lee",
                "citations": 3
            },
            {
                "title": "Emergent Visual-Semantic Hierarchies in Image-Text Representations",
                "abstract": "While recent vision-and-language models (VLMs) like CLIP are a powerful tool for analyzing text and images in a shared semantic space, they do not explicitly model the hierarchical nature of the set of texts which may describe an image. Conversely, existing multimodal hierarchical representation learning methods require costly training from scratch, failing to leverage the knowledge encoded by state-of-the-art multimodal foundation models. In this work, we study the knowledge of existing foundation models, finding that they exhibit emergent understanding of visual-semantic hierarchies despite not being directly trained for this purpose. We propose the Radial Embedding (RE) framework for probing and optimizing hierarchical understanding, and contribute the HierarCaps dataset, a benchmark facilitating the study of hierarchical knowledge in image--text representations, constructed automatically via large language models. Our results show that foundation VLMs exhibit zero-shot hierarchical understanding, surpassing the performance of prior models explicitly designed for this purpose. Furthermore, we show that foundation models may be better aligned to hierarchical reasoning via a text-only fine-tuning phase, while retaining pretraining knowledge.",
                "authors": "Morris Alper, Hadar Averbuch-Elor",
                "citations": 3
            },
            {
                "title": "WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks",
                "abstract": "The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress toward capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus.",
                "authors": "L'eo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier De Chezelles, Quentin Cappart, Nicolas Chapados, Alexandre Lacoste, Alexandre Drouin",
                "citations": 3
            },
            {
                "title": "Seeing the Unseen: Visual Common Sense for Semantic Placement",
                "abstract": "Computer vision tasks typically involve describing what is present in an image (e.g. classification, detection, segmentation, and captioning). We study a visual common sense task that requires understanding ‘what is not present’. Specif-ically, given an image (e.g. of a living room) and a name of an object (“cushion ”), a vision system is asked to predict semantically-meaningful regions (masks or bounding boxes) in the image where that object could be placed or is likely be placed by humans (e.g. on the sofa). We call this task: Se-mantic Placement (SP) and believe that such common-sense visual understanding is critical for assitive robots (tidying a house), AR devices (automatically rendering an object in the user's space), and visually-grounded chatbots with common sense. Studying the invisible is hard. Datasets for image description are typically constructed by curating relevant images (e.g. via image search with object names) and asking humans to annotate the contents of the image; neither of those two steps are straightforward for objects not present in the image. We overcome this challenge by operating in the opposite direction: we start with an image of an object in context, which is easy to find online, and then remove that ob-ject from the image via inpainting. This automated pipeline converts unstructured web data into a dataset comprising pairs of images with/without the object. With this proposed data generation pipeline, we collect a novel dataset, containing ~1.3M images across 9 object categories. We then train a SP prediction model, called CLIP-UNet, on our dataset. The CLIP-UNet outperforms existing VLMs and baselines that combine semantic priors with object detectors, gener-alizes well to real-world and simulated images and exhibits semantics-aware reasoning for object placement. In our user studies, we find that the SP masks predicted by CLIP-UNet are favored 43.7% and 31.3% times when comparing against the 4 SP baselines on real and simulated images. In addition, leveraging SP mask predictions from CLIP-UNet enables downstream applications like building tidying robots in indoor environments.",
                "authors": "Ram Ramrakhya, Aniruddha Kembhavi, Dhruv Batra, Z. Kira, Kuo-Hao Zeng, Luca Weihs",
                "citations": 3
            },
            {
                "title": "FLoRA: Enhancing Vision-Language Models with Parameter-Efficient Federated Learning",
                "abstract": "In the rapidly evolving field of artificial intelligence, multimodal models, e.g., integrating vision and language into visual-language models (VLMs), have become pivotal for many applications, ranging from image captioning to multimodal search engines. Among these models, the Contrastive Language-Image Pre-training (CLIP) model has demonstrated remarkable performance in understanding and generating nuanced relationships between text and images. However, the conventional training of such models often requires centralized aggregation of vast datasets, posing significant privacy and data governance challenges. To address these concerns, this paper proposes a novel approach that leverages Federated Learning and parameter-efficient adapters, i.e., Low-Rank Adaptation (LoRA), to train VLMs. This methodology preserves data privacy by training models across decentralized data sources and ensures model adaptability and efficiency through LoRA's parameter-efficient fine-tuning. Our approach accelerates training time by up to 34.72 times and requires 2.47 times less memory usage than full fine-tuning.",
                "authors": "Duy Phuong Nguyen, J. P. Muñoz, Ali Jannesari",
                "citations": 3
            },
            {
                "title": "PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization",
                "abstract": "Domain Generalization (DG) aims to resolve distribution shifts between source and target domains, and current DG methods are default to the setting that data from source and target domains share identical categories. Nevertheless, there exists unseen classes from target domains in practical scenarios. To address this issue, Open Set Domain Generalization (OSDG) has emerged and several methods have been exclusively proposed. However, most existing methods adopt complex architectures with slight improvement compared with DG methods. Recently, vision-language models (VLMs) have been introduced in DG following the fine-tuning paradigm, but consume huge training overhead with large vision models. Therefore, in this paper, we innovate to transfer knowledge from VLMs to lightweight vision models and improve the robustness by introducing Perturbation Distillation (PD) from three perspectives, including Score, Class and Instance (SCI), named SCI-PD. Moreover, previous methods are oriented by the benchmarks with identical and fixed splits, ignoring the divergence between source domains. These methods are revealed to suffer from sharp performance decay with our proposed new benchmark Hybrid Domain Generalization (HDG) and a novel metric H2-CV, which construct various splits to comprehensively assess the robustness of algorithms. Extensive experiments demonstrate that our method outperforms state-of-the-art algorithms on multiple datasets, especially improving the robustness when confronting data scarcity.",
                "authors": "Zining Chen, Weiqiu Wang, Zhicheng Zhao, Fei Su, Aidong Men, Hongying Meng",
                "citations": 3
            },
            {
                "title": "Hyperbolic Learning with Synthetic Captions for Open-World Detection",
                "abstract": "Open-world detection poses significant challenges, as it requires the detection of any object using either object class labels or free-form texts. Existing related works often use large-scale manual annotated caption datasets for training, which are extremely expensive to collect. Instead, we propose to transfer knowledge from vision-language models (VLMs) to enrich the open-vocabulary descriptions au-tomatically. Specifically, we bootstrap dense synthetic captions using pretrained VLMs to provide rich descriptions on different regions in images, and incorporate these captions to train a novel detector that generalizes to novel concepts. To mitigate the noise caused by hallucination in syn-thetic captions, we also propose a novel hyperbolic vision-language learning approach to impose a hierarchy between visual and caption embeddings. We call our detector “Hy-perLearner”. We conduct extensive experiments on a wide variety of open-world detection benchmarks (COCO, LVIS, Object Detection in the Wild, RefCoCo) and our results show that our model consistently outperforms existing state-of-the-art methods, such as GLIP, GLIPv2 and Grounding DINO, when using the same backbone.",
                "authors": "Fanjie Kong, Yanbei Chen, Jiarui Cai, Davide Modolo",
                "citations": 3
            },
            {
                "title": "Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments",
                "abstract": "In this work, we tackle the limitations of current LiDAR-based 3D object detection systems, which are hindered by a restricted class vocabulary and the high costs associated with annotating new object classes. Our exploration of open-vocabulary (OV) learning in urban environments aims to capture novel instances using pre-trained vision-language models (VLMs) with multi-sensor data. We design and benchmark a set of four potential solutions as baselines, categorizing them into either top-down or bottom-up approaches based on their input data strategies. While effective, these methods exhibit certain limitations, such as missing novel objects in 3D box estimation or applying rigorous priors, leading to biases towards objects near the camera or of rectangular geometries. To overcome these limitations, we introduce a universal \\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the recall of novel objects and propagating this detection capability to more distant areas thereby progressively capturing more. In particular, we utilize a greedy box seeker to search against 3D novel boxes of varying orientations and depth in each generated frustum and ensure the reliability of newly identified boxes by cross alignment and density ranker. Additionally, the inherent bias towards camera-proximal objects is alleviated by the proposed remote simulator, which randomly diversifies pseudo-labeled novel instances in the self-training process, combined with the fusion of base samples in the memory bank. Extensive experiments demonstrate a 53% improvement in novel recall across diverse OV settings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold increase in Average Precision (AP) for novel object classes. The source code is made available at https://github.com/djamahl99/findnpropagate.",
                "authors": "Djamahl Etchegaray, Zi Huang, Tatsuya Harada, Yadan Luo",
                "citations": 3
            },
            {
                "title": "Open-Vocabulary Spatio-Temporal Action Detection",
                "abstract": "Spatio-temporal action detection (STAD) is an important fine-grained video understanding task. Current methods require box and label supervision for all action classes in advance. However, in real-world applications, it is very likely to come across new action classes not seen in training because the action category space is large and hard to enumerate. Also, the cost of data annotation and model training for new classes is extremely high for traditional methods, as we need to perform detailed box annotations and re-train the whole network from scratch. In this paper, we propose a new challenging setting by performing open-vocabulary STAD to better mimic the situation of action detection in an open world. Open-vocabulary spatio-temporal action detection (OV-STAD) requires training a model on a limited set of base classes with box and label supervision, which is expected to yield good generalization performance on novel action classes. For OV-STAD, we build two benchmarks based on the existing STAD datasets and propose a simple but effective method based on pretrained video-language models (VLM). To better adapt the holistic VLM for the fine-grained action detection task, we carefully fine-tune it on the localized video region-text pairs. This customized fine-tuning endows the VLM with better motion understanding, thus contributing to a more accurate alignment between video regions and texts. Local region feature and global video feature fusion before alignment is adopted to further improve the action detection performance by providing global context. Our method achieves a promising performance on novel classes.",
                "authors": "Tao Wu, Shuqiu Ge, Jie Qin, Gangshan Wu, Limin Wang",
                "citations": 3
            },
            {
                "title": "Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps",
                "abstract": "Open-Vocabulary Mobile Manipulation (OVMM) is a crucial capability for autonomous robots, especially when faced with the challenges posed by unknown and dynamic environments. This task requires robots to explore and build a semantic understanding of their surroundings, generate feasible plans to achieve manipulation goals, adapt to environmental changes, and comprehend natural language instructions from humans. To address these challenges, we propose a novel framework that leverages the zero-shot detection and grounded recognition capabilities of pretraining visual-language models (VLMs) combined with dense 3D entity reconstruction to build 3D semantic maps. Additionally, we utilize large language models (LLMs) for spatial region abstraction and online planning, incorporating human instructions and spatial semantic context. We have built a 10-DoF mobile manipulation robotic platform JSR-1 and demonstrated in real-world robot experiments that our proposed framework can effectively capture spatial semantics and process natural language user instructions for zero-shot OVMM tasks under dynamic environment settings, with an overall navigation and task success rate of 80.95% and 73.33% over 105 episodes, and better SFT and SPL by 157.18% and 19.53% respectively compared to the baseline. Furthermore, the framework is capable of replanning towards the next most probable candidate location based on the spatial semantic context derived from the 3D semantic map when initial plans fail, keeping an average success rate of 76.67%.",
                "authors": "Dicong Qiu, Wenzong Ma, Zhenfu Pan, Hui Xiong, Junwei Liang",
                "citations": 3
            },
            {
                "title": "Zero shot VLMs for hate meme detection: Are we there yet?",
                "abstract": "Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we observe that large VLMs are still vulnerable for zero-shot hate meme detection.",
                "authors": "Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee",
                "citations": 3
            },
            {
                "title": "Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case",
                "abstract": "Recently, large language model (LLM)-based agents have made significant advances across various fields. One of the most popular research areas involves applying these agents to video games. Traditionally, these methods have relied on game APIs to access in-game environmental and action data. However, this approach is limited by the availability of APIs and does not reflect how humans play games. With the advent of vision language models (VLMs), agents now have enhanced visual understanding capabilities, enabling them to interact with games using only visual inputs. Despite these advances, current approaches still face challenges in action-oriented tasks, particularly in action role-playing games (ARPGs), where reinforcement learning methods are prevalent but suffer from poor generalization and require extensive training. To address these limitations, we select an ARPG, ``Black Myth: Wukong'', as a research platform to explore the capability boundaries of existing VLMs in scenarios requiring visual-only input and complex action output. We define 12 tasks within the game, with 75% focusing on combat, and incorporate several state-of-the-art VLMs into this benchmark. Additionally, we will release a human operation dataset containing recorded gameplay videos and operation logs, including mouse and keyboard actions. Moreover, we propose a novel VARP (Vision Action Role-Playing) agent framework, consisting of an action planning system and a visual trajectory system. Our framework demonstrates the ability to perform basic tasks and succeed in 90% of easy and medium-level combat scenarios. This research aims to provide new insights and directions for applying multimodal agents in complex action game environments. The code and datasets will be made available at https://varp-agent.github.io/.",
                "authors": "Peng Chen, Pi Bu, Jun Song, Yuan Gao, Bo Zheng",
                "citations": 3
            },
            {
                "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
                "abstract": "Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.",
                "authors": "Rui Zhao, Hangjie Yuan, Yujie Wei, Shiwei Zhang, Yuchao Gu, L. Ran, Xiang Wang, Zhangjie Wu, Junhao Zhang, Yingya Zhang, Mike Zheng Shou",
                "citations": 2
            },
            {
                "title": "Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything",
                "abstract": "Large Visual Language Model\\textbfs (VLMs) such as GPT-4V have achieved remarkable success in generating comprehensive and nuanced responses. Researchers have proposed various benchmarks for evaluating the capabilities of VLMs. With the integration of visual and text inputs in VLMs, new security issues emerge, as malicious attackers can exploit multiple modalities to achieve their objectives. This has led to increasing attention on the vulnerabilities of VLMs to jailbreak. Most existing research focuses on generating adversarial images or nonsensical image to jailbreak these models. However, no researchers evaluate whether logic understanding capabilities of VLMs in flowchart can influence jailbreak. Therefore, to fill this gap, this paper first introduces a novel dataset Flow-JD specifically designed to evaluate the logic-based flowchart jailbreak capabilities of VLMs. We conduct an extensive evaluation on GPT-4o, GPT-4V, other 5 SOTA open source VLMs and the jailbreak rate is up to 92.8%. Our research reveals significant vulnerabilities in current VLMs concerning image-to-text jailbreak and these findings underscore the the urgency for the development of robust and effective future defenses.",
                "authors": "Xiaotian Zou, Yongkang Chen",
                "citations": 2
            },
            {
                "title": "MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations",
                "abstract": "With the emergence of LLMs and their integration with other data modalities, multi-modal 3D perception attracts more attention due to its connectivity to the physical world and makes rapid progress. However, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3D scene. To tackle this problem, this paper builds the first largest ever multi-modal 3D scene dataset and benchmark with hierarchical grounded language annotations, MMScan. It is constructed based on a top-down logic, from region to object level, from a single target to inter-target relationships, covering holistic aspects of spatial and attribute understanding. The overall pipeline incorporates powerful VLMs via carefully designed prompts to initialize the annotations efficiently and further involve humans' correction in the loop to ensure the annotations are natural, correct, and comprehensive. Built upon existing 3D scanning data, the resulting multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding and question-answering benchmarks. We evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. Furthermore, we use this high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.",
                "authors": "Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, Jiangmiao Pang",
                "citations": 2
            },
            {
                "title": "Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models",
                "abstract": "Learning-based methods have achieved strong performance for quadrupedal locomotion. However, several challenges prevent quadrupeds from learning helpful indoor skills that require interaction with environments and humans: lack of end-effectors for manipulation, limited semantic understanding using only simulation data, and low traversability and reachability in indoor environments. We present a system for quadrupedal mobile manipulation in indoor environments. It uses a front-mounted gripper for object manipulation, a low-level controller trained in simulation using egocentric depth for agile skills like climbing and whole-body tilting, and pre-trained vision-language models (VLMs) with a third-person fisheye and an egocentric RGB camera for semantic understanding and command generation. We evaluate our system in two unseen environments without any real-world data collection or training. Our system can zero-shot generalize to these environments and complete tasks, like following user's commands to fetch a randomly placed stuff toy after climbing over a queen-sized bed, with a 60% success rate. Project website: https://helpful-doggybot.github.io/",
                "authors": "Qi Wu, Zipeng Fu, Xuxin Cheng, Xiaolong Wang, Chelsea Finn",
                "citations": 2
            },
            {
                "title": "Open-Set Recognition in the Age of Vision-Language Models",
                "abstract": "Are vision-language models (VLMs) for open-vocabulary perception inherently open-set models because they are trained on internet-scale datasets? We answer this question with a clear no - VLMs introduce closed-set assumptions via their finite query set, making them vulnerable to open-set conditions. We systematically evaluate VLMs for open-set recognition and find they frequently misclassify objects not contained in their query set, leading to alarmingly low precision when tuned for high recall and vice versa. We show that naively increasing the size of the query set to contain more and more classes does not mitigate this problem, but instead causes diminishing task performance and open-set performance. We establish a revised definition of the open-set problem for the age of VLMs, define a new benchmark and evaluation protocol to facilitate standardised evaluation and research in this important area, and evaluate promising baseline approaches based on predictive uncertainty and dedicated negative embeddings on a range of open-vocabulary VLM classifiers and object detectors.",
                "authors": "Dimity Miller, Niko Sünderhauf, Alex Kenna, Keita Mason",
                "citations": 2
            },
            {
                "title": "Towards Open-World Grasping with Large Vision-Language Models",
                "abstract": "The ability to grasp objects in-the-wild from open-ended language instructions constitutes a fundamental challenge in robotics. An open-world grasping system should be able to combine high-level contextual with low-level physical-geometric reasoning in order to be applicable in arbitrary scenarios. Recent works exploit the web-scale knowledge inherent in large language models (LLMs) to plan and reason in robotic context, but rely on external vision and action models to ground such knowledge into the environment and parameterize actuation. This setup suffers from two major bottlenecks: a) the LLM's reasoning capacity is constrained by the quality of visual grounding, and b) LLMs do not contain low-level spatial understanding of the world, which is essential for grasping in contact-rich scenarios. In this work we demonstrate that modern vision-language models (VLMs) are capable of tackling such limitations, as they are implicitly grounded and can jointly reason about semantics and geometry. We propose OWG, an open-world grasping pipeline that combines VLMs with segmentation and grasp synthesis models to unlock grounded world understanding in three stages: open-ended referring segmentation, grounded grasp planning and grasp ranking via contact reasoning, all of which can be applied zero-shot via suitable visual prompting mechanisms. We conduct extensive evaluation in cluttered indoor scene datasets to showcase OWG's robustness in grounding from open-ended language, as well as open-world robotic grasping experiments in both simulation and hardware that demonstrate superior performance compared to previous supervised and zero-shot LLM-based methods. Project material is available at https://gtziafas.github.io/OWG_project/ .",
                "authors": "Georgios Tziafas, H. Kasaei",
                "citations": 2
            },
            {
                "title": "Image Analysis in Autonomous Vehicles: A Review of the Latest AI Solutions and Their Comparison",
                "abstract": "The integration of advanced image analysis using artificial intelligence (AI) is pivotal for the evolution of autonomous vehicles (AVs). This article provides a thorough review of the most significant datasets and latest state-of-the-art AI solutions employed in image analysis for AVs. Datasets such as Cityscapes, NuScenes, CARLA, and Talk2Car form the benchmarks for training and evaluating different AI models, with unique characteristics catering to various aspects of autonomous driving. Key AI methodologies, including Convolutional Neural Networks (CNNs), Transformer models, Generative Adversarial Networks (GANs), and Vision Language Models (VLMs), are discussed. The article also presents a comparative analysis of various AI techniques in real-world scenarios, focusing on semantic image segmentation, 3D object detection, vehicle control in virtual environments, and vehicle interaction using natural language. Simultaneously, the roles of multisensor datasets and simulation platforms like AirSim, TORCS, and SUMMIT in enriching the training data and testing environments for AVs are highlighted. By synthesizing information on datasets, AI solutions, and comparative performance evaluations, this article serves as a crucial resource for researchers, developers, and industry stakeholders, offering a clear view of the current landscape and future directions in autonomous vehicle image analysis technologies.",
                "authors": "Michał Kozłowski, S. Racewicz, Sławomir Wierzbicki",
                "citations": 2
            },
            {
                "title": "ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning",
                "abstract": "Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon tasks. To address these limitations, we present ExACT, an approach to combine test-time search and self-learning to build o1-like models for agentic applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test time algorithm designed to enhance AI agents' ability to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate for reliable state evaluation. Next, we introduce Exploratory Learning, a novel learning strategy to teach agents to search at inference time without relying on any external search algorithms. On the challenging VisualWebArena benchmark, our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge and experience gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. After Exploratory Learning, GPT-4o 1) demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success, and 2) matches 87% of R-MCTS's performance while using significantly less compute. Notably, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' capabilities for agentic applications via test-time search and self-learning.",
                "authors": "Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu",
                "citations": 2
            },
            {
                "title": "The Minimum Information about CLinical Artificial Intelligence Checklist for Generative Modeling Research (MI-CLAIM-GEN)",
                "abstract": "Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data (\"zero-\"or\"few-shot\"approaches), as well as the open-ended nature of their outputs, necessitate the development of new guidelines for robust reporting of clinical generative model research. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the original MI-CLAIM checklist. The new checklist, MI-CLAIM-GEN (Table 1), aims to address differences in training, evaluation, interpretability, and reproducibility of new generative models compared to non-generative (\"predictive\") AI models. This MI-CLAIM-GEN checklist also seeks to clarify cohort selection reporting with unstructured clinical data and adds additional items on alignment with ethical standards for clinical AI research.",
                "authors": "Brenda Y Miao, Irene Y. Chen, Christopher Y K Williams, Jaysón M. Davidson, Augusto Garcia-Agundez, Harry Sun, T. Zack, A. Butte, Madhumita Sushil",
                "citations": 2
            },
            {
                "title": "Cobra Effect in Reference-Free Image Captioning Metrics",
                "abstract": "Evaluating the compatibility between textual descriptions and corresponding images represents a core endeavor within multi-modal research. In recent years, a proliferation of reference-free methods, leveraging visual-language pre-trained models (VLMs), has emerged. Empirical evidence has substantiated that these innovative approaches exhibit a higher correlation with human judgment, marking a significant advancement in the field. However, does a higher correlation with human evaluations alone sufficiently denote the complete of a metric? In response to this question, in this paper, we study if there are any deficiencies in reference-free metrics. Specifically, inspired by the Cobra Effect, we utilize metric scores as rewards to direct the captioning model toward generating descriptions that closely align with the metric's criteria. If a certain metric has flaws, it will be exploited by the model and reflected in the generated sentences. Our findings reveal that descriptions guided by these metrics contain significant flaws, e.g. incoherent statements and excessive repetition. Subsequently, we propose a novel method termed Self-Improving to rectify the identified shortcomings within these metrics. We employ GPT-4V as an evaluative tool to assess generated sentences and the result reveals that our approach achieves state-of-the-art (SOTA) performance. In addition, we also introduce a challenging evaluation benchmark called Flaws Caption to evaluate reference-free image captioning metrics comprehensively. Our code is available at https://github.com/aaronma2020/robust_captioning_metric",
                "authors": "Zheng Ma, Changxin Wang, Yawen Ouyang, Fei Zhao, Jianbing Zhang, Shujian Huang, Jiajun Chen",
                "citations": 2
            },
            {
                "title": "GalLoP: Learning Global and Local Prompts for Vision-Language Models",
                "abstract": "Prompt learning has been widely adopted to efficiently adapt vision-language models (VLMs), e.g. CLIP, for few-shot image classification. Despite their success, most prompt learning methods trade-off between classification accuracy and robustness, e.g. in domain generalization or out-of-distribution (OOD) detection. In this work, we introduce Global-Local Prompts (GalLoP), a new prompt learning method that learns multiple diverse prompts leveraging both global and local visual features. The training of the local prompts relies on local features with an enhanced vision-text alignment. To focus only on pertinent features, this local alignment is coupled with a sparsity strategy in the selection of the local features. We enforce diversity on the set of prompts using a new ``prompt dropout'' technique and a multiscale strategy on the local prompts. GalLoP outperforms previous prompt learning methods on accuracy on eleven datasets in different few shots settings and with various backbones. Furthermore, GalLoP shows strong robustness performances in both domain generalization and OOD detection, even outperforming dedicated OOD detection methods. Code and instructions to reproduce our results: https://github.com/MarcLafon/gallop.",
                "authors": "Marc Lafon, Elias Ramzi, Clément Rambour, Nicolas Audebert, Nicolas Thome",
                "citations": 2
            },
            {
                "title": "DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection",
                "abstract": "Vision-language models (VLMs), such as CLIP, have demonstrated impressive zero-shot capabilities for various downstream tasks. Their performance can be further enhanced through few-shot prompt tuning methods. However, current studies evaluate the performance of learned prompts separately on base and new classes. This evaluation lacks practicality for real-world applications since downstream tasks cannot determine whether the data belongs to base or new classes in advance. In this paper, we explore a problem setting called Open-world Prompt Tuning (OPT), which involves tuning prompts on base classes and evaluating on a combination of base and new classes. By introducing Decomposed Prompt Tuning framework (DePT), we theoretically demonstrate that OPT can be solved by incorporating out-of-distribution detection into prompt tuning, thereby enhancing the base-to-new discriminability. Based on DePT, we present a novel prompt tuning approach, namely, Decomposed Context Optimization (DeCoOp), which introduces new-class detectors and sub-classifiers to further enhance the base-class and new-class discriminability. Experimental results on 11 benchmark datasets validate the effectiveness of DePT and demonstrate that DeCoOp outperforms current state-of-the-art methods, providing a significant 2% average accuracy improvement.",
                "authors": "Zhi Zhou, Ming Yang, Jiang-Xin Shi, Lan-Zhe Guo, Yu-Feng Li",
                "citations": 2
            },
            {
                "title": "HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation",
                "abstract": "Enabling robots to autonomously perform hybrid motions in diverse environments can be beneficial for long-horizon tasks such as material handling, household chores, and work assistance. This requires extensive exploitation of intrinsic motion capabilities, extraction of affordances from rich environmental information, and planning of physical interaction behaviors. Despite recent progress has demonstrated impressive humanoid whole-body control abilities, they struggle to achieve versatility and adaptability for new tasks. In this work, we propose HYPERmotion, a framework that learns, selects and plans behaviors based on tasks in different scenarios. We combine reinforcement learning with whole-body optimization to generate motion for 38 actuated joints and create a motion library to store the learned skills. We apply the planning and reasoning features of the large language models (LLMs) to complex loco-manipulation tasks, constructing a hierarchical task graph that comprises a series of primitive behaviors to bridge lower-level execution with higher-level planning. By leveraging the interaction of distilled spatial geometry and 2D observation with a visual language model (VLM) to ground knowledge into a robotic morphology selector to choose appropriate actions in single- or dual-arm, legged or wheeled locomotion. Experiments in simulation and real-world show that learned motions can efficiently adapt to new tasks, demonstrating high autonomy from free-text commands in unstructured scenes. Videos and website: hy-motion.github.io/",
                "authors": "Jin Wang, Rui Dai, Weijie Wang, Luca Rossini, Francesco Ruscelli, Nikos Tsagarakis",
                "citations": 2
            },
            {
                "title": "Neptune: The Long Orbit to Benchmarking Long Video Understanding",
                "abstract": "We introduce Neptune, a benchmark for long video understanding that requires reasoning over long time horizons and across different modalities. Many existing video datasets and models are focused on short clips (10s-30s). While some long video datasets do exist, they can often be solved by powerful image models applied per frame (and often to very few frames) in a video, and are usually manually annotated at high cost. In order to mitigate both these problems, we propose a scalable dataset creation pipeline which leverages large models (VLMs and LLMs), to automatically generate dense, time-aligned video captions, as well as tough question answer decoy sets for video segments (up to 15 minutes in length). Our dataset Neptune covers a broad range of long video reasoning abilities and consists of a subset that emphasizes multimodal reasoning. Since existing metrics for open-ended question answering are either rule-based or may rely on proprietary models, we provide a new open source model-based metric GEM to score open-ended responses on Neptune. Benchmark evaluations reveal that most current open-source long video models perform poorly on Neptune, particularly on questions testing temporal ordering, counting and state changes. Through Neptune, we aim to spur the development of more advanced models capable of understanding long videos. The dataset is available at https://github.com/google-deepmind/neptune",
                "authors": "Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung Nitesh, Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, Tobias Weyand, †. GoogleResearch",
                "citations": 2
            },
            {
                "title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning",
                "abstract": "With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, fine-tuning large pre-trained models has recently become a prevalent strategy in Continual Learning. This has led to the development of numerous prompting strategies to adapt transformer-based models without incurring catastrophic forgetting. However, these strategies often compromise the original zero-shot capabilities of the pre-trained CLIP model and struggle to adapt to domains that significantly deviate from the pre-training data. In this work, we propose Continual Generative training for Incremental prompt-Learning, a simple and novel approach to mitigate forgetting while adapting CLIP. Briefly, we employ Variational Autoencoders (VAEs) to learn class-conditioned distributions within the embedding space of the visual encoder. We then exploit these distributions to sample new synthetic visual embeddings and train the corresponding class-specific textual prompts during subsequent tasks. Through extensive experiments on different domains, we show that such a generative replay approach can adapt to new tasks while improving zero-shot capabilities, evaluated using a novel metric tailored for CL scenarios. Notably, further analysis reveals that our approach can bridge the gap with joint prompt tuning. The codebase is available at https://github.com/aimagelab/mammoth.",
                "authors": "Emanuele Frascaroli, Aniello Panariello, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara",
                "citations": 2
            },
            {
                "title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation",
                "abstract": "In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons, sketches and scientific figures. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs and (f) comparison with VLMs on rasterized representations. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced.",
                "authors": "Bocheng Zou, Mu Cai, Jianrui Zhang, Yong Jae Lee",
                "citations": 2
            },
            {
                "title": "VideoAgent: Self-Improving Video Generation",
                "abstract": "Video generation has been used to generate visual plans for controlling robotic systems. Given an image observation and a language instruction, previous work has generated video plans which are then converted to robot controls to be executed. However, a major bottleneck in leveraging video generation for control lies in the quality of the generated videos, which often suffer from hallucinatory content and unrealistic physics, resulting in low task success when control actions are extracted from the generated videos. While scaling up dataset and model size provides a partial solution, integrating external feedback is both natural and essential for grounding video generation in the real world. With this observation, we propose VideoAgent for self-improving generated video plans based on external feedback. Instead of directly executing the generated video plan, VideoAgent first refines the generated video plans using a novel procedure which we call self-conditioning consistency, utilizing feedback from a pretrained vision-language model (VLM). As the refined video plan is being executed, VideoAgent collects additional data from the environment to further improve video plan generation. Experiments in simulated robotic manipulation from MetaWorld and iTHOR show that VideoAgent drastically reduces hallucination, thereby boosting success rate of downstream manipulation tasks. We further illustrate that VideoAgent can effectively refine real-robot videos, providing an early indicator that robotics can be an effective tool in grounding video generation in the physical world.",
                "authors": "Achint Soni, Sreyas Venkataraman, Abhranil Chandra, Sebastian Fischmeister, Percy Liang, Bo Dai, Sherry Yang",
                "citations": 2
            },
            {
                "title": "Multimodal Foundation Models for Medical Imaging - A Systematic Review and Implementation Guidelines",
                "abstract": "Advancements in artificial intelligence (AI) offer promising solutions for enhancing clinical workflows and patient care, potentially revolutionizing healthcare delivery. However, the traditional paradigm of AI integration in healthcare is limited by models that rely on single input modalities during training and require extensive labeled data, failing to capture the multimodal nature of medical practice. Multimodal foundation models, particularly Large Vision Language Models (VLMs), have the potential to overcome these limitations by processing diverse data types and learning from large-scale unlabeled datasets or natural pairs of different modalities, thereby significantly contributing to the development of more robust and versatile AI systems in healthcare. In this review, we establish a unified terminology for multimodal foundation models for medical imaging applications and provide a systematic analysis of papers published between 2012 and 2024. In total, we screened 1,144 papers from medical and AI domains and extracted data from 97 included studies. Our comprehensive effort aggregates the collective knowledge of prior work, evaluates the current state of multimodal AI in healthcare, and delineates both prevailing limitations and potential growth areas. We provide implementation guidelines and actionable recommendations for various stakeholders, including model developers, clinicians, policymakers, and dataset curators.",
                "authors": "S.-C. Huang, M. E. Jensen, S. Yeung-Levy, M. Lungren, H. Poon, A. Chaudhari",
                "citations": 2
            },
            {
                "title": "Advancing Object Detection in Transportation with Multimodal Large Language Models (MLLMs): A Comprehensive Review and Empirical Testing",
                "abstract": "This study aims to comprehensively review and empirically evaluate the application of multimodal large language models (MLLMs) and Large Vision Models (VLMs) in object detection for transportation systems. In the first fold, we provide a background about the potential benefits of MLLMs in transportation applications and conduct a comprehensive review of current MLLM technologies in previous studies. We highlight their effectiveness and limitations in object detection within various transportation scenarios. The second fold involves providing an overview of the taxonomy of end-to-end object detection in transportation applications and future directions. Building on this, we proposed empirical analysis for testing MLLMs on three real-world transportation problems that include object detection tasks namely, road safety attributes extraction, safety-critical event detection, and visual reasoning of thermal images. Our findings provide a detailed assessment of MLLM performance, uncovering both strengths and areas for improvement. Finally, we discuss practical limitations and challenges of MLLMs in enhancing object detection in transportation, thereby offering a roadmap for future research and development in this critical area.",
                "authors": "Huthaifa I. Ashqar, Ahmed Jaber, Taqwa I. Alhadidi, Mohammed Elhenawy",
                "citations": 2
            },
            {
                "title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks",
                "abstract": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite its importance and practicality. In this work, we aim to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets covering both in-distribution and out-of-distribution tasks, and (2) VLM2Vec (Vision-Language Model ->Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, which encodes text or images independently without any task instruction, VLM2Vec can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. We build a series of VLM2Vec models on SoTA VLMs like Phi-3.5-V, LLaVA-1.6 and evaluate them on MMEB's evaluation split. Our results show that VLM2Vec achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB. We show that VLMs are secretly strong embedding models.",
                "authors": "Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, Wenhu Chen",
                "citations": 2
            },
            {
                "title": "AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) are a significant technique for Artificial General Intelligence (AGI). With the fast growth of AGI, the security problem become one of the most important challenges for VLMs. In this paper, through extensive experiments, we demonstrate the vulnerability of the conventional adaptation methods for VLMs, which may bring significant security risks. In addition, as the size of the VLMs increases, performing conventional adversarial adaptation techniques on VLMs results in high computational costs. To solve these problems, we propose a parameter-efficient \\underline{Adv}ersarial adaptation method named \\underline{AdvLoRA} by \\underline{Lo}w-\\underline{R}ank \\underline{A}daptation. At first, we investigate and reveal the intrinsic low-rank property during the adversarial adaptation for VLMs. Different from LoRA, we improve the efficiency and robustness of adversarial adaptation by designing a novel reparameterizing method based on parameter clustering and parameter alignment. In addition, an adaptive parameter update strategy is proposed to further improve the robustness. By these settings, our proposed AdvLoRA alleviates the model security and high resource waste problems. Extensive experiments demonstrate the effectiveness and efficiency of the AdvLoRA.",
                "authors": "Yuheng Ji, Yue Liu, Zhicheng Zhang, Zhao Zhang, Yuting Zhao, Gang Zhou, Xingwei Zhang, Xinwang Liu, Xiaolong Zheng",
                "citations": 2
            },
            {
                "title": "VLDadaptor: Domain Adaptive Object Detection With Vision-Language Model Distillation",
                "abstract": "Domain adaptive object detection (DAOD) aims to develop a detector trained on labeled source domains to identify objects in unlabeled target domains. A primary challenge in DAOD is the domain shift problem. Most existing methods learn domain-invariant features within single domain embedding space, often resulting in heavy model biases due to the intrinsic data properties of source domains. To mitigate the model biases, this paper proposes VLDadaptor, a domain adaptive object detector based on vision-language models (VLMs) distillation. Firstly, the proposed method integrates domain-mixed contrastive knowledge distillation between the visual encoder of CLIP and the detector by transferring category-level instance features, which guarantees the detector can extract domain-invariant visual instance features across domains. Then, VLDadaptor employs domain-mixed consistency distillation between the text encoder of CLIP and detector by aligning text prompt embeddings with visual instance features, which helps to maintain the category-level feature consistency among the detector, text encoder and the visual encoder of VLMs. Finally, the proposed method further promotes the adaptation ability by adopting a prompt-based memory bank to generate semantic-complete features for graph matching. These contributions enable VLDadaptor to extract visual features into the visual-language embedding space without any evident model bias towards specific domains. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art performance on Pascal VOC to Clipart adaptation tasks and exhibits high accuracy on driving scenario tasks with significantly less training time.",
                "authors": "Junjie Ke, Lihuo He, Bo Han, Jie Li, Di Wang, Xinbo Gao",
                "citations": 2
            },
            {
                "title": "CLIPArTT: Adaptation of CLIP to New Domains at Test Time",
                "abstract": "Pre-trained vision-language models (VLMs), exemplified by CLIP, demonstrate remarkable adaptability across zero-shot classification tasks without additional training. However, their performance diminishes in the presence of domain shifts. In this study, we introduce CLIP Adaptation duRing Test-Time (CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which involves automatic text prompts construction during inference for their use as text supervision. Our method employs a unique, minimally invasive text prompt tuning process, wherein multiple predicted classes are aggregated into a single new text prompt, used as \\emph{pseudo label} to re-classify inputs in a transductive manner. Additionally, we pioneer the standardization of TTA benchmarks (e.g., TENT) in the realm of VLMs. Our findings demonstrate that, without requiring additional transformations nor new trainable modules, CLIPArTT enhances performance dynamically across non-corrupted datasets such as CIFAR-100, corrupted datasets like CIFAR-100-C and ImageNet-C, alongside synthetic datasets such as VisDA-C. This research underscores the potential for improving VLMs' adaptability through novel test-time strategies, offering insights for robust performance across varied datasets and environments. The code can be found at: https://github.com/dosowiechi/CLIPArTT.git",
                "authors": "G. A. V. Hakim, David Osowiechi, Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers",
                "citations": 2
            },
            {
                "title": "NVILA: Efficient Frontier Visual Language Models",
                "abstract": "Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This\"scale-then-compress\"approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.",
                "authors": "Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, Yao Lu",
                "citations": 2
            },
            {
                "title": "TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings",
                "abstract": "Currently, inspired by the success of vision-language models (VLMs), an increasing number of researchers are focusing on improving VLMs and have achieved promising results. However, most existing methods concentrate on optimizing the connector and enhancing the language model component, while neglecting improvements to the vision encoder itself. In contrast, we propose Text Guided LLaVA (TG-LLaVA) in this paper, which optimizes VLMs by guiding the vision encoder with text, offering a new and orthogonal optimization direction. Specifically, inspired by the purpose-driven logic inherent in human behavior, we use learnable latent embeddings as a bridge to analyze textual instruction and add the analysis results to the vision encoder as guidance, refining it. Subsequently, another set of latent embeddings extracts additional detailed text-guided information from high-resolution local patches as auxiliary information. Finally, with the guidance of text, the vision encoder can extract text-related features, similar to how humans focus on the most relevant parts of an image when considering a question. This results in generating better answers. Experiments on various datasets validate the effectiveness of the proposed method. Remarkably, without the need for additional training data, our propsoed method can bring more benefits to the baseline (LLaVA-1.5) compared with other concurrent methods. Furthermore, the proposed method consistently brings improvement in different settings.",
                "authors": "Dawei Yan, Pengcheng Li, Yang Li, Hao Chen, Qingguo Chen, Weihua Luo, Wei Dong, Qingsen Yan, Haokui Zhang, Chunhua Shen",
                "citations": 2
            },
            {
                "title": "Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns",
                "abstract": "Recent advancements in Computer Assisted Diagnosis have shown promising performance in medical imaging tasks, particularly in chest X-ray analysis. However, the interaction between these models and radiologists has been primarily limited to input images. This work proposes a novel approach to enhance human-computer interaction in chest X-ray analysis using Vision-Language Models (VLMs) enhanced with radiologists' attention by incorporating eye gaze data alongside textual prompts. Our approach leverages heatmaps generated from eye gaze data, overlaying them onto medical images to highlight areas of intense radiologist's focus during chest X-ray evaluation. We evaluate this methodology in tasks such as visual question answering, chest X-ray report automation, error detection, and differential diagnosis. Our results demonstrate the inclusion of eye gaze information significantly enhances the accuracy of chest X-ray analysis. Also, the impact of eye gaze on fine-tuning was confirmed as it outperformed other medical VLMs in all tasks except visual question answering. This work marks the potential of leveraging both the VLM's capabilities and the radiologist's domain knowledge to improve the capabilities of AI models in medical imaging, paving a novel way for Computer Assisted Diagnosis with a human-centred AI.",
                "authors": "Yunsoo Kim, Jinge Wu, Yusuf Abdulle, Yue Gao, Honghan Wu",
                "citations": 2
            },
            {
                "title": "Conditional Prototype Rectification Prompt Learning",
                "abstract": "Pre-trained large-scale vision-language models (VLMs) have acquired profound understanding of general visual concepts. Recent advancements in efficient transfer learning (ETL) have shown remarkable success in fine-tuning VLMs within the scenario of limited data, introducing only a few parameters to harness task-specific insights from VLMs. Despite significant progress, current leading ETL methods tend to overfit the narrow distributions of base classes seen during training and encounter two primary challenges: (i) only utilizing uni-modal information to modeling task-specific knowledge; and (ii) using costly and time-consuming methods to supplement knowledge. To address these issues, we propose a Conditional Prototype Rectification Prompt Learning (CPR) method to correct the bias of base examples and augment limited data in an effective way. Specifically, we alleviate overfitting on base classes from two aspects. First, each input image acquires knowledge from both textual and visual prototypes, and then generates sample-conditional text tokens. Second, we extract utilizable knowledge from unlabeled data to further refine the prototypes. These two strategies mitigate biases stemming from base classes, yielding a more effective classifier. Extensive experiments on 11 benchmark datasets show that our CPR achieves state-of-the-art performance on both few-shot classification and base-to-new generalization tasks. Our code is avaliable at \\url{https://github.com/chenhaoxing/CPR}.",
                "authors": "Haoxing Chen, Yaohui Li, Zizheng Huang, Yan Hong, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Huijia Zhu, Weiqiang Wang",
                "citations": 2
            },
            {
                "title": "Unified Generative and Discriminative Training for Multi-modal Large Language Models",
                "abstract": "In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM's hidden state. This approach enhances the MLLM's ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling.",
                "authors": "Wei Chow, Juncheng Li, Qifan Yu, Kaihang Pan, Hao Fei, Zhiqi Ge, Shuai Yang, Siliang Tang, Hanwang Zhang, Qianru Sun",
                "citations": 2
            },
            {
                "title": "Training-free Video Temporal Grounding using Large-scale Pre-trained Models",
                "abstract": "Video temporal grounding aims to identify video segments within untrimmed videos that are most relevant to a given natural language query. Existing video temporal localization models rely on specific datasets for training and have high data collection costs, but they exhibit poor generalization capability under the across-dataset and out-of-distribution (OOD) settings. In this paper, we propose a Training-Free Video Temporal Grounding (TFVTG) approach that leverages the ability of pre-trained large models. A naive baseline is to enumerate proposals in the video and use the pre-trained visual language models (VLMs) to select the best proposal according to the vision-language alignment. However, most existing VLMs are trained on image-text pairs or trimmed video clip-text pairs, making it struggle to (1) grasp the relationship and distinguish the temporal boundaries of multiple events within the same video; (2) comprehend and be sensitive to the dynamic transition of events (the transition from one event to another) in the video. To address these issues, we propose leveraging large language models (LLMs) to analyze multiple sub-events contained in the query text and analyze the temporal order and relationships between these events. Secondly, we split a sub-event into dynamic transition and static status parts and propose the dynamic and static scoring functions using VLMs to better evaluate the relevance between the event and the description. Finally, for each sub-event description, we use VLMs to locate the top-k proposals and leverage the order and relationships between sub-events provided by LLMs to filter and integrate these proposals. Our method achieves the best performance on zero-shot video temporal grounding on Charades-STA and ActivityNet Captions datasets without any training and demonstrates better generalization capabilities in cross-dataset and OOD settings.",
                "authors": "Minghang Zheng, Xinhao Cai, Qingchao Chen, Yuxin Peng, Yang Liu",
                "citations": 2
            },
            {
                "title": "Open-Vocabulary Calibration for Fine-tuned CLIP",
                "abstract": "Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes. The experiments with 7 distinct prompt learning methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed. Our code is available at https://github.com/ml-stat-Sustech/CLIP_Calibration.",
                "authors": "Shuoyuan Wang, Jindong Wang, Guoqing Wang, Bob Zhang, Kaiyang Zhou, Hongxin Wei",
                "citations": 2
            },
            {
                "title": "MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices",
                "abstract": "The attainment of autonomous operations in mobile computing devices has consistently been a goal of human pursuit. With the development of Large Language Models (LLMs) and Visual Language Models (VLMs), this aspiration is progressively turning into reality. While contemporary research has explored automation of simple tasks on mobile devices via VLMs, there remains significant room for improvement in handling complex tasks and reducing high reasoning costs. In this paper, we introduce MobileExperts, which for the first time introduces tool formulation and multi-agent collaboration to address the aforementioned challenges. More specifically, MobileExperts dynamically assembles teams based on the alignment of agent portraits with the human requirements. Following this, each agent embarks on an independent exploration phase, formulating its tools to evolve into an expert. Lastly, we develop a dual-layer planning mechanism to establish coordinate collaboration among experts. To validate our effectiveness, we design a new benchmark of hierarchical intelligence levels, offering insights into algorithm's capability to address tasks across a spectrum of complexity. Experimental results demonstrate that MobileExperts performs better on all intelligence levels and achieves ~ 22% reduction in reasoning costs, thus verifying the superiority of our design.",
                "authors": "Jiayi Zhang, Chuang Zhao, Yihan Zhao, Zhaoyang Yu, Ming He, Jianpin Fan",
                "citations": 2
            },
            {
                "title": "Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with Multimodal Loss",
                "abstract": "The fusion of vision and language has brought about a transformative shift in computer vision through the emergence of Vision-Language Models (VLMs). However, the resource-intensive nature of existing VLMs poses a significant challenge. We need an accessible method for developing the next generation of VLMs. To address this issue, we propose Zoom-shot, a novel method for transferring the zero-shot capabilities of CLIP to any pre-trained vision encoder. We do this by exploiting the multimodal information (i.e. text and image) present in the CLIP latent space through the use of specifically designed multimodal loss functions. These loss functions are (1) cycle-consistency loss and (2) our novel prompt-guided knowledge distillation loss (PG-KD). PG-KD combines the concept of knowledge distillation with CLIP's zero-shot classification, to capture the interactions between text and image features. With our multimodal losses, we train a $\\textbf{linear mapping}$ between the CLIP latent space and the latent space of a pre-trained vision encoder, for only a $\\textbf{single epoch}$. Furthermore, Zoom-shot is entirely unsupervised and is trained using $\\textbf{unpaired}$ data. We test the zero-shot capabilities of a range of vision encoders augmented as new VLMs, on coarse and fine-grained classification datasets, outperforming the previous state-of-the-art in this problem domain. In our ablations, we find Zoom-shot allows for a trade-off between data and compute during training; and our state-of-the-art results can be obtained by reducing training from 20% to 1% of the ImageNet training data with 20 epochs. All code and models are available on GitHub.",
                "authors": "Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, C. Fookes",
                "citations": 2
            },
            {
                "title": "Language-Guided Manipulation with Diffusion Policies and Constrained Inpainting",
                "abstract": "Diffusion policies have demonstrated robust performance in generative modeling, prompting their application in robotic manipulation controlled via language descriptions. In this paper, we introduce a zero-shot, open-vocabulary diffusion policy method for robot manipulation. Using Vision-Language Models (VLMs), our method transforms linguistic task descriptions into actionable keyframes in 3D space. These keyframes serve to guide the diffusion process via inpainting. However, naively enforcing the diffusion process to adhere to the generated keyframes is problematic: the keyframes from the VLMs may be incorrect and lead to action sequences where the diffusion model performs poorly. To address these challenges, we develop an inpainting optimization strategy that balances adherence to the keyframes v.s. the training data distribution. Experimental evaluations demonstrate that our approach surpasses the performance of traditional fine-tuned language-conditioned methods in both simulated and real-world settings.",
                "authors": "Ce Hao, Kelvin Lin, Siyuan Luo, Harold Soh",
                "citations": 2
            },
            {
                "title": "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models",
                "abstract": "Existing works examining Vision-Language Models (VLMs) for social biases predominantly focus on a limited set of documented bias associations, such as gender:profession or race:crime. This narrow scope often overlooks a vast range of unexamined implicit associations, restricting the identification and, hence, mitigation of such biases. We address this gap by probing VLMs to (1) uncover hidden, implicit associations across 9 bias dimensions. We systematically explore diverse input and output modalities and (2) demonstrate how biased associations vary in their negativity, toxicity, and extremity. Our work (3) identifies subtle and extreme biases that are typically not recognized by existing methodologies. We make the Dataset of retrieved associations, (Dora), publicly available here https://github.com/chahatraj/BiasDora.",
                "authors": "Chahat Raj, A. Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu",
                "citations": 2
            },
            {
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models",
                "abstract": "Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing classification accuracy. A notable aspect of our framework is its significantly reduced memory and computational demands when compared to conventional text-prompt tuning methods. Extensive evaluations across 15 image classification datasets involving natural distribution shifts and cross-dataset generalization, as well as in context-dependent visual reasoning, demonstrate TPS's superior performance, achieving state-of-the-art results while reducing resource requirements.",
                "authors": "Elaine Sui, Xiaohan Wang, S. Yeung-Levy",
                "citations": 2
            },
            {
                "title": "μ-Bench: A Vision-Language Benchmark for Microscopy Understanding",
                "abstract": "Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing researchers' efficiency, identifying new image biomarkers, and accelerating hypothesis generation and scientific discovery. However, there is a lack of standardized, diverse, and large-scale vision-language benchmarks to evaluate VLMs' perception and cognition capabilities in biological image understanding. To address this gap, we introduce {\\mu}-Bench, an expert-curated benchmark encompassing 22 biomedical tasks across various scientific disciplines (biology, pathology), microscopy modalities (electron, fluorescence, light), scales (subcellular, cellular, tissue), and organisms in both normal and abnormal states. We evaluate state-of-the-art biomedical, pathology, and general VLMs on {\\mu}-Bench and find that: i) current models struggle on all categories, even for basic tasks such as distinguishing microscopy modalities; ii) current specialist models fine-tuned on biomedical data often perform worse than generalist models; iii) fine-tuning in specific microscopy domains can cause catastrophic forgetting, eroding prior biomedical knowledge encoded in their base model. iv) weight interpolation between fine-tuned and pre-trained models offers one solution to forgetting and improves general performance across biomedical tasks. We release {\\mu}-Bench under a permissive license to accelerate the research and development of microscopy foundation models.",
                "authors": "Alejandro Lozano, Jeffrey J Nirschl, James Burgess, S. Gupte, Yuhui Zhang, Alyssa Unell, S. Yeung-Levy",
                "citations": 2
            },
            {
                "title": "VALE: A Multimodal Visual and Language Explanation Framework for Image Classifiers using eXplainable AI and Language Models",
                "abstract": "Deep Neural Networks (DNNs) have revolutionized various fields by enabling task automation and reducing human error. However, their internal workings and decision-making processes remain obscure due to their black box nature. Consequently, the lack of interpretability limits the application of these models in high-risk scenarios. To address this issue, the emerging field of eXplainable Artificial Intelligence (XAI) aims to explain and interpret the inner workings of DNNs. Despite advancements, XAI faces challenges such as the semantic gap between machine and human understanding, the trade-off between interpretability and performance, and the need for context-specific explanations. To overcome these limitations, we propose a novel multimodal framework named VALE Visual and Language Explanation. VALE integrates explainable AI techniques with advanced language models to provide comprehensive explanations. This framework utilizes visual explanations from XAI tools, an advanced zero-shot image segmentation model, and a visual language model to generate corresponding textual explanations. By combining visual and textual explanations, VALE bridges the semantic gap between machine outputs and human interpretation, delivering results that are more comprehensible to users. In this paper, we conduct a pilot study of the VALE framework for image classification tasks. Specifically, Shapley Additive Explanations (SHAP) are used to identify the most influential regions in classified images. The object of interest is then extracted using the Segment Anything Model (SAM), and explanations are generated using state-of-the-art pre-trained Vision-Language Models (VLMs). Extensive experimental studies are performed on two datasets: the ImageNet dataset and a custom underwater SONAR image dataset, demonstrating VALEs real-world applicability in underwater image classification.",
                "authors": "Purushothaman Natarajan, Athira Nambiar",
                "citations": 2
            },
            {
                "title": "Enhancing Vision-Language Few-Shot Adaptation with Negative Learning",
                "abstract": "Large-scale pre-trained Vision-Language Models (VLMs) have exhibited impressive zero-shot performance and transferability, allowing them to adapt to downstream tasks in a data-efficient manner. However, when only a few labeled samples are available, adapting VLMs to distinguish subtle differences between similar classes in specific downstream tasks remains challenging. In this work, we propose a Simple yet effective Negative Learning approach, SimNL, to more efficiently exploit the task-specific knowledge from few-shot labeled samples. Unlike previous methods that focus on identifying a set of representative positive features defining\"what is a {CLASS}\", SimNL discovers a complementary set of negative features that define\"what is not a {CLASS}\", providing additional insights that supplement the positive features to enhance task-specific recognition capability. Further, we identify that current adaptation approaches are particularly vulnerable to potential noise in the few-shot sample set. To mitigate this issue, we introduce a plug-and-play few-shot instance reweighting technique to suppress noisy outliers and amplify clean samples for more stable adaptation. Our extensive experimental results across 15 datasets validate that the proposed SimNL outperforms existing state-of-the-art methods on both few-shot learning and domain generalization tasks while achieving competitive computational efficiency. Code is available at https://github.com/zhangce01/SimNL.",
                "authors": "Ce Zhang, Simon Stepputtis, Katia P. Sycara, Yaqi Xie",
                "citations": 2
            },
            {
                "title": "ECOR: Explainable CLIP for Object Recognition",
                "abstract": "Large Vision Language Models (VLMs), such as CLIP, have significantly contributed to various computer vision tasks, including object recognition and object detection. Their open vocabulary feature enhances their value. However, their black-box nature and lack of explainability in predictions make them less trustworthy in critical domains. Recently, some work has been done to force VLMs to provide reasonable rationales for object recognition, but this often comes at the expense of classification accuracy. In this paper, we first propose a mathematical definition of explainability in the object recognition task based on the joint probability distribution of categories and rationales, then leverage this definition to fine-tune CLIP in an explainable manner. Through evaluations of different datasets, our method demonstrates state-of-the-art performance in explainable classification. Notably, it excels in zero-shot settings, showcasing its adaptability. This advancement improves explainable object recognition, enhancing trust across diverse applications. The code will be made available online upon publication.",
                "authors": "Ali Rasekh, Sepehr Kazemi Ranjbar, Milad Heidari, Wolfgang Nejdl",
                "citations": 2
            },
            {
                "title": "Boosting Vision-Language Models with Transduction",
                "abstract": "Transduction is a powerful paradigm that leverages the structure of unlabeled data to boost predictive accuracy. We present TransCLIP, a novel and computationally efficient transductive approach designed for Vision-Language Models (VLMs). TransCLIP is applicable as a plug-and-play module on top of popular inductive zero- and few-shot models, consistently improving their performances. Our new objective function can be viewed as a regularized maximum-likelihood estimation, constrained by a KL divergence penalty that integrates the text-encoder knowledge and guides the transductive learning process. We further derive an iterative Block Majorize-Minimize (BMM) procedure for optimizing our objective, with guaranteed convergence and decoupled sample-assignment updates, yielding computationally efficient transduction for large-scale datasets. We report comprehensive evaluations, comparisons, and ablation studies that demonstrate: (i) Transduction can greatly enhance the generalization capabilities of inductive pretrained zero- and few-shot VLMs; (ii) TransCLIP substantially outperforms standard transductive few-shot learning methods relying solely on vision features, notably due to the KL-based language constraint.",
                "authors": "Maxime Zanella, Benoit G'erin, Ismail Ben Ayed",
                "citations": 2
            },
            {
                "title": "Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization",
                "abstract": "The vocabulary size in temporal action localization (TAL) is limited by the scarcity of large-scale annotated datasets. To overcome this, recent works integrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL (OV-TAL). However, despite the success of VLMs trained on extensive datasets, existing OV-TAL methods still rely on human-labeled TAL datasets of limited size to train action localizers, limiting their generalizability. In this paper, we explore the scalability of self-training with unlabeled YouTube videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic action localizer is trained on a human-labeled TAL dataset to generate pseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled dataset is then used to train the localizer. Extensive experiments demonstrate that leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer. Additionally, we identify limitations in existing OV-TAL evaluation schemes and propose a new benchmark for thorough assessment. Finally, we showcase the TAL performance of the large multimodal model Gemini-1.5 on our new benchmark. Code is released at https://github.com/HYUNJS/STOV-TAL.",
                "authors": "Jeongseok Hyun, Su Ho Han, Hyolim Kang, Joon-Young Lee, Seon Joo Kim",
                "citations": 2
            },
            {
                "title": "Zero-shot Building Age Classification from Facade Image Using GPT-4",
                "abstract": "Abstract. A building’s age of construction is crucial for supporting many geospatial applications. Much current research focuses on estimating building age from facade images using deep learning. However, building an accurate deep learning model requires a considerable amount of labelled training data, and the trained models often have geographical constraints. Recently, large pre-trained vision language models (VLMs) such as GPT-4 Vision, which demonstrate significant generalisation capabilities, have emerged as potential training-free tools for dealing with specific vision tasks, but their applicability and reliability for building information remain unexplored. In this study, a zero-shot building age classifier for facade images is developed using prompts that include logical instructions. Taking London as a test case, we introduce a new dataset, FI-London, comprising facade images and building age epochs. Although the training-free classifier achieved a modest accuracy of 39.69%, the mean absolute error of 0.85 decades indicates that the model can predict building age epochs successfully albeit with a small bias. The ensuing discussion reveals that the classifier struggles to predict the age of very old buildings and is challenged by fine-grained predictions within 2 decades. Overall, the classifier utilising GPT-4 Vision is capable of predicting the rough age epoch of a building from a single facade image without any training. The code and dataset are available at https://zichaozeng.github.io/ba_classifier.\n",
                "authors": "Zichao Zeng, June Moh Goo, Xinglei Wang, Bin Chi, Meihui Wang, Jan Boehm",
                "citations": 2
            },
            {
                "title": "Visual-Language Decision System Through Integration of Foundation Models for Service Robot Navigation",
                "abstract": "This study aims to build a system that bridges the gap between robotics and environmental understanding by integrating various foundation models. While current visual-language models (VLMs) and large language models (LLMs) have demonstrated robust capabilities in image recognition and language comprehension, challenges remain in integrating them into practical robotic applications. Therefore, we propose a visual-language decision (VLD) system that allows a robot to autonomously analyze its surroundings using three VLMs (CLIP, OFA, and PaddleOCR) to generate semantic information. This information is further processed using the GPT-3 LLM, which allows the robot to make judgments during autonomous navigation. The contribution is twofold: 1) We show that integrating CLIP, OFA, and PaddleOCR into a robotic system can generate task-critical information in unexplored environments; 2) We explore how to effectively use GPT-3 to match the results generated by specific VLMs and make navigation decisions based on environmental information. We also implement a photorealistic training environment using Isaac Sim to test and validate the proposed VLD system in simulation. Finally, we demonstrate VLD-based real-world navigation in an unexplored environment using a TurtleBot3 robot equipped with a lidar and an RGB camera.",
                "authors": "Peiyuan Zhu, Lotfi El Hafi, T. Taniguchi",
                "citations": 2
            },
            {
                "title": "Understanding Figurative Meaning through Explainable Visual Entailment",
                "abstract": "Large Vision-Language Models (VLMs) have demonstrated strong capabilities in tasks requiring a fine-grained understanding of literal meaning in images and text, such as visual question-answering or visual entailment. However, there has been little exploration of these models' capabilities when presented with images and captions containing figurative meaning, such as metaphors or humor. To close this gap, we propose a new task framing the figurative meaning understanding problem as an explainable visual entailment task, where the model has to predict whether the image (premise) entails a caption (hypothesis) and justify the predicted label with a textual explanation. The figurative phenomena can be present either in the image, the caption, or both. Utilizing a human-AI collaboration approach, we build the accompanying expert-verified dataset V-FLUTE, containing 6,027 {image, caption, label, explanation} instances spanning five diverse figurative phenomena: metaphors, similes, idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs struggle to generalize from literal to figurative meaning, particularly when it is present in images. Further, we identify common types of errors in VLM reasoning via human evaluation.",
                "authors": "Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, S. Muresan",
                "citations": 2
            },
            {
                "title": "CityLLaVA: Efficient Fine-Tuning for VLMs in City Scenario",
                "abstract": "In the vast and dynamic landscape of urban settings, Traffic Safety Description and Analysis plays a pivotal role in applications ranging from insurance inspection to accident prevention. This paper introduces CityLLaVA, a novel fine-tuning framework for Visual Language Models (VLMs) designed for urban scenarios. CityLLaVA enhances model comprehension and prediction accuracy through (1) employing bounding boxes for optimal visual data preprocessing, including video best-view selection and visual prompt engineering during both training and testing phases; (2) constructing concise Question-Answer sequences and designing textual prompts to refine instruction comprehension; (3) implementing block expansion to finetune large VLMs efficiently; and (4) advancing prediction accuracy via a unique sequential questioning-based prediction augmentation. Demonstrating top-tier performance, our method achieved a benchmark score of 33.4308, securing the leading position on the leaderboard. The code will be released soon.",
                "authors": "Zhizhao Duan, Hao Cheng, Duo Xu, Xi Wu, Xiangxie Zhang, Xi Ye, Zhen Xie",
                "citations": 2
            },
            {
                "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models",
                "abstract": "Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.",
                "authors": "Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Christopher Callison-Burch, Andrew Head, Rose Hendrix, F. Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Christopher Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hanna Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi",
                "citations": 2
            },
            {
                "title": "Task Vectors are Cross-Modal",
                "abstract": "We investigate the internal representations of vision-and-language models (VLMs) and how they encode task representations. We consider tasks specified through examples or instructions, using either text or image inputs. Surprisingly, we find that conceptually similar tasks are mapped to similar task vector representations, regardless of how they are specified. Our findings suggest that to output answers, tokens in VLMs undergo three distinct phases: input, task, and answer, a process which is consistent across different modalities and specifications. The task vectors we identify in VLMs are general enough to be derived in one modality (e.g., text) and transferred to another (e.g., image). Additionally, we find that ensembling exemplar and instruction based task vectors produce better task representations. Taken together, these insights shed light on the underlying mechanisms of VLMs, particularly their ability to represent tasks in a shared manner across different modalities and task specifications. Project page: https://task-vectors-are-cross-modal.github.io.",
                "authors": "Grace Luo, Trevor Darrell, Amir Bar",
                "citations": 2
            },
            {
                "title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations",
                "abstract": "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation.",
                "authors": "Nick Jiang, Anish Kachinthaya, Suzie Petryk, Yossi Gandelsman",
                "citations": 2
            },
            {
                "title": "Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector",
                "abstract": "Visual Language Models (VLMs) are vulnerable to adversarial attacks, especially those from adversarial images, which is however under-explored in literature. To facilitate research on this critical safety problem, we first construct a new laRge-scale Adervsarial images dataset with Diverse hArmful Responses (RADAR), given that existing datasets are either small-scale or only contain limited types of harmful responses. With the new RADAR dataset, we further develop a novel and effective iN-time Embedding-based AdveRSarial Image DEtection (NEARSIDE) method, which exploits a single vector that distilled from the hidden states of VLMs, which we call the attacking direction, to achieve the detection of adversarial images against benign ones in the input. Extensive experiments with two victim VLMs, LLaVA and MiniGPT-4, well demonstrate the effectiveness, efficiency, and cross-model transferrability of our proposed method. Our code is available at https://github.com/mob-scu/RADAR-NEARSIDE",
                "authors": "Youcheng Huang, Fengbin Zhu, Jingkun Tang, Pan Zhou, Wenqiang Lei, Jiancheng Lv, Tat-seng Chua",
                "citations": 2
            },
            {
                "title": "Representing Online Handwriting for Recognition in Large Vision-Language Models",
                "abstract": "The adoption of tablets with touchscreens and styluses is increasing, and a key feature is converting handwriting to text, enabling search, indexing, and AI assistance. Meanwhile, vision-language models (VLMs) are now the go-to solution for image understanding, thanks to both their state-of-the-art performance across a variety of tasks and the simplicity of a unified approach to training, fine-tuning, and inference. While VLMs obtain high performance on image-based tasks, they perform poorly on handwriting recognition when applied naively, i.e., by rendering handwriting as an image and performing optical character recognition (OCR). In this paper, we study online handwriting recognition with VLMs, going beyond naive OCR. We propose a novel tokenized representation of digital ink (online handwriting) that includes both a time-ordered sequence of strokes as text, and as image. We show that this representation yields results comparable to or better than state-of-the-art online handwriting recognizers. Wide applicability is shown through results with two different VLM families, on multiple public datasets. Our approach can be applied to off-the-shelf VLMs, does not require any changes in their architecture, and can be used in both fine-tuning and parameter-efficient tuning. We perform a detailed ablation study to identify the key elements of the proposed representation.",
                "authors": "Anastasiia Fadeeva, Philippe Schlattner, Andrii Maksai, Mark Collier, Efi Kokiopoulou, Jesse Berent, C. Musat",
                "citations": 2
            },
            {
                "title": "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) are becoming increasingly vulnerable to adversarial attacks as various novel attack strategies are being proposed against these models. While existing defenses excel in unimodal contexts, they currently fall short in safeguarding VLMs against adversarial threats. To mitigate this vulnerability, we propose a novel, yet elegantly simple approach for detecting adversarial samples in VLMs. Our method leverages Text-to-Image (T2I) models to generate images based on captions produced by target VLMs. Subsequently, we calculate the similarities of the embeddings of both input and generated images in the feature space to identify adversarial samples. Empirical evaluations conducted on different datasets validate the efficacy of our approach, outperforming baseline methods adapted from image classification domains. Furthermore, we extend our methodology to classification tasks, showcasing its adaptability and model-agnostic nature. Theoretical analyses and empirical findings also show the resilience of our approach against adaptive attacks, positioning it as an excellent defense mechanism for real-world deployment against adversarial threats.",
                "authors": "Samar Fares, Klea Ziu, Toluwani Aremu, N. Durasov, Martin Tak'avc, Pascal Fua, Karthik Nandakumar, Ivan Laptev",
                "citations": 2
            },
            {
                "title": "Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data",
                "abstract": "Fine-tuning vision-language models (VLMs) with abundant unlabeled data recently has attracted increasing attention. Existing methods that resort to the pseudolabeling strategy would suffer from heavily incorrect hard pseudolabels when VLMs exhibit low zero-shot performance in downstream tasks. To alleviate this issue, we propose a Candidate Pseudolabel Learning method, termed CPL, to fine-tune VLMs with suitable candidate pseudolabels of unlabeled data in downstream tasks. The core of our method lies in the generation strategy of candidate pseudolabels, which progressively generates refined candidate pseudolabels by both intra- and inter-instance label selection, based on a confidence score matrix for all unlabeled data. This strategy can result in better performance in true label inclusion and class-balanced instance selection. In this way, we can directly apply existing loss functions to learn with generated candidate psueudolabels. Extensive experiments on nine benchmark datasets with three learning paradigms demonstrate the effectiveness of our method. Our code can be found at https://github.com/vanillaer/CPL-ICML2024.",
                "authors": "Jiahan Zhang, Qi Wei, Feng Liu, Lei Feng",
                "citations": 2
            },
            {
                "title": "Improving Zero-Shot Generalization for CLIP with Variational Adapter",
                "abstract": null,
                "authors": "Ziqian Lu, Fengli Shen, Mushui Liu, Yunlong Yu, Xi Li",
                "citations": 2
            },
            {
                "title": "Generating Out-Of-Distribution Scenarios Using Language Models",
                "abstract": "The deployment of autonomous vehicles controlled by machine learning techniques requires extensive testing in diverse real-world environments, robust handling of edge cases and out-of-distribution scenarios, and comprehensive safety validation to ensure that these systems can navigate safely and effectively under unpredictable conditions. Addressing Out-Of-Distribution (OOD) driving scenarios is essential for enhancing safety, as OOD scenarios help validate the reliability of the models within the vehicle's autonomy stack. However, generating OOD scenarios is challenging due to their long-tailed distribution and rarity in urban driving dataset. Recently, Large Language Models (LLMs) have shown promise in autonomous driving, particularly for their zero-shot generalization and common-sense reasoning capabilities. In this paper, we leverage these LLM strengths to introduce a framework for generating diverse OOD driving scenarios. Our approach uses LLMs to construct a branching tree, where each branch represents a unique OOD scenario. These scenarios are then simulated in the CARLA simulator using an automated framework that aligns scene augmentation with the corresponding textual descriptions. We evaluate our framework through extensive simulations, and assess its performance via a diversity metric that measures the richness of the scenarios. Additionally, we introduce a new\"OOD-ness\"metric, which quantifies how much the generated scenarios deviate from typical urban driving conditions. Furthermore, we explore the capacity of modern Vision-Language Models (VLMs) to interpret and safely navigate through the simulated OOD scenarios. Our findings offer valuable insights into the reliability of language models in addressing OOD scenarios within the context of urban driving.",
                "authors": "Erfan Aasi, Phat Nguyen, Shiva Sreeram, G. Rosman, S. Karaman, Daniela Rus",
                "citations": 2
            },
            {
                "title": "Multi-agent Planning using Visual Language Models",
                "abstract": "Large Language Models (LLMs) and Visual Language Models (VLMs) are attracting increasing interest due to their improving performance and applications across various domains and tasks. However, LLMs and VLMs can produce erroneous results, especially when a deep understanding of the problem domain is required. For instance, when planning and perception are needed simultaneously, these models often struggle because of difficulties in merging multi-modal information. To address this issue, fine-tuned models are typically employed and trained on specialized data structures representing the environment. This approach has limited effectiveness, as it can overly complicate the context for processing. In this paper, we propose a multi-agent architecture for embodied task planning that operates without the need for specific data structures as input. Instead, it uses a single image of the environment, handling free-form domains by leveraging commonsense knowledge. We also introduce a novel, fully automatic evaluation procedure, PG2S, designed to better assess the quality of a plan. We validated our approach using the widely recognized ALFRED dataset, comparing PG2S to the existing KAS metric to further evaluate the quality of the generated plans.",
                "authors": "Michele Brienza, F. Argenziano, Vincenzo Suriani, D. Bloisi, Daniele Nardi",
                "citations": 2
            },
            {
                "title": "D\\'ej\\`a Vu Memorization in Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d\\'ej\\`a vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d\\'ej\\`a vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance.",
                "authors": "Bargav Jayaraman, Chuan Guo, Kamalika Chaudhuri",
                "citations": 2
            },
            {
                "title": "WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines",
                "abstract": "Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.",
                "authors": "Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, N. Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, C. Mawalim, Ching Lam Cheng, D. Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, Shi-Xiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, D. Adelani, En-Shiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, Chong-Wah Ngo",
                "citations": 2
            },
            {
                "title": "A Vision–Language Model-Based Traffic Sign Detection Method for High-Resolution Drone Images: A Case Study in Guyuan, China",
                "abstract": "As a fundamental element of the transportation system, traffic signs are widely used to guide traffic behaviors. In recent years, drones have emerged as an important tool for monitoring the conditions of traffic signs. However, the existing image processing technique is heavily reliant on image annotations. It is time consuming to build a high-quality dataset with diverse training images and human annotations. In this paper, we introduce the utilization of Vision–language Models (VLMs) in the traffic sign detection task. Without the need for discrete image labels, the rapid deployment is fulfilled by the multi-modal learning and large-scale pretrained networks. First, we compile a keyword dictionary to explain traffic signs. The Chinese national standard is used to suggest the shape and color information. Our program conducts Bootstrapping Language-image Pretraining v2 (BLIPv2) to translate representative images into text descriptions. Second, a Contrastive Language-image Pretraining (CLIP) framework is applied to characterize not only drone images but also text descriptions. Our method utilizes the pretrained encoder network to create visual features and word embeddings. Third, the category of each traffic sign is predicted according to the similarity between drone images and keywords. Cosine distance and softmax function are performed to calculate the class probability distribution. To evaluate the performance, we apply the proposed method in a practical application. The drone images captured from Guyuan, China, are employed to record the conditions of traffic signs. Further experiments include two widely used public datasets. The calculation results indicate that our vision–language model-based method has an acceptable prediction accuracy and low training cost.",
                "authors": "Jianqun Yao, Jinming Li, Yuxuan Li, Mingzhu Zhang, Chen Zuo, Shi Dong, Zhe Dai",
                "citations": 2
            },
            {
                "title": "Direct Preference Optimization for Suppressing Hallucinated Prior Exams in Radiology Report Generation",
                "abstract": "Recent advances in generative vision-language models (VLMs) have exciting potential implications for AI in radiology, yet VLMs are also known to produce hallucinations, nonsensical text, and other unwanted behaviors that can waste clinicians' time and cause patient harm. Drawing on recent work on direct preference optimization (DPO), we propose a simple method for modifying the behavior of pretrained VLMs performing radiology report generation by suppressing unwanted types of generations. We apply our method to the prevention of hallucinations of prior exams, addressing a long-established problem behavior in models performing chest X-ray report generation. Across our experiments, we find that DPO fine-tuning achieves a 3.2-4.8x reduction in lines hallucinating prior exams while maintaining model performance on clinical accuracy metrics. Our work is, to the best of our knowledge, the first work to apply DPO to medical VLMs, providing a data- and compute- efficient way to suppress problem behaviors while maintaining overall clinical accuracy.",
                "authors": "Oishi Banerjee, Hong-Yu Zhou, Subathra Adithan, Stephen Kwak, Kay Wu, P. Rajpurkar",
                "citations": 2
            },
            {
                "title": "ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models",
                "abstract": "Assessing the quality of outputs generated by generative models, such as large language models and vision language models, presents notable challenges. Traditional methods for evaluation typically rely on either human assessments, which are resource-intensive, or automatic metrics that often show a low correlation with human judgment. Another common approach is to use deep learning systems, which not only consume a substantial amount of compute and time but also require extensive training data. In this study, we introduce a tuning-free framework called ReFeR, designed to evaluate generative outputs, including both text and images, by leveraging a 2-level hierarchy of LLMs and VLMs themselves. We rigorously evaluate our framework, ReFeR, across four diverse evaluation tasks. The framework not only improves the accuracy of these evaluations, surpassing previous benchmarks but also generates constructive feedback. Interestingly, the framework is also applicable to reasoning tasks. Experiments on four reasoning tasks demonstrate superior collective reasoning abilities of the framework. We present two variants of the framework: ReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a more cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient while being comparably accurate to ReFeR-Turbo. We make code, data and PIP package publicly available. See this PIP URL https://pypi.org/project/refer-agents/ and this Git URL https://github.com/yaswanth-iitkgp/ReFeR_Code .",
                "authors": "Yaswanth Narsupalli, Abhranil Chandra, Sreevatsa Muppirala, Manish Gupta, Pawan Goyal",
                "citations": 2
            },
            {
                "title": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models",
                "abstract": "Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands. Recent studies suggest decomposing a fine-tuned LLM into a base model and corresponding delta weights, which are then compressed using low-rank or low-bit approaches to reduce costs. In this work, we observe that existing low-rank and low-bit compression methods can significantly harm the model performance for task-specific fine-tuned LLMs (e.g., WizardMath for math problems). Motivated by the long-tail distribution of singular values in the delta weights, we propose a delta quantization approach using mixed-precision. This method employs higher-bit representation for singular vectors corresponding to larger singular values. We evaluate our approach on various fine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs. Experimental results demonstrate that our approach performs comparably to full fine-tuned LLMs, surpassing both low-rank and low-bit baselines by a considerable margin. Additionally, we show that our method is compatible with various backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its generalizability.",
                "authors": "Bowen Ping, Shuo Wang, Hanqing Wang, Xu Han, Yuzhuang Xu, Yukun Yan, Yun Chen, Baobao Chang, Zhiyuan Liu, Maosong Sun",
                "citations": 1
            },
            {
                "title": "Change in Mindfulness Profiles After Mindfulness-Based Cognitive Therapy for Major Depressive Disorder",
                "abstract": null,
                "authors": "Jelle Lubbers, Philip Spinhoven, M. Cladder-Micus, Jan Spijker, A. Speckens, D. Geurts",
                "citations": 1
            },
            {
                "title": "Words2Contact: Identifying Support Contacts from Verbal Instructions Using Foundation Models",
                "abstract": "This paper presents Words2Contact, a language-guided multi-contact placement pipeline leveraging large language models and vision language models. Our method is a key component for language-assisted teleoperation and human-robot cooperation, where human operators can instruct the robots where to place their support contacts before whole-body reaching or manipulation using natural language. Words2Contact transforms the verbal instructions of a human operator into contact placement predictions; it also deals with iterative corrections, until the human is satisfied with the contact location identified in the robot’s field of view. We benchmark state-of-the-art LLMs and VLMs for size and performance in contact prediction. We demonstrate the effectiveness of the iterative correction process, showing that users, even naive, quickly learn how to instruct the system to obtain accurate locations. Finally, we validate Words2Contact in real-world experiments with the Talos humanoid robot, instructed by human operators to place support contacts on different locations and surfaces to avoid falling when reaching for distant objects.",
                "authors": "Dionis Totsila, Quentin Rouxel, Jean-Baptiste Mouret, S. Ivaldi",
                "citations": 1
            },
            {
                "title": "CountCLIP - [Re] Teaching CLIP to Count to Ten",
                "abstract": "Large vision-language models (VLMs) are shown to learn rich joint image-text representations enabling high performances in relevant downstream tasks. However, they fail to showcase their quantitative understanding of objects, and they lack good counting-aware representation. This paper conducts a reproducibility study of 'Teaching CLIP to Count to Ten' (Paiss et al., 2023), which presents a method to finetune a CLIP model (Radford et al., 2021) to improve zero-shot counting accuracy in an image while maintaining the performance for zero-shot classification by introducing a counting-contrastive loss term. We improve the model's performance on a smaller subset of their training data with lower computational resources. We verify these claims by reproducing their study with our own code. The implementation can be found at https://github.com/SforAiDl/CountCLIP.",
                "authors": "Harshvardhan Mestha, Tejas Agrawal, Karan Bania, V. Shreyas, Yash Bhisikar",
                "citations": 1
            },
            {
                "title": "DegustaBot: Zero-Shot Visual Preference Estimation for Personalized Multi-Object Rearrangement",
                "abstract": "De gustibus non est disputandum (\"there is no accounting for others' tastes\") is a common Latin maxim describing how many solutions in life are determined by people's personal preferences. Many household tasks, in particular, can only be considered fully successful when they account for personal preferences such as the visual aesthetic of the scene. For example, setting a table could be optimized by arranging utensils according to traditional rules of Western table setting decorum, without considering the color, shape, or material of each object, but this may not be a completely satisfying solution for a given person. Toward this end, we present DegustaBot, an algorithm for visual preference learning that solves household multi-object rearrangement tasks according to personal preference. To do this, we use internet-scale pre-trained vision-and-language foundation models (VLMs) with novel zero-shot visual prompting techniques. To evaluate our method, we collect a large dataset of naturalistic personal preferences in a simulated table-setting task, and conduct a user study in order to develop two novel metrics for determining success based on personal preference. This is a challenging problem and we find that 50% of our model's predictions are likely to be found acceptable by at least 20% of people.",
                "authors": "Benjamin A. Newman, Pranay Gupta, Kris Kitani, Yonatan Bisk, H. Admoni, Chris Paxton",
                "citations": 1
            },
            {
                "title": "Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training",
                "abstract": "In rapidly evolving field of vision-language models (VLMs), contrastive language-image pre-training (CLIP) has made significant strides, becoming foundation for various downstream tasks. However, relying on one-to-one (image, text) contrastive paradigm to learn alignment from large-scale messy web data, CLIP faces a serious myopic dilemma, resulting in biases towards monotonous short texts and shallow visual expressivity. To overcome these issues, this paper advances CLIP into one novel holistic paradigm, by updating both diverse data and alignment optimization. To obtain colorful data with low cost, we use image-to-text captioning to generate multi-texts for each image, from multiple perspectives, granularities, and hierarchies. Two gadgets are proposed to encourage textual diversity. To match such (image, multi-texts) pairs, we modify the CLIP image encoder into multi-branch, and propose multi-to-multi contrastive optimization for image-text part-to-part matching. As a result, diverse visual embeddings are learned for each image, bringing good interpretability and generalization. Extensive experiments and ablations across over ten benchmarks indicate that our holistic CLIP significantly outperforms existing myopic CLIP, including image-text retrieval, open-vocabulary classification, and dense visual tasks.",
                "authors": "Haicheng Wang, Chen Ju, Weixiong Lin, Shuai Xiao, Mengting Chen, Yixuan Huang, Chang Liu, Mingshuai Yao, Jinsong Lan, Ying Chen, Qingwen Liu, Yanfeng Wang",
                "citations": 1
            },
            {
                "title": "VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models",
                "abstract": "The recent developments in deep learning led to the integration of natural language processing (NLP) with computer vision, resulting in powerful integrated Vision and Language Models (VLMs). Despite their remarkable capabilities, these models are frequently regarded as black boxes within the machine learning research community. This raises a critical question: which parts of an image correspond to specific segments of text, and how can we decipher these associations? Understanding these connections is essential for enhancing model transparency, interpretability, and trustworthiness. To answer this question, we present an image-text aligned human visual attention dataset that maps specific associations between image regions and corresponding text segments. We then compare the internal heatmaps generated by VL models with this dataset, allowing us to analyze and better understand the model's decision-making process. This approach aims to enhance model transparency, interpretability, and trustworthiness by providing insights into how these models align visual and linguistic information. We conducted a comprehensive study on text-guided visual saliency detection in these VL models. This study aims to understand how different models prioritize and focus on specific visual elements in response to corresponding text segments, providing deeper insights into their internal mechanisms and improving our ability to interpret their outputs.",
                "authors": "Tolga Tasdizen, Josh OpenAI, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Aleman, Almeida Janko, Sam Altenschmidt, Shyamal Altman, Red Anad-kat, Igor Avila, Suchir Babuschkin, Valerie Bal-aji, Paul Balcom, Haim-ing Baltescu, Bao Mohammad, Jeff Bavarian, Ir-wan Belgum, Bello Jake, Gabriel Berdine, Christo-pher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Made-laine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brock-man, Miles Brooks, Kevin Brundage, Button Trevor, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Chen Sully, Ruby Chen, Jason Chen, Mark Chen, Chen Ben, Chester Chess, Casey Cho, Hyung Won Chu, D. Chung, Jeremiah Cummings, Currier, Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-hannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, B. Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kan-itscheider, Nitish Shirish, Tabarak Keskar, Logan Khan, J. Kilpatrick, Christina Kim, Kim Yongjik, Jan Hendrik Kim, Jamie Kirchner, Kiros Matt, Daniel Knight, Łukasz Kokotajlo, Kondraciuk Andrew, Aris Kondrich, Kyle Konstantinidis, Kosic Gretchen, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Chak Ming Levy, Rachel Li, Molly Lim, Lin Stephanie, Mateusz Lin, Theresa Litwin, Lopez Ryan, Patricia Lowe, Anna Lue, Kim Makanju, S. Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Bob Mayne, Scott Mayer McGrew, McKinney Christine, Paul McLeavey, Jake McMillan, McNeil David, Aalok Medina, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Vinnie Mishkin, Evan Monaco, Daniel Morikawa, Tong Mossing, Mira Mu, Oleg Murati, David Murk, Ashvin Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Cullen Ouyang, Jakub O’Keefe, Alex Pachocki, J. Paino, Ashley Palermo, Giambat-tista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Filipe Perel-man, de Avila Belbute, Michael Peres, Petrov Henrique, Pondé, Michael Oliveira Pinto, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack W. Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rim-bach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Shep-pard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Stau-dacher, F. Such, Natalie Summers, I. Sutskever, Jie Tang, N. Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe, Cerón Uribe, Andrea Vallone, Matt Weng, Dave Wiethoff, Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, Barret Zoph. 2024, Solomon Neyshabur, Adrian Kim, Priyanka Hutter, Alex Agrawal, George Castro-Ros, Tao van den Driess-che, Fan Wang, Shuo yiin Yang, Paul Chang, Ross Komarek, Mario McIlroy, Guodong Luˇci´c, Wael Zhang, Michael Farhan, Paul Sharman, Paul Nat-sev, Yamini Michel, Siyuan Bansal, Kris Qiao, Siamak Cao, Christina Shakeri, Justin Butterfield, Paul Kishan Chung, Shivani Rubenstein, Agrawal Arthur, Kedar Mensch, Karel Soparkar, Timothy Lenc, Aedan Chung, Loren Pope, Jackie Maggiore, Priya Kay, Shibo Jhakra, Joshua Wang, Maynez Mary, Taylor Phuong, Andrea Tobin, Maja Tacchetti, Kevin Trebacz, Yash Robinson, S. Katariya, Paige Riedel, Ke-fan Bailey, Nimesh Xiao, Lora Ghe-lani, Ambrose Aroyo, Neil Slone, Houlsby Xuehan, Zhen Xiong, Elena Yang, Gribovskaya Jonas, Mateo Adler, Lisa Wirth, M. Lee, Li Thais, Jay Kagohara, So-phie Pavagadhi, Bridgers Anna, Sanjay Bortsova, Zafarali Ghemawat, Ahmed Tianqi, Richard Liu, Vijay Powell, Mariko Bolina, Polina Iinuma, James Zablotskaia, Da-Woon Besley, Timothy Chung, Ramona Dozat, Xiance Comanescu, Jeremy Si, Guolong Greer, Martin Su, Polacek Raphaël Lopez, Simon Kaufman, Hexiang Tokumine, Elena Hu, Yingjie Buchatskaya, Mohamed Miao, Aditya Elhawaty, Nenad Siddhant, Tomasev Jinwei, Christina Xing, Helen Greer, Shereen Miller, Aurko Ashraf, Zizhao Roy, Ada Zhang, An-gelos Ma, Milos Filos, Rory Besta, Ted Blevins, Chih-Kuan Kli-menko, Soravit Yeh, Jiaqi Changpinyo, Oscar Mu, Mantas Chang, Carrie Pajarskas, Muir Vered, Charline Cohen, Krishna Le Lan, Haridasan Amit, Steven Marathe, Sholto Hansen, Ra-jkumar Douglas, Mingqiu Samuel, Sophia Wang, Austin Chang, Jiepu Lan, Justin Jiang, Jaime Alonso Chiu, Lars Lowe Lorenzo, Sébastien Sjösund, Zach Cevey, Thi Gleicher, Anudhyan Avrahami, Hansa Boral, Vittorio Srinivasan, Rhys Selo, Kon-stantinos May, Léonard Aisopos, Hussenot, Livio Baldini, Soares Kate, Michael B Baumli, A. Chang, Recasens Ben, Alexander Caine, Filip Pritzel, Fabio Pavetic, Anita Pardo, Justin Gergely, Vinay Frye, Ramasesh Dan, Kartikeya Horgan, Nora Badola, Kassner, Ab-hishek Jindal, S. Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxi-aoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, Ça˘glar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson, Maxwell Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Ro-driguez, T. Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara N. Sainath, Maribeth Rauh, Sayed Hadi, Hashemi Richard, Yana Ives, Eric Hasson, Yuan Noland, N. Cao, Le Byrd, Qingze Hou, Thibault Wang, Michela Sottiaux, Jean-Baptiste Paganini, Lespiau Alexandre, Samer Moufarek, Kaushik Hassan, Joost Shiv-akumar, Amol van Amersfoort, Mandhane Pratik, Anirudh Joshi, Matthew Goyal, Andrew Tung, Hannah Brock, Vedant Sheahan, Cheng Misra, Nemanja Li, Mostafa Raki´cevi´c, Fangyu Dehghani, Sid Liu, Junhyuk Mittal, Seb Oh, Eren Noury, Fantine Sezener, Matthew Huot, Nicola Lamm, Charlie De Cao, Sidharth Chen, Romina Mudgal, Stella Kevin, Gautam Brooks, Chenxi Vasudevan, Liu Mainak, Nivedita Chain, Aaron Melinkeri, Cohen Venus, Kristie Wang, Sergey Seymore, Zubkov Rahul, Summer Goel, Sai Yue, Krishnakumaran Brian, Nate Albert, Motoki Hurley, Anhad Sano, Jonah Mohananey, Egor Joughin, Tomasz Filonov, Yomna K˛epa, Jiawern Eldawy, Rahul Lim, Rishi Shirin, Taylor Badiezadegan, J. Bos, Chang Sanil, Sri Jain, G. Sundara, P. Subha, Kalpesh Puttagunta, Leslie Krishna, Baker Norbert, Vamsi Kalb, Adam Bedapudi, Kurzrok Shuntong, Anthony Lei, Oren Yu, Xiang Litvin, Zhichun Zhou, Sam Wu, A. Sobell, Siciliano Alan, Robby Papir, Jonas Neale, Tej Bragagnolo, Tina Toor, Valentin Chen, Feiran Anklin, Wang Richie, Milad Feng",
                "citations": 1
            },
            {
                "title": "Towards Foundation Models for 3D Vision: How Close Are We?",
                "abstract": "Building a foundation model for 3D vision is a complex challenge that remains unsolved. Towards that goal, it is important to understand the 3D reasoning capabilities of current models as well as identify the gaps between these models and humans. Therefore, we construct a new 3D visual understanding benchmark named UniQA-3D. UniQA-3D covers fundamental 3D vision tasks in the Visual Question Answering (VQA) format. We evaluate state-of-the-art Vision-Language Models (VLMs), specialized models, and human subjects on it. Our results show that VLMs generally perform poorly, while the specialized models are accurate but not robust, failing under geometric perturbations. In contrast, human vision continues to be the most reliable 3D visual system. We further demonstrate that neural networks align more closely with human 3D vision mechanisms compared to classical computer vision methods, and Transformer-based networks such as ViT align more closely with human 3D vision mechanisms than CNNs. We hope our study will benefit the future development of foundation models for 3D vision. Code is available at https://github.com/princeton-vl/UniQA-3D .",
                "authors": "Yiming Zuo, Karhan Kayan, Maggie Wang, Kevin Jeon, Jia Deng, Thomas L. Griffiths",
                "citations": 1
            },
            {
                "title": "Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models",
                "abstract": "Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has demonstrated potent applicability across diverse downstream tasks. This lightweight approach has quickly gained traction from federated learning (FL) researchers who seek to efficiently adapt VLMs to heterogeneous scenarios. However, current federated prompt learning methods are habitually restricted to the traditional FL paradigm, where the participating clients are generally only allowed to download a single globally aggregated model from the server. While justifiable for training full-sized models under federated settings, in this work, we argue that this paradigm is ill-suited for lightweight prompts. By facilitating the clients to download multiple pre-aggregated prompts as fixed non-local experts, we propose Personalized Federated Mixture of Adaptive Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning process through the lens of Mixture of Experts (MoE). pFedMoAP implements a local attention-based gating network that learns to generate enhanced text features for better alignment with local image data on the client, benefiting from both local and downloaded non-local adaptive prompt experts. The non-local experts are sparsely selected from a server-maintained pool, fostering collaborative learning across clients. To evaluate the proposed algorithm, we conduct extensive experiments across 9 datasets under various heterogeneous federated settings. The results show that pFedMoAP consistently outperforms the state-of-the-art alternatives, underscoring its efficacy in personalizing prompt learning for CLIP within the federated learning paradigm.",
                "authors": "Jun Luo, Chen Chen, Shandong Wu",
                "citations": 1
            },
            {
                "title": "Towards Generalizable Vision-Language Robotic Manipulation: A Benchmark and LLM-guided 3D Policy",
                "abstract": "Generalizing language-conditioned robotic policies to new tasks remains a significant challenge, hampered by the lack of suitable simulation benchmarks. In this paper, we address this gap by introducing GemBench, a novel benchmark to assess generalization capabilities of vision-language robotic manipulation policies. GemBench incorporates seven general action primitives and four levels of generalization, spanning novel placements, rigid and articulated objects, and complex long-horizon tasks. We evaluate state-of-the-art approaches on GemBench and also introduce a new method. Our approach 3D-LOTUS leverages rich 3D information for action prediction conditioned on language. While 3D-LOTUS excels in both efficiency and performance on seen tasks, it struggles with novel tasks. To address this, we present 3D-LOTUS++, a framework that integrates 3D-LOTUS's motion planning capabilities with the task planning capabilities of LLMs and the object grounding accuracy of VLMs. 3D-LOTUS++ achieves state-of-the-art performance on novel tasks of GemBench, setting a new standard for generalization in robotic manipulation. The benchmark, codes and trained models are available at \\url{https://www.di.ens.fr/willow/research/gembench/}.",
                "authors": "Ricardo Garcia, Shizhe Chen, Cordelia Schmid",
                "citations": 1
            },
            {
                "title": "Data Modeling Review: The case of Vaccine Lifecycle Management",
                "abstract": "Vaccination is essential, particularly in light of global challenges such as the COVID-19 pandemic. Effective vaccine management is critical; however, creating a comprehensive model for seamless data integration throughout the vaccine lifecycle presents significant challenges. This paper seeks to address these challenges by highlighting the need for a model that comprehensively tackles the complex nature of the vaccine lifecycle. By evaluating existing approaches, we suggest a framework based on Data Models and Product Lifecycle Management to improve vaccine lifecycle. This study offers a thorough examination of the recent situation and explores potential directions for future studies.In summary, our study highlights the crucial role of data models in vaccine lifecycle management, which are vital tools from development to distribution. Existing models such as STEP, CPM, and IPPOP provide valuable frameworks but require adaptation to fit the highly regulated environment of vaccine lifecycle management. Our future research will focus on refining data models for Vaccine Lifecycle Management (VLM) to boost efficiency and collaboration among stakeholders, addressing the specific demands of vaccine development and management",
                "authors": "AL Sanae, El Bouzekri El Idrissi Adiba, Sekhari Seklouli Aicha",
                "citations": 1
            },
            {
                "title": "Soft Prompt Generation for Domain Generalization",
                "abstract": "Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt or residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely Soft Prompt Generation (SPG). Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt label for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that SPG achieves state-of-the-art performance. The code is available at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.",
                "authors": "Shuanghao Bai, Yuedi Zhang, Wanqi Zhou, Zhirong Luan, Badong Chen",
                "citations": 1
            },
            {
                "title": "ABM: Attention before Manipulation",
                "abstract": "Vision-language models (VLMs) show promising generalization and zero-shot capabilities, offering a potential solution to the impracticality and cost of enabling robots to comprehend diverse human instructions and scene semantics in the real world. Existing approaches most directly integrate the semantic representations from pre-trained VLMs with policy learning. However, these methods are limited to the labeled data learned, resulting in poor generalization ability to unseen instructions and objects. To address the above limitation, we propose a simple method called \"Attention before Manipulation\" (ABM), which fully leverages the object knowledge encoded in CLIP to extract information about the target object in the image. It constructs an Object Mask Field, serving as a better representation of the target object for the model to separate visual grounding from action prediction and acquire specific manipulation skills effectively. We train ABM for 8 RLBench tasks and 2 real-world tasks via behavior cloning. Extensive experiments show that our method significantly outperforms the baselines in the zero-shot and compositional generalization experiment settings.",
                "authors": "Fan Zhuo, Ying He, F. Yu, Pengteng Li, Zheyi Zhao, Xilong Sun",
                "citations": 1
            },
            {
                "title": "FlexAttention for Efficient High-Resolution Vision-Language Models",
                "abstract": "Current high-resolution vision-language models encode images as high-resolution image tokens and exhaustively take all these tokens to compute attention, which significantly increases the computational cost. To address this problem, we propose FlexAttention, a flexible attention mechanism for efficient high-resolution vision-language models. Specifically, a high-resolution image is encoded both as high-resolution tokens and low-resolution tokens, where only the low-resolution tokens and a few selected high-resolution tokens are utilized to calculate the attention map, which greatly shrinks the computational cost. The high-resolution tokens are selected via a high-resolution selection module which could retrieve tokens of relevant regions based on an input attention map. The selected high-resolution tokens are then concatenated to the low-resolution tokens and text tokens, and input to a hierarchical self-attention layer which produces an attention map that could be used for the next-step high-resolution token selection. The hierarchical self-attention process and high-resolution token selection process are performed iteratively for each attention layer. Experiments on multimodal benchmarks prove that our FlexAttention outperforms existing high-resolution VLMs (e.g., relatively ~9% in V* Bench, ~7% in TextVQA), while also significantly reducing the computational cost by nearly 40%.",
                "authors": "Junyan Li, Delin Chen, Tianle Cai, Peihao Chen, Yining Hong, Zhenfang Chen, Yikang Shen, Chuang Gan",
                "citations": 1
            },
            {
                "title": "A mRNA Vaccine for Crimean–Congo Hemorrhagic Fever Virus Expressing Non-Fusion GnGc Using NSm Linker Elicits Unexpected Immune Responses in Mice",
                "abstract": "Crimean–Congo hemorrhagic fever (CCHF), caused by Crimean–Congo Hemorrhagic virus (CCHFV), is listed in the World Health Organization’s list of priority diseases. The high fatality rate in humans, the widespread distribution of CCHFV, and the lack of approved specific vaccines are the primary concerns regarding this disease. We used microfluidic technology to optimize the mRNA vaccine delivery system and demonstrated that vaccination with nucleoside-modified CCHFV mRNA vaccines encoding GnNSmGc (vLMs), Gn (vLMn), or Gc (vLMc) induced different immune responses. We found that both T-cell and B-cell immune responses induced by vLMc were better than those induced by vLMn. Interestingly, immune responses were found to be lower for vLMs, which employed NSm to link Gn and Gc for non-fusion expression, compared to those for vLMc. In conclusion, our results indicated that NSm could be a factor that leads to decreased specific immune responses in the host and should be avoided in the development of CCHFV vaccine antigens.",
                "authors": "Tong Chen, Zhe Ding, Xuejie Li, Yingwen Li, Jiaming Lan, Gary Wong",
                "citations": 1
            },
            {
                "title": "PALM: Few-Shot Prompt Learning for Audio Language Models",
                "abstract": "Audio-Language Models (ALMs) have recently achieved remarkable success in zero-shot audio recognition tasks, which match features of audio waveforms with class-specific text prompt features, inspired by advancements in Vision-Language Models (VLMs). Given the sensitivity of zero-shot performance to the choice of hand-crafted text prompts, many prompt learning techniques have been developed for VLMs. We explore the efficacy of these approaches in ALMs and propose a novel method, Prompt Learning in Audio Language Models (PALM), which optimizes the feature space of the text encoder branch. Unlike existing methods that work in the input space, our approach results in greater training efficiency. We demonstrate the effectiveness of our approach on 11 audio recognition datasets, encompassing a variety of speech-processing tasks, and compare the results with three baselines in a few-shot learning setup. Our method is either on par with or outperforms other approaches while being computationally less demanding. Our code is publicly available at https://asif-hanif.github.io/palm/.",
                "authors": "Asif Hanif, M. Agro, Mohammad Areeb Qazi, Hanan Aldarmaki",
                "citations": 1
            },
            {
                "title": "Knowledge Graph Extraction from Total Synthesis Documents",
                "abstract": "Knowledge graphs (KGs) have emerged as a powerful tool for organizing and integrating complex information, making it a suitable format for scientific knowledge. However, translating scientific knowledge into KGs is challenging as a wide variety of styles and elements to present data and ideas is used. Although efforts for KG extraction (KGE) from scientific documents exist, evaluation remains challenging and field-dependent; and existing benchmarks do not focuse on scientific information. Furthermore, establishing a general benchmark for this task is challenging as not all scientific knowledge has a ground-truth KG representation, making any benchmark prone to ambiguity. Here we propose Graph of Organic Synthesis Benchmark (GOSyBench), a benchmark for KG extraction from scientific documents in chemistry, that leverages the native KG-like structure of synthetic routes in organic chemistry. We develop KG-extraction algorithms based on LLMs (GPT-4, Claude, Mistral) and VLMs (GPT-4o), the best of which reaches 73% recovery accuracy and 59% precision, leaving a lot of room for improvement. We expect GOSyBench can serve as a valuable resource for evaluating and advancing KGE methods in the scientific domain, ultimately facilitating better organization, integration, and discovery of scientific knowledge.",
                "authors": "Andres M Bran, Zlatko Jončev, Philippe Schwaller",
                "citations": 1
            },
            {
                "title": "Efficient Generation of Targeted and Transferable Adversarial Examples for Vision-Language Models via Diffusion Models",
                "abstract": "Adversarial attacks, particularly targeted transfer-based attacks, can be used to assess the adversarial robustness of large visual-language models (VLMs), allowing for a more thorough examination of potential security flaws before deployment. However, previous transfer-based adversarial attacks incur high costs due to high iteration counts and complex method structure. Furthermore, due to the unnaturalness of adversarial semantics, the generated adversarial examples have low transferability. These issues limit the utility of existing methods for assessing robustness. To address these issues, we propose AdvDiffVLM, which uses diffusion models to generate natural, unrestricted and targeted adversarial examples via score matching. Specifically, AdvDiffVLM uses Adaptive Ensemble Gradient Estimation (AEGE) to modify the score during the diffusion model’s reverse generation process, ensuring that the produced adversarial examples have natural adversarial targeted semantics, which improves their transferability. Simultaneously, to improve the quality of adversarial examples, we use the GradCAM-guided Mask Generation (GCMG) to disperse adversarial semantics throughout the image rather than concentrating them in a single area. Finally, AdvDiffVLM embeds more target semantics into adversarial examples after multiple iterations. Experimental results show that our method generates adversarial examples 5x to 10x faster than state-of-the-art (SOTA) transfer-based adversarial attacks while maintaining higher quality adversarial examples. Furthermore, compared to previous transfer-based adversarial attacks, the adversarial examples generated by our method have better transferability. Notably, AdvDiffVLM can successfully attack a variety of commercial VLMs in a black-box environment, including GPT-4V. The code is available at https://github.com/gq-max/AdvDiffVLM",
                "authors": "Qi Guo, Shanmin Pang, Xiaojun Jia, Yang Liu, Qing Guo",
                "citations": 1
            },
            {
                "title": "Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models",
                "abstract": "In this paper, we extended the method proposed in [21] to enable humans to interact naturally with autonomous agents through vocal and textual conversations. Our extended method exploits the inherent capabilities of pre-trained large language models (LLMs), multimodal visual language models (VLMs), and speech recognition (SR) models to decode the high-level natural language conversations and semantic understanding of the robot's task environment, and abstract them to the robot's actionable commands or queries. We performed a quantitative evaluation of our framework's natural vocal conversation understanding with participants from different racial backgrounds and English language accents. The participants interacted with the robot using both spoken and textual instructional commands. Based on the logged interaction data, our framework achieved 87.55% vocal commands decoding accuracy, 86.27% commands execution success, and an average latency of 0.89 seconds from receiving the participants' vocal chat commands to initiating the robot's actual physical action. The video demonstrations of this paper can be found at https://linusnep.github.io/MTCC-IRoNL/.",
                "authors": "Linus Nwankwo, Elmar Rueckert",
                "citations": 1
            },
            {
                "title": "Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning",
                "abstract": "Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcut. Hence, contrastive losses are not sufficient to learn task-optimal representations, i.e., representations that contain all task-relevant information shared between the image and associated captions. We examine two methods to reduce shortcut learning in our training and evaluation framework: (i) latent target decoding and (ii) implicit feature modification. We show empirically that both methods improve performance on the evaluation task, but only partly reduce shortcut learning when training and evaluating with our shortcut learning framework. Hence, we show the difficulty and challenge of our shortcut learning framework for contrastive vision-language representation learning.",
                "authors": "Maurits J. R. Bleeker, Mariya Hendriksen, Andrew Yates, M. D. Rijke",
                "citations": 1
            },
            {
                "title": "MultiHateClip: A Multilingual Benchmark Dataset for Hateful Video Detection on YouTube and Bilibili",
                "abstract": "Hate speech is a pressing issue in modern society, with significant effects both online and offline. Recent research in hate speech detection has primarily centered on text-based media, largely overlooking multimodal content such as videos. Existing studies on hateful video datasets have predominantly focused on English content within a Western context and have been limited to binary labels (hateful or non-hateful), lacking detailed contextual information. This study presents MultiHateClip1 , an novel multilingual dataset created through hate lexicons and human annotation. It aims to enhance the detection of hateful videos on platforms such as YouTube and Bilibili, including content in both English and Chinese languages. Comprising 2,000 videos annotated for hatefulness, offensiveness, and normalcy, this dataset provides a cross-cultural perspective on gender-based hate speech. Through a detailed examination of human annotation results, we discuss the differences between Chinese and English hateful videos and underscore the importance of different modalities in hateful and offensive video analysis. Evaluations of state-of-the-art video classification models, such as VLM, GPT-4V and Qwen-VL, on MultiHateClip highlight the existing challenges in accurately distinguishing between hateful and offensive content and the urgent need for models that are both multimodally and culturally nuanced. MultiHateClip represents a foundational advance in enhancing hateful video detection by underscoring the necessity of a multimodal and culturally sensitive approach in combating online hate speech.",
                "authors": "Han Wang, Tan Rui Yang, Usman Naseem, Roy Ka-Wei Lee",
                "citations": 1
            },
            {
                "title": "MePT: Multi-Representation Guided Prompt Tuning for Vision-Language Model",
                "abstract": "Recent advancements in pre-trained Vision-Language Models (VLMs) have highlighted the significant potential of prompt tuning for adapting these models to a wide range of downstream tasks. However, existing prompt tuning methods typically map an image to a single representation, limiting the model's ability to capture the diverse ways an image can be described. To address this limitation, we investigate the impact of visual prompts on the model's generalization capability and introduce a novel method termed Multi-Representation Guided Prompt Tuning (MePT). Specifically, MePT employs a three-branch framework that focuses on diverse salient regions, uncovering the inherent knowledge within images which is crucial for robust generalization. Further, we employ efficient self-ensemble techniques to integrate these versatile image representations, allowing MePT to learn all conditional, marginal, and fine-grained distributions effectively. We validate the effectiveness of MePT through extensive experiments, demonstrating significant improvements on both base-to-novel class prediction and domain generalization tasks.",
                "authors": "Xinyang Wang, Yi Yang, Minfeng Zhu, Kecheng Zheng, Shihong Liu, Wei Chen",
                "citations": 1
            },
            {
                "title": "Cap4Video++: Enhancing Video Understanding with Auxiliary Captions.",
                "abstract": "Understanding videos, especially aligning them with textual data, presents a significant challenge in computer vision. The advent of vision-language models (VLMs) like CLIP has sparked interest in leveraging their capabilities for enhanced video understanding, showing marked advancements in both performance and efficiency. However, current methods often neglect vital user-generated metadata such as video titles. In this paper, we present Cap4Video++, a universal framework that leverages auxiliary captions to enrich video understanding. More recently, we witness the flourishing of large language models (LLMs) like ChatGPT. Cap4Video++ harnesses the synergy of vision-language models (VLMs) and large language models (LLMs) to generate video captions, utilized in three key phases: (i) Input stage employs Semantic Pair Sampling to extract beneficial samples from captions, aiding contrastive learning. (ii) Intermediate stage sees Video-Caption Cross-modal Interaction and Adaptive Caption Selection work together to bolster video and caption representations. (iii) Output stage introduces a Complementary Caption-Text Matching branch, enhancing the primary video branch by improving similarity calculations. Our comprehensive experiments on text-video retrieval and video action recognition across nine benchmarks clearly demonstrate Cap4Video++'s superiority over existing models, highlighting its effectiveness in utilizing automatically generated captions to advance video understanding.",
                "authors": "Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, Wanli Ouyang",
                "citations": 1
            },
            {
                "title": "GeoGround: A Unified Large Vision-Language Model. for Remote Sensing Visual Grounding",
                "abstract": "Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an object's position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. To support model training, we present refGeo, a large-scale RS visual instruction-following dataset containing 161k image-text pairs. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching or surpassing the performance of specialized methods on multiple benchmarks. Code available at https://github.com/zytx121/GeoGround",
                "authors": "Yue Zhou, Mengcheng Lan, Xiang Li, Yiping Ke, Xue Jiang, Litong Feng, Wayne Zhang",
                "citations": 1
            },
            {
                "title": "DA-Ada: Learning Domain-Aware Adapter for Domain Adaptive Object Detection",
                "abstract": "Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. As the visual-language models (VLMs) can provide essential general knowledge on unseen images, freezing the visual encoder and inserting a domain-agnostic adapter can learn domain-invariant knowledge for DAOD. However, the domain-agnostic adapter is inevitably biased to the source domain. It discards some beneficial knowledge discriminative on the unlabelled domain, i.e., domain-specific knowledge of the target domain. To solve the issue, we propose a novel Domain-Aware Adapter (DA-Ada) tailored for the DAOD task. The key point is exploiting domain-specific knowledge between the essential general knowledge and domain-invariant knowledge. DA-Ada consists of the Domain-Invariant Adapter (DIA) for learning domain-invariant knowledge and the Domain-Specific Adapter (DSA) for injecting the domain-specific knowledge from the information discarded by the visual encoder. Comprehensive experiments over multiple DAOD tasks show that DA-Ada can efficiently infer a domain-aware visual encoder for boosting domain adaptive object detection. Our code is available at https://github.com/Therock90421/DA-Ada.",
                "authors": "Haochen Li, Rui Zhang, Hantao Yao, Xin Zhang, Yifan Hao, Xinkai Song, Xiaqing Li, Yongwei Zhao, Ling-ling Li, Yunji Chen",
                "citations": 1
            },
            {
                "title": "Foundation Models to the Rescue: Deadlock Resolution in Connected Multi-Robot Systems",
                "abstract": "Connected multi-agent robotic systems (MRS) are prone to deadlocks in an obstacle environment where the robots can get stuck away from their desired locations under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, a low-level control policy cannot resolve such deadlocks. Utilizing the generalizability and low data requirements of foundation models, this paper explores the possibility of using text-based models, i.e., large language models (LLMs), and text-and-image-based models, i.e., vision-language models (VLMs), as high-level planners for deadlock resolution. We propose a hierarchical control framework where a foundation model-based high-level planner helps to resolve deadlocks by assigning a leader to the MRS along with a set of waypoints for the MRS leader. Then, a low-level distributed control policy based on graph neural networks is executed to safely follow these waypoints, thereby evading the deadlock. We conduct extensive experiments on various MRS environments using the best available pre-trained LLMs and VLMs. We compare their performance with a graph-based planner in terms of effectiveness in helping the MRS reach their target locations and computational time. Our results illustrate that, compared to grid-based planners, the foundation models perform better in terms of the goal-reaching rate and computational time for complex environments, which helps us conclude that foundation models can assist MRS operating in complex obstacle-cluttered environments to resolve deadlocks efficiently.",
                "authors": "Kunal Garg, Songyuan Zhang, Jacob Arkin, Chuchu Fan",
                "citations": 1
            },
            {
                "title": "FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models",
                "abstract": "Vision language models (VLMs) have achieved impressive progress in diverse applications, becoming a prevalent research direction. In this paper, we build FIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn conversations that are derived from 27 source datasets, empowering VLMs to spontaneously refine their responses based on user feedback across diverse tasks. To scale up the data collection, FIRE is collected in two components: FIRE-100K and FIRE-1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is freely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a benchmark to comprehensively evaluate the feedback-refining capability of VLMs, which contains 11K feedback-refinement conversations as the test data, two evaluation settings, and a model to provide feedback for VLMs. We develop the FIRE-LLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows remarkable feedback-refining capability on FIRE-Bench and outperforms untrained VLMs by 50%, making more efficient user-agent interactions and underscoring the significance of the FIRE dataset.",
                "authors": "Pengxiang Li, Zhi Gao, Bofei Zhang, Tao Yuan, Yuwei Wu, Mehrtash Harandi, Yunde Jia, Song-Chun Zhu, Qing Li",
                "citations": 1
            },
            {
                "title": "ManufVisSGG: A Vision-Language-Model Approach for Cognitive Scene Graph Generation in Manufacturing Systems",
                "abstract": "To establish a cognitive manufacturing system, scene graph generation (SGG) that lets machines/robots understand objects and their relations under varied scenarios is an essential task. Existing research on SGG primarily focuses on detection and panoptic segmentation approaches, where objects are identified through bounding boxes or panoptic segmentation, followed by the prediction of their pairwise relationships. This process means that the quality of the final scene graph predictions is heavily influenced by the quality of costly annotations. To tackle this issue, we propose the Manufacturing Visual Scene Graph Generation (ManufVisSGG) method, a simple yet powerful approach that leverages the capabilities of Vision-Language Models (VLMs) to generate scene graphs quickly and accurately without any additional object annotations. Furthermore, leveraging the ManufVisSGG method, we have implemented a meticulous annotation procedure to compile a high-quality manufacturing scene graph generation (MSG) dataset, comprising 10,000 images of manufacturing and other industrial scenes. Through comparisons with various scene graph generation methods and benchmarks across two other datasets, we have showcased the superiority of the ManufVisSGG method and underscored the benefits of the MSG dataset over existing datasets.",
                "authors": "Zhijie Yan, Zuoxu Wang, Shufei Li, Mingrui Li, Xinxin Liang, Jihong Liu",
                "citations": 1
            },
            {
                "title": "Multi-modal Situated Reasoning in 3D Scenes",
                "abstract": "Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.",
                "authors": "Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, Siyuan Huang",
                "citations": 1
            },
            {
                "title": "They're All Doctors: Synthesizing Diverse Counterfactuals to Mitigate Associative Bias",
                "abstract": "Vision Language Models (VLMs) such as CLIP are powerful models; however they can exhibit unwanted biases, making them less safe when deployed directly in applications such as text-to-image, text-to-video retrievals, reverse search, or classification tasks. In this work, we propose a novel framework to generate synthetic counterfactual images to create a diverse and balanced dataset that can be used to fine-tune CLIP. Given a set of diverse synthetic base images from text-to-image models, we leverage off-the-shelf segmentation and inpainting models to place humans with diverse visual appearances in context. We show that CLIP trained on such datasets learns to disentangle the human appearance from the context of an image, i.e., what makes a doctor is not correlated to the person's visual appearance, like skin color or body type, but to the context, such as background, the attire they are wearing, or the objects they are holding. We demonstrate that our fine-tuned CLIP model, $CF_\\alpha$, improves key fairness metrics such as MaxSkew, MinSkew, and NDKL by 40-66\\% for image retrieval tasks, while still achieving similar levels of performance in downstream tasks. We show that, by design, our model retains maximal compatibility with the original CLIP models, and can be easily controlled to support different accuracy versus fairness trade-offs in a plug-n-play fashion.",
                "authors": "Salma Abdel Magid, Jui-Hsien Wang, Kushal Kafle, Hanspeter Pfister",
                "citations": 1
            },
            {
                "title": "Task-oriented Robotic Manipulation with Vision Language Models",
                "abstract": "Vision-Language Models (VLMs) play a crucial role in robotic manipulation by enabling robots to understand and interpret the visual properties of objects and their surroundings, allowing them to perform manipulation based on this multimodal understanding. However, understanding object attributes and spatial relationships is a non-trivial task but is critical in robotic manipulation tasks. In this work, we present a new dataset focused on spatial relationships and attribute assignment and a novel method to utilize VLMs to perform object manipulation with task-oriented, high-level input. In this dataset, the spatial relationships between objects are manually described as captions. Additionally, each object is labeled with multiple attributes, such as fragility, mass, material, and transparency, derived from a fine-tuned vision language model. The embedded object information from captions are automatically extracted and transformed into a data structure (in this case, tree, for demonstration purposes) that captures the spatial relationships among the objects within each image. The tree structures, along with the object attributes, are then fed into a language model to transform into a new tree structure that determines how these objects should be organized in order to accomplish a specific (high-level) task. We demonstrate that our method not only improves the comprehension of spatial relationships among objects in the visual environment but also enables robots to interact with these objects more effectively. As a result, this approach significantly enhances spatial reasoning in robotic manipulation tasks. To our knowledge, this is the first method of its kind in the literature, offering a novel solution that allows robots to more efficiently organize and utilize objects in their surroundings.",
                "authors": "Nurhan Bulus Guran, Hanchi Ren, Jingjing Deng, Xianghua Xie",
                "citations": 1
            },
            {
                "title": "Enhancing Robustness of Vision-Language Models through Orthogonality Learning and Cross-Regularization",
                "abstract": "Efficient finetuning of vision-language models (VLMs) like CLIP for specific downstream tasks is gaining significant attention. Previous works primarily focus on prompt learning to adapt the CLIP into a variety of downstream tasks, however, suffering from task overfitting when finetuned on a small data set. In this paper, we introduce an orthogonal finetuning method for efficiently updating pretrained weights which enhances robustness and generalization, while a cross-regularization strategy is further exploited to maintain the stability in terms of zero-shot generalization of VLMs, dubbed OrthCR . Specifically, trainable orthogonal matrices are injected seamlessly into the transformer architecture and enforced with orthogonality constraint using Cayley parameterization, benefiting from the norm-preserving property and thus leading to stable and faster convergence. To alleviate deviation from orthogonal constraint during training, a cross-regularization strategy is further employed with initial pretrained weights within a bypass manner. In addition, to enrich the sample diversity for downstream tasks, we first explore Cutout data augmentation to boost the efficient finetuning and comprehend how our approach improves the specific downstream performance and maintains the generalizability in the perspective of Orthogonality Learning. Beyond existing prompt learning techniques, we conduct extensive experiments to demonstrate that our method explicitly steers pretrained weight space to represent the task-specific knowledge and presents competitive generalizability under base-to-base/base-to-new , cross-dataset transfer and domain generalization evaluations.",
                "authors": "Jinlong Li, Zequn Jie, Elisa Ricci, Lin Ma, N. Sebe",
                "citations": 1
            },
            {
                "title": "OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation",
                "abstract": "In the realm of food computing, segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients, the emergence of new ingredients, and the high annotation costs as-sociated with large food segmentation datasets. Existing approaches primarily utilize a closed-vocabulary and static text embeddings setting. These methods often fall short in effectively handling the ingredients, particularly new and diverse ones. In response to these limitations, we introduce OVFoodSeg, a framework that adopts an open-vocabulary setting and enhances text embeddings with visual context. By integrating vision-language models (VLMs), our approach enriches text embedding with image-specific infor-mation through two innovative modules, e.g., an image-to-text learner FoodLearner and an Image-Informed Text Encoder. The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLearner and the sub-sequent learning phase for segmentation. The pre-training phase equips FoodLearner with the capability to align visual information with corresponding textual representations that are specifically related to food, while the second phase adapts both the FoodLearner and the Image-Informed Text Encoder for the segmentation task. By addressing the de-ficiencies of previous models, OVFoodSeg demonstrates a significant improvement, achieving an 4.9% increase in mean Intersection over Union (mIoU) on the FoodSeg103 dataset, setting a new milestone for food image segmentation.",
                "authors": "Xiongwei Wu, Sicheng Yu, Ee-Peng Lim, C. Ngo",
                "citations": 1
            },
            {
                "title": "GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events",
                "abstract": "The recognition and understanding of traffic incidents, particularly traffic accidents, is a topic of paramount importance in the realm of intelligent transportation systems and intelligent vehicles. This area has continually captured the extensive focus of both the academic and industrial sectors. Identifying and comprehending complex traffic events is highly challenging, primarily due to the intricate nature of traffic environments, diverse observational perspectives, and the multifaceted causes of accidents. These factors have persistently impeded the development of effective solutions. The advent of large vision-language models (VLMs) such as GPT-4V, has introduced innovative approaches to addressing this issue. In this paper, we explore the ability of GPT-4V with a set of representative traffic incident videos and delve into the model's capacity of understanding these complex traffic situations. We observe that GPT-4V demonstrates remarkable cognitive, reasoning, and decision-making ability in certain classic traffic events. Concurrently, we also identify certain limitations of GPT-4V, which constrain its understanding in more intricate scenarios. These limitations merit further exploration and resolution.",
                "authors": "Xingcheng Zhou, Alois C. Knoll",
                "citations": 1
            },
            {
                "title": "Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction",
                "abstract": "Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fail short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE$^2$, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE$^2$ outperforms the conventional pre-trained metrics in capturing caption quality and demonstrates superior consistency with human judgment.",
                "authors": "Koki Maeda, Shuhei Kurita, Taiki Miyanishi, Naoaki Okazaki",
                "citations": 1
            },
            {
                "title": "Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts",
                "abstract": "Existing perception models achieve great success by learning from large amounts of labeled data, but they still struggle with open-world scenarios. To alleviate this issue, researchers introduce open-set perception tasks to detect or segment unseen objects in the training set. However, these models require predefined object categories as inputs during inference, which are not available in real-world scenarios. Recently, researchers pose a new and more practical problem, \\textit{i.e.}, open-ended object detection, which discovers unseen objects without any object categories as inputs. In this paper, we present VL-SAM, a training-free framework that combines the generalized object recognition model (\\textit{i.e.,} Vision-Language Model) with the generalized object localization model (\\textit{i.e.,} Segment-Anything Model), to address the open-ended object detection and segmentation task. Without additional training, we connect these two generalized models with attention maps as the prompts. Specifically, we design an attention map generation module by employing head aggregation and a regularized attention flow to aggregate and propagate attention maps across all heads and layers in VLM, yielding high-quality attention maps. Then, we iteratively sample positive and negative points from the attention maps with a prompt generation module and send the sampled points to SAM to segment corresponding objects. Experimental results on the long-tail instance segmentation dataset (LVIS) show that our method surpasses the previous open-ended method on the object detection task and can provide additional instance segmentation masks. Besides, VL-SAM achieves favorable performance on the corner case object detection dataset (CODA), demonstrating the effectiveness of VL-SAM in real-world applications. Moreover, VL-SAM exhibits good model generalization that can incorporate various VLMs and SAMs.",
                "authors": "Zhiwei Lin, Yongtao Wang, Zhi Tang",
                "citations": 1
            },
            {
                "title": "READoc: A Unified Benchmark for Realistic Document Structured Extraction",
                "abstract": "Document Structured Extraction (DSE) aims to extract structured content from raw documents. Despite the emergence of numerous DSE systems, their unified evaluation remains inadequate, significantly hindering the field's advancement. This problem is largely attributed to existing benchmark paradigms, which exhibit fragmented and localized characteristics. To address these limitations and offer a thorough evaluation of DSE systems, we introduce a novel benchmark named READoc, which defines DSE as a realistic task of converting unstructured PDFs into semantically rich Markdown. The READoc dataset is derived from 2,233 diverse and real-world documents from arXiv and GitHub. In addition, we develop a DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring modules, to conduct a unified evaluation of state-of-the-art DSE approaches. By evaluating a range of pipeline tools, expert visual models, and general VLMs, we identify the gap between current work and the unified, realistic DSE objective for the first time. We aspire that READoc will catalyze future research in DSE, fostering more comprehensive and practical solutions.",
                "authors": "Zichao Li, Aizier Abulaiti, Yaojie Lu, Xuanang Chen, Jia Zheng, Hongyu Lin, Xianpei Han, Le Sun",
                "citations": 1
            },
            {
                "title": "Visual Prompt Engineering for Medical Vision Language Models in Radiology",
                "abstract": "Medical image classification in radiology faces significant challenges, particularly in generalizing to unseen pathologies. In contrast, CLIP offers a promising solution by leveraging multimodal learning to improve zero-shot classification performance. However, in the medical domain, lesions can be small and might not be well represented in the embedding space. Therefore, in this paper, we explore the potential of visual prompt engineering to enhance the capabilities of Vision Language Models (VLMs) in radiology. Leveraging BiomedCLIP, trained on extensive biomedical image-text pairs, we investigate the impact of embedding visual markers directly within radiological images to guide the model's attention to critical regions. Our evaluation on the JSRT dataset, focusing on lung nodule malignancy classification, demonstrates that incorporating visual prompts $\\unicode{x2013}$ such as arrows, circles, and contours $\\unicode{x2013}$ significantly improves classification metrics including AUROC, AUPRC, F1 score, and accuracy. Moreover, the study provides attention maps, showcasing enhanced model interpretability and focus on clinically relevant areas. These findings underscore the efficacy of visual prompt engineering as a straightforward yet powerful approach to advance VLM performance in medical image analysis.",
                "authors": "Stefan Denner, Markus Bujotzek, Dimitrios Bounias, David Zimmerer, Raphael Stock, Paul F. Jäger, Klaus H. Maier-Hein",
                "citations": 1
            },
            {
                "title": "Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models",
                "abstract": "Image-based advertisements are complex multimodal stimuli that often contain unusual visual elements and figurative language. Previous research on automatic ad understanding has reported impressive zero-shot accuracy of contrastive vision-and-language models (VLMs) on an ad-explanation retrieval task. Here, we examine the original task setup and show that contrastive VLMs can solve it by exploiting grounding heuristics. To control for this confound, we introduce TRADE, a new evaluation test set with adversarial grounded explanations. While these explanations look implausible to humans, we show that they\"fool\"four different contrastive VLMs. Our findings highlight the need for an improved operationalisation of automatic ad understanding that truly evaluates VLMs' multimodal reasoning abilities. We make our code and TRADE available at https://github.com/dmg-illc/trade .",
                "authors": "A. Bavaresco, A. Testoni, R. Fern'andez",
                "citations": 1
            },
            {
                "title": "Multimodal foundation world models for generalist embodied agents",
                "abstract": "Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be functional, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle toward developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain’s dynamics, and learns the corresponding behaviors in imagination. As assessed through large-scale multi-task benchmarking, GenRL exhibits strong multi-task generalization performance in several locomotion and manipulation domains. Furthermore, by introducing a data-free RL strategy, it lays the groundwork for foundation model-based RL for generalist embodied agents.",
                "authors": "Pietro Mazzaglia, Tim Verbelen, B. Dhoedt, Aaron C. Courville, Sai Rajeswar",
                "citations": 1
            },
            {
                "title": "Creating a Lens of Chinese Culture: A Multimodal Dataset for Chinese Pun Rebus Art Understanding",
                "abstract": "Large vision-language models (VLMs) have demonstrated remarkable abilities in understanding everyday content. However, their performance in the domain of art, particularly culturally rich art forms, remains less explored. As a pearl of human wisdom and creativity, art encapsulates complex cultural narratives and symbolism. In this paper, we offer the Pun Rebus Art Dataset, a multimodal dataset for art understanding deeply rooted in traditional Chinese culture. We focus on three primary tasks: identifying salient visual elements, matching elements with their symbolic meanings, and explanations for the conveyed messages. Our evaluation reveals that state-of-the-art VLMs struggle with these tasks, often providing biased and hallucinated explanations and showing limited improvement through in-context learning. By releasing the Pun Rebus Art Dataset, we aim to facilitate the development of VLMs that can better understand and interpret culturally specific content, promoting greater inclusiveness beyond English-based corpora.",
                "authors": "Tuo Zhang, Tiantian Feng, Yibin Ni, Mengqin Cao, Ruying Liu, Katharine Butler, Yanjun Weng, Mi Zhang, Shrikanth S. Narayanan, S. Avestimehr",
                "citations": 1
            },
            {
                "title": "CAST: Cross-modal Alignment Similarity Test for Vision Language Models",
                "abstract": "Vision Language Models (VLMs) are typically evaluated with Visual Question Answering (VQA) tasks which assess a model's understanding of scenes. Good VQA performance is taken as evidence that the model will perform well on a broader range of tasks that require both visual and language inputs. However, scene-aware VQA does not fully capture input biases or assess hallucinations caused by a misalignment between modalities. To address this, we propose a Cross-modal Alignment Similarity Test (CAST) to probe VLMs for self-consistency across modalities. This test involves asking the models to identify similarities between two scenes through text-only, image-only, or both and then assess the truthfulness of the similarities they generate. Since there is no ground-truth to compare against, this evaluation does not focus on objective accuracy but rather on whether VLMs are internally consistent in their outputs. We argue that while not all self-consistent models are capable or accurate, all capable VLMs must be self-consistent.",
                "authors": "Gautier Dagan, Olga Loginova, Anil Batra",
                "citations": 1
            },
            {
                "title": "Diagnosing the Compositional Knowledge of Vision Language Models from a Game-Theoretic View",
                "abstract": "Compositional reasoning capabilities are usually considered as fundamental skills to characterize human perception. Recent studies show that current Vision Language Models (VLMs) surprisingly lack sufficient knowledge with respect to such capabilities. To this end, we propose to thoroughly diagnose the composition representations encoded by VLMs, systematically revealing the potential cause for this weakness. Specifically, we propose evaluation methods from a novel game-theoretic view to assess the vulnerability of VLMs on different aspects of compositional understanding, e.g., relations and attributes. Extensive experimental results demonstrate and validate several insights to understand the incapabilities of VLMs on compositional reasoning, which provide useful and reliable guidance for future studies. The deliverables will be updated at https://vlms-compositionality-gametheory.github.io/.",
                "authors": "Jin Wang, Shichao Dong, Yapeng Zhu, Kelu Yao, Weidong Zhao, Chao Li, Ping Luo",
                "citations": 1
            },
            {
                "title": "Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning",
                "abstract": "Vision language models (VLMs) have demonstrated impressive performance across a wide range of downstream tasks. However, their proficiency in spatial reasoning remains limited, despite its crucial role in tasks involving navigation and interaction with physical environments. Specifically, most of these tasks rely on the core spatial reasoning capabilities in two-dimensional (2D) environments, and our evaluation reveals that state-of-the-art VLMs frequently generate implausible and incorrect responses to composite spatial reasoning problems, including simple pathfinding tasks that humans can solve effortlessly at a glance. To address this, we explore an effective approach to enhance 2D spatial reasoning within VLMs by training the model solely on basic spatial capabilities. We begin by disentangling the key components of 2D spatial reasoning: direction comprehension, distance estimation, and localization. Our central hypothesis is that mastering these basic spatial capabilities can significantly enhance a model's performance on composite spatial tasks requiring advanced spatial understanding and combinatorial problem-solving, with generalized improvements in visual-spatial tasks. To investigate this hypothesis, we introduce Sparkle, a framework that fine-tunes VLMs on these three basic spatial capabilities by synthetic data generation and targeted supervision to form an instruction dataset for each capability. Our experiments demonstrate that VLMs fine-tuned with Sparkle achieve significant performance gains, not only in the basic tasks themselves but also in generalizing to composite and out-of-distribution spatial reasoning tasks. These findings underscore the effectiveness of mastering basic spatial capabilities in enhancing composite spatial problem-solving, offering insights into systematic strategies for improving VLMs' spatial reasoning capabilities.",
                "authors": "Yihong Tang, Ao Qu, Zhaokai Wang, Dingyi Zhuang, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, Jinhua Zhao",
                "citations": 1
            },
            {
                "title": "Backdooring Vision-Language Models with Out-Of-Distribution Data",
                "abstract": "The emergence of Vision-Language Models (VLMs) represents a significant advancement in integrating computer vision with Large Language Models (LLMs) to generate detailed text descriptions from visual inputs. Despite their growing importance, the security of VLMs, particularly against backdoor attacks, is under explored. Moreover, prior works often assume attackers have access to the original training data, which is often unrealistic. In this paper, we address a more practical and challenging scenario where attackers must rely solely on Out-Of-Distribution (OOD) data. We introduce VLOOD (Backdooring Vision-Language Models with Out-of-Distribution Data), a novel approach with two key contributions: (1) demonstrating backdoor attacks on VLMs in complex image-to-text tasks while minimizing degradation of the original semantics under poisoned inputs, and (2) proposing innovative techniques for backdoor injection without requiring any access to the original training data. Our evaluation on image captioning and visual question answering (VQA) tasks confirms the effectiveness of VLOOD, revealing a critical security vulnerability in VLMs and laying the foundation for future research on securing multimodal models against sophisticated threats.",
                "authors": "Weimin Lyu, Jiachen Yao, Saumya Gupta, Lu Pang, Tao Sun, Lingjie Yi, Lijie Hu, Haibin Ling, Chao Chen",
                "citations": 1
            },
            {
                "title": "A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks",
                "abstract": "Recent advancements in Vision-Language Models (VLMs) have enabled complex multimodal tasks by processing text and image data simultaneously, significantly enhancing the field of artificial intelligence. However, these models often exhibit biases that can skew outputs towards societal stereotypes, thus necessitating debiasing strategies. Existing debiasing methods focus narrowly on specific modalities or tasks, and require extensive retraining. To address these limitations, this paper introduces Selective Feature Imputation for Debiasing (SFID), a novel methodology that integrates feature pruning and low confidence imputation (LCI) to effectively reduce biases in VLMs. SFID is versatile, maintaining the semantic integrity of outputs and costly effective by eliminating the need for retraining. Our experimental results demonstrate SFID's effectiveness across various VLMs tasks including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, by significantly reducing gender biases without compromising performance. This approach not only enhances the fairness of VLMs applications but also preserves their efficiency and utility across diverse scenarios.",
                "authors": "Hoin Jung, T. Jang, Xiaoqian Wang",
                "citations": 1
            },
            {
                "title": "DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving",
                "abstract": "The advancement of autonomous driving technologies necessitates increasingly sophisticated methods for understanding and predicting real-world scenarios. Vision language models (VLMs) are emerging as revolutionary tools with significant potential to influence autonomous driving. In this paper, we propose the DriveGenVLM framework to generate driving videos and use VLMs to understand them. To achieve this, we employ a video generation framework grounded in denoising diffusion probabilistic models (DDPM) aimed at predicting real-world video sequences. We then explore the adequacy of our generated videos for use in VLMs by employing a pre-trained model known as Efficient In-context Learning on Egocentric Videos (EILEV). The diffusion model is trained with the Waymo open dataset and evaluated using the Fréchet Video Distance (FVD) score to ensure the quality and realism of the generated videos. Corresponding narrations are provided by EILEV for these generated videos, which may be beneficial in the autonomous driving domain. These narrations can enhance traffic scene understanding, aid in navigation, and improve planning capabilities. The integration of video generation with VLMs in the DriveGenVLM framework represents a significant step forward in leveraging advanced AI models to address complex challenges in autonomous driving.",
                "authors": "Yongjie Fu, Anmol Jain, Xuan Di, Xu Chen, Zhaobin Mo",
                "citations": 1
            },
            {
                "title": "An Empirical Study Into What Matters for Calibrating Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) have emerged as the dominant approach for zero-shot recognition, adept at handling diverse scenarios and significant distribution changes. However, their deployment in risk-sensitive areas requires a deeper understanding of their uncertainty estimation capabilities, a relatively uncharted area. In this study, we explore the calibration properties of VLMs across different architectures, datasets, and training strategies. In particular, we analyze the uncertainty estimation performance of VLMs when calibrated in one domain, label set or hierarchy level, and tested in a different one. Our findings reveal that while VLMs are not inherently calibrated for uncertainty, temperature scaling significantly and consistently improves calibration, even across shifts in distribution and changes in label set. Moreover, VLMs can be calibrated with a very small set of examples. Through detailed experimentation, we highlight the potential applications and importance of our insights, aiming for more reliable and effective use of VLMs in critical, real-world scenarios.",
                "authors": "Weijie Tu, Weijian Deng, Dylan Campbell, Stephen Gould, Tom Gedeon",
                "citations": 1
            },
            {
                "title": "Aligning Medical Images with General Knowledge from Large Language Models",
                "abstract": "Pre-trained large vision-language models (VLMs) like CLIP have revolutionized visual representation learning using natural language as supervisions, and demonstrated promising generalization ability. In this work, we propose ViP, a novel visual symptom-guided prompt learning framework for medical image analysis, which facilitates general knowledge transfer from CLIP. ViP consists of two key components: a visual symptom generator (VSG) and a dual-prompt network. Specifically, VSG aims to extract explicable visual symptoms from pre-trained large language models, while the dual-prompt network utilizes these visual symptoms to guide the training on two learnable prompt modules, i.e., context prompt and merge prompt, which effectively adapts our framework to medical image analysis via large VLMs. Extensive experimental results demonstrate that ViP can outperform state-of-the-art methods on two challenging datasets.",
                "authors": "Xiao Fang, Yi-Mou Lin, Dong-Ming Zhang, Kwang-Ting Cheng, Hao Chen",
                "citations": 1
            },
            {
                "title": "Towards Specific Domain Prompt Learning via Improved Text Label Optimization",
                "abstract": "Prompt learning has emerged as a thriving parameter-efficient fine-tuning technique for adapting pre-trained vision-language models (VLMs) to various downstream tasks. However, existing prompt learning approaches still exhibit limited capability for adapting foundational VLMs to specific domains that require specialized and expert-level knowledge. Since this kind of specific knowledge is primarily embedded in the pre-defined text labels, we infer that foundational VLMs cannot directly interpret semantic meaningful information from these specific text labels, which causes the above limitation. From this perspective, this paper additionally models text labels with learnable tokens and casts this operation into traditional prompt learning framework. By optimizing label tokens, semantic meaningful text labels are automatically learned for each class. Nevertheless, directly optimizing text label still remains two critical problems, i.e., insufficient optimization and biased optimization. We further address these problems by proposing Modality Interaction Text Label Optimization (MITLOp) and Color-based Consistency Augmentation (CCAug) respectively, thereby effectively improving the quality of the optimized text labels. Extensive experiments indicate that our proposed method achieves significant improvements in VLM adaptation on specific domains.",
                "authors": "Liangchen Liu, Nannan Wang, Decheng Liu, Xi Yang, Xinbo Gao, Tongliang Liu",
                "citations": 1
            },
            {
                "title": "Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers",
                "abstract": "Recent advancements in vision-language models (VLMs) have expanded their potential for real-world applications, enabling these models to perform complex reasoning on images. In the widely used fully autoregressive transformer-based models like LLaVA, projected visual tokens are prepended to textual tokens. Oftentimes, visual tokens are significantly more than prompt tokens, resulting in increased computational overhead during both training and inference. In this paper, we propose Visual Compact Token Registers (Victor), a method that reduces the number of visual tokens by summarizing them into a smaller set of register tokens. Victor adds a few learnable register tokens after the visual tokens and summarizes the visual information into these registers using the first few layers in the language tower of VLMs. After these few layers, all visual tokens are discarded, significantly improving computational efficiency for both training and inference. Notably, our method is easy to implement and requires a small number of new trainable parameters with minimal impact on model performance. In our experiment, with merely 8 visual registers--about 1% of the original tokens--Victor shows less than a 4% accuracy drop while reducing the total training time by 43% and boosting the inference throughput by 3.3X.",
                "authors": "Yuxin Wen, Qingqing Cao, Qichen Fu, Sachin Mehta, Mahyar Najibi",
                "citations": 1
            },
            {
                "title": "Improving Multi-label Recognition using Class Co-Occurrence Probabilities",
                "abstract": "Multi-label Recognition (MLR) involves the identification of multiple objects within an image. To address the additional complexity of this problem, recent works have leveraged information from vision-language models (VLMs) trained on large text-images datasets for the task. These methods learn an independent classifier for each object (class), overlooking correlations in their occurrences. Such co-occurrences can be captured from the training data as conditional probabilities between a pair of classes. We propose a framework to extend the independent classifiers by incorporating the co-occurrence information for object pairs to improve the performance of independent classifiers. We use a Graph Convolutional Network (GCN) to enforce the conditional probabilities between classes, by refining the initial estimates derived from image and text sources obtained using VLMs. We validate our method on four MLR datasets, where our approach outperforms all state-of-the-art methods.",
                "authors": "Samyak Rawlekar, Shubhang Bhatnagar, Vishnuvardhan Pogunulu Srinivasulu, Narendra Ahuja",
                "citations": 1
            },
            {
                "title": "Advancing Prompt Learning through an External Layer",
                "abstract": "Prompt learning represents a promising method for adapting pre-trained vision-language models (VLMs) to various downstream tasks by learning a set of text embeddings. One challenge inherent to these methods is the poor generalization performance due to the invalidity of the learned text embeddings for unseen tasks. A straightforward approach to bridge this gap is to freeze the text embeddings in prompts, which results in a lack of capacity to adapt VLMs for downstream tasks. To address this dilemma, we propose a paradigm called EnPrompt with a novel External Layer (EnLa). Specifically, we propose a textual external layer and learnable visual embeddings for adapting VLMs to downstream tasks. The learnable external layer is built upon valid embeddings of pre-trained CLIP. This design considers the balance of learning capabilities between the two branches. To align the textual and visual features, we propose a novel two-pronged approach: i) we introduce the optimal transport as the discrepancy metric to align the vision and text modalities, and ii) we introduce a novel strengthening feature to enhance the interaction between these two modalities. Four representative experiments (i.e., base-to-novel generalization, few-shot learning, cross-dataset generalization, domain shifts generalization) across 15 datasets demonstrate that our method outperforms the existing prompt learning method.",
                "authors": "Fangming Cui, Xun Yang, Chao Wu, Liang Xiao, Xinmei Tian",
                "citations": 1
            },
            {
                "title": "VisMin: Visual Minimal-Change Understanding",
                "abstract": "Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). Existing benchmarks primarily focus on evaluating VLMs' capability to distinguish between two very similar captions given an image. In this paper, we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. The image pair and caption pair contain minimal changes, i.e., only one aspect changes at a time from among the following: object, attribute, count, and spatial relation. These changes test the models' understanding of objects, attributes (such as color, material, shape), counts, and spatial relationships between objects. We built an automatic framework using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. We also generate a large-scale training dataset to finetune CLIP and Idefics2, showing significant improvements in fine-grained understanding across benchmarks and in CLIP's general image-text alignment. We release all resources, including the benchmark, training data, and finetuned model checkpoints, at https://vismin.net/.",
                "authors": "Rabiul Awal, Saba Ahmadi, Le Zhang, Aishwarya Agrawal",
                "citations": 1
            },
            {
                "title": "Generative Semantic Communication via Textual Prompts: Latency Performance Tradeoffs",
                "abstract": "This paper develops an edge-device collaborative Generative Semantic Communications (Gen SemCom) framework leveraging pre-trained Multi-modal/Vision Language Models (M/VLMs) for ultra-low-rate semantic communication via textual prompts. The proposed framework optimizes the use of M/VLMs on the wireless edge/device to generate high-fidelity textual prompts through visual captioning/question answering, which are then transmitted over a wireless channel for SemCom. Specifically, we develop a multi-user Gen SemCom framework using pre-trained M/VLMs, and formulate a joint optimization problem of prompt generation offloading, communication and computation resource allocation to minimize the latency and maximize the resulting semantic quality. Due to the nonconvex nature of the problem with highly coupled discrete and continuous variables, we decompose it as a two-level problem and propose a low-complexity swap/leaving/joining (SLJ)-based matching algorithm. Simulation results demonstrate significant performance improvements over the conventional semanticunaware/non-collaborative offloading benchmarks.",
                "authors": "Mengmeng Ren, Li Qiao, Long Yang, Zhen Gao, Jian Chen, Mahdi Boloursaz Mashhadi, Pei Xiao, Rahim Tafazolli, Mehdi Bennis",
                "citations": 1
            },
            {
                "title": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models",
                "abstract": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to “hear” via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for in silico experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size.",
                "authors": "Tyler Loakman, Yucheng Li, Chenghua Lin",
                "citations": 1
            },
            {
                "title": "Benchmarking VLMs' Reasoning About Persuasive Atypical Images",
                "abstract": "Vision language models (VLMs) have shown strong zero-shot generalization across various tasks, especially when integrated with large language models (LLMs). However, their ability to comprehend rhetorical and persuasive visual media, such as advertisements, remains understudied. Ads often employ atypical imagery, using surprising object juxtapositions to convey shared properties. For example, Fig. 1 (e) shows a beer with a feather-like texture. This requires advanced reasoning to deduce that this atypical representation signifies the beer's lightness. We introduce three novel tasks, Multi-label Atypicality Classification, Atypicality Statement Retrieval, and Aypical Object Recognition, to benchmark VLMs' understanding of atypicality in persuasive images. We evaluate how well VLMs use atypicality to infer an ad's message and test their reasoning abilities by employing semantically challenging negatives. Finally, we pioneer atypicality-aware verbalization by extracting comprehensive image descriptions sensitive to atypical elements. Our findings reveal that: (1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple, effective strategies can extract atypicality-aware information, leading to comprehensive image verbalization; (3) atypicality aids persuasive advertisement understanding. Code and data will be made available.",
                "authors": "Sina Malakouti, Aysan Aghazadeh, Ashmit Khandelwal, Adriana Kovashka",
                "citations": 1
            },
            {
                "title": "Hard Cases Detection in Motion Prediction by Vision-Language Foundation Models",
                "abstract": "Addressing hard cases in autonomous driving, such as anomalous road users, extreme weather conditions, and complex traffic interactions, presents significant challenges. To ensure safety, it is crucial to detect and manage these scenarios effectively for autonomous driving systems. However, the rarity and high-risk nature of these cases demand extensive, diverse datasets for training robust models. Vision-Language Foundation Models (VLMs) have shown remarkable zero-shot capabilities as being trained on extensive datasets. This work explores the potential of VLMs in detecting hard cases in autonomous driving. We demonstrate the capability of VLMs such as GPT-4v in detecting hard cases in traffic participant motion prediction on both agent and scenario levels. We introduce a feasible pipeline where VLMs, fed with sequential image frames with designed prompts, effectively identify challenging agents or scenarios, which are verified by existing prediction models. Moreover, by taking advantage of this detection of hard cases by VLMs, we further improve the training efficiency of the existing motion prediction pipeline by performing data selection for the training samples suggested by GPT. We show the effectiveness and feasibility of our pipeline incorporating VLMs with state-of-the-art methods on NuScenes datasets. The code is accessible at https://github.com/KTH-RPL/Detect_VLM.",
                "authors": "Yi Yang, Qingwen Zhang, Kei Ikemura, Nazre Batool, John Folkesson",
                "citations": 1
            },
            {
                "title": "Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models",
                "abstract": "With the prevalence of large-scale pretrained vision-language models (VLMs), such as CLIP, soft-prompt tuning has become a popular method for adapting these models to various downstream tasks. However, few works delve into the inherent properties of learnable soft-prompt vectors, specifically the impact of their norms to the performance of VLMs. This motivates us to pose an unexplored research question: ``Do we need to normalize the soft prompts in VLMs?'' To fill this research gap, we first uncover a phenomenon, called the \\textbf{Low-Norm Effect} by performing extensive corruption experiments, suggesting that reducing the norms of certain learned prompts occasionally enhances the performance of VLMs, while increasing them often degrades it. To harness this effect, we propose a novel method named \\textbf{N}ormalizing th\\textbf{e} soft-pro\\textbf{m}pt v\\textbf{e}ctors of vi\\textbf{si}on-language model\\textbf{s} (\\textbf{Nemesis}) to normalize soft-prompt vectors in VLMs. To the best of our knowledge, our work is the first to systematically investigate the role of norms of soft-prompt vector in VLMs, offering valuable insights for future research in soft-prompt tuning. The code is available at \\texttt{\\href{https://github.com/ShyFoo/Nemesis}{https://github.com/ShyFoo/Nemesis}}.",
                "authors": "Shuai Fu, Xiequn Wang, Qiushi Huang, Yu Zhang",
                "citations": 1
            },
            {
                "title": "Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities",
                "abstract": "This paper explores capabilities of Vision Language Models on spreadsheet comprehension. We propose three self-supervised challenges with corresponding evaluation metrics to comprehensively evaluate VLMs on Optical Character Recognition (OCR), spatial perception, and visual format recognition. Additionally, we utilize the spreadsheet table detection task to assess the overall performance of VLMs by integrating these challenges. To probe VLMs more finely, we propose three spreadsheet-to-image settings: column width adjustment, style change, and address augmentation. We propose variants of prompts to address the above tasks in different settings. Notably, to leverage the strengths of VLMs in understanding text rather than two-dimensional positioning, we propose to decode cell values on the four boundaries of the table in spreadsheet boundary detection. Our findings reveal that VLMs demonstrate promising OCR capabilities but produce unsatisfactory results due to cell omission and misalignment, and they notably exhibit insufficient spatial and format recognition skills, motivating future work to enhance VLMs’ spreadsheet data comprehension capabilities using our methods to generate extensive spreadsheet-image pairs in various settings.",
                "authors": "Shiyu Xia, Junyu Xiong, Haoyu Dong, Jianbo Zhao, Yuzhang Tian, Mengyu Zhou, Yeye He, Shi Han, Dongmei Zhang",
                "citations": 1
            },
            {
                "title": "Harnessing the Power of Large Vision Language Models for Synthetic Image Detection",
                "abstract": "In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the potential misuse of these images, including the creation of misleading content such as fake news and propaganda. This study investigates the effectiveness of using advanced vision-language models (VLMs) for synthetic image identification. Specifically, the focus is on tuning state-of-the-art image captioning models for synthetic image detection. By harnessing the robust understanding capabilities of large VLMs, the aim is to distinguish authentic images from synthetic images produced by diffusion-based models. This study contributes to the advancement of synthetic image detection by exploiting the capabilities of visual language models such as BLIP-2 and ViTGPT2. By tailoring image captioning models, we address the challenges associated with the potential misuse of synthetic images in real-world applications. Results described in this paper highlight the promising role of VLMs in the field of synthetic image detection, outperforming conventional image-based detection techniques. Code and models can be found at https://github.com/Mamadou-Keita/VLM-DETECT.",
                "authors": "Mamadou Keita, W. Hamidouche, Hassen Bougueffa, Abdenour Hadid, Abdelmalik Taleb-Ahmed",
                "citations": 1
            },
            {
                "title": "Privacy-Aware Visual Language Models",
                "abstract": "This paper aims to advance our understanding of how Visual Language Models (VLMs) handle privacy-sensitive information, a crucial concern as these technologies become integral to everyday life. To this end, we introduce a new benchmark PrivBench, which contains images from 8 sensitive categories such as passports, or fingerprints. We evaluate 10 state-of-the-art VLMs on this benchmark and observe a generally limited understanding of privacy, highlighting a significant area for model improvement. Based on this we introduce PrivTune, a new instruction-tuning dataset aimed at equipping VLMs with knowledge about visual privacy. By tuning two pretrained VLMs, TinyLLaVa and MiniGPT-v2, on this small dataset, we achieve strong gains in their ability to recognize sensitive content, outperforming even GPT4-V. At the same time, we show that privacy-tuning only minimally affects the VLMs performance on standard benchmarks such as VQA. Overall, this paper lays out a crucial challenge for making VLMs effective in handling real-world data safely and provides a simple recipe that takes the first step towards building privacy-aware VLMs.",
                "authors": "Laurens Samson, Nimrod Barazani, S. Ghebreab, Yukiyasu Asano",
                "citations": 1
            },
            {
                "title": "Automating Robot Failure Recovery Using Vision-Language Models With Optimized Prompts",
                "abstract": "Current robot autonomy struggles to operate beyond the assumed Operational Design Domain (ODD), the specific set of conditions and environments in which the system is designed to function, while the real-world is rife with uncertainties that may lead to failures. Automating recovery remains a significant challenge. Traditional methods often rely on human intervention to manually address failures or require exhaustive enumeration of failure cases and the design of specific recovery policies for each scenario, both of which are labor-intensive. Foundational Vision-Language Models (VLMs), which demonstrate remarkable common-sense generalization and reasoning capabilities, have broader, potentially unbounded ODDs. However, limitations in spatial reasoning continue to be a common challenge for many VLMs when applied to robot control and motion-level error recovery. In this paper, we investigate how optimizing visual and text prompts can enhance the spatial reasoning of VLMs, enabling them to function effectively as black-box controllers for both motion-level position correction and task-level recovery from unknown failures. Specifically, the optimizations include identifying key visual elements in visual prompts, highlighting these elements in text prompts for querying, and decomposing the reasoning process for failure detection and control generation. In experiments, prompt optimizations significantly outperform pre-trained Vision-Language-Action Models in correcting motion-level position errors and improve accuracy by 65.78% compared to VLMs with unoptimized prompts. Additionally, for task-level failures, optimized prompts enhanced the success rate by 5.8%, 5.8%, and 7.5% in VLMs' abilities to detect failures, analyze issues, and generate recovery plans, respectively, across a wide range of unknown errors in Lego assembly.",
                "authors": "Hongyi Chen, Yunchao Yao, Ruixuan Liu, Changliu Liu, Jeffrey Ichnowski",
                "citations": 1
            },
            {
                "title": "I Know About \"Up\"! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction",
                "abstract": "Visual Language Models (VLMs) are essential for various tasks, particularly visual reasoning tasks, due to their robust multi-modal information integration, visual reasoning capabilities, and contextual awareness. However, existing \\VLMs{}' visual spatial reasoning capabilities are often inadequate, struggling even with basic tasks such as distinguishing left from right. To address this, we propose the \\ours{} model, designed to enhance the visual spatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D reconstruction model for obtaining different views of the input images and incorporates a prompting mechanism to further improve visual spatial reasoning. Experimental results on four visual spatial reasoning datasets show that our \\ours{} achieves up to 19.48% accuracy improvement, which indicates the effectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.",
                "authors": "Zaiqiao Meng, Hao Zhou, Yifang Chen",
                "citations": 1
            },
            {
                "title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models",
                "abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in comprehending complex visual content. However, the mechanisms underlying how VLMs process visual information remain largely unexplored. In this paper, we conduct a thorough empirical analysis, focusing on attention modules across layers. We reveal several key insights about how these models process visual data: (i) the internal representation of the query tokens (e.g., representations of\"describe the image\"), is utilized by VLMs to store global image information; we demonstrate that these models generate surprisingly descriptive responses solely from these tokens, without direct access to image tokens. (ii) Cross-modal information flow is predominantly influenced by the middle layers (approximately 25% of all layers), while early and late layers contribute only marginally.(iii) Fine-grained visual attributes and object details are directly extracted from image tokens in a spatially localized manner, i.e., the generated tokens associated with a specific object or attribute attend strongly to their corresponding regions in the image. We propose novel quantitative evaluation to validate our observations, leveraging real-world complex visual scenes. Finally, we demonstrate the potential of our findings in facilitating efficient visual processing in state-of-the-art VLMs.",
                "authors": "Omri Kaduri, Shai Bagon, Tali Dekel",
                "citations": 1
            },
            {
                "title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
                "abstract": "We present LoCoVQA, a dynamic benchmark generator for evaluating long-context extractive reasoning in vision language models (VLMs). LoCoVQA augments test examples for mathematical reasoning, VQA, and character recognition tasks with increasingly long visual contexts composed of both in-distribution and out-of-distribution distractor images. Across these tasks, a diverse set of VLMs rapidly lose performance as the visual context length grows, often exhibiting a striking logarithmic decay trend. This test assesses how well VLMs can ignore irrelevant information when answering queries -- a task that is quite easy for language models (LMs) in the text domain -- demonstrating that current state-of-the-art VLMs lack this essential capability for many long-context applications.",
                "authors": "Aditya Sharma, Michael Stephen Saxon, William Yang Wang, Haim-ing Bao, Mo Bavarian, J. Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Made-laine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, M. Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, B. Chess, Chester Cho, Hyung Casey Chu, Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Shixiang Shane Gross, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Harris Yuchen, Mike He, Johannes Heaton, C. Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Hoeschele Brandon, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Jain Joanne, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Hee-woo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Ka-mali, Kanitscheider, Nitish Shirish, Keskar Tabarak, Logan Khan, J. Kilpatrick, Kim Christina, Yongjik Kim, Jan Hendrik Kim, Jamie Kirch-ner, Matt Kiros, Daniel Knight, Kokotajlo Łukasz, A. Kondraciuk, Aris Kondrich, Kyle Kon-stantinidis, G. Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Chak Ming Levy, Li Rachel, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Lue Anna, Kim Makanju, S. Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Bob Mayne, Scott Mayer McGrew, Christine McKinney, Paul McLeavey, McMillan Jake, David McNeil, Aalok Medina, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Vinnie Mishkin, Evan Monaco, Daniel Morikawa, Tong Mossing, Mira Mu, Oleg Murati, David Murk, Ashvin Mély, Reiichiro Nair, Rajeev Nakano, Nayak Arvind, Richard Neelakantan, Hyeonwoo Ngo, Noh Long, Cullen Ouyang, Jakub O’Keefe, Alex Pachocki, J. Paino, Ashley Palermo, Giambat-tista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Filipe Perel-man, de Avila Belbute, Michael Peres, Petrov Henrique, Pondé, Michael Oliveira Pinto, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack W. Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Tianhao Shengjia Zhao",
                "citations": 1
            },
            {
                "title": "Investigating Compositional Challenges in Vision-Language Models for Visual Grounding",
                "abstract": "Pre-trained vision-language models (VLMs) have achieved high performance on various downstream tasks, which have been widely used for visual grounding tasks in a weakly supervised manner. However, despite the per-formance gains contributed by large vision and language pre-training, we find that state-of-the-art VLMs struggle with compositional reasoning on grounding tasks. To demonstrate this, we propose Attribute, Relation, and Pri-ority grounding (ARPGrounding) benchmark to test VLMs' compositional reasoning ability on visual grounding tasks. ARPGrounding contains 11,425 samples and evaluates the compositional understanding of VLMs in three dimensions: 1) attribute, denoting comprehension of objects' properties; 2) relation, indicating an understanding of relation between objects; 3) priority, reflecting an awareness of the part of speech associated with nouns. Using the ARPGrounding benchmark, we evaluate several mainstream VLMs. We empirically find that these models perform quite well on conventional visual grounding datasets, achieving performance comparable to or surpassing state-of-the-art methods but showing strong deficiencies in compositional reasoning. Furthermore, we propose a composition-aware fine-tuning pipeline, demonstrating the potential to lever-age cost-effective image-text annotations for enhancing the compositional understanding of VLMs in grounding tasks. Code is available at link.",
                "authors": "Yunan Zeng, Yan Huang, Jinjin Zhang, Zequn Jie, Zhenhua Chai, Liang Wang",
                "citations": 1
            },
            {
                "title": "MotIF: Motion Instruction Fine-tuning",
                "abstract": "While success in many robotics tasks can be determined by only observing the final state and how it differs from the initial state - e.g., if an apple is picked up - many tasks require observing the full motion of the robot to correctly determine success. For example, brushing hair requires repeated strokes that correspond to the contours and type of hair. Prior works often use off-the-shelf vision-language models (VLMs) as success detectors; however, when success depends on the full trajectory, VLMs struggle to make correct judgments for two reasons. First, modern VLMs are trained only on single frames, and cannot capture changes over a full trajectory. Second, even if we provide state-of-the-art VLMs with an aggregate input of multiple frames, they still fail to detect success due to a lack of robot data. Our key idea is to fine-tune VLMs using abstract representations that are able to capture trajectory-level information such as the path the robot takes by overlaying keypoint trajectories on the final image. We propose motion instruction fine-tuning (MotIF), a method that fine-tunes VLMs using the aforementioned abstract representations to semantically ground the robot's behavior in the environment. To benchmark and fine-tune VLMs for robotic motion understanding, we introduce the MotIF-1K dataset containing 653 human and 369 robot demonstrations across 13 task categories. MotIF assesses the success of robot motion given the image observation of the trajectory, task instruction, and motion description. Our model significantly outperforms state-of-the-art VLMs by at least twice in precision and 56.1% in recall, generalizing across unseen motions, tasks, and environments. Finally, we demonstrate practical applications of MotIF in refining and terminating robot planning, and ranking trajectories on how they align with task and motion descriptions. Project page: https://motif-1k.github.io",
                "authors": "Minyoung Hwang, Joey Hejna, Dorsa Sadigh, Yonatan Bisk",
                "citations": 1
            },
            {
                "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
                "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.",
                "authors": "Chengke Zou, Xing-ming Guo, Rui Yang, Junyu Zhang, Bin Hu, Huan Zhang",
                "citations": 1
            },
            {
                "title": "Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks",
                "abstract": "While Vision-Language Models (VLMs) have shown remarkable abilities in visual and language reasoning tasks, they invariably generate flawed responses. Self-correction that instructs models to refine their outputs presents a promising solution to this issue. Previous studies have mainly concentrated on Large Language Models (LLMs), while the self-correction abilities of VLMs, particularly concerning both visual and linguistic information, remain largely unexamined. This study investigates the self-correction capabilities of VLMs during both inference and fine-tuning stages. We introduce a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Specifically, we collect preferred and disfavored samples based on the correctness of initial and refined responses, which are obtained by two-turn self-correction with VLMs during the inference stage. Experimental results demonstrate that although VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning when their self-generated self-correction data are categorized into preferred and disfavored samples. This study emphasizes that self-correction is not merely a refinement process; rather, it should enhance the reasoning abilities of models through additional training, enabling them to generate high-quality responses directly without further refinement.",
                "authors": "Jiayi He, Hehai Lin, Qingyun Wang, Y. Fung, Heng Ji",
                "citations": 1
            },
            {
                "title": "Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics",
                "abstract": "Large language models (LLMs) and vision-language models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored. In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors. We investigate whether LLMs can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation. In this paper, we verify that 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images. Our experiments show that GPT-4 can estimate an object's reflectance using only text input with a mean error of 14.7%, lower than the image-only ResNet. Moreover, CLIP achieved the lowest mean error of 11.8%, while GPT-3.5 obtained a competitive 19.9% compared to ResNet's 17.8%. These results suggest that the distributional semantics in LLMs and VLMs increases their generalization capabilities, and the knowledge acquired by VLMs benefits from the latent structure of language.",
                "authors": "Masashi Osada, G. A. G. Ricardez, Yosuke Suzuki, Tadahiro Taniguchi",
                "citations": 1
            },
            {
                "title": "Vision Language Models Are Few-Shot Audio Spectrogram Classifiers",
                "abstract": "We demonstrate that vision language models (VLMs) are capable of recognizing the content in audio recordings when given corresponding spectrogram images. Specifically, we instruct VLMs to perform audio classification tasks in a few-shot setting by prompting them to classify a spectrogram image given example spectrogram images of each class. By carefully designing the spectrogram image representation and selecting good few-shot examples, we show that GPT-4o can achieve 59.00% cross-validated accuracy on the ESC-10 environmental sound classification dataset. Moreover, we demonstrate that VLMs currently outperform the only available commercial audio language model with audio understanding capabilities (Gemini-1.5) on the equivalent audio classification task (59.00% vs. 49.62%), and even perform slightly better than human experts on visual spectrogram classification (73.75% vs. 72.50% on first fold). We envision two potential use cases for these findings: (1) combining the spectrogram and language understanding capabilities of VLMs for audio caption augmentation, and (2) posing visual spectrogram classification as a challenge task for VLMs.",
                "authors": "Satvik Dixit, Laurie M. Heller, Chris Donahue",
                "citations": 1
            },
            {
                "title": "ViLBias: A Comprehensive Framework for Bias Detection through Linguistic and Visual Cues , presenting Annotation Strategies, Evaluation, and Key Challenges",
                "abstract": "The integration of Large Language Models (LLMs) and Vision-Language Models (VLMs) opens new avenues for addressing complex challenges in multimodal content analysis, particularly in biased news detection. This study introduces VLBias, a framework that leverages state-of-the-art LLMs and VLMs to detect linguistic and visual biases in news content. We present a multimodal dataset comprising textual content and corresponding images from diverse news sources. We propose a hybrid annotation framework that combines LLM-based annotations with human review to ensure high-quality labeling while reducing costs and enhancing scalability. Our evaluation compares the performance of state-of-the-art SLMs and LLMs for both modalities (text and images) and the results reveal that while SLMs are computationally efficient, LLMs demonstrate superior accuracy in identifying subtle framing and text-visual inconsistencies. Furthermore, empirical analysis shows that incorporating visual cues alongside textual data improves bias detection accuracy by 3 to 5%. This study provides a comprehensive exploration of LLMs, SLMs, and VLMs as tools for detecting multimodal biases in news content and highlights their respective strengths, limitations, and potential for future applications",
                "authors": "Shaina Raza, Caesar Saleh, Emrul Hasan, Franklin Ogidi, Maximus Powers, Veronica Chatrath, Marcelo Lotif, Roya Javadi, Anam Zahid, Vahid Reza Khazaie",
                "citations": 1
            },
            {
                "title": "SenCLIP: Enhancing zero-shot land-use mapping for Sentinel-2 with ground-level prompting",
                "abstract": "Pre-trained vision-language models (VLMs), such as CLIP, demonstrate impressive zero-shot classification capabilities with free-form prompts and even show some generalization in specialized domains. However, their performance on satellite imagery is limited due to the underrepresentation of such data in their training sets, which predominantly consist of ground-level images. Existing prompting techniques for satellite imagery are often restricted to generic phrases like a satellite image of ..., limiting their effectiveness for zero-shot land-use and land-cover (LULC) mapping. To address these challenges, we introduce SenCLIP, which transfers CLIPs representation to Sentinel-2 imagery by leveraging a large dataset of Sentinel-2 images paired with geotagged ground-level photos from across Europe. We evaluate SenCLIP alongside other SOTA remote sensing VLMs on zero-shot LULC mapping tasks using the EuroSAT and BigEarthNet datasets with both aerial and ground-level prompting styles. Our approach, which aligns ground-level representations with satellite imagery, demonstrates significant improvements in classification accuracy across both prompt styles, opening new possibilities for applying free-form textual descriptions in zero-shot LULC mapping.",
                "authors": "P. Jain, Dino Ienco, R. Interdonato, Tristan Berchoux, Diego Marcos",
                "citations": 1
            },
            {
                "title": "Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities",
                "abstract": "This paper explores capabilities of Vision Language Models on spreadsheet comprehension. We propose three self-supervised challenges with corresponding evaluation metrics to comprehensively evaluate VLMs on Optical Character Recognition (OCR), spatial perception, and visual format recognition. Additionally, we utilize the spreadsheet table detection task to assess the overall performance of VLMs by integrating these challenges. To probe VLMs more finely, we propose three spreadsheet-to-image settings: column width adjustment, style change, and address augmentation. We propose variants of prompts to address the above tasks in different settings. Notably, to leverage the strengths of VLMs in understanding text rather than two-dimensional positioning, we propose to decode cell values on the four boundaries of the table in spreadsheet boundary detection. Our findings reveal that VLMs demonstrate promising OCR capabilities but produce unsatisfactory results due to cell omission and misalignment, and they notably exhibit insufficient spatial and format recognition skills, motivating future work to enhance VLMs’ spreadsheet data comprehension capabilities using our methods to generate extensive spreadsheet-image pairs in various settings.",
                "authors": "Shiyu Xia, Junyu Xiong, Haoyu Dong, Jianbo Zhao, Yuzhang Tian, Mengyu Zhou, Yeye He, Shi Han, Dongmei Zhang",
                "citations": 1
            },
            {
                "title": "I Know About \"Up\"! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction",
                "abstract": "Visual Language Models (VLMs) are essential for various tasks, particularly visual reasoning tasks, due to their robust multi-modal information integration, visual reasoning capabilities, and contextual awareness. However, existing \\VLMs{}' visual spatial reasoning capabilities are often inadequate, struggling even with basic tasks such as distinguishing left from right. To address this, we propose the \\ours{} model, designed to enhance the visual spatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D reconstruction model for obtaining different views of the input images and incorporates a prompting mechanism to further improve visual spatial reasoning. Experimental results on four visual spatial reasoning datasets show that our \\ours{} achieves up to 19.48% accuracy improvement, which indicates the effectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.",
                "authors": "Zaiqiao Meng, Hao Zhou, Yifang Chen",
                "citations": 1
            },
            {
                "title": "Automating Robot Failure Recovery Using Vision-Language Models With Optimized Prompts",
                "abstract": "Current robot autonomy struggles to operate beyond the assumed Operational Design Domain (ODD), the specific set of conditions and environments in which the system is designed to function, while the real-world is rife with uncertainties that may lead to failures. Automating recovery remains a significant challenge. Traditional methods often rely on human intervention to manually address failures or require exhaustive enumeration of failure cases and the design of specific recovery policies for each scenario, both of which are labor-intensive. Foundational Vision-Language Models (VLMs), which demonstrate remarkable common-sense generalization and reasoning capabilities, have broader, potentially unbounded ODDs. However, limitations in spatial reasoning continue to be a common challenge for many VLMs when applied to robot control and motion-level error recovery. In this paper, we investigate how optimizing visual and text prompts can enhance the spatial reasoning of VLMs, enabling them to function effectively as black-box controllers for both motion-level position correction and task-level recovery from unknown failures. Specifically, the optimizations include identifying key visual elements in visual prompts, highlighting these elements in text prompts for querying, and decomposing the reasoning process for failure detection and control generation. In experiments, prompt optimizations significantly outperform pre-trained Vision-Language-Action Models in correcting motion-level position errors and improve accuracy by 65.78% compared to VLMs with unoptimized prompts. Additionally, for task-level failures, optimized prompts enhanced the success rate by 5.8%, 5.8%, and 7.5% in VLMs' abilities to detect failures, analyze issues, and generate recovery plans, respectively, across a wide range of unknown errors in Lego assembly.",
                "authors": "Hongyi Chen, Yunchao Yao, Ruixuan Liu, Changliu Liu, Jeffrey Ichnowski",
                "citations": 1
            },
            {
                "title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models",
                "abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in comprehending complex visual content. However, the mechanisms underlying how VLMs process visual information remain largely unexplored. In this paper, we conduct a thorough empirical analysis, focusing on attention modules across layers. We reveal several key insights about how these models process visual data: (i) the internal representation of the query tokens (e.g., representations of\"describe the image\"), is utilized by VLMs to store global image information; we demonstrate that these models generate surprisingly descriptive responses solely from these tokens, without direct access to image tokens. (ii) Cross-modal information flow is predominantly influenced by the middle layers (approximately 25% of all layers), while early and late layers contribute only marginally.(iii) Fine-grained visual attributes and object details are directly extracted from image tokens in a spatially localized manner, i.e., the generated tokens associated with a specific object or attribute attend strongly to their corresponding regions in the image. We propose novel quantitative evaluation to validate our observations, leveraging real-world complex visual scenes. Finally, we demonstrate the potential of our findings in facilitating efficient visual processing in state-of-the-art VLMs.",
                "authors": "Omri Kaduri, Shai Bagon, Tali Dekel",
                "citations": 1
            },
            {
                "title": "GRS: Generating Robotic Simulation Tasks from Real-World Images",
                "abstract": "We introduce GRS (Generating Robotic Simulation tasks), a novel system to address the challenge of real-to-sim in robotics, computer vision, and AR/VR. GRS enables the creation of digital twin simulations from single real-world RGB-D observations, complete with diverse, solvable tasks for virtual agent training. We use state-of-the-art vision-language models (VLMs) to achieve a comprehensive real-to-sim pipeline. GRS operates in three stages: 1) scene comprehension using SAM2 for object segmentation and VLMs for object description, 2) matching identified objects with simulation-ready assets, and 3) generating contextually appropriate robotic tasks. Our approach ensures simulations align with task specifications by generating test suites designed to verify adherence to the task specification. We introduce a router that iteratively refines the simulation and test code to ensure the simulation is solvable by a robot policy while remaining aligned to the task specification. Our experiments demonstrate the system's efficacy in accurately identifying object correspondence, which allows us to generate task environments that closely match input environments, and enhance automated simulation task generation through our novel router mechanism.",
                "authors": "Alex Zook, Fan-Yun Sun, Josef Spjut, Valts Blukis, Stanley T. Birchfield, Jonathan Tremblay",
                "citations": 1
            },
            {
                "title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
                "abstract": "We present LoCoVQA, a dynamic benchmark generator for evaluating long-context extractive reasoning in vision language models (VLMs). LoCoVQA augments test examples for mathematical reasoning, VQA, and character recognition tasks with increasingly long visual contexts composed of both in-distribution and out-of-distribution distractor images. Across these tasks, a diverse set of VLMs rapidly lose performance as the visual context length grows, often exhibiting a striking logarithmic decay trend. This test assesses how well VLMs can ignore irrelevant information when answering queries -- a task that is quite easy for language models (LMs) in the text domain -- demonstrating that current state-of-the-art VLMs lack this essential capability for many long-context applications.",
                "authors": "Aditya Sharma, Michael Stephen Saxon, William Yang Wang, Haim-ing Bao, Mo Bavarian, J. Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Made-laine Boyd, Anna-Luisa Brakman, Greg Brock-man, Tim Brooks, M. Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, B. Chess, Chester Cho, Hyung Casey Chu, Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Shixiang Shane Gross, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Harris Yuchen, Mike He, Johannes Heaton, C. Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Hoeschele Brandon, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Jain Joanne, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Hee-woo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Ka-mali, Kanitscheider, Nitish Shirish, Keskar Tabarak, Logan Khan, J. Kilpatrick, Kim Christina, Yongjik Kim, Jan Hendrik Kim, Jamie Kirch-ner, Matt Kiros, Daniel Knight, Kokotajlo Łukasz, A. Kondraciuk, Aris Kondrich, Kyle Kon-stantinidis, G. Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Chak Ming Levy, Li Rachel, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Lue Anna, Kim Makanju, S. Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Bob Mayne, Scott Mayer McGrew, Christine McKinney, Paul McLeavey, McMillan Jake, David McNeil, Aalok Medina, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Vinnie Mishkin, Evan Monaco, Daniel Morikawa, Tong Mossing, Mira Mu, Oleg Murati, David Murk, Ashvin Mély, Reiichiro Nair, Rajeev Nakano, Nayak Arvind, Richard Neelakantan, Hyeonwoo Ngo, Noh Long, Cullen Ouyang, Jakub O’Keefe, Alex Pachocki, J. Paino, Ashley Palermo, Giambat-tista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Filipe Perel-man, de Avila Belbute, Michael Peres, Petrov Henrique, Pondé, Michael Oliveira Pinto, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-ell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack W. Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-der, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Tianhao Shengjia Zhao",
                "citations": 1
            },
            {
                "title": "DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization",
                "abstract": "Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data. With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG. Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains. With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.",
                "authors": "Feng Hou, Jin Yuan, Ying Yang, Yang Liu, Yang Zhang, Cheng Zhong, Zhongchao Shi, Jianping Fan, Yong Rui, Zhiqiang He",
                "citations": 1
            },
            {
                "title": "Exploring Text-Guided Single Image Editing for Remote Sensing Images",
                "abstract": "Artificial intelligence generative content (AIGC) has significantly impacted image generation in the field of remote sensing. However, the equally important area of remote sensing image (RSI) editing has not received sufficient attention. Deep learning based editing methods generally involve two sequential stages: generation and editing. During the generation stage, consistency in content and details between the original and edited images must be maintained, while in the editing stage, controllability and accuracy of the edits should be ensured. For natural images, these challenges can be tackled by training generative backbones on large-scale benchmark datasets and using text guidance based on vision-language models (VLMs). However, these previously effective approaches become less viable for RSIs due to two reasons: First, existing generative RSI benchmark datasets do not fully capture the diversity of remote sensing scenarios, particularly in terms of variations in sensors, object types, and resolutions. Consequently, the generalization capacity of the trained backbone model is often inadequate for universal editing tasks on RSIs. Second, the large spatial resolution of RSIs exacerbates the problem in VLMs where a single text semantic corresponds to multiple image semantics, leading to the introduction of incorrect semantics when using text to guide RSI editing. To solve above problems, this paper proposes a text-guided RSI editing method that is controllable but stable, and can be trained using only a single image. It adopts a multi-scale training approach to preserve consistency without the need for training on extensive benchmark datasets, while leveraging RSI pre-trained VLMs and prompt ensembling (PE) to ensure accuracy and controllability in the text-guided editing process.",
                "authors": "Fangzhou Han, Lingyu Si, Hongwei Dong, Lamei Zhang, Hao Chen, Bo Du",
                "citations": 1
            },
            {
                "title": "Investigating Compositional Challenges in Vision-Language Models for Visual Grounding",
                "abstract": "Pre-trained vision-language models (VLMs) have achieved high performance on various downstream tasks, which have been widely used for visual grounding tasks in a weakly supervised manner. However, despite the per-formance gains contributed by large vision and language pre-training, we find that state-of-the-art VLMs struggle with compositional reasoning on grounding tasks. To demonstrate this, we propose Attribute, Relation, and Pri-ority grounding (ARPGrounding) benchmark to test VLMs' compositional reasoning ability on visual grounding tasks. ARPGrounding contains 11,425 samples and evaluates the compositional understanding of VLMs in three dimensions: 1) attribute, denoting comprehension of objects' properties; 2) relation, indicating an understanding of relation between objects; 3) priority, reflecting an awareness of the part of speech associated with nouns. Using the ARPGrounding benchmark, we evaluate several mainstream VLMs. We empirically find that these models perform quite well on conventional visual grounding datasets, achieving performance comparable to or surpassing state-of-the-art methods but showing strong deficiencies in compositional reasoning. Furthermore, we propose a composition-aware fine-tuning pipeline, demonstrating the potential to lever-age cost-effective image-text annotations for enhancing the compositional understanding of VLMs in grounding tasks. Code is available at link.",
                "authors": "Yunan Zeng, Yan Huang, Jinjin Zhang, Zequn Jie, Zhenhua Chai, Liang Wang",
                "citations": 1
            },
            {
                "title": "MotIF: Motion Instruction Fine-tuning",
                "abstract": "While success in many robotics tasks can be determined by only observing the final state and how it differs from the initial state - e.g., if an apple is picked up - many tasks require observing the full motion of the robot to correctly determine success. For example, brushing hair requires repeated strokes that correspond to the contours and type of hair. Prior works often use off-the-shelf vision-language models (VLMs) as success detectors; however, when success depends on the full trajectory, VLMs struggle to make correct judgments for two reasons. First, modern VLMs are trained only on single frames, and cannot capture changes over a full trajectory. Second, even if we provide state-of-the-art VLMs with an aggregate input of multiple frames, they still fail to detect success due to a lack of robot data. Our key idea is to fine-tune VLMs using abstract representations that are able to capture trajectory-level information such as the path the robot takes by overlaying keypoint trajectories on the final image. We propose motion instruction fine-tuning (MotIF), a method that fine-tunes VLMs using the aforementioned abstract representations to semantically ground the robot's behavior in the environment. To benchmark and fine-tune VLMs for robotic motion understanding, we introduce the MotIF-1K dataset containing 653 human and 369 robot demonstrations across 13 task categories. MotIF assesses the success of robot motion given the image observation of the trajectory, task instruction, and motion description. Our model significantly outperforms state-of-the-art VLMs by at least twice in precision and 56.1% in recall, generalizing across unseen motions, tasks, and environments. Finally, we demonstrate practical applications of MotIF in refining and terminating robot planning, and ranking trajectories on how they align with task and motion descriptions. Project page: https://motif-1k.github.io",
                "authors": "Minyoung Hwang, Joey Hejna, Dorsa Sadigh, Yonatan Bisk",
                "citations": 1
            },
            {
                "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
                "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.",
                "authors": "Chengke Zou, Xing-ming Guo, Rui Yang, Junyu Zhang, Bin Hu, Huan Zhang",
                "citations": 1
            },
            {
                "title": "Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks",
                "abstract": "While Vision-Language Models (VLMs) have shown remarkable abilities in visual and language reasoning tasks, they invariably generate flawed responses. Self-correction that instructs models to refine their outputs presents a promising solution to this issue. Previous studies have mainly concentrated on Large Language Models (LLMs), while the self-correction abilities of VLMs, particularly concerning both visual and linguistic information, remain largely unexamined. This study investigates the self-correction capabilities of VLMs during both inference and fine-tuning stages. We introduce a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Specifically, we collect preferred and disfavored samples based on the correctness of initial and refined responses, which are obtained by two-turn self-correction with VLMs during the inference stage. Experimental results demonstrate that although VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning when their self-generated self-correction data are categorized into preferred and disfavored samples. This study emphasizes that self-correction is not merely a refinement process; rather, it should enhance the reasoning abilities of models through additional training, enabling them to generate high-quality responses directly without further refinement.",
                "authors": "Jiayi He, Hehai Lin, Qingyun Wang, Y. Fung, Heng Ji",
                "citations": 1
            },
            {
                "title": "Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics",
                "abstract": "Large language models (LLMs) and vision-language models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored. In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors. We investigate whether LLMs can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation. In this paper, we verify that 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images. Our experiments show that GPT-4 can estimate an object's reflectance using only text input with a mean error of 14.7%, lower than the image-only ResNet. Moreover, CLIP achieved the lowest mean error of 11.8%, while GPT-3.5 obtained a competitive 19.9% compared to ResNet's 17.8%. These results suggest that the distributional semantics in LLMs and VLMs increases their generalization capabilities, and the knowledge acquired by VLMs benefits from the latent structure of language.",
                "authors": "Masashi Osada, G. A. G. Ricardez, Yosuke Suzuki, Tadahiro Taniguchi",
                "citations": 1
            },
            {
                "title": "Vision Language Models Are Few-Shot Audio Spectrogram Classifiers",
                "abstract": "We demonstrate that vision language models (VLMs) are capable of recognizing the content in audio recordings when given corresponding spectrogram images. Specifically, we instruct VLMs to perform audio classification tasks in a few-shot setting by prompting them to classify a spectrogram image given example spectrogram images of each class. By carefully designing the spectrogram image representation and selecting good few-shot examples, we show that GPT-4o can achieve 59.00% cross-validated accuracy on the ESC-10 environmental sound classification dataset. Moreover, we demonstrate that VLMs currently outperform the only available commercial audio language model with audio understanding capabilities (Gemini-1.5) on the equivalent audio classification task (59.00% vs. 49.62%), and even perform slightly better than human experts on visual spectrogram classification (73.75% vs. 72.50% on first fold). We envision two potential use cases for these findings: (1) combining the spectrogram and language understanding capabilities of VLMs for audio caption augmentation, and (2) posing visual spectrogram classification as a challenge task for VLMs.",
                "authors": "Satvik Dixit, Laurie M. Heller, Chris Donahue",
                "citations": 1
            },
            {
                "title": "A mRNA Vaccine for Crimean–Congo Hemorrhagic Fever Virus Expressing Non-Fusion GnGc Using NSm Linker Elicits Unexpected Immune Responses in Mice",
                "abstract": "Crimean–Congo hemorrhagic fever (CCHF), caused by Crimean–Congo Hemorrhagic virus (CCHFV), is listed in the World Health Organization’s list of priority diseases. The high fatality rate in humans, the widespread distribution of CCHFV, and the lack of approved specific vaccines are the primary concerns regarding this disease. We used microfluidic technology to optimize the mRNA vaccine delivery system and demonstrated that vaccination with nucleoside-modified CCHFV mRNA vaccines encoding GnNSmGc (vLMs), Gn (vLMn), or Gc (vLMc) induced different immune responses. We found that both T-cell and B-cell immune responses induced by vLMc were better than those induced by vLMn. Interestingly, immune responses were found to be lower for vLMs, which employed NSm to link Gn and Gc for non-fusion expression, compared to those for vLMc. In conclusion, our results indicated that NSm could be a factor that leads to decreased specific immune responses in the host and should be avoided in the development of CCHFV vaccine antigens.",
                "authors": "Tong Chen, Zhe Ding, Xuejie Li, Yingwen Li, Jiaming Lan, Gary Wong",
                "citations": 1
            },
            {
                "title": "BlenderAlchemy: Editing 3D Graphics with Vision-Language Models",
                "abstract": "Graphics design is important for various applications, including movie production and game design. To create a high-quality scene, designers usually need to spend hours in software like Blender, in which they might need to interleave and repeat operations, such as connecting material nodes, hundreds of times. Moreover, slightly different design goals may require completely different sequences, making automation difficult. In this paper, we propose a system that leverages Vision-Language Models (VLMs), like GPT-4V, to intelligently search the design action space to arrive at an answer that can satisfy a user's intent. Specifically, we design a vision-based edit generator and state evaluator to work together to find the correct sequence of actions to achieve the goal. Inspired by the role of visual imagination in the human design process, we supplement the visual reasoning capabilities of VLMs with\"imagined\"reference images from image-generation models, providing visual grounding of abstract language descriptions. In this paper, we provide empirical evidence suggesting our system can produce simple but tedious Blender editing sequences for tasks such as editing procedural materials and geometry from text and/or reference images, as well as adjusting lighting configurations for product renderings in complex scenes.",
                "authors": "Ian Huang, Guandao Yang, Leonidas J. Guibas",
                "citations": 1
            },
            {
                "title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation",
                "abstract": "Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization to unseen tasks, but fall short of the performance of supervised methods in generalizing to downstream tasks with limited data. Prompt learning is emerging as a parameter-efficient method for adapting VLMs, but state-of-the-art approaches require annotated samples. In this paper we propose a novel approach to prompt learning based on unsupervised knowledge distillation from more powerful models. Our approach, which we call Knowledge Distillation Prompt Learning (KDPL), can be integrated into existing prompt learning techniques and eliminates the need for labeled examples during adaptation. Our experiments on more than ten standard benchmark datasets demonstrate that KDPL is very effective at improving generalization of learned prompts for zero-shot domain generalization, zero-shot cross-dataset generalization, and zero-shot base-to-novel class generalization problems. KDPL requires no ground-truth labels for adaptation, and moreover we show that even in the absence of any knowledge of training class names it can be used to effectively transfer knowledge. The code is publicly available at https://github.com/miccunifi/KDPL.",
                "authors": "Marco Mistretta, Alberto Baldrati, M. Bertini, Andrew D. Bagdanov",
                "citations": 1
            },
            {
                "title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models",
                "abstract": "Cognitive textual and visual reasoning tasks, including puzzles, series, and analogies, demand the ability to quickly reason, decipher, and evaluate patterns both textually and spatially. Due to extensive training on vast amounts of human-curated data, LLMs and VLMs excel in common-sense reasoning tasks, however still struggle with more complex reasoning that demands deeper cognitive understanding. We introduce NTSEBench, a new dataset designed to evaluate cognitive multi-modal reasoning and problem-solving skills of large models. The dataset contains 2728 multiple-choice questions, accompanied by a total of 4,642 images, categorized into 26 different types. These questions are drawn from the nationwide NTSE examination in India and feature a mix of visual and textual general aptitude challenges, designed to assess intelligence and critical thinking skills beyond mere rote learning. We establish baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison between open source and propriety models, we propose four distinct modeling strategies to handle different modalities -- text and images -- in the dataset instances.",
                "authors": "Pranshu Pandya, Agney S Talwarr, Vatsal Gupta, Tushar Kataria, Vivek Gupta, Dan Roth",
                "citations": 1
            },
            {
                "title": "Generative Semantic Communication via Textual Prompts: Latency Performance Tradeoffs",
                "abstract": "This paper develops an edge-device collaborative Generative Semantic Communications (Gen SemCom) framework leveraging pre-trained Multi-modal/Vision Language Models (M/VLMs) for ultra-low-rate semantic communication via textual prompts. The proposed framework optimizes the use of M/VLMs on the wireless edge/device to generate high-fidelity textual prompts through visual captioning/question answering, which are then transmitted over a wireless channel for SemCom. Specifically, we develop a multi-user Gen SemCom framework using pre-trained M/VLMs, and formulate a joint optimization problem of prompt generation offloading, communication and computation resource allocation to minimize the latency and maximize the resulting semantic quality. Due to the nonconvex nature of the problem with highly coupled discrete and continuous variables, we decompose it as a two-level problem and propose a low-complexity swap/leaving/joining (SLJ)-based matching algorithm. Simulation results demonstrate significant performance improvements over the conventional semanticunaware/non-collaborative offloading benchmarks.",
                "authors": "Mengmeng Ren, Li Qiao, Long Yang, Zhen Gao, Jian Chen, Mahdi Boloursaz Mashhadi, Pei Xiao, Rahim Tafazolli, Mehdi Bennis",
                "citations": 1
            },
            {
                "title": "Privacy-Aware Visual Language Models",
                "abstract": "This paper aims to advance our understanding of how Visual Language Models (VLMs) handle privacy-sensitive information, a crucial concern as these technologies become integral to everyday life. To this end, we introduce a new benchmark PrivBench, which contains images from 8 sensitive categories such as passports, or fingerprints. We evaluate 10 state-of-the-art VLMs on this benchmark and observe a generally limited understanding of privacy, highlighting a significant area for model improvement. Based on this we introduce PrivTune, a new instruction-tuning dataset aimed at equipping VLMs with knowledge about visual privacy. By tuning two pretrained VLMs, TinyLLaVa and MiniGPT-v2, on this small dataset, we achieve strong gains in their ability to recognize sensitive content, outperforming even GPT4-V. At the same time, we show that privacy-tuning only minimally affects the VLMs performance on standard benchmarks such as VQA. Overall, this paper lays out a crucial challenge for making VLMs effective in handling real-world data safely and provides a simple recipe that takes the first step towards building privacy-aware VLMs.",
                "authors": "Laurens Samson, Nimrod Barazani, S. Ghebreab, Yukiyasu Asano",
                "citations": 1
            },
            {
                "title": "IndiFoodVQA: Advancing Visual Question Answering and Reasoning with a Knowledge-Infused Synthetic Data Generation Pipeline",
                "abstract": "Large Vision Language Models (VLMs) like GPT-4, LLaVA, and InstructBLIP exhibit extraordinary capabilities for both knowledge understanding and reasoning. However, the reasoning capabilities of such models on sophisticated problems that require external knowledge of a specific domain have not been assessed well, due to the unavailability of necessary datasets. In this work, we release a first-of-its-kind dataset called IndiFoodVQA with around 16.7k data samples, consisting of explicit knowledge-infused questions, answers, and reasons. We also release IndiFoodKG, a related Knowledge Graph (KG) with 79k triples. The data has been created with minimal human intervention via an automated pipeline based on InstructBlip and GPT-3.5. We also present a methodology to extract knowledge from the KG and use it to both answer and reason upon the questions. We employ different models to report baseline zero-shot and fine-tuned results. Fine-tuned VLMs on our data showed an improvement of ~25% over the corresponding base model, highlighting the fact that current VLMs need domain-specific fine-tuning to excel in specialized settings. Our findings reveal that (1) explicit knowledge infusion during question generation helps in making questions that have more grounded knowledge, and (2) proper knowledge retrieval can often lead to better-answering potential in such cases. The data and code is available at https://github.com/SLSravanthi/IndifoodVQA.",
                "authors": "Pulkit Agarwal, S. Sravanthi, Pushpak Bhattacharyya",
                "citations": 1
            },
            {
                "title": "VisualRWKV-HD and UHD: Advancing High-Resolution Processing for Visual Language Models",
                "abstract": "Accurately understanding complex visual information is crucial for visual language models (VLMs). Enhancing image resolution can improve visual perception capabilities, not only reducing hallucinations but also boosting performance in tasks that demand high resolution, such as text-rich or document analysis. In this paper, we present VisualRWKV-HD and VisualRWKV-UHD, two advancements in the VisualRWKV model family, specifically designed to process high-resolution visual inputs. For VisualRWKV-HD, we developed a lossless downsampling method to effectively integrate a high-resolution vision encoder with low-resolution encoders, without extending the input sequence length. For the VisualRWKV-UHD model, we enhanced image representation by dividing the image into four segments, which are then recombined with the original image. This technique allows the model to incorporate both high-resolution and low-resolution features, effectively balancing coarse and fine-grained information. As a result, the model supports resolutions up to 4096 x 4096 pixels, offering a more detailed and comprehensive visual processing capability. Both VisualRWKV-HD and VisualRWKV-UHD not only achieve strong results on VLM benchmarks but also show marked improvements in performance for text-rich tasks.",
                "authors": "Zihang Li, Haowen Hou",
                "citations": 1
            },
            {
                "title": "Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem",
                "abstract": "Recent work has documented striking heterogeneity in the performance of state-of-the-art vision language models (VLMs), including both multimodal language models and text-to-image models. These models are able to describe and generate a diverse array of complex, naturalistic images, yet they exhibit surprising failures on basic multi-object reasoning tasks -- such as counting, localization, and simple forms of visual analogy -- that humans perform with near perfect accuracy. To better understand this puzzling pattern of successes and failures, we turn to theoretical accounts of the binding problem in cognitive science and neuroscience, a fundamental problem that arises when a shared set of representational resources must be used to represent distinct entities (e.g., to represent multiple objects in an image), necessitating the use of serial processing to avoid interference. We find that many of the puzzling failures of state-of-the-art VLMs can be explained as arising due to the binding problem, and that these failure modes are strikingly similar to the limitations exhibited by rapid, feedforward processing in the human brain.",
                "authors": "Declan Campbell, Sunayana Rane, Tyler Giallanza, Nicolò De Sabbata, Kia Ghods, Amogh Joshi, Alexander Ku, Steven M. Frankland, Thomas L. Griffiths, Jonathan D. Cohen, Taylor Webb",
                "citations": 1
            },
            {
                "title": "An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation",
                "abstract": "Radiological services are experiencing unprecedented demand, leading to increased interest in automating radiology report generation. Existing Vision-Language Models (VLMs) suffer from hallucinations, lack interpretability, and require expensive fine-tuning. We introduce SAE-Rad, which uses sparse autoencoders (SAEs) to decompose latent representations from a pre-trained vision transformer into human-interpretable features. Our hybrid architecture combines state-of-the-art SAE advancements, achieving accurate latent reconstructions while maintaining sparsity. Using an off-the-shelf language model, we distil ground-truth reports into radiological descriptions for each SAE feature, which we then compile into a full report for each image, eliminating the need for fine-tuning large models for this task. To the best of our knowledge, SAE-Rad represents the first instance of using mechanistic interpretability techniques explicitly for a downstream multi-modal reasoning task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific metrics compared to state-of-the-art models while using significantly fewer computational resources for training. Qualitative analysis reveals that SAE-Rad learns meaningful visual concepts and generates reports aligning closely with expert interpretations. Our results suggest that SAEs can enhance multimodal reasoning in healthcare, providing a more interpretable alternative to existing VLMs.",
                "authors": "Ahmed Abdulaal, Hugo Fry, Nina Montaña Brown, Ayodeji Ijishakin, Jack Gao, Stephanie L. Hyland, Daniel C. Alexander, Daniel C. Castro",
                "citations": 1
            },
            {
                "title": "CLIP feature-based randomized control using images and text for multiple tasks and robots",
                "abstract": "This study presents a control framework leveraging vision language models (VLMs) for multiple tasks and robots. Notably, existing control methods using VLMs have achieved high performance in various tasks and robots in the training environment. However, these methods incur high costs for learning control policies for tasks and robots other than those in the training environment. Considering the application of industrial and household robots, learning in novel environments where robots are introduced is challenging. To address this issue, we propose a control framework that does not require learning control policies. Our framework combines the vision-language CLIP model with a randomized control. CLIP computes the similarity between images and texts by embedding them in the feature space. This study employs CLIP to compute the similarity between camera images and text representing the target state. In our method, the robot is controlled by a randomized controller that simultaneously explores and increases the similarity gradients. Moreover, we fine-tune the CLIP to improve the performance of the proposed method. Consequently, we confirm the effectiveness of our approach through a multitask simulation and a real robot experiment using a two-wheeled robot and robot arm.",
                "authors": "Kazuki Shibata, Hideki Deguchi, Shun Taguchi",
                "citations": 1
            },
            {
                "title": "Hard Cases Detection in Motion Prediction by Vision-Language Foundation Models",
                "abstract": "Addressing hard cases in autonomous driving, such as anomalous road users, extreme weather conditions, and complex traffic interactions, presents significant challenges. To ensure safety, it is crucial to detect and manage these scenarios effectively for autonomous driving systems. However, the rarity and high-risk nature of these cases demand extensive, diverse datasets for training robust models. Vision-Language Foundation Models (VLMs) have shown remarkable zero-shot capabilities as being trained on extensive datasets. This work explores the potential of VLMs in detecting hard cases in autonomous driving. We demonstrate the capability of VLMs such as GPT-4v in detecting hard cases in traffic participant motion prediction on both agent and scenario levels. We introduce a feasible pipeline where VLMs, fed with sequential image frames with designed prompts, effectively identify challenging agents or scenarios, which are verified by existing prediction models. Moreover, by taking advantage of this detection of hard cases by VLMs, we further improve the training efficiency of the existing motion prediction pipeline by performing data selection for the training samples suggested by GPT. We show the effectiveness and feasibility of our pipeline incorporating VLMs with state-of-the-art methods on NuScenes datasets. The code is accessible at https://github.com/KTH-RPL/Detect_VLM.",
                "authors": "Yi Yang, Qingwen Zhang, Kei Ikemura, Nazre Batool, John Folkesson",
                "citations": 1
            },
            {
                "title": "Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models",
                "abstract": "With the prevalence of large-scale pretrained vision-language models (VLMs), such as CLIP, soft-prompt tuning has become a popular method for adapting these models to various downstream tasks. However, few works delve into the inherent properties of learnable soft-prompt vectors, specifically the impact of their norms to the performance of VLMs. This motivates us to pose an unexplored research question: ``Do we need to normalize the soft prompts in VLMs?'' To fill this research gap, we first uncover a phenomenon, called the \\textbf{Low-Norm Effect} by performing extensive corruption experiments, suggesting that reducing the norms of certain learned prompts occasionally enhances the performance of VLMs, while increasing them often degrades it. To harness this effect, we propose a novel method named \\textbf{N}ormalizing th\\textbf{e} soft-pro\\textbf{m}pt v\\textbf{e}ctors of vi\\textbf{si}on-language model\\textbf{s} (\\textbf{Nemesis}) to normalize soft-prompt vectors in VLMs. To the best of our knowledge, our work is the first to systematically investigate the role of norms of soft-prompt vector in VLMs, offering valuable insights for future research in soft-prompt tuning. The code is available at \\texttt{\\href{https://github.com/ShyFoo/Nemesis}{https://github.com/ShyFoo/Nemesis}}.",
                "authors": "Shuai Fu, Xiequn Wang, Qiushi Huang, Yu Zhang",
                "citations": 1
            },
            {
                "title": "Harnessing the Power of Large Vision Language Models for Synthetic Image Detection",
                "abstract": "In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the potential misuse of these images, including the creation of misleading content such as fake news and propaganda. This study investigates the effectiveness of using advanced vision-language models (VLMs) for synthetic image identification. Specifically, the focus is on tuning state-of-the-art image captioning models for synthetic image detection. By harnessing the robust understanding capabilities of large VLMs, the aim is to distinguish authentic images from synthetic images produced by diffusion-based models. This study contributes to the advancement of synthetic image detection by exploiting the capabilities of visual language models such as BLIP-2 and ViTGPT2. By tailoring image captioning models, we address the challenges associated with the potential misuse of synthetic images in real-world applications. Results described in this paper highlight the promising role of VLMs in the field of synthetic image detection, outperforming conventional image-based detection techniques. Code and models can be found at https://github.com/Mamadou-Keita/VLM-DETECT.",
                "authors": "Mamadou Keita, W. Hamidouche, Hassen Bougueffa, Abdenour Hadid, Abdelmalik Taleb-Ahmed",
                "citations": 1
            },
            {
                "title": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models",
                "abstract": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have demonstrated aptitude as potential substitutes for human participants in experiments testing psycholinguistic phenomena. However, an understudied question is to what extent models that only have access to vision and text modalities are able to implicitly understand sound-based phenomena via abstract reasoning from orthography and imagery alone. To investigate this, we analyse the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise a non-arbitrary link between sounds and concepts) as well as their ability to “hear” via the interplay of the language and vision modules of open and closed-source multimodal models. We perform multiple experiments, including replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism tasks and comparing human judgements of linguistic iconicity with that of LLMs. Our results show that VLMs demonstrate varying levels of agreement with human labels, and more task information may be required for VLMs versus their human counterparts for in silico experimentation. We additionally see through higher maximum agreement levels that Magnitude Symbolism is an easier pattern for VLMs to identify than Shape Symbolism, and that an understanding of linguistic iconicity is highly dependent on model size.",
                "authors": "Tyler Loakman, Yucheng Li, Chenghua Lin",
                "citations": 1
            },
            {
                "title": "Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers",
                "abstract": "Recent advancements in vision-language models (VLMs) have expanded their potential for real-world applications, enabling these models to perform complex reasoning on images. In the widely used fully autoregressive transformer-based models like LLaVA, projected visual tokens are prepended to textual tokens. Oftentimes, visual tokens are significantly more than prompt tokens, resulting in increased computational overhead during both training and inference. In this paper, we propose Visual Compact Token Registers (Victor), a method that reduces the number of visual tokens by summarizing them into a smaller set of register tokens. Victor adds a few learnable register tokens after the visual tokens and summarizes the visual information into these registers using the first few layers in the language tower of VLMs. After these few layers, all visual tokens are discarded, significantly improving computational efficiency for both training and inference. Notably, our method is easy to implement and requires a small number of new trainable parameters with minimal impact on model performance. In our experiment, with merely 8 visual registers--about 1% of the original tokens--Victor shows less than a 4% accuracy drop while reducing the total training time by 43% and boosting the inference throughput by 3.3X.",
                "authors": "Yuxin Wen, Qingqing Cao, Qichen Fu, Sachin Mehta, Mahyar Najibi",
                "citations": 1
            },
            {
                "title": "Benchmarking VLMs' Reasoning About Persuasive Atypical Images",
                "abstract": "Vision language models (VLMs) have shown strong zero-shot generalization across various tasks, especially when integrated with large language models (LLMs). However, their ability to comprehend rhetorical and persuasive visual media, such as advertisements, remains understudied. Ads often employ atypical imagery, using surprising object juxtapositions to convey shared properties. For example, Fig. 1 (e) shows a beer with a feather-like texture. This requires advanced reasoning to deduce that this atypical representation signifies the beer's lightness. We introduce three novel tasks, Multi-label Atypicality Classification, Atypicality Statement Retrieval, and Aypical Object Recognition, to benchmark VLMs' understanding of atypicality in persuasive images. We evaluate how well VLMs use atypicality to infer an ad's message and test their reasoning abilities by employing semantically challenging negatives. Finally, we pioneer atypicality-aware verbalization by extracting comprehensive image descriptions sensitive to atypical elements. Our findings reveal that: (1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple, effective strategies can extract atypicality-aware information, leading to comprehensive image verbalization; (3) atypicality aids persuasive advertisement understanding. Code and data will be made available.",
                "authors": "Sina Malakouti, Aysan Aghazadeh, Ashmit Khandelwal, Adriana Kovashka",
                "citations": 1
            },
            {
                "title": "CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering",
                "abstract": "Recent Vision-Language Models (VLMs) have demonstrated remarkable capabilities in visual understanding and reasoning, and in particular on multiple-choice Visual Question Answering (VQA). Still, these models can make distinctly unnatural errors, for example, providing (wrong) answers to unanswerable VQA questions, such as questions asking about objects that do not appear in the image. To address this issue, we propose CLIP-UP: CLIP-based Unanswerable Problem detection, a novel lightweight method for equipping VLMs with the ability to withhold answers to unanswerable questions. By leveraging CLIP to extract question-image alignment information, CLIP-UP requires only efficient training of a few additional layers, while keeping the original VLMs' weights unchanged. Tested across LLaVA models, CLIP-UP achieves state-of-the-art results on the MM-UPD benchmark for assessing unanswerability in multiple-choice VQA, while preserving the original performance on other tasks.",
                "authors": "Ben Vardi, O. Nir, Ariel Shamir",
                "citations": 0
            },
            {
                "title": "Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning",
                "abstract": "This paper proposes the first video-grounded entailment tree reasoning method for commonsense video question answering (VQA). Despite the remarkable progress of large visual-language models (VLMs), there are growing concerns that they learn spurious correlations between videos and likely answers, reinforced by their black-box nature and remaining benchmarking biases. Our method explicitly grounds VQA tasks to video fragments in four steps: entailment tree construction, video-language entailment verification, tree reasoning, and dynamic tree expansion. A vital benefit of the method is its generalizability to current video and image-based VLMs across reasoning types. To support fair evaluation, we devise a de-biasing procedure based on large-language models that rewrites VQA benchmark answer sets to enforce model reasoning. Systematic experiments on existing and de-biased benchmarks highlight the impact of our method components across benchmarks, VLMs, and reasoning types.",
                "authors": "Huabin Liu, Filip Ilievski, Cees G. M. Snoek",
                "citations": 0
            },
            {
                "title": "SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptation",
                "abstract": "We present SelfPrompt, a novel prompt-tuning approach for vision-language models (VLMs) in a semi-supervised learning setup. Existing methods for tuning VLMs in semi-supervised setups struggle with the negative impact of the miscalibrated VLMs on pseudo-labelling, and the accumulation of noisy pseudo-labels. SelfPrompt addresses these challenges by introducing a cluster-guided pseudo-labelling method that improves pseudo-label accuracy, and a confidence-aware semi-supervised learning module that maximizes the utilization of unlabelled data by combining supervised learning and weakly-supervised learning. Additionally, we investigate our method in an active semi-supervised learning setup, where the labelled set is strategically selected to ensure the best utilization of a limited labelling budget. To this end, we propose a weakly-supervised sampling technique that selects a diverse and representative labelled set, which can be seamlessly integrated into existing methods to enhance their performance. We conduct extensive evaluations across 13 datasets, significantly surpassing state-of-the-art performances with average improvements of 6.23% in standard semi-supervised learning, 6.25% in active semi-supervised learning, and 4.9% in base-to-novel generalization, using a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in single-shot settings, achieving an average improvement of 11.78%.",
                "authors": "Shuvendu Roy, A. Etemad",
                "citations": 0
            },
            {
                "title": "FGAseg: Fine-Grained Pixel-Text Alignment for Open-Vocabulary Semantic Segmentation",
                "abstract": "Open-vocabulary segmentation aims to identify and segment specific regions and objects based on text-based descriptions. A common solution is to leverage powerful vision-language models (VLMs), such as CLIP, to bridge the gap between vision and text information. However, VLMs are typically pretrained for image-level vision-text alignment, focusing on global semantic features. In contrast, segmentation tasks require fine-grained pixel-level alignment and detailed category boundary information, which VLMs alone cannot provide. As a result, information extracted directly from VLMs can't meet the requirements of segmentation tasks. To address this limitation, we propose FGAseg, a model designed for fine-grained pixel-text alignment and category boundary supplementation. The core of FGAseg is a Pixel-Level Alignment module that employs a cross-modal attention mechanism and a text-pixel alignment loss to refine the coarse-grained alignment from CLIP, achieving finer-grained pixel-text semantic alignment. Additionally, to enrich category boundary information, we introduce the alignment matrices as optimizable pseudo-masks during forward propagation and propose Category Information Supplementation module. These pseudo-masks, derived from cosine and convolutional similarity, provide essential global and local boundary information between different categories. By combining these two strategies, FGAseg effectively enhances pixel-level alignment and category boundary information, addressing key challenges in open-vocabulary segmentation. Extensive experiments demonstrate that FGAseg outperforms existing methods on open-vocabulary semantic segmentation benchmarks.",
                "authors": "Bingyu Li, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li",
                "citations": 0
            },
            {
                "title": "Vision-Language Models Do Not Understand Negation",
                "abstract": "Many practical vision-language applications require models that understand negation, e.g., when using natural language to retrieve images which contain certain objects but not others. Despite advancements in vision-language models (VLMs) through large-scale training, their ability to comprehend negation remains underexplored. This study addresses the question: how well do current VLMs understand negation? We introduce NegBench, a new benchmark designed to evaluate negation understanding across 18 task variations and 79k examples spanning image, video, and medical datasets. The benchmark consists of two core tasks designed to evaluate negation understanding in diverse multimodal settings: Retrieval with Negation and Multiple Choice Questions with Negated Captions. Our evaluation reveals that modern VLMs struggle significantly with negation, often performing at chance level. To address these shortcomings, we explore a data-centric approach wherein we finetune CLIP models on large-scale synthetic datasets containing millions of negated captions. We show that this approach can result in a 10% increase in recall on negated queries and a 40% boost in accuracy on multiple-choice questions with negated captions.",
                "authors": "Kumail Alhamoud, Shaden S Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi",
                "citations": 0
            },
            {
                "title": "IllusionBench: A Large-scale and Comprehensive Benchmark for Visual Illusion Understanding in Vision-Language Models",
                "abstract": "Current Visual Language Models (VLMs) show impressive image understanding but struggle with visual illusions, especially in real-world scenarios. Existing benchmarks focus on classical cognitive illusions, which have been learned by state-of-the-art (SOTA) VLMs, revealing issues such as hallucinations and limited perceptual abilities. To address this gap, we introduce IllusionBench, a comprehensive visual illusion dataset that encompasses not only classic cognitive illusions but also real-world scene illusions. This dataset features 1,051 images, 5,548 question-answer pairs, and 1,051 golden text descriptions that address the presence, causes, and content of the illusions. We evaluate ten SOTA VLMs on this dataset using true-or-false, multiple-choice, and open-ended tasks. In addition to real-world illusions, we design trap illusions that resemble classical patterns but differ in reality, highlighting hallucination issues in SOTA models. The top-performing model, GPT-4o, achieves 80.59% accuracy on true-or-false tasks and 76.75% on multiple-choice questions, but still lags behind human performance. In the semantic description task, GPT-4o's hallucinations on classical illusions result in low scores for trap illusions, even falling behind some open-source models. IllusionBench is, to the best of our knowledge, the largest and most comprehensive benchmark for visual illusions in VLMs to date.",
                "authors": "Yiming Zhang, Zicheng Zhang, Xinyi Wei, Xiaohong Liu, Guangtao Zhai, Xiongkuo Min",
                "citations": 0
            },
            {
                "title": "Embodied Scene Understanding for Vision Language Models via MetaVQA",
                "abstract": "Vision Language Models (VLMs) demonstrate significant potential as embodied AI agents for various mobility applications. However, a standardized, closed-loop benchmark for evaluating their spatial reasoning and sequential decision-making capabilities is lacking. To address this, we present MetaVQA: a comprehensive benchmark designed to assess and enhance VLMs' understanding of spatial relationships and scene dynamics through Visual Question Answering (VQA) and closed-loop simulations. MetaVQA leverages Set-of-Mark prompting and top-down view ground-truth annotations from nuScenes and Waymo datasets to automatically generate extensive question-answer pairs based on diverse real-world traffic scenarios, ensuring object-centric and context-rich instructions. Our experiments show that fine-tuning VLMs with the MetaVQA dataset significantly improves their spatial reasoning and embodied scene comprehension in safety-critical simulations, evident not only in improved VQA accuracies but also in emerging safety-aware driving maneuvers. In addition, the learning demonstrates strong transferability from simulation to real-world observation. Code and data will be publicly available at https://metadriverse.github.io/metavqa .",
                "authors": "Weizhen Wang, Chenda Duan, Zhenghao Peng, Yuxin Liu, Bolei Zhou",
                "citations": 0
            },
            {
                "title": "Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving",
                "abstract": "Vision-language models (VLMs) have significantly advanced autonomous driving (AD) by enhancing reasoning capabilities; however, these models remain highly susceptible to adversarial attacks. While existing research has explored white-box attacks to some extent, the more practical and challenging black-box scenarios remain largely underexplored due to their inherent difficulty. In this paper, we take the first step toward designing black-box adversarial attacks specifically targeting VLMs in AD. We identify two key challenges for achieving effective black-box attacks in this context: the effectiveness across driving reasoning chains in AD systems and the dynamic nature of driving scenarios. To address this, we propose Cascading Adversarial Disruption (CAD). It first introduces Decision Chain Disruption, which targets low-level reasoning breakdown by generating and injecting deceptive semantics, ensuring the perturbations remain effective across the entire decision-making chain. Building on this, we present Risky Scene Induction, which addresses dynamic adaptation by leveraging a surrogate VLM to understand and construct high-level risky scenarios that are likely to result in critical errors in the current driving contexts. Extensive experiments conducted on multiple AD VLMs and benchmarks demonstrate that CAD achieves state-of-the-art attack effectiveness, significantly outperforming existing methods (+13.43% on average). Moreover, we validate its practical applicability through real-world attacks on AD vehicles powered by VLMs, where the route completion rate drops by 61.11% and the vehicle crashes directly into the obstacle vehicle with adversarial patches. Finally, we release CADA dataset, comprising 18,808 adversarial visual-question-answer pairs, to facilitate further evaluation and research in this critical domain. Our codes and dataset will be available after paper's acceptance.",
                "authors": "Lu Wang, Tianyuan Zhang, Yang Qu, Siyuan Liang, Yuwei Chen, Aishan Liu, Xianglong Liu, Dacheng Tao",
                "citations": 0
            },
            {
                "title": "Efficient Architectures for High Resolution Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.",
                "authors": "Miguel Carvalho, Bruno Martins",
                "citations": 0
            },
            {
                "title": "Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?",
                "abstract": "While Vision Language Models (VLMs) are impressive in tasks such as visual question answering (VQA) and image captioning, their ability to apply multi-step reasoning to images has lagged, giving rise to perceptions of modality imbalance or brittleness. Towards systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning (AVR), comprising three tasks: Table Readout, Grid Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for frontier VLMs. We seek strategies for training on the SIMPLE version of the tasks that improve performance on the corresponding HARD task, i.e., S2H generalization. This synthetic framework, where each task also has a text-only version, allows a quantification of the modality imbalance, and how it is impacted by training strategy. Ablations highlight the importance of explicit image-to-text conversion in promoting S2H generalization when using auto-regressive training. We also report results of mechanistic study of this phenomenon, including a measure of gradient alignment that seems to identify training strategies that promote better S2H generalization.",
                "authors": "Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, Sanjeev Arora",
                "citations": 0
            },
            {
                "title": "CuTCP: Custom Text Generation-based Class-aware Prompt Tuning for visual-language models",
                "abstract": null,
                "authors": "Min Huang, Chen Yang, Xiaoyan Yu",
                "citations": 0
            },
            {
                "title": "Realistic Test-Time Adaptation of Vision-Language Models",
                "abstract": "The zero-shot capabilities of Vision-Language Models (VLMs) have been widely leveraged to improve predictive performance. However, previous works on transductive or test-time adaptation (TTA) often make strong assumptions about the data distribution, such as the presence of all classes. Our work challenges these favorable deployment scenarios, and introduces a more realistic evaluation framework, including: (i) a variable number of effective classes for adaptation within a single batch, and (ii) non-i.i.d. batches of test samples in online adaptation settings. We provide comprehensive evaluations, comparisons, and ablation studies that demonstrate how current transductive or TTA methods for VLMs systematically compromise the models' initial zero-shot robustness across various realistic scenarios, favoring performance gains under advantageous assumptions about the test samples' distributions. Furthermore, we introduce StatA, a versatile method that could handle a wide range of deployment scenarios, including those with a variable number of effective classes at test time. Our approach incorporates a novel regularization term designed specifically for VLMs, which acts as a statistical anchor preserving the initial text-encoder knowledge, particularly in low-data regimes. Code available at https://github.com/MaxZanella/StatA.",
                "authors": "Maxime Zanella, Cl'ement Fuchs, C. Vleeschouwer, Ismail Ben Ayed",
                "citations": 0
            },
            {
                "title": "Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations",
                "abstract": "With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder architectures, leading to the emergence of works like LLava-Med. However, these works primarily operate at the whole-image level, aligning general information from 2D medical images without attending to finer details. As a result, these models often provide irrelevant or non-clinically valuable information while missing critical details. Medical vision-language tasks differ significantly from general images, particularly in their focus on fine-grained details, while excluding irrelevant content. General domain VLMs tend to prioritize global information due to their design, which compresses the entire image into a multi-token representation that is passed into the LLM decoder. Therefore, current VLMs all lack the capability to restrict their attention to particular areas. To address this critical issue in the medical domain, we introduce MedVP, an visual prompt generation and fine-tuning framework, which involves extract medical entities, generate visual prompts, and adapt datasets for visual prompt guided fine-tuning. To the best of our knowledge, this is the first work to explicitly introduce visual prompt into medical VLMs, and we successfully outperform recent state-of-the-art large models across multiple medical VQA datasets. Extensive experiments are conducted to analyze the impact of different visual prompt forms and how they contribute to performance improvement. The results demonstrate both the effectiveness and clinical significance of our approach",
                "authors": "Kangyu Zhu, Ziyuan Qin, Huahui Yi, Zekun Jiang, Qicheng Lao, Shaoting Zhang, Kang Li",
                "citations": 0
            },
            {
                "title": "Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding",
                "abstract": "Vision-language Models (VLMs) have shown remarkable capabilities in advancing general artificial intelligence, yet the irrational encoding of visual positions persists in inhibiting the models' comprehensive perception performance across different levels of granularity. In this work, we propose Pyramid-descent Visual Position Encoding (PyPE), a novel approach designed to enhance the perception of visual tokens within VLMs. By assigning visual position indexes from the periphery to the center and expanding the central receptive field incrementally, PyPE addresses the limitations of traditional raster-scan methods and mitigates the long-term decay effects induced by Rotary Position Embedding (RoPE). Our method reduces the relative distance between interrelated visual elements and instruction tokens, promoting a more rational allocation of attention weights and allowing for a multi-granularity perception of visual elements and countering the over-reliance on anchor tokens. Extensive experimental evaluations demonstrate that PyPE consistently improves the general capabilities of VLMs across various sizes. Code is available at https://github.com/SakuraTroyChen/PyPE.",
                "authors": "Zhanpeng Chen, Mingxiao Li, Ziyang Chen, Nan Du, Xiaolong Li, Yuexian Zou",
                "citations": 0
            },
            {
                "title": "SimLabel: Consistency-Guided OOD Detection with Pretrained Vision-Language Models",
                "abstract": "Detecting out-of-distribution (OOD) data is crucial in real-world machine learning applications, particularly in safety-critical domains. Existing methods often leverage language information from vision-language models (VLMs) to enhance OOD detection by improving confidence estimation through rich class-wise text information. However, when building OOD detection score upon on in-distribution (ID) text-image affinity, existing works either focus on each ID class or whole ID label sets, overlooking inherent ID classes' connection. We find that the semantic information across different ID classes is beneficial for effective OOD detection. We thus investigate the ability of image-text comprehension among different semantic-related ID labels in VLMs and propose a novel post-hoc strategy called SimLabel. SimLabel enhances the separability between ID and OOD samples by establishing a more robust image-class similarity metric that considers consistency over a set of similar class labels. Extensive experiments demonstrate the superior performance of SimLabel on various zero-shot OOD detection benchmarks. The proposed model is also extended to various VLM-backbones, demonstrating its good generalization ability. Our demonstration and implementation codes are available at: https://github.com/ShuZou-1/SimLabel.",
                "authors": "Shu Zou, Xinyu Tian, Qinyu Zhao, Zhaoyuan Yang, Jing Zhang",
                "citations": 0
            },
            {
                "title": "Text-driven Online Action Detection",
                "abstract": "Detecting actions as they occur is essential for applications like video surveillance, autonomous driving, and human-robot interaction. Known as online action detection, this task requires classifying actions in streaming videos, handling background noise, and coping with incomplete actions. Transformer architectures are the current state-of-the-art, yet the potential of recent advancements in computer vision, particularly vision-language models (VLMs), remains largely untapped for this problem, partly due to high computational costs. In this paper, we introduce TOAD: a Text-driven Online Action Detection architecture that supports zero-shot and few-shot learning. TOAD leverages CLIP (Contrastive Language-Image Pretraining) textual embeddings, enabling efficient use of VLMs without significant computational overhead. Our model achieves 82.46% mAP on the THUMOS14 dataset, outperforming existing methods, and sets new baselines for zero-shot and few-shot performance on the THUMOS14 and TVSeries datasets.",
                "authors": "Manuel Benavent-Lledó, David Mulero-P'erez, David Ortiz-Perez, Jose Garcia-Rodriguez",
                "citations": 0
            },
            {
                "title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation",
                "abstract": "The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.",
                "authors": "Yuhui Zhang, Yuchang Su, Yiming Liu, Xiaohan Wang, James Burgess, Elaine Sui, Chenyu Wang, Josiah Aklilu, Alejandro Lozano, Anjiang Wei, Ludwig Schmidt, S. Yeung-Levy",
                "citations": 0
            },
            {
                "title": "Advancing the Understanding and Evaluation of AR-Generated Scenes: When Vision-Language Models Shine and Stumble",
                "abstract": "Augmented Reality (AR) enhances the real world by integrating virtual content, yet ensuring the quality, usability, and safety of AR experiences presents significant challenges. Could Vision-Language Models (VLMs) offer a solution for the automated evaluation of AR-generated scenes? Could Vision-Language Models (VLMs) offer a solution for the automated evaluation of AR-generated scenes? In this study, we evaluate the capabilities of three state-of-the-art commercial VLMs -- GPT, Gemini, and Claude -- in identifying and describing AR scenes. For this purpose, we use DiverseAR, the first AR dataset specifically designed to assess VLMs' ability to analyze virtual content across a wide range of AR scene complexities. Our findings demonstrate that VLMs are generally capable of perceiving and describing AR scenes, achieving a True Positive Rate (TPR) of up to 93\\% for perception and 71\\% for description. While they excel at identifying obvious virtual objects, such as a glowing apple, they struggle when faced with seamlessly integrated content, such as a virtual pot with realistic shadows. Our results highlight both the strengths and the limitations of VLMs in understanding AR scenarios. We identify key factors affecting VLM performance, including virtual content placement, rendering quality, and physical plausibility. This study underscores the potential of VLMs as tools for evaluating the quality of AR experiences.",
                "authors": "Lin Duan, Yanming Xiu, Maria Gorlatova",
                "citations": 0
            },
            {
                "title": "Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey",
                "abstract": "Multimodal Vision Language Models (VLMs) have emerged as a transformative technology at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP, Claude, and GPT-4V demonstrate strong reasoning and understanding abilities on visual and textual data and beat classical single modality vision models on zero-shot classification. Despite their rapid advancements in research and growing popularity in applications, a comprehensive survey of existing studies on VLMs is notably lacking, particularly for researchers aiming to leverage VLMs in their specific domains. To this end, we provide a systematic overview of VLMs in the following aspects: model information of the major VLMs developed over the past five years (2019-2024); the main architectures and training methods of these VLMs; summary and categorization of the popular benchmarks and evaluation metrics of VLMs; the applications of VLMs including embodied agents, robotics, and video generation; the challenges and issues faced by current VLMs such as hallucination, fairness, and safety. Detailed collections including papers and model repository links are listed in https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.",
                "authors": "Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, Guangyao Shi",
                "citations": 0
            },
            {
                "title": "AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention Mixture",
                "abstract": "The success of VLMs often relies on the dynamic high-resolution schema that adaptively augments the input images to multiple crops, so that the details of the images can be retained. However, such approaches result in a large number of redundant visual tokens, thus significantly reducing the efficiency of the VLMs. To improve the VLMs' efficiency without introducing extra training costs, many research works are proposed to reduce the visual tokens by filtering the uninformative visual tokens or aggregating their information. Some approaches propose to reduce the visual tokens according to the self-attention of VLMs, which are biased, to result in inaccurate responses. The token reduction approaches solely rely on visual cues are text-agnostic, and fail to focus on the areas that are most relevant to the question, especially when the queried objects are non-salient to the image. In this work, we first conduct experiments to show that the original text embeddings are aligned with the visual tokens, without bias on the tailed visual tokens. We then propose a self-adaptive cross-modality attention mixture mechanism that dynamically leverages the effectiveness of visual saliency and text-to-image similarity in the pre-LLM layers to select the visual tokens that are informative. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art training-free VLM acceleration performance, especially when the reduction rate is sufficiently large.",
                "authors": "Jiayi Han, Liang Du, Yiwen Wu, Xiangguo Zhou, Hongwei Du, Weibo Zheng",
                "citations": 0
            },
            {
                "title": "Visual Language Models as Operator Agents in the Space Domain",
                "abstract": "This paper explores the application of Vision-Language Models (VLMs) as operator agents in the space domain, focusing on both software and hardware operational paradigms. Building on advances in Large Language Models (LLMs) and their multimodal extensions, we investigate how VLMs can enhance autonomous control and decision-making in space missions. In the software context, we employ VLMs within the Kerbal Space Program Differential Games (KSPDG) simulation environment, enabling the agent to interpret visual screenshots of the graphical user interface to perform complex orbital maneuvers. In the hardware context, we integrate VLMs with robotic systems equipped with cameras to inspect and diagnose physical space objects, such as satellites. Our results demonstrate that VLMs can effectively process visual and textual data to generate contextually appropriate actions, competing with traditional methods and non-multimodal LLMs in simulation tasks, and showing promise in real-world applications.",
                "authors": "Alejandro Carrasco, Marco Nedungadi, Enrico M. Zucchelli, Amit Jain, Victor Rodríguez-Fernández, Richard Linares",
                "citations": 0
            },
            {
                "title": "Can Vision-Language Models Evaluate Handwritten Math?",
                "abstract": "Recent advancements in Vision-Language Models (VLMs) have opened new possibilities in automatic grading of handwritten student responses, particularly in mathematics. However, a comprehensive study to test the ability of VLMs to evaluate and reason over handwritten content remains absent. To address this gap, we introduce FERMAT, a benchmark designed to assess the ability of VLMs to detect, localize and correct errors in handwritten mathematical content. FERMAT spans four key error dimensions - computational, conceptual, notational, and presentation - and comprises over 2,200 handwritten math solutions derived from 609 manually curated problems from grades 7-12 with intentionally introduced perturbations. Using FERMAT we benchmark nine VLMs across three tasks: error detection, localization, and correction. Our results reveal significant shortcomings in current VLMs in reasoning over handwritten text, with Gemini-1.5-Pro achieving the highest error correction rate (77%). We also observed that some models struggle with processing handwritten content, as their accuracy improves when handwritten inputs are replaced with printed text or images. These findings highlight the limitations of current VLMs and reveal new avenues for improvement. We release FERMAT and all the associated resources in the open-source to drive further research.",
                "authors": "Oikantik Nath, Hanani Bathina, Mohammed Safi Ur Rahman Khan, Mitesh M. Khapra",
                "citations": 0
            },
            {
                "title": "Cognitive Paradigms for Evaluating VLMs on Visual Reasoning Task",
                "abstract": "Evaluating the reasoning capabilities of Vision-Language Models (VLMs) in complex visual tasks provides valuable insights into their potential and limitations. In this work, we assess the performance of VLMs on the challenging Bongard Openworld Problems benchmark, which involves reasoning over natural images. We propose and evaluate three human-inspired paradigms: holistic analysis (global context processing), deductive rule learning (explicit rule derivation and application), and componential analysis (structured decomposition of images into components). Our results demonstrate that state-of-the-art models, including GPT-4o and Gemini, not only surpass human benchmarks but also excel in structured reasoning tasks, with componential analysis proving especially effective. However, ablation studies reveal key challenges, such as handling synthetic images, making fine-grained distinctions, and interpreting nuanced contextual information. These insights underscore the need for further advancements in model robustness and generalization, while highlighting the transformative potential of structured reasoning approaches in enhancing VLM capabilities.",
                "authors": "Mohit Vaishnav, T. Tammet",
                "citations": 0
            },
            {
                "title": "Vision-language model-based human-robot collaboration for smart manufacturing: A state-of-the-art survey",
                "abstract": null,
                "authors": "Junming Fan, Yue Yin, Tian Wang, Wenhang Dong, Pai Zheng, Lihui Wang",
                "citations": 0
            },
            {
                "title": "CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification",
                "abstract": "Dense features, customized for different business scenarios, are essential in short video classification. However, their complexity, specific adaptation requirements, and high computational costs make them resource-intensive and less accessible during online inference. Consequently, these dense features are categorized as `Privileged Dense Features'.Meanwhile, end-to-end multi-modal models have shown promising results in numerous computer vision tasks. In industrial applications, prioritizing end-to-end multi-modal features, can enhance efficiency but often leads to the loss of valuable information from historical privileged dense features. To integrate both features while maintaining efficiency and manageable resource costs, we present Confidence-aware Privileged Feature Distillation (CPFD), which empowers features of an end-to-end multi-modal model by adaptively distilling privileged features during training. Unlike existing privileged feature distillation (PFD) methods, which apply uniform weights to all instances during distillation, potentially causing unstable performance across different business scenarios and a notable performance gap between teacher model (Dense Feature enhanced multimodal-model DF-X-VLM) and student model (multimodal-model only X-VLM), our CPFD leverages confidence scores derived from the teacher model to adaptively mitigate the performance variance with the student model. We conducted extensive offline experiments on five diverse tasks demonstrating that CPFD improves the video classification F1 score by 6.76% compared with end-to-end multimodal-model (X-VLM) and by 2.31% with vanilla PFD on-average. And it reduces the performance gap by 84.6% and achieves results comparable to teacher model DF-X-VLM. The effectiveness of CPFD is further substantiated by online experiments, and our framework has been deployed in production systems for over a dozen models.",
                "authors": "Jinghao Shi, Xiang Shen, Kaili Zhao, Xuedong Wang, Vera Wen, Zixuan Wang, Yifan Wu, Zhixin Zhang",
                "citations": 0
            },
            {
                "title": "Discovering Object Attributes by Prompting Large Language Models with Perception-Action APIs",
                "abstract": "There has been a lot of interest in grounding natural language to physical entities through visual context. While Vision Language Models (VLMs) can ground linguistic instructions to visual sensory information, they struggle with grounding non-visual attributes, like the weight of an object. Our key insight is that non-visual attribute detection can be effectively achieved by active perception guided by visual reasoning. To this end, we present a perception-action programming API that consists of VLMs and Large Language Models (LLMs) as backbones, together with a set of robot control functions. When prompted with this API and a natural language query, an LLM generates a program to actively identify attributes given an input image. Offline testing on the Odd-One-Out dataset demonstrates that our framework outperforms vanilla VLMs in detecting attributes like relative object location, size, and weight. Online testing in realistic household scenes on AI2-THOR and a real robot demonstration on a DJI RoboMaster EP robot highlight the efficacy of our approach.",
                "authors": "A. Mavrogiannis, Dehao Yuan, Y. Aloimonos",
                "citations": 0
            },
            {
                "title": "Audio Description Generation in the Era of LLMs and VLMs: A Review of Transferable Generative AI Technologies",
                "abstract": "Audio descriptions (ADs) function as acoustic commentaries designed to assist blind persons and persons with visual impairments in accessing digital media content on television and in movies, among other settings. As an accessibility service typically provided by trained AD professionals, the generation of ADs demands significant human effort, making the process both time-consuming and costly. Recent advancements in natural language processing (NLP) and computer vision (CV), particularly in large language models (LLMs) and vision-language models (VLMs), have allowed for getting a step closer to automatic AD generation. This paper reviews the technologies pertinent to AD generation in the era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV technologies can be applied to generate ADs and identify essential research directions for the future.",
                "authors": "Yingqiang Gao, Lukas Fischer, Alexa Lintner, Sarah Ebling",
                "citations": 0
            },
            {
                "title": "EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues",
                "abstract": "Automated analysis of vast Earth observation data via interactive Vision-Language Models (VLMs) can unlock new opportunities for environmental monitoring, disaster response, and resource management. Existing generic VLMs do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs remain restricted to a fixed resolution and few sensor modalities. In this paper, we introduce EarthDial, a conversational assistant specifically designed for Earth Observation (EO) data, transforming complex, multi-sensory Earth observations into interactive, natural language dialogues. EarthDial supports multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide range of remote sensing tasks, including classification, detection, captioning, question answering, visual reasoning, and visual grounding. To achieve this, we introduce an extensive instruction tuning dataset comprising over 11.11M instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore, EarthDial handles bi-temporal and multi-temporal sequence analysis for applications like change detection. Our extensive experimental results on 37 downstream applications demonstrate that EarthDial outperforms existing generic and domain-specific models, achieving better generalization across various EO tasks.",
                "authors": "Sagar Soni, Akshay Dudhane, Hiyam Debary, M. Fiaz, Muhammad Akhtar Munir, M. S. Danish, Paolo Fraccaro, Campbell D Watson, Levente J. Klein, F. Khan, Salman Khan",
                "citations": 0
            },
            {
                "title": "AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models",
                "abstract": "Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks, particularly targeted adversarial images that manipulate the model to generate harmful content specified by the adversary. Current attack methods rely on predefined target labels to create targeted adversarial attacks, which limits their scalability and applicability for large-scale robustness evaluations. In this paper, we propose AnyAttack , a self-supervised framework that generates targeted adversarial images for VLMs without label supervision, allowing any image to serve as a target for the attack . To address the limitation of existing methods that require label supervision, we introduce a contrastive loss that trains a generator on a large-scale unlabeled image dataset, LAION-400M dataset, for generating targeted adversarial noise. This large-scale pre-training endows our method with powerful transferability across a wide range of VLMs. Extensive experiments on five mainstream open-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks (image-text retrieval, multimodal classification, and image captioning) demonstrate the effectiveness of our attack. Additionally, we successfully transfer AnyAttack to multiple commercial VLMs, including Google’s Gemini, Claude’s Sonnet, and Microsoft’s Copilot. These re-sults reveal an unprecedented risk to VLMs, highlighting the need for effective countermeasures.",
                "authors": "Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Jitao Sang, Dit-Yan Yeung",
                "citations": 0
            },
            {
                "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
                "abstract": "Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planning. A common approach to address this problem is through the use of hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language and imagined observations. However, language often fails to effectively convey spatial information, while generating future images with sufficient accuracy remains challenging. To address these limitations, we propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from both past and present observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, with real-time object tracking provided by SAM-2. Our method unlocks the full potential of VLMs’ visual-language reasoning abilities, enabling them to solve complex creative tasks, especially those heavily reliant on spatial understanding. Experiments in Minecraft demonstrate that our approach allows agents to accomplish previously unattainable tasks, highlighting the effectiveness of visual-temporal context prompting in embodied decision-making. Codes and demos will be available on the project page:",
                "authors": "Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, Yitao Liang",
                "citations": 0
            },
            {
                "title": "Right this way: Can VLMs Guide Us to See More to Answer Questions?",
                "abstract": "In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating ``where to know'' scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans.",
                "authors": "Li Liu, Diji Yang, Sijia Zhong, Kalyana Suma Sree Tholeti, Lei Ding, Yi Zhang, Leilani Gilpin",
                "citations": 0
            },
            {
                "title": "Evaluating Vision-Language Models for hematology image Classification: Performance Analysis of CLIP and its Biomedical AI Variants",
                "abstract": "Vision-language models (VLMs) have shown remarkable potential in various domains, particularly in zero-shot learning applications. This research focuses on evaluating the performance of notable VLMs—CLIP, PLIP, and BiomedCLIP—in the classification of blood cells, with a specific emphasis on distinguishing between normal and malignant (cancerous) cells datasets. While CLIP demonstrates robust zero-shot capabilities in general tasks, this study probes its biomedical adaptations, PLIP and BiomedCLIP, to assess their effectiveness in specialized medical tasks, such as hematological image classification. Additionally, we investigate the impact of prompt engineering on model performance, exploring how variations in prompt construction influence accuracy across these biomedical datasets. Extensive experiments were conducted on a variety of biomedical images, including microscopic blood cell images, brain MRIs, and chest X-rays, providing a comprehensive evaluation of the VLMs’ applicability in medical imaging. Our findings reveal that while CLIP, trained on general datasets, performs well in broader contexts, PLIP and BiomedCLIP—optimized for medical imagery—demonstrate enhanced accuracy in medical settings, particularly in hematology. The results underscore the strengths and limitations of these models, offering valuable insights into their adaptability, precision, and potential for future applications in medical image classification.",
                "authors": "Tanviben Patel, Hoda El-Sayed, Md. Kamruzzaman Sarker",
                "citations": 0
            },
            {
                "title": "GeoMeter: Probing Depth and Height Perception of Large Visual-Language Models",
                "abstract": "Geometric understanding is crucial for navigating and interacting with our environment. While large Vision Language Models (VLMs) demonstrate impressive capabilities, deploying them in real-world scenarios necessitates a comparable geometric understanding in visual perception. In this work, we focus on the geometric comprehension of these models; specifically targeting the depths and heights of objects within a scene. Our observations reveal that, although VLMs excel in basic geometric properties perception such as shape and size, they encounter significant challenges in reasoning about the depth and height of objects. To address this, we introduce GeoMeter, a suite of benchmark datasets encompassing Synthetic 2D, Synthetic 3D, and Real-World scenarios to rigorously evaluate these aspects. We benchmark 17 state-of-the-art VLMs using these datasets and find that they consistently struggle with both depth and height perception. Our key insights include detailed analyses of the shortcomings in depth and height reasoning capabilities of VLMs and the inherent bias present in these models. This study aims to pave the way for the development of VLMs with enhanced geometric understanding, crucial for real-world applications.",
                "authors": "Shehreen Azad, Yash Jain, Rishit Garg, Y. S. Rawat, Vibhav Vineet",
                "citations": 0
            },
            {
                "title": "Locality Alignment Improves Vision-Language Models",
                "abstract": "Vision language models (VLMs) have seen growing adoption in recent years, but many still struggle with basic spatial reasoning errors. We hypothesize that this is due to VLMs adopting pre-trained vision backbones, specifically vision transformers (ViTs) trained with image-level supervision and minimal inductive biases. Such models may fail to encode the class contents at each position in the image, and our goal is to resolve this by ensuring that the vision backbone effectively captures both local and global image semantics. Our main insight is that we do not require new supervision to learn this capability -- pre-trained models contain significant knowledge of local semantics that we can extract and use for scalable self-supervision. We propose a new efficient post-training stage for ViTs called locality alignment and a novel fine-tuning procedure called MaskEmbed that uses a masked reconstruction loss to learn semantic contributions for each image patch. We first evaluate locality alignment with a vision-only benchmark, finding that it improves a model's performance at a patch-level semantic segmentation task, especially for strong backbones trained with image-caption pairs (e.g., CLIP and SigLIP). We then train a series of VLMs with and without locality alignment, and show that locality-aligned backbones improve performance across a range of benchmarks, particularly ones that involve spatial understanding (e.g., RefCOCO, OCID-Ref, TallyQA, VSR, AI2D). Overall, we demonstrate that we can efficiently learn local semantic extraction via a locality alignment stage, and that this procedure complements existing VLM training recipes that use off-the-shelf vision backbones.",
                "authors": "Ian Covert, Tony Sun, James Zou, Tatsunori Hashimoto",
                "citations": 0
            },
            {
                "title": "ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights",
                "abstract": "Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot in-context learning for decision making and instruction following. However, they require high-quality exemplar demonstrations to be included in their context window. In this work, we ask: Can LLMs and VLMs generate their own prompt examples from generic, sub-optimal demonstrations? We propose In-Context Abstraction Learning (ICAL), a method that builds a memory of multi-modal experience insights from sub-optimal demonstrations and human feedback. Given a noisy demonstration in a new domain, VLMs abstract the trajectory into a general program by fixing inefficient actions and annotating cognitive abstractions: task relationships, object state changes, temporal subgoals, and task construals. These abstractions are refined and adapted interactively through human feedback while the agent attempts to execute the trajectory in a similar environment. The resulting abstractions, when used as exemplars in the prompt, significantly improve decision-making in retrieval-augmented LLM and VLM agents. Our ICAL agent surpasses the state-of-the-art in dialogue-based instruction following in TEACh, multimodal web agents in VisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6% improvement in goal-condition success. In Visual-WebArena, our task success rate improves over the SOTA from 14.3% to 22.7%. In Ego4D action forecasting, we improve over few-shot GPT-4V and remain competitive with supervised models. We show finetuning our retrieval-augmented in-context agent yields additional improvements. Our approach significantly reduces reliance on expert-crafted examples and consistently outperforms in-context learning from action plans that lack such insights.",
                "authors": "Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, Katerina Fragkiadaki",
                "citations": 0
            },
            {
                "title": "Addressing Failures in Robotics using Vision-Based Language Models (VLMs) and Behavior Trees (BT)",
                "abstract": "In this paper, we propose an approach that combines Vision Language Models (VLMs) and Behavior Trees (BTs) to address failures in robotics. Current robotic systems can handle known failures with pre-existing recovery strategies, but they are often ill-equipped to manage unknown failures or anomalies. We introduce VLMs as a monitoring tool to detect and identify failures during task execution. Additionally, VLMs generate missing conditions or skill templates that are then incorporated into the BT, ensuring the system can autonomously address similar failures in future tasks. We validate our approach through simulations in several failure scenarios.",
                "authors": "Faseeh Ahmad, J. Styrud, Volker Krueger",
                "citations": 0
            },
            {
                "title": "A pen mark is all you need - Incidental prompt injection attacks on Vision Language Models in real-life histopathology",
                "abstract": "Vision-language models (VLMs) can analyze multimodal medical data. However, a significant weakness of VLMs, as we have recently described, is their susceptibility to prompt injection attacks. Here, the model receives conflicting instructions, leading to potentially harmful outputs. In this study, we hypothesized that handwritten labels and watermarks on pathological images could act as inadvertent prompt injections, influencing decision-making in histopathology. We conducted a quantitative study with a total of N = 3888 observations on the state-of-the-art VLMs Claude 3 Opus, Claude 3.5 Sonnet and GPT-4o. We designed various real-world inspired scenarios in which we show that VLMs rely entirely on (false) labels and watermarks if presented with those next to the tissue. All models reached almost perfect accuracies (90 - 100 %) for ground-truth leaking labels and abysmal accuracies (0 - 10 %) for misleading watermarks, despite baseline accuracies between 30-65 % for various multiclass problems. Overall, all VLMs accepted human-provided labels as infallible, even when those inputs contained obvious errors. Furthermore, these effects could not be mitigated by prompt engineering. It is therefore imperative to consider the presence of labels or other influencing features during future evaluation of VLMs in medicine and other fields.",
                "authors": "J. Clusmann, S. J. K. Schulz, D. Ferber, I. Wiest, A. Fernandez, M. Eckstein, F. Lange, N. G. Reitsam, F. Kellers, M. Schmitt, P. Neidlinger, P.-H. Koop, C. V. Schneider, D. Truhn, W. Roth, M. Jesinghaus, J. N. Kather, S. Foersch",
                "citations": 0
            },
            {
                "title": "Enhancing Cross-Prompt Transferability in Vision-Language Models through Contextual Injection of Target Tokens",
                "abstract": "Vision-language models (VLMs) seamlessly integrate visual and textual data to perform tasks such as image classification, caption generation, and visual question answering. However, adversarial images often struggle to deceive all prompts effectively in the context of cross-prompt migration attacks, as the probability distribution of the tokens in these images tends to favor the semantics of the original image rather than the target tokens. To address this challenge, we propose a Contextual-Injection Attack (CIA) that employs gradient-based perturbation to inject target tokens into both visual and textual contexts, thereby improving the probability distribution of the target tokens. By shifting the contextual semantics towards the target tokens instead of the original image semantics, CIA enhances the cross-prompt transferability of adversarial images.Extensive experiments on the BLIP2, InstructBLIP, and LLaVA models show that CIA outperforms existing methods in cross-prompt transferability, demonstrating its potential for more effective adversarial strategies in VLMs.",
                "authors": "Xikang Yang, Xuehai Tang, Fuqing Zhu, Jizhong Han, Songlin Hu",
                "citations": 0
            },
            {
                "title": "COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training",
                "abstract": "Vision-Language Models (VLMs) trained with contrastive loss have achieved significant advancements in various vision and language tasks. However, the global nature of contrastive loss makes VLMs focus predominantly on foreground objects, neglecting other crucial information in the image, which limits their effectiveness in downstream tasks. To address these challenges, we propose COSMOS: CrOSs-MOdality Self-distillation for vision-language pre-training that integrates a novel text-cropping strategy and cross-attention module into a self-supervised learning framework. We create global and local views of images and texts (i.e., multi-modal augmentations), which are essential for self-distillation in VLMs. We further introduce a cross-attention module, enabling COSMOS to learn comprehensive cross-modal representations optimized via a cross-modality self-distillation loss. COSMOS consistently outperforms previous strong baselines on various zero-shot downstream tasks, including retrieval, classification, and semantic segmentation. Additionally, it surpasses CLIP-based models trained on larger datasets in visual perception and contextual understanding tasks.",
                "authors": "Sanghwan Kim, Rui Xiao, Mariana-Iuliana Georgescu, Stephan Alaniz, Zeynep Akata",
                "citations": 0
            },
            {
                "title": "VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images",
                "abstract": "Images are increasingly becoming the currency for documenting biodiversity on the planet, providing novel opportunities for accelerating scientific discoveries in the field of organismal biology, especially with the advent of large vision-language models (VLMs). We ask if pre-trained VLMs can aid scientists in answering a range of biologically relevant questions without any additional fine-tuning. In this paper, we evaluate the effectiveness of 12 state-of-the-art (SOTA) VLMs in the field of organismal biology using a novel dataset, VLM4Bio, consisting of 469K question-answer pairs involving 30K images from three groups of organisms: fishes, birds, and butterflies, covering five biologically relevant tasks. We also explore the effects of applying prompting techniques and tests for reasoning hallucination on the performance of VLMs, shedding new light on the capabilities of current SOTA VLMs in answering biologically relevant questions using images. The code and datasets for running all the analyses reported in this paper can be found at https://github.com/sammarfy/VLM4Bio.",
                "authors": "M. Maruf, Arka Daw, Kazi Sajeed Mehrab, Harish Babu Manogaran, Abhilash Neog, Medha Sawhney, Mridul Khurana, J. Balhoff, Yasin Bakiş, B. Altıntaş, Matthew J. Thompson, Elizabeth G. Campolongo, J. Uyeda, H. Lapp, Henry L. Bart, Paula M. Mabee, Yu Su, Wei-Lun Chao, Charles Stewart, Tanya Y. Berger-Wolf, W. Dahdul, A. Karpatne",
                "citations": 0
            },
            {
                "title": "@Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology",
                "abstract": "As Vision-Language Models (VLMs) advance, human-centered Assistive Technologies (ATs) for helping People with Visual Impairments (PVIs) are evolving into generalists, capable of performing multiple tasks simultaneously. However, benchmarking VLMs for ATs remains under-explored. To bridge this gap, we first create a novel AT benchmark (@Bench). Guided by a pre-design user study with PVIs, our benchmark includes the five most crucial vision-language tasks: Panoptic Segmentation, Depth Estimation, Optical Character Recognition (OCR), Image Captioning, and Visual Question Answering (VQA). Besides, we propose a novel AT model (@Model) that addresses all tasks simultaneously and can be expanded to more assistive functions for helping PVIs. Our framework exhibits outstanding performance across tasks by integrating multi-modal information, and it offers PVIs a more comprehensive assistance. Extensive experiments prove the effectiveness and generalizability of our framework.",
                "authors": "Xin Jiang, Junwei Zheng, Ruiping Liu, Jiahang Li, Jiaming Zhang, Sven Matthiesen, Rainer Stiefelhagen",
                "citations": 0
            },
            {
                "title": "Disease-informed Adaptation of Vision-Language Models",
                "abstract": "Expertise scarcity and high cost of data annotation hinder the development of artificial intelligence (AI) foundation models for medical image analysis. Transfer learning provides a way to utilize the off-the-shelf foundation models to address the clinical challenges. However, such models encounter difficulties when adapting to new diseases not presented in their original pre-training datasets. Compounding this challenge is the limited availability of example cases for a new disease, which further leads to the poor performance of the existing transfer learning techniques. This paper proposes a novel method for transfer learning of foundation Vision-Language Models (VLMs) to efficiently adapt them to a new disease with only a few examples. Such an effective adaptation of VLMs hinges on learning the nuanced representation of new disease concepts. By capitalizing on the joint visual-linguistic capabilities of VLMs, we introduce disease-informed contextual prompting in a novel disease prototype learning framework, which enables VLMs to quickly grasp the concept of the new disease, even with limited data. Extensive experiments across multiple pre-trained medical VLMs and multiple tasks showcase the notable enhancements in performance compared to other existing adaptation techniques. The code will be made publicly available at https://github.com/ RPIDIAL/Disease-informed-VLM-Adaptation.",
                "authors": "Jiajin Zhang, Ge Wang, M. Kalra, P. Yan",
                "citations": 0
            },
            {
                "title": "CVLUE: A New Benchmark Dataset for Chinese Vision-Language Understanding Evaluation",
                "abstract": "Despite the rapid development of Chinese vision-language models (VLMs), most existing Chinese vision-language (VL) datasets are constructed on Western-centric images from existing English VL datasets. The cultural bias in the images makes these datasets unsuitable for evaluating VLMs in Chinese culture. To remedy this issue, we present a new Chinese Vision- Language Understanding Evaluation (CVLUE) benchmark dataset, where the selection of object categories and images is entirely driven by Chinese native speakers, ensuring that the source images are representative of Chinese culture. The benchmark contains four distinct VL tasks ranging from image-text retrieval to visual question answering, visual grounding and visual dialogue. We present a detailed statistical analysis of CVLUE and provide a baseline performance analysis with several open-source multilingual VLMs on CVLUE and its English counterparts to reveal their performance gap between English and Chinese. Our in-depth category-level analysis reveals a lack of Chinese cultural knowledge in existing VLMs. We also find that fine-tuning on Chinese culture-related VL datasets effectively enhances VLMs' understanding of Chinese culture.",
                "authors": "Yuxuan Wang, Yijun Liu, Fei Yu, Chen Huang, Kexin Li, Zhiguo Wan, Wanxiang Che",
                "citations": 0
            },
            {
                "title": "ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities",
                "abstract": "Large language models(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model’s reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.",
                "authors": "Ying Su, Zhan Ling, Haochen Shi, Jiayang Cheng, Yauwai Yim, Yangqiu Song",
                "citations": 0
            },
            {
                "title": "Image-conditioned human language comprehension and psychometric benchmarking of visual language models",
                "abstract": "Large language model (LLM)s’ next-word predictions have shown impressive performance in capturing human expectations during real-time language comprehension. This finding has enabled a line of research on psychometric benchmarking of LLMs against human language-comprehension data in order to reverse-engineer humans’ linguistic subjective probability distributions and representations. However, to date, this work has exclusively involved unimodal (language-only) comprehension data, whereas much human language use takes place in rich multimodal contexts. Here we extend psychometric benchmarking to visual language models (VLMs). We develop a novel experimental paradigm, \\textit{Image-Conditioned Maze Reading}, in which participants first view an image and then read a text describing an image within the Maze paradigm, yielding word-by-word reaction-time measures with high signal-to-noise ratio and good localization of expectation-driven language processing effects. We find a large facilitatory effect of correct image context on language comprehension, not only for words such as concrete nouns that are directly grounded in the image but even for ungrounded words in the image descriptions. Furthermore, we find that VLM surprisal captures most to all of this effect. We use these findings to benchmark a range of VLMs, showing that models with lower perplexity generally have better psychometric performance, but that among the best VLMs tested perplexity and psychometric performance dissociate. Overall, our work offers new possibilities for connecting psycholinguistics with multimodal LLMs for both scientific and engineering goals.",
                "authors": "Subha Nawer Pushpita, Roger Levy",
                "citations": 0
            },
            {
                "title": "NeIn: Telling What You Don't Want",
                "abstract": "Negation is a fundamental linguistic concept used by humans to convey information that they do not desire. Despite this, there has been minimal research specifically focused on negation within vision-language tasks. This lack of research means that vision-language models (VLMs) may struggle to understand negation, implying that they struggle to provide accurate results. One barrier to achieving human-level intelligence is the lack of a standard collection by which research into negation can be evaluated. This paper presents the first large-scale dataset, Negative Instruction (NeIn), for studying negation within the vision-language domain. Our dataset comprises 530,694 quadruples, i.e., source image, original caption, negative sentence, and target image in total, including 495,694 queries for training and 35,000 queries for benchmarking across multiple vision-language tasks. Specifically, we automatically generate NeIn based on a large, existing vision-language dataset, MS-COCO, via two steps: generation and filtering. During the generation phase, we leverage two VLMs, BLIP and MagicBrush, to generate the target image and a negative clause that expresses the content of the source image. In the subsequent filtering phase, we apply BLIP to remove erroneous samples. Additionally, we introduce an evaluation protocol for negation understanding of image editing models. Extensive experiments using our dataset across multiple VLMs for instruction-based image editing tasks demonstrate that even recent state-of-the-art VLMs struggle to understand negative queries. The project page is: https://tanbuinhat.github.io/NeIn/",
                "authors": "Nhat-Tan Bui, Dinh-Hieu Hoang, Quoc-Huy Trinh, Minh-Triet Tran, Truong Nguyen, Susan Gauch",
                "citations": 0
            },
            {
                "title": "Exploring Foundation Models in Detecting Concerning Daily Functioning in Psychotherapeutic Context Based on Images from Smart Home Devices",
                "abstract": "The surge of cyber-physical systems (CPS) and smart home devices (e.g., vacuum robots and pet cameras) equipped in U.S. households opens up the potential to screen the day-to-day functioning of individuals in the smart home environment and to provide precautionary assistance to individuals who may need psychotherapies. Meanwhile, recent advances in foundation models (FMs) enable the large language models (LLMs) and vision-language models (VLMs) to have strong reasoning capabilities even in complex scenarios. In this paper, we investigate the integration of FMs and photos taken from the perspective of vacuum robots to screen the behaviors indicative of mental state that need further attention from therapists and health care practitioners. Specifically, we explore the possibility of using VLMs and LLM-based reasoners to accurately detect two of the most concerning behaviors at home: smoking- and drinking-alone. Compared to existing methods based on object detection, we demonstrate that the integration of LLMs and VLMs can significantly enhance detection accuracy, especially in complex home environments with ambiguous patterns and distinguishing concerning events from benign events. We showcase the potential of employing FMs in CPS to discern nuanced insights into day-to-day functioning behaviors in the psychotherapeutic contexts.",
                "authors": "Yuang Fan, Jingping Nie, Xinghua Sun, Xiaofan Jiang",
                "citations": 0
            },
            {
                "title": "Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot Generalization of Vision-Language Models",
                "abstract": "The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time involves tuning learnable prompts, ie, test-time prompt tuning. This paper introduces Test-Time Low-rank adaptation (TTL) as an alternative to prompt tuning for zero-shot generalization of large-scale VLMs. Taking inspiration from recent advancements in efficiently fine-tuning large language models, TTL offers a test-time parameter-efficient adaptation approach that updates the attention weights of the transformer encoder by maximizing prediction confidence. The self-supervised confidence maximization objective is specified using a weighted entropy loss that enforces consistency among predictions of augmented samples. TTL introduces only a small amount of trainable parameters for low-rank adapters in the model space while keeping the prompts and backbone frozen. Extensive experiments on a variety of natural distribution and cross-domain tasks show that TTL can outperform other techniques for test-time optimization of VLMs in strict zero-shot settings. Specifically, TTL outperforms test-time prompt tuning baselines with a significant improvement on average. Our code is available at at https://github.com/Razaimam45/TTL-Test-Time-Low-Rank-Adaptation.",
                "authors": "Raza Imam, Hanan Gani, Muhammad Huzaifa, Karthik Nandakumar",
                "citations": 0
            },
            {
                "title": "Generalization Boosted Adapter for Open-Vocabulary Segmentation",
                "abstract": "Vision-language models (VLMs) have demonstrated remarkable open-vocabulary object recognition capabilities, motivating their adaptation for dense prediction tasks like segmentation. However, directly applying VLMs to such tasks remains challenging due to their lack of pixel-level granularity and the limited data available for fine-tuning, leading to overfitting and poor generalization. To address these limitations, we propose Generalization Boosted Adapter (GBA), a novel adapter strategy that enhances the generalization and robustness of VLMs for open-vocabulary segmentation. GBA comprises two core components: (1) a Style Diversification Adapter (SDA) that decouples features into amplitude and phase components, operating solely on the amplitude to enrich the feature space representation while preserving semantic consistency; and (2) a Correlation Constraint Adapter (CCA) that employs cross-attention to establish tighter semantic associations between text categories and target regions, suppressing irrelevant low-frequency ``noise'' information and avoiding erroneous associations. Through the synergistic effect of the shallow SDA and the deep CCA, GBA effectively alleviates overfitting issues and enhances the semantic relevance of feature representations. As a simple, efficient, and plug-and-play component, GBA can be flexibly integrated into various CLIP-based methods, demonstrating broad applicability and achieving state-of-the-art performance on multiple open-vocabulary segmentation benchmarks.",
                "authors": "Wenhao Xu, Changwei Wang, Xuxiang Feng, Rongtao Xu, Longzhao Huang, Zherui Zhang, Li Guo, Shibiao Xu",
                "citations": 0
            },
            {
                "title": "Learning from True-False Labels via Multi-modal Prompt Retrieving",
                "abstract": "Weakly supervised learning has recently achieved considerable success in reducing annotation costs and label noise. Unfortunately, existing weakly supervised learning methods are short of ability in generating reliable labels via pre-trained vision-language models (VLMs). In this paper, we propose a novel weakly supervised labeling setting, namely True-False Labels (TFLs) which can achieve high accuracy when generated by VLMs. The TFL indicates whether an instance belongs to the label, which is randomly and uniformly sampled from the candidate label set. Specifically, we theoretically derive a risk-consistent estimator to explore and utilize the conditional probability distribution information of TFLs. Besides, we propose a convolutional-based Multi-modal Prompt Retrieving (MRP) method to bridge the gap between the knowledge of VLMs and target learning tasks. Experimental results demonstrate the effectiveness of the proposed TFL setting and MRP learning method. The code to reproduce the experiments is at https://github.com/Tranquilxu/TMP.",
                "authors": "Zhongnian Li, Jinghao Xu, Peng Ying, Meng Wei, Tongfeng Sun, Xinzheng Xu",
                "citations": 0
            },
            {
                "title": "Structured Spatial Reasoning with Open Vocabulary Object Detectors",
                "abstract": "Reasoning about spatial relationships between objects is essential for many real-world robotic tasks, such as fetch-and-delivery, object rearrangement, and object search. The ability to detect and disambiguate different objects and identify their location is key to successful completion of these tasks. Several recent works have used powerful Vision and Language Models (VLMs) to unlock this capability in robotic agents. In this paper we introduce a structured probabilistic approach that integrates rich 3D geometric features with state-of-the-art open-vocabulary object detectors to enhance spatial reasoning for robotic perception. The approach is evaluated and compared against zero-shot performance of the state-of-the-art Vision and Language Models (VLMs) on spatial reasoning tasks. To enable this comparison, we annotate spatial clauses in real-world RGB-D Active Vision Dataset [1] and conduct experiments on this and the synthetic Semantic Abstraction [2] dataset. Results demonstrate the effectiveness of the proposed method, showing superior performance of grounding spatial relations over state of the art open-source VLMs by more than 20%.",
                "authors": "Negar Nejatishahidin, Madhukar Reddy Vongala, Jana Kosecka",
                "citations": 0
            },
            {
                "title": "Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models",
                "abstract": "Despite inheriting security measures from underlying language models, Vision-Language Models (VLMs) may still be vulnerable to safety alignment issues. Through empirical analysis, we uncover two critical findings: scenario-matched images can significantly amplify harmful outputs, and contrary to common assumptions in gradient-based attacks, minimal loss values do not guarantee optimal attack effectiveness. Building on these insights, we introduce MLAI (Multi-Loss Adversarial Images), a novel jailbreak framework that leverages scenario-aware image generation for semantic alignment, exploits flat minima theory for robust adversarial image selection, and employs multi-image collaborative attacks for enhanced effectiveness. Extensive experiments demonstrate MLAI's significant impact, achieving attack success rates of 77.75% on MiniGPT-4 and 82.80% on LLaVA-2, substantially outperforming existing methods by margins of 34.37% and 12.77% respectively. Furthermore, MLAI shows considerable transferability to commercial black-box VLMs, achieving up to 60.11% success rate. Our work reveals fundamental visual vulnerabilities in current VLMs safety mechanisms and underscores the need for stronger defenses. Warning: This paper contains potentially harmful example text.",
                "authors": "Shuyang Hao, Bryan Hooi, Jun Liu, Kai-Wei Chang, Zi Huang, Yujun Cai",
                "citations": 0
            },
            {
                "title": "MAPWise: Evaluating Vision-Language Models for Advanced Map Queries",
                "abstract": "Vision-language models (VLMs) excel at tasks requiring joint understanding of visual and linguistic information. A particularly promising yet under-explored application for these models lies in answering questions based on various kinds of maps. This study investigates the efficacy of VLMs in answering questions based on choropleth maps, which are widely used for data analysis and representation. To facilitate and encourage research in this area, we introduce a novel map-based question-answering benchmark, consisting of maps from three geographical regions (United States, India, China), each containing 1000 questions. Our benchmark incorporates 43 diverse question templates, requiring nuanced understanding of relative spatial relationships, intricate map features, and complex reasoning. It also includes maps with discrete and continuous values, encompassing variations in color-mapping, category ordering, and stylistic patterns, enabling comprehensive analysis. We evaluate the performance of multiple VLMs on this benchmark, highlighting gaps in their abilities and providing insights for improving such models.",
                "authors": "Srija Mukhopadhyay, Abhishek Rajgaria, Prerana Khatiwada, Vivek Gupta, Dan Roth",
                "citations": 0
            },
            {
                "title": "Improving Scene Graph Generation with Relation Words' Debiasing in Vision-Language Models",
                "abstract": "Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between various objects. However, this complexity and diversity in SGG also leads to underrepresentation, where part of test triplets are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose using the SGG models with pre-trained vision-language models (VLMs) to enhance representation. However, due to the gap between the pre-training and SGG, directly em-sembling the pre-trained VLMs leads to severe biases across relation words. Thus, we introduce LM Estimation to approximate the words’ distribution underlies in the pre-training language sets, and then use the distribution for debiasing. After that, we ensemble VLMs with SGG models to enhance representation. Considering that each model may represent better at different samples, we use a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our method effectively addresses the words biases, enhances SGG’s representation, and achieve markable performance enhancements. It is training-free and integrates well with existing SGG models.",
                "authors": "Yuxuan Wang, Xiaoyuan Liu",
                "citations": 0
            },
            {
                "title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification",
                "abstract": "Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the parameters of VLMs with few-shot samples corrupts the pre-trained knowledge since fine-tuning the CLIP model even degrades performance. In this paper, we revisit this viewpoint, and propose a new perspective: fine-tuning the specific parameters instead of all will uncover the power of classic model fine-tuning on VLMs. Through our meticulous study, we propose ClipFit, a simple yet effective method to fine-tune CLIP without introducing any overhead of extra parameters. We demonstrate that by only fine-tuning the specific bias terms and normalization layers, ClipFit can improve the performance of zero-shot CLIP by 7.27% average harmonic mean accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the pre-trained models, we conducted extensive experimental analyses w.r.t. changes in internal parameters and representations. We found that low-level text bias layers and the first layer normalization layer change much more than other layers. The code will be released.",
                "authors": "Ming Li, Jike Zhong, Chenxing Li, Liuzhuozheng Li, Nie Lin, Masashi Sugiyama",
                "citations": 0
            },
            {
                "title": "Vision-Language Model Based Handwriting Verification",
                "abstract": "Handwriting Verification is a critical in document forensics. Deep learning based approaches often face skepticism from forensic document examiners due to their lack of explainability and reliance on extensive training data and handcrafted features. This paper explores using Vision Language Models (VLMs), such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By leveraging their Visual Question Answering capabilities and 0-shot Chain-of-Thought (CoT) reasoning, our goal is to provide clear, human-understandable explanations for model decisions. Our experiments on the CEDAR handwriting dataset demonstrate that VLMs offer enhanced interpretability, reduce the need for large training datasets, and adapt better to diverse handwriting styles. However, results show that the CNN-based ResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach with GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy: 71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings highlight the potential of VLMs in generating human-interpretable decisions while underscoring the need for further advancements to match the performance of specialized deep learning models.",
                "authors": "Mihir Chauhan, Abhishek Satbhai, Mohammad Abuzar Hashemi, Mir Basheer Ali, B. Ramamurthy, Mingchen Gao, Siwei Lyu, Sargur Srihari",
                "citations": 0
            },
            {
                "title": "Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label Learning",
                "abstract": "Due to the lack of extensive precisely-annotated multi-label data in real word, semi-supervised multi-label learning (SSMLL) has gradually gained attention. Abundant knowledge embedded in vision-language models (VLMs) pre-trained on large-scale image-text pairs could alleviate the challenge of limited labeled data under SSMLL setting.Despite existing methods based on fine-tuning VLMs have achieved advances in weakly-supervised multi-label learning, they failed to fully leverage the information from labeled data to enhance the learning of unlabeled data. In this paper, we propose a context-based semantic-aware alignment method to solve the SSMLL problem by leveraging the knowledge of VLMs. To address the challenge of handling multiple semantics within an image, we introduce a novel framework design to extract label-specific image features. This design allows us to achieve a more compact alignment between text features and label-specific image features, leading the model to generate high-quality pseudo-labels. To incorporate the model with comprehensive understanding of image, we design a semi-supervised context identification auxiliary task to enhance the feature representation by capturing co-occurrence information. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our proposed method.",
                "authors": "Heng-Bo Fan, Ming-Kun Xie, Jia-Hao Xiao, Sheng-Jun Huang",
                "citations": 0
            },
            {
                "title": "SCoTT: Wireless-Aware Path Planning with Vision Language Models and Strategic Chains-of-Thought",
                "abstract": "Path planning is a complex problem for many practical applications, particularly in robotics. Existing algorithms, however, are exhaustive in nature and become increasingly complex when additional side constraints are incorporated alongside distance minimization. In this paper, a novel approach using vision language models (VLMs) is proposed for enabling path planning in complex wireless-aware environments. To this end, insights from a digital twin (DT) with real-world wireless ray tracing data are explored in order to guarantee an average path gain threshold while minimizing the trajectory length. First, traditional approaches such as A* are compared to several wireless-aware extensions, and an optimal iterative dynamic programming approach (DP-WA*) is derived, which fully takes into account all path gains and distance metrics within the DT. On the basis of these baselines, the role of VLMs as an alternative assistant for path planning is investigated, and a strategic chain-of-thought tasking (SCoTT) approach is proposed. SCoTT divides the complex planning task into several subproblems and solves each with advanced CoT prompting. Results show that SCoTT achieves very close average path gains compared to DP-WA* while at the same time yielding consistently shorter path lengths. The results also show that VLMs can be used to accelerate DP-WA* by efficiently reducing the algorithm's search space and thus saving up to 62\\% in execution time. This work underscores the potential of VLMs in future digital systems as capable assistants for solving complex tasks, while enhancing user interaction and accelerating rapid prototyping under diverse wireless constraints.",
                "authors": "Aladin Djuhera, Vlad-Costin Andrei, Amin Seffo, Holger Boche, Walid Saad",
                "citations": 0
            },
            {
                "title": "GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models",
                "abstract": "Vision-language models (VLMs) are intensively used in many downstream tasks, including those requiring assessments of individuals appearing in the images. While VLMs perform well in simple single-person scenarios, in real-world applications, we often face complex situations in which there are persons of different genders doing different activities. We show that in such cases, VLMs are biased towards identifying the individual with the expected gender (according to ingrained gender stereotypes in the model or other forms of sample selection bias) as the performer of the activity. We refer to this bias in associating an activity with the gender of its actual performer in an image or text as the Gender-Activity Binding (GAB) bias and analyze how this bias is internalized in VLMs. To assess this bias, we have introduced the GAB dataset with approximately 5500 AI-generated images that represent a variety of activities, addressing the scarcity of real-world images for some scenarios. To have extensive quality control, the generated images are evaluated for their diversity, quality, and realism. We have tested 12 renowned pre-trained VLMs on this dataset in the context of text-to-image and image-to-text retrieval to measure the effect of this bias on their predictions. Additionally, we have carried out supplementary experiments to quantify the bias in VLMs' text encoders and to evaluate VLMs' capability to recognize activities. Our experiments indicate that VLMs experience an average performance decline of about 13.2% when confronted with gender-activity binding bias.",
                "authors": "Ali Abdollahi, Mahdi Ghaznavi, Mohammad Reza Karimi Nejad, Arash Mari Oriyad, Reza Abbasi, Ali Salesi, Melika Behjati, M. H. Rohban, M. Baghshah",
                "citations": 0
            },
            {
                "title": "COREval: A Comprehensive and Objective Benchmark for Evaluating the Remote Sensing Capabilities of Large Vision-Language Models",
                "abstract": "With the rapid development of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing Earth observation, have demonstrated exceptional perception and reasoning abilities within this specific field. However, the current absence of a comprehensive benchmark for holistically evaluating the remote sensing capabilities of these VLMs represents a significant gap. To bridge this gap, we propose COREval, the first benchmark designed to comprehensively and objectively evaluate the hierarchical remote sensing capabilities of VLMs. Concentrating on 2 primary capability dimensions essential to remote sensing: perception and reasoning, we further categorize 6 secondary dimensions and 22 leaf tasks to ensure a well-rounded assessment coverage for this specific field. COREval guarantees the quality of the total of 6,263 problems through a rigorous process of data collection from 50 globally distributed cities, question construction and quality control, and the format of multiple-choice questions with definitive answers allows for an objective and straightforward evaluation of VLM performance. We conducted a holistic evaluation of 13 prominent open-source VLMs from both the general and remote sensing domains, highlighting current shortcomings in their remote sensing capabilities and providing directions for improvements in their application within this specialized context. We hope that COREval will serve as a valuable resource and offer deeper insights into the challenges and potential of VLMs in the field of remote sensing.",
                "authors": "Xiao An, Jiaxing Sun, Zihan Gui, Wei He",
                "citations": 0
            },
            {
                "title": "Find Everything: A General Vision Language Model Approach to Multi-Object Search",
                "abstract": "The Multi-Object Search (MOS) problem involves navigating to a sequence of locations to maximize the likelihood of finding target objects while minimizing travel costs. In this paper, we introduce a novel approach to the MOS problem, called Finder, which leverages vision language models (VLMs) to locate multiple objects across diverse environments. Specifically, our approach introduces multi-channel score maps to track and reason about multiple objects simultaneously during navigation, along with a score fusion technique that combines scene-level and object-level semantic correlations. Experiments in both simulated and real-world settings showed that Finder outperforms existing methods using deep reinforcement learning and VLMs. Ablation and scalability studies further validated our design choices and robustness with increasing numbers of target objects, respectively. Website: https://find-all-my-things.github.io/",
                "authors": "Daniel Choi, Angus Fung, Haitong Wang, Aaron Hao Tan",
                "citations": 0
            },
            {
                "title": "Can VLMs be used on videos for action recognition? LLMs are Visual Reasoning Coordinators",
                "abstract": "Recent advancements have introduced multiple vision-language models (VLMs) demonstrating impressive commonsense reasoning across various domains. Despite their individual capabilities, the potential of synergizing these complementary VLMs remains underexplored. The Cola Framework addresses this by showcasing how a large language model (LLM) can efficiently coordinate multiple VLMs through natural language communication, leveraging their distinct strengths. We have verified this claim on the challenging A-OKVQA dataset, confirming the effectiveness of such coordination. Building on this, our study investigates whether the same methodology can be applied to surveillance videos for action recognition. Specifically, we explore if leveraging the combined knowledge base of VLMs and LLM can effectively deduce actions from a video when presented with only a few selectively important frames and minimal temporal information. Our experiments demonstrate that LLM, when coordinating different VLMs, can successfully recognize patterns and deduce actions in various scenarios despite the weak temporal signals. However, our findings suggest that to enhance this approach as a viable alternative solution, integrating a stronger temporal signal and exposing the models to slightly more frames would be beneficial.",
                "authors": "Harsh Lunia",
                "citations": 0
            },
            {
                "title": "Interleaved-Modal Chain-of-Thought",
                "abstract": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to produce a series of intermediate reasoning steps before arriving at the final answer. However, when transitioning to vision-language models (VLMs), their text-only rationales struggle to express the fine-grained associations with the original image. In this paper, we propose an image-incorporated multimodal Chain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)}, which generates sequential reasoning steps consisting of paired visual and textual rationales to infer the final answer. Intuitively, the novel ICoT requires VLMs to enable the generation of fine-grained interleaved-modal content, which is hard for current VLMs to fulfill. Considering that the required visual information is usually part of the input image, we propose \\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs. ADS intelligently inserts regions of the input image to generate the interleaved-modal reasoning steps with ignorable additional latency. ADS relies solely on the attention map of VLMs without the need for parameterization, and therefore it is a plug-and-play strategy that can be generalized to a spectrum of VLMs. We apply ADS to realize ICoT on two popular VLMs of different architectures. Extensive evaluations of three benchmarks have shown that ICoT prompting achieves substantial performance (up to 14\\%) and interpretability improvements compared to existing multimodal CoT prompting methods.",
                "authors": "Jun Gao, Yongqing Li, Ziqiang Cao, Wenjie Li",
                "citations": 0
            },
            {
                "title": "Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?",
                "abstract": "Large vision-language models (VLMs) have become state-of-the-art for many computer vision tasks, with in-context learning (ICL) as a popular adaptation strategy for new ones. But can VLMs learn novel concepts purely from visual demonstrations, or are they limited to adapting to the output format of ICL examples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks (SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks in-context. We find that VLMs fail to do this zero-shot, and sometimes continue to fail after finetuning. However, adding simpler data to the training by curriculum learning leads to improved ICL performance.",
                "authors": "Bowen Zhao, Leo Parker Dirac, Paulina Varshavskaya",
                "citations": 0
            },
            {
                "title": "From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning",
                "abstract": "Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at https://github.com/baiyang4/VITask.",
                "authors": "Yang Bai, Yang Zhou, Jun Zhou, R. Goh, Daniel Shu Wei Ting, Yong Liu",
                "citations": 0
            },
            {
                "title": "P4Q: Learning to Prompt for Quantization in Visual-language Models",
                "abstract": "Large-scale pre-trained Vision-Language Models (VLMs) have gained prominence in various visual and multimodal tasks, yet the deployment of VLMs on downstream application platforms remains challenging due to their prohibitive requirements of training samples and computing resources. Fine-tuning and quantization of VLMs can substantially reduce the sample and computation costs, which are in urgent need. There are two prevailing paradigms in quantization, Quantization-Aware Training (QAT) can effectively quantize large-scale VLMs but incur a huge training cost, while low-bit Post-Training Quantization (PTQ) suffers from a notable performance drop. We propose a method that balances fine-tuning and quantization named ``Prompt for Quantization'' (P4Q), in which we design a lightweight architecture to leverage contrastive loss supervision to enhance the recognition performance of a PTQ model. Our method can effectively reduce the gap between image features and text features caused by low-bit quantization, based on learnable prompts to reorganize textual representations and a low-bit adapter to realign the distributions of image and text features. We also introduce a distillation loss based on cosine similarity predictions to distill the quantized model using a full-precision teacher. Extensive experimental results demonstrate that our P4Q method outperforms prior arts, even achieving comparable results to its full-precision counterparts. For instance, our 8-bit P4Q can theoretically compress the CLIP-ViT/B-32 by 4 $\\times$ while achieving 66.94\\% Top-1 accuracy, outperforming the learnable prompt fine-tuned full-precision model by 2.24\\% with negligible additional parameters on the ImageNet dataset.",
                "authors": "H. Sun, Runqi Wang, Yanjing Li, Xianbin Cao, Xiaolong Jiang, Yao Hu, Baochang Zhang",
                "citations": 0
            },
            {
                "title": "Advancing Vehicle Plate Recognition: Multitasking Visual Language Models with VehiclePaliGemma",
                "abstract": "License plate recognition (LPR) involves automated systems that utilize cameras and computer vision to read vehicle license plates. Such plates collected through LPR can then be compared against databases to identify stolen vehicles, uninsured drivers, crime suspects, and more. The LPR system plays a significant role in saving time for institutions such as the police force. In the past, LPR relied heavily on Optical Character Recognition (OCR), which has been widely explored to recognize characters in images. Usually, collected plate images suffer from various limitations, including noise, blurring, weather conditions, and close characters, making the recognition complex. Existing LPR methods still require significant improvement, especially for distorted images. To fill this gap, we propose utilizing visual language models (VLMs) such as OpenAI GPT4o, Google Gemini 1.5, Google PaliGemma (Pathways Language and Image model + Gemma model), Meta Llama 3.2, Anthropic Claude 3.5 Sonnet, LLaVA, NVIDIA VILA, and moondream2 to recognize such unclear plates with close characters. This paper evaluates the VLM's capability to address the aforementioned problems. Additionally, we introduce ``VehiclePaliGemma'', a fine-tuned Open-sourced PaliGemma VLM designed to recognize plates under challenging conditions. We compared our proposed VehiclePaliGemma with state-of-the-art methods and other VLMs using a dataset of Malaysian license plates collected under complex conditions. The results indicate that VehiclePaliGemma achieved superior performance with an accuracy of 87.6\\%. Moreover, it is able to predict the car's plate at a speed of 7 frames per second using A100-80GB GPU. Finally, we explored the multitasking capability of VehiclePaliGemma model to accurately identify plates containing multiple cars of various models and colors, with plates positioned and oriented in different directions.",
                "authors": "Nouar Aldahoul, M. J. Tan, Raghava Reddy Tera, Hezerul Bin Abdul Karim, Chee How Lim, Manish Kumar Mishra, Yasir Zaki",
                "citations": 0
            },
            {
                "title": "Unraveling the Truth: Do LLMs really Understand Charts? A Deep Dive into Consistency and Robustness",
                "abstract": "Chart question answering (CQA) is a crucial area of Visual Language Understanding. However, the robustness and consistency of current Visual Language Models (VLMs) in this field remain under-explored. This paper evaluates state-of-the-art VLMs on comprehensive datasets, developed specifically for this study, encompassing diverse question categories and chart formats. We investigate two key aspects: 1) the models' ability to handle varying levels of chart and question complexity, and 2) their robustness across different visual representations of the same underlying data. Our analysis reveals significant performance variations based on question and chart types, highlighting both strengths and weaknesses of current models. Additionally, we identify areas for improvement and propose future research directions to build more robust and reliable CQA systems. This study sheds light on the limitations of current models and paves the way for future advancements in the field.",
                "authors": "Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, Vivek Gupta, Dan Roth",
                "citations": 0
            },
            {
                "title": "Advancing Vehicle Plate Recognition: Multitasking Visual Language Models with VehiclePaliGemma",
                "abstract": "License plate recognition (LPR) involves automated systems that utilize cameras and computer vision to read vehicle license plates. Such plates collected through LPR can then be compared against databases to identify stolen vehicles, uninsured drivers, crime suspects, and more. The LPR system plays a significant role in saving time for institutions such as the police force. In the past, LPR relied heavily on Optical Character Recognition (OCR), which has been widely explored to recognize characters in images. Usually, collected plate images suffer from various limitations, including noise, blurring, weather conditions, and close characters, making the recognition complex. Existing LPR methods still require significant improvement, especially for distorted images. To fill this gap, we propose utilizing visual language models (VLMs) such as OpenAI GPT4o, Google Gemini 1.5, Google PaliGemma (Pathways Language and Image model + Gemma model), Meta Llama 3.2, Anthropic Claude 3.5 Sonnet, LLaVA, NVIDIA VILA, and moondream2 to recognize such unclear plates with close characters. This paper evaluates the VLM's capability to address the aforementioned problems. Additionally, we introduce ``VehiclePaliGemma'', a fine-tuned Open-sourced PaliGemma VLM designed to recognize plates under challenging conditions. We compared our proposed VehiclePaliGemma with state-of-the-art methods and other VLMs using a dataset of Malaysian license plates collected under complex conditions. The results indicate that VehiclePaliGemma achieved superior performance with an accuracy of 87.6\\%. Moreover, it is able to predict the car's plate at a speed of 7 frames per second using A100-80GB GPU. Finally, we explored the multitasking capability of VehiclePaliGemma model to accurately identify plates containing multiple cars of various models and colors, with plates positioned and oriented in different directions.",
                "authors": "Nouar Aldahoul, M. J. Tan, Raghava Reddy Tera, Hezerul Bin Abdul Karim, Chee How Lim, Manish Kumar Mishra, Yasir Zaki",
                "citations": 0
            },
            {
                "title": "Find Everything: A General Vision Language Model Approach to Multi-Object Search",
                "abstract": "The Multi-Object Search (MOS) problem involves navigating to a sequence of locations to maximize the likelihood of finding target objects while minimizing travel costs. In this paper, we introduce a novel approach to the MOS problem, called Finder, which leverages vision language models (VLMs) to locate multiple objects across diverse environments. Specifically, our approach introduces multi-channel score maps to track and reason about multiple objects simultaneously during navigation, along with a score fusion technique that combines scene-level and object-level semantic correlations. Experiments in both simulated and real-world settings showed that Finder outperforms existing methods using deep reinforcement learning and VLMs. Ablation and scalability studies further validated our design choices and robustness with increasing numbers of target objects, respectively. Website: https://find-all-my-things.github.io/",
                "authors": "Daniel Choi, Angus Fung, Haitong Wang, Aaron Hao Tan",
                "citations": 0
            },
            {
                "title": "GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) building upon the foundation of powerful large language models have made rapid progress in reasoning across visual and textual data. While VLMs perform well on vision tasks that they are trained on, our results highlight key challenges in abstract pattern recognition. We present GlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of visual patterns from 40 writing systems with three visual presentation styles. GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models to understand and judge natural language descriptions of visual patterns. GlyphPattern patterns are drawn from a large-scale cognitive science investigation of human writing systems; as a result, they are rich in spatial reference and compositionality. Our experiments show that GlyphPattern is challenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with marginal gains from few-shot prompting. Our detailed error analysis reveals challenges at multiple levels, including visual processing, natural language understanding, and pattern generalization.",
                "authors": "Zixuan Wu, Yoolim Kim, C. Anderson",
                "citations": 0
            },
            {
                "title": "Unraveling the Truth: Do LLMs really Understand Charts? A Deep Dive into Consistency and Robustness",
                "abstract": "Chart question answering (CQA) is a crucial area of Visual Language Understanding. However, the robustness and consistency of current Visual Language Models (VLMs) in this field remain under-explored. This paper evaluates state-of-the-art VLMs on comprehensive datasets, developed specifically for this study, encompassing diverse question categories and chart formats. We investigate two key aspects: 1) the models' ability to handle varying levels of chart and question complexity, and 2) their robustness across different visual representations of the same underlying data. Our analysis reveals significant performance variations based on question and chart types, highlighting both strengths and weaknesses of current models. Additionally, we identify areas for improvement and propose future research directions to build more robust and reliable CQA systems. This study sheds light on the limitations of current models and paves the way for future advancements in the field.",
                "authors": "Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, Vivek Gupta, Dan Roth",
                "citations": 0
            },
            {
                "title": "Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark",
                "abstract": "We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual Network Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision Language Models (VLMs) on 5 tasks related to three foundational network science concepts: identifying nodes of maximal degree on a rendered graph, identifying whether signed triads are balanced or unbalanced, and counting components. The tasks are structured to be easy for a human who understands the underlying graph theoretic concepts, and can all be solved by counting the appropriate elements in graphs. We find that while GPT-4 consistently outperforms LLaVa, both models struggle with every visual network analysis task we propose. We publicly release the first benchmark for the evaluation of VLMs on foundational VNA tasks.",
                "authors": "Evan M. Williams, K. Carley",
                "citations": 0
            },
            {
                "title": "Enabling Data-Driven and Empathetic Interactions: A Context-Aware 3D Virtual Agent in Mixed Reality for Enhanced Financial Customer Experience",
                "abstract": "In this paper, we introduce a novel system designed to enhance customer service in the financial and retail sectors through a context-aware 3D virtual agent, utilizing Mixed Reality (MR) and Vision Language Models (VLMs). Our approach focuses on enabling data-driven and empathetic interactions that ensure customer satisfaction by introducing situational awareness of the physical location, personalized interactions based on customer profiles, and rigorous privacy and security standards. We discuss our design considerations critical for deployment in real-world customer service environments, addressing challenges in user data management and sensitive information handling. We also outline the system architecture and key features unique to banking and retail environments. Our work demonstrates the potential of integrating MR and VLMs in service industries, offering practical insights in customer service delivery while maintaining high standards of security and personalization.",
                "authors": "Cindy Xu, Mengyu Chen, Pranav Deshpande, Elvir Azanli, Runqing Yang, Joseph Ligman",
                "citations": 0
            },
            {
                "title": "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values",
                "abstract": "This paper introduces VIVA, a benchmark for VIsion-grounded decision-making driven by human VA. While most large vision-language models (VLMs) focus on physical-level skills, our work is the first to examine their multimodal capabilities in leveraging human values to make decisions under a vision-depicted situation. VIVA contains 1,062 images depicting diverse real-world situations and the manually annotated decisions grounded in them. Given an image there, the model should select the most appropriate action to address the situation and provide the relevant human values and reason underlying the decision. Extensive experiments based on VIVA show the limitation of VLMs in using human values to make multimodal decisions. Further analyses indicate the potential benefits of exploiting action consequences and predicted human values.",
                "authors": "Zhe Hu, Yixiao Ren, Jing Li, Yu Yin",
                "citations": 0
            },
            {
                "title": "HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic Evaluation Using a Vision Language Model",
                "abstract": "Vision-language models (VLMs) have shown impressive abilities in text and image understanding. However, existing metrics for evaluating the text generated by VLMs focus exclusively on overall quality, leading to two limitations: 1) it is challenging to identify which aspects of the text need improvement from the overall score; 2) metrics may overlook specific evaluation criteria when predicting an overall score. To address these limitations, we propose HarmonicEval, a reference-free evaluation metric that aggregates criterion-wise scores to produce the overall score in a bottom-up manner. Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert human judgments across four vision-language tasks. Our experiments demonstrate that HarmonicEval achieves higher correlations with human judgments than conventional metrics while providing numerical scores for each criterion.",
                "authors": "Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue",
                "citations": 0
            },
            {
                "title": "Sensitivity of Generative VLMs to Semantically and Lexically Altered Prompts",
                "abstract": "Despite the significant influx of prompt-tuning techniques for generative vision-language models (VLMs), it remains unclear how sensitive these models are to lexical and semantic alterations in prompts. In this paper, we evaluate the ability of generative VLMs to understand lexical and semantic changes in text using the SugarCrepe++ dataset. We analyze the sensitivity of VLMs to lexical alterations in prompts without corresponding semantic changes. Our findings demonstrate that generative VLMs are highly sensitive to such alterations. Additionally, we show that this vulnerability affects the performance of techniques aimed at achieving consistency in their outputs.",
                "authors": "Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, E. Milios, Sageev Oore, Hassan Sajjad",
                "citations": 0
            },
            {
                "title": "Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs",
                "abstract": "In the study of LLMs, sycophancy represents a prevalent hallucination that poses significant challenges to these models. Specifically, LLMs often fail to adhere to original correct responses, instead blindly agreeing with users' opinions, even when those opinions are incorrect or malicious. However, research on sycophancy in visual language models (VLMs) has been scarce. In this work, we extend the exploration of sycophancy from LLMs to VLMs, introducing the MM-SY benchmark to evaluate this phenomenon. We present evaluation results from multiple representative models, addressing the gap in sycophancy research for VLMs. To mitigate sycophancy, we propose a synthetic dataset for training and employ methods based on prompts, supervised fine-tuning, and DPO. Our experiments demonstrate that these methods effectively alleviate sycophancy in VLMs. Additionally, we probe VLMs to assess the semantic impact of sycophancy and analyze the attention distribution of visual tokens. Our findings indicate that the ability to prevent sycophancy is predominantly observed in higher layers of the model. The lack of attention to image knowledge in these higher layers may contribute to sycophancy, and enhancing image attention at high layers proves beneficial in mitigating this issue.",
                "authors": "Shuo Li, Tao Ji, Xiaoran Fan, Linsheng Lu, Leyi Yang, Yuming Yang, Zhiheng Xi, Rui Zheng, Yuran Wang, Xiaohui Zhao, Tao Gui, Qi Zhang, Xuanjing Huang",
                "citations": 0
            },
            {
                "title": "Can VLMs be used on videos for action recognition? LLMs are Visual Reasoning Coordinators",
                "abstract": "Recent advancements have introduced multiple vision-language models (VLMs) demonstrating impressive commonsense reasoning across various domains. Despite their individual capabilities, the potential of synergizing these complementary VLMs remains underexplored. The Cola Framework addresses this by showcasing how a large language model (LLM) can efficiently coordinate multiple VLMs through natural language communication, leveraging their distinct strengths. We have verified this claim on the challenging A-OKVQA dataset, confirming the effectiveness of such coordination. Building on this, our study investigates whether the same methodology can be applied to surveillance videos for action recognition. Specifically, we explore if leveraging the combined knowledge base of VLMs and LLM can effectively deduce actions from a video when presented with only a few selectively important frames and minimal temporal information. Our experiments demonstrate that LLM, when coordinating different VLMs, can successfully recognize patterns and deduce actions in various scenarios despite the weak temporal signals. However, our findings suggest that to enhance this approach as a viable alternative solution, integrating a stronger temporal signal and exposing the models to slightly more frames would be beneficial.",
                "authors": "Harsh Lunia",
                "citations": 0
            },
            {
                "title": "AnyAttack: Targeted Adversarial Attacks on Vision-Language Models toward Any Images",
                "abstract": "Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks, particularly targeted adversarial images that manipulate the model to generate harmful content specified by the adversary. Current attack methods rely on predefined target labels to create targeted adversarial attacks, which limits their scalability and applicability for large-scale robustness evaluations. In this paper, we propose AnyAttack, a self-supervised framework that generates targeted adversarial images for VLMs without label supervision, allowing any image to serve as a target for the attack. Our framework employs the pre-training and fine-tuning paradigm, with the adversarial noise generator pre-trained on the large-scale LAION-400M dataset. This large-scale pre-training endows our method with powerful transferability across a wide range of VLMs. Extensive experiments on five mainstream open-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks (image-text retrieval, multimodal classification, and image captioning) demonstrate the effectiveness of our attack. Additionally, we successfully transfer AnyAttack to multiple commercial VLMs, including Google Gemini, Claude Sonnet, Microsoft Copilot and OpenAI GPT. These results reveal an unprecedented risk to VLMs, highlighting the need for effective countermeasures.",
                "authors": "Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Jitao Sang, Dit-Yan Yeung",
                "citations": 0
            },
            {
                "title": "Evaluating Model Perception of Color Illusions in Photorealistic Scenes",
                "abstract": "We study the perception of color illusions by vision-language models. Color illusion, where a person's visual system perceives color differently from actual color, is well-studied in human vision. However, it remains underexplored whether vision-language models (VLMs), trained on large-scale human data, exhibit similar perceptual biases when confronted with such color illusions. We propose an automated framework for generating color illusion images, resulting in RCID (Realistic Color Illusion Dataset), a dataset of 19,000 realistic illusion images. Our experiments show that all studied VLMs exhibit perceptual biases similar human vision. Finally, we train a model to distinguish both human perception and actual pixel differences.",
                "authors": "Lingjun Mao, Zineng Tang, Alane Suhr",
                "citations": 0
            },
            {
                "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
                "abstract": "Large language models have become increasingly prominent, also signaling a shift towards multimodality as the next frontier in artificial intelligence, where their embeddings are harnessed as prompts to generate textual content. Vision-language models (VLMs) stand at the forefront of this advancement, offering innovative ways to combine visual and textual data for enhanced understanding and interaction. However, this integration also enlarges the attack surface. Patch-based adversarial attack is considered the most realistic threat model in physical vision applications, as demonstrated in many existing literature. In this paper, we propose to address patched visual prompt injection, where adversaries exploit adversarial patches to generate target content in VLMs. Our investigation reveals that patched adversarial prompts exhibit sensitivity to pixel-wise randomization, a trait that remains robust even against adaptive attacks designed to counteract such defenses. Leveraging this insight, we introduce SmoothVLM, a defense mechanism rooted in smoothing techniques, specifically tailored to protect VLMs from the threat of patched visual prompt injectors. Our framework significantly lowers the attack success rate to a range between 0% and 5.0% on two leading VLMs, while achieving around 67.3% to 95.0% context recovery of the benign images, demonstrating a balance between security and usability.",
                "authors": "Jiachen Sun, Changsheng Wang, Jiong Wang, Yiwei Zhang, Chaowei Xiao",
                "citations": 0
            },
            {
                "title": "Rethinking Pruning for Vision-Language Models: Strategies for Effective Sparsity and Performance Restoration",
                "abstract": "Vision-Language Models (VLMs) integrate information from multiple modalities and have shown remarkable success across various tasks. However, deploying large-scale VLMs in resource-constrained scenarios is challenging. Pruning followed by finetuning offers a potential solution but remains underexplored for VLMs. This study addresses two key questions: how to distribute sparsity across different modality-specific models, and how to restore the performance of pruned sparse VLMs. Our preliminary studies identified two effective pruning settings: applying the same sparsity to both vision and language models, and pruning only the language models. While LoRA finetuning aims to restore sparse models, it faces challenges due to incompatibility with sparse models, disrupting the pruned sparsity. To overcome these issues, we propose SparseLoRA, which applies sparsity directly to LoRA weights. Our experimental results demonstrate significant improvements, including an 11.3\\% boost under 2:4 sparsity and a 47.6\\% enhancement under unstructured 70\\% sparsity. Code is released at: \\url{https://github.com/Shwai-He/VLM-Compression}.",
                "authors": "Shwai He, Tianlong Chen",
                "citations": 0
            },
            {
                "title": "MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models",
                "abstract": "Warning:This paper contains unsafe model responses. As deep learning advances, Large Language Models (LLMs) and their multimodal counter-parts, Vision-Language Models (VLMs), have shown exceptional performance in many real-world tasks. However, VLMs face significant security challenges, such as jailbreak attacks, where attackers attempt to bypass the model’s safety alignment to elicit harmful responses. The threat of jailbreak attacks on VLMs arises from both the inherent vulnerabilities of LLMs and the multiple information channels that VLMs process. While various attacks and defenses have been proposed, there is a notable gap in unified and comprehensive evaluations, as each method is evaluated on different dataset and metrics, making it impossible to compare the effectiveness of each method. To address this gap, we introduce MMJ-Bench , a unified pipeline for evaluating jailbreak attacks and defense techniques for VLMs. Through extensive experiments, we assess the effectiveness of various attack methods against SoTA VLMs and evaluate the impact of defense mechanisms on both defense effectiveness and model utility for normal tasks. Our comprehensive evaluation contribute to the field by offering a unified and systematic evaluation framework and the first public-available benchmark for VLM jailbreak research. We also demonstrate several insightful findings that highlights directions for future studies.",
                "authors": "Fenghua Weng, Yue Xu, Chengyan Fu, Wenjie Wang",
                "citations": 0
            },
            {
                "title": "Bootstrapping Vision-language Models for Self-supervised Remote Physiological Measurement",
                "abstract": "Facial video-based remote physiological measurement is a promising research area for detecting human vital signs (e.g., heart rate, respiration frequency) in a non-contact way. Conventional approaches are mostly supervised learning, requiring extensive collections of facial videos and synchronously recorded photoplethysmography (PPG) signals. To tackle it, self-supervised learning has recently gained attentions; due to the lack of ground truth PPG signals, its performance is however limited. In this paper, we propose a novel self-supervised framework that successfully integrates the popular vision-language models (VLMs) into the remote physiological measurement task. Given a facial video, we first augment its positive and negative video samples with varying rPPG signal frequencies. Next, we introduce a frequency-oriented vision-text pair generation method by carefully creating contrastive spatio-temporal maps from positive and negative samples and designing proper text prompts to describe their relative ratios of signal frequencies. A pre-trained VLM is employed to extract features for these formed vision-text pairs and estimate rPPG signals thereafter. We develop a series of generative and contrastive learning mechanisms to optimize the VLM, including the text-guided visual map reconstruction task, the vision-text contrastive learning task, and the frequency contrastive and ranking task. Overall, our method for the first time adapts VLMs to digest and align the frequency-related knowledge in vision and text modalities. Extensive experiments on four benchmark datasets demonstrate that it significantly outperforms state of the art self-supervised methods.",
                "authors": "Zijie Yue, Miaojing Shi, Hanli Wang, Shuai Ding, Qijun Chen, Shanlin Yang",
                "citations": 0
            },
            {
                "title": "The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals",
                "abstract": "The glyphic writing system of Chinese incorporates information-rich visual features in each character, such as radicals that provide hints about meaning or pronunciation. However, there has been no investigation into whether contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can harness these sub-character features in Chinese through prompting. In this study, we establish a benchmark to evaluate LLMs' and VLMs' understanding of visual elements in Chinese characters, including radicals, composition structures, strokes, and stroke counts. Our results reveal that models surprisingly exhibit some, but still limited, knowledge of the visual information, regardless of whether images of characters are provided. To incite models' ability to use radicals, we further experiment with incorporating radicals into the prompts for Chinese language processing (CLP) tasks. We observe consistent improvement in Part-Of-Speech tagging when providing additional information about radicals, suggesting the potential to enhance CLP by integrating sub-character information.",
                "authors": "Xiaofeng Wu, Karl Stratos, Wei Xu",
                "citations": 0
            },
            {
                "title": "Can Vision-Language Models Replace Human Annotators: A Case Study with CelebA Dataset",
                "abstract": "This study evaluates the capability of Vision-Language Models (VLMs) in image data annotation by comparing their performance on the CelebA dataset in terms of quality and cost-effectiveness against manual annotation. Annotations from the state-of-the-art LLaVA-NeXT model on 1000 CelebA images are in 79.5% agreement with the original human annotations. Incorporating re-annotations of disagreed cases into a majority vote boosts AI annotation consistency to 89.1% and even higher for more objective labels. Cost assessments demonstrate that AI annotation significantly reduces expenditures compared to traditional manual methods -- representing less than 1% of the costs for manual annotation in the CelebA dataset. These findings support the potential of VLMs as a viable, cost-effective alternative for specific annotation tasks, reducing both financial burden and ethical concerns associated with large-scale manual data annotation. The AI annotations and re-annotations utilized in this study are available on https://github.com/evev2024/EVEV2024_CelebA.",
                "authors": "Haoming Lu, Feifei Zhong",
                "citations": 0
            },
            {
                "title": "InsightSee: Advancing Multi-agent Vision-Language Models for Enhanced Visual Understanding",
                "abstract": "Accurate visual understanding is imperative for advancing autonomous systems and intelligent robots. Despite the powerful capabilities of vision-language models (VLMs) in processing complex visual scenes, precisely recognizing obscured or ambiguously presented visual elements remains challenging. To tackle such issues, this paper proposes InsightSee, a multi-agent framework to enhance VLMs’ interpretative capabilities in handling complex visual understanding scenarios. The framework comprises a description agent, two reasoning agents, and a decision agent, which are integrated to refine the process of visual information interpretation. The design of these agents and the mechanisms by which they can be enhanced in visual information processing are presented. Experimental results demonstrate that the InsightSee framework not only boosts performance on specific visual tasks but also retains the original models’ strength. The proposed framework outperforms state-of-the-art algorithms in 6 out of 9 benchmark tests, with a substantial advancement in multimodal understanding.",
                "authors": "Huaxiang Zhang, Yaojia Mu, Guo-Niu Zhu, Zhongxue Gan",
                "citations": 0
            },
            {
                "title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?",
                "abstract": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an understanding of diverse multi-vision sensor data, such as thermal, depth, and X-ray information, is essential. However, we find that current VLMs process multi-vision sensor images without deep understanding of sensor information, disregarding each sensor's unique physical properties. This limitation restricts their capacity to interpret and respond to complex questions requiring multi-vision sensor reasoning. To address this, we propose a novel Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark, assessing VLMs on their capacity for sensor-specific reasoning. Moreover, we introduce Diverse Negative Attributes (DNA) optimization to enable VLMs to perform deep reasoning on multi-vision sensor tasks, helping to bridge the core information gap between images and sensor data. Extensive experimental results validate that the proposed DNA method can significantly improve the multi-vision sensor reasoning for VLMs.",
                "authors": "Sangyun Chung, Youngjoon Yu, Youngchae Chee, Se Yeon Kim, Byung-Kwan Lee, Yonghyun Ro",
                "citations": 0
            },
            {
                "title": "V-RoAst: A New Dataset for Visual Road Assessment",
                "abstract": "Road traffic crashes cause millions of deaths annually and have a significant economic impact, particularly in low- and middle-income countries (LMICs). This paper presents an approach using Vision Language Models (VLMs) for road safety assessment, overcoming the limitations of traditional Convolutional Neural Networks (CNNs). We introduce a new task ,V-RoAst (Visual question answering for Road Assessment), with a real-world dataset. Our approach optimizes prompt engineering and evaluates advanced VLMs, including Gemini-1.5-flash and GPT-4o-mini. The models effectively examine attributes for road assessment. Using crowdsourced imagery from Mapillary, our scalable solution influentially estimates road safety levels. In addition, this approach is designed for local stakeholders who lack resources, as it does not require training data. It offers a cost-effective and automated methods for global road safety assessments, potentially saving lives and reducing economic burdens.",
                "authors": "Natchapon Jongwiriyanurak, Zichao Zeng, June Moh Goo, Xinglei Wang, Ilya Ilyankou, Kerkritt Srirrongvikrai, Meihui Wang, James Haworth",
                "citations": 0
            },
            {
                "title": "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models",
                "abstract": "Smaller-scale Vision-Language Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the “Uncontextualized Uncommon Objects” (UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs. Code and project details for UOUO can be found at https://zoezheng126.github.io/UOUO-Website/.",
                "authors": "Xinyu Pi, Mingyuan Wu, Jize Jiang, Haozhen Zheng, Beitong Tian, Chengxiang Zhai, Klara Nahrstedt, Zhiting Hu",
                "citations": 0
            },
            {
                "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models",
                "abstract": "Vision Language Models (VLMs) have shown impressive performances on numerous tasks. However, their zero-shot capabilities can be limited compared to dedicated or fine-tuned models. In this work, we propose LLM-wrapper , a novel approach to adapt VLMs in a black-box manner by leveraging large language models (LLMs) so as to reason on their outputs. LLM-wrapper fine-tunes LLMs in a light and efficient way. We demonstrate the effectiveness of LLM-wrapper on Refer-ring Expression Comprehension (REC), a challenging open-vocabulary task that requires spatial and semantic reasoning. Our approach significantly boosts off-the-shelf models, resulting in competitive results. The code and instructions to reproduce our results will be open-sourced.",
                "authors": "Amaia Cardiel, Éloi Zablocki, Oriane Siméoni, Elias Ramzi, Matthieu Cord",
                "citations": 0
            },
            {
                "title": "An Intelligent Agentic System for Complex Image Restoration Problems",
                "abstract": "Real-world image restoration (IR) is inherently complex and often requires combining multiple specialized models to address diverse degradations. Inspired by human problem-solving, we propose AgenticIR, an agentic system that mimics the human approach to image processing by following five key stages: Perception, Scheduling, Execution, Reflection, and Rescheduling. AgenticIR leverages large language models (LLMs) and vision-language models (VLMs) that interact via text generation to dynamically operate a toolbox of IR models. We fine-tune VLMs for image quality analysis and employ LLMs for reasoning, guiding the system step by step. To compensate for LLMs' lack of specific IR knowledge and experience, we introduce a self-exploration method, allowing the LLM to observe and summarize restoration results into referenceable documents. Experiments demonstrate AgenticIR's potential in handling complex IR tasks, representing a promising path toward achieving general intelligence in visual processing.",
                "authors": "Kaiwen Zhu, Jinjin Gu, Zhiyuan You, Yu Qiao, Chao Dong",
                "citations": 0
            },
            {
                "title": "Zero-Shot Pornographic Detection Using Vision-Language Models",
                "abstract": "The proliferation of pornographic content online challenges content moderation efforts, especially in sensitive contexts. Traditional detection methods often require extensive labeled datasets and struggle with nuanced content. This paper proposes a zero-shot classification approach using Vision-Language Models (VLMs) like CLIP and Open CLIP, which leverage visual-textual alignment to classify pornographic content without task-specific training. Evaluated on the LSPD dataset, our research examined various aspects of using VLMs, including the effects of keyword choice, prompt construction, model size, and pre-training data resolution. Our method achieved comparable or better performance than traditional models, with the best accuracy reaching 91.6% using the key-word “erotica“ and specific descriptive prompts. This approach reduces dependency on large datasets, offering a robust solution for detecting explicit content.",
                "authors": "Dinh-Duy Phan, Hoang-Loc Tran, Thanh-Thien Nguyen, Duc-Lung Vu",
                "citations": 0
            },
            {
                "title": "Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual Ability of Vision Language Models",
                "abstract": "Large language models (LLMs) have increased interest in vision language models (VLMs), which process image-text pairs as input. Studies investigating the visual understanding ability of VLMs have been proposed, but such studies are still preliminary because existing datasets do not permit a comprehensive evaluation of the fine-grained visual linguistic abilities of VLMs across multiple languages. To further explore the strengths of VLMs, such as GPT-4V \\cite{openai2023GPT4}, we developed new datasets for the systematic and qualitative analysis of VLMs. Our contribution is four-fold: 1) we introduced nine vision-and-language (VL) tasks (including object recognition, image-text matching, and more) and constructed multilingual visual-text datasets in four languages: English, Japanese, Swahili, and Urdu through utilizing templates containing \\textit{questions} and prompting GPT4-V to generate the \\textit{answers} and the \\textit{rationales}, 2) introduced a new VL task named \\textit{unrelatedness}, 3) introduced rationales to enable human understanding of the VLM reasoning process, and 4) employed human evaluation to measure the suitability of proposed datasets for VL tasks. We show that VLMs can be fine-tuned on our datasets. Our work is the first to conduct such analyses in Swahili and Urdu. Also, it introduces \\textit{rationales} in VL analysis, which played a vital role in the evaluation.",
                "authors": "Jesse Atuhurra, Iqra Ali, Tatsuya Hiraoka, Hidetaka Kamigaito, Tomoya Iwakura, Taro Watanabe",
                "citations": 0
            },
            {
                "title": "Improving Fine-grained Visual Understanding in VLMs through Text-Only Training",
                "abstract": "Visual-Language Models (VLMs) have become a powerful tool for bridging the gap between visual and linguistic understanding. However, the conventional learning approaches for VLMs often suffer from limitations, such as the high resource requirements of collecting and training image-text paired data. Recent research has suggested that language understanding plays a crucial role in the performance of VLMs, potentially indicating that text-only training could be a viable approach. In this work, we investigate the feasibility of enhancing fine-grained visual understanding in VLMs through text-only training. Inspired by how humans develop visual concept understanding, where rich textual descriptions can guide visual recognition, we hypothesize that VLMs can also benefit from leveraging text-based representations to improve their visual recognition abilities. We conduct comprehensive experiments on two distinct domains: fine-grained species classification and cultural visual understanding tasks. Our findings demonstrate that text-only training can be comparable to conventional image-text training while significantly reducing computational costs. This suggests a more efficient and cost-effective pathway for advancing VLM capabilities, particularly valuable in resource-constrained environments.",
                "authors": "Dasol Choi, Guijin Son, Soo Yong Kim, Gio Paik, Seunghyeok Hong",
                "citations": 0
            },
            {
                "title": "ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities",
                "abstract": "Large language models(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model’s reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.",
                "authors": "Ying Su, Zhan Ling, Haochen Shi, Jiayang Cheng, Yauwai Yim, Yangqiu Song",
                "citations": 0
            },
            {
                "title": "AI-Driven Early Mental Health Screening: Analyzing Selfies of Pregnant Women",
                "abstract": "Major Depressive Disorder and anxiety disorders affect millions globally, contributing significantly to the burden of mental health issues. Early screening is crucial for effective intervention, as timely identification of mental health issues can significantly improve treatment outcomes. Artificial intelligence (AI) can be valuable for improving the screening of mental disorders, enabling early intervention and better treatment outcomes. AI-driven screening can leverage the analysis of multiple data sources, including facial features in digital images. However, existing methods often rely on controlled environments or specialized equipment, limiting their broad applicability. This study explores the potential of AI models for ubiquitous depression-anxiety screening given face-centric selfies. The investigation focuses on high-risk pregnant patients, a population that is particularly vulnerable to mental health issues. To cope with limited training data resulting from our clinical setup, pre-trained models were utilized in two different approaches: fine-tuning convolutional neural networks (CNNs) originally designed for facial expression recognition and employing vision-language models (VLMs) for zero-shot analysis of facial expressions. Experimental results indicate that the proposed VLM-based method significantly outperforms CNNs, achieving an accuracy of 77.6%. Although there is significant room for improvement, the results suggest that VLMs can be a promising approach for mental health screening.",
                "authors": "Gustavo A. Bas'ilio, Thiago B. Pereira, A. L. Koerich, Hermano Tavares, Ludmila Dias, Maria das Graccas da S. Teixeira, Rafael T. Sousa, W. H. Hisatugu, Amanda S. Mota, Anilton S. Garcia, Marco Aur'elio K. Galletta, Thiago M. Paixao",
                "citations": 0
            },
            {
                "title": "Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models",
                "abstract": "Test-time adaptation, which enables models to generalize to diverse data with unlabeled test samples, holds significant value in real-world scenarios. Recently, researchers have applied this setting to advanced pre-trained vision-language models (VLMs), developing approaches such as test-time prompt tuning to further extend their practical applicability. However, these methods typically focus solely on adapting VLMs from a single modality and fail to accumulate task-specific knowledge as more samples are processed. To address this, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation approach for VLMs that effectively accumulates task-specific knowledge from multi-modalities. Specifically, we create and evolve two sets of prototypes--textual and visual--to progressively capture more accurate multi-modal representations for target classes during test time. Moreover, to promote consistent multi-modal representations, we introduce and optimize learnable residuals for each test sample to align the prototypes from both modalities. Extensive experimental results on 15 benchmark datasets demonstrate that our proposed DPE consistently outperforms previous state-of-the-art methods while also exhibiting competitive computational efficiency. Code is available at https://github.com/zhangce01/DPE-CLIP.",
                "authors": "Ce Zhang, Simon Stepputtis, Katia P. Sycara, Yaqi Xie",
                "citations": 0
            },
            {
                "title": "CVLUE: A New Benchmark Dataset for Chinese Vision-Language Understanding Evaluation",
                "abstract": "Despite the rapid development of Chinese vision-language models (VLMs), most existing Chinese vision-language (VL) datasets are constructed on Western-centric images from existing English VL datasets. The cultural bias in the images makes these datasets unsuitable for evaluating VLMs in Chinese culture. To remedy this issue, we present a new Chinese Vision- Language Understanding Evaluation (CVLUE) benchmark dataset, where the selection of object categories and images is entirely driven by Chinese native speakers, ensuring that the source images are representative of Chinese culture. The benchmark contains four distinct VL tasks ranging from image-text retrieval to visual question answering, visual grounding and visual dialogue. We present a detailed statistical analysis of CVLUE and provide a baseline performance analysis with several open-source multilingual VLMs on CVLUE and its English counterparts to reveal their performance gap between English and Chinese. Our in-depth category-level analysis reveals a lack of Chinese cultural knowledge in existing VLMs. We also find that fine-tuning on Chinese culture-related VL datasets effectively enhances VLMs' understanding of Chinese culture.",
                "authors": "Yuxuan Wang, Yijun Liu, Fei Yu, Chen Huang, Kexin Li, Zhiguo Wan, Wanxiang Che",
                "citations": 0
            },
            {
                "title": "Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation",
                "abstract": "Large Vision-Language Models (VLMs) have demonstrated remarkable performance across multimodal tasks by integrating vision encoders with large language models (LLMs). However, these models remain vulnerable to adversarial attacks. Among such attacks, Universal Adversarial Perturbations (UAPs) are especially powerful, as a single optimized perturbation can mislead the model across various input images. In this work, we introduce a novel UAP specifically designed for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP), capable of universally deceiving VLMs across both image and text inputs. To successfully disrupt the vision encoder's fundamental process, we analyze the core components of the attention mechanism. After identifying value vectors in the middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a label-free manner with a frozen model. Despite being developed as a black-box to the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently outperforming baseline methods across vision-language tasks. Extensive ablation studies and analyses further demonstrate the robustness of Doubly-UAP and provide insights into how it influences internal attention mechanisms.",
                "authors": "Heeseon Kim, Minbeom Kim, Changick Kim",
                "citations": 0
            },
            {
                "title": "Active Learning for Vision-Language Models",
                "abstract": "Pre-trained vision-language models (VLMs) like CLIP have demonstrated impressive zero-shot performance on a wide range of downstream computer vision tasks. However, there still exists a considerable performance gap between these models and a supervised deep model trained on a downstream dataset. To bridge this gap, we propose a novel active learning (AL) framework that enhances the zero-shot classification performance of VLMs by selecting only a few informative samples from the unlabeled data for annotation during training. To achieve this, our approach first calibrates the predicted entropy of VLMs and then utilizes a combination of self-uncertainty and neighbor-aware uncertainty to calculate a reliable uncertainty measure for active sample selection. Our extensive experiments show that the proposed approach outperforms existing AL approaches on several image classification datasets, and significantly enhances the zero-shot performance of VLMs.",
                "authors": "Bardia Safaei, Vishal M. Patel",
                "citations": 0
            },
            {
                "title": "ViLBias: A Framework for Bias Detection using Linguistic and Visual Cues",
                "abstract": "The integration of Large Language Models (LLMs) and Vision-Language Models (VLMs) opens new avenues for addressing complex challenges in multimodal content analysis, particularly in biased news detection. This study introduces ViLBias , a framework that leverages state-of-the-art LLMs and VLMs to detect linguistic and visual biases in news content, addressing the limitations of traditional text-only approaches. Our contributions include a novel dataset pairing textual content with accompanying visuals from diverse news sources, and a hybrid annotation framework, combining LLM-based annotations with human review, enhancing quality while reducing costs and improving scalability. We evaluate the efficacy of LLMs and VLMs in identifying biases, revealing their strengths in detecting subtle framing and text-visual inconsistencies. Empirical analysis demonstrates that incorporating visual cues alongside text enhances bias detection accuracy by 3–5%, show-casing the complementary strengths of LLMs in generative reasoning and Small Language Models (SLMs) in classification. This study offers a comprehensive exploration of LLMs and VLMs as tools for detecting multimodal biases in news content, highlighting both their potential and limitations. Our research paves the way for more robust, scalable, and nuanced approaches to media bias detection, contributing to the broader field of natural language processing and multimodal analysis.",
                "authors": "Shaina Raza, Caesar Saleh, Emrul Hasan, Franklin Ogidi, Maximus Powers, Veronica Chatrath, Marcelo Lotif, Roya Javadi, Anam Zahid, Vahid Reza Khazaie",
                "citations": 0
            },
            {
                "title": "SuctionPrompt: Visual-assisted Robotic Picking with a Suction Cup Using Vision-Language Models and Facile Hardware Design",
                "abstract": "The development of large language models and vision-language models (VLMs) has resulted in the increasing use of robotic systems in various fields. However, the effective integration of these models into real-world robotic tasks is a key challenge. We developed a versatile robotic system called SuctionPrompt that utilizes prompting techniques of VLMs combined with 3D detections to perform product-picking tasks in diverse and dynamic environments. Our method highlights the importance of integrating 3D spatial information with adaptive action planning to enable robots to approach and manipulate objects in novel environments. In the validation experiments, the system accurately selected suction points 75.4%, and achieved a 65.0% success rate in picking common items. This study highlights the effectiveness of VLMs in robotic manipulation tasks, even with simple 3D processing.",
                "authors": "Tomohiro Motoda, Takahide Kitamura, Ryo Hanai, Y. Domae",
                "citations": 0
            },
            {
                "title": "Post-hoc Probabilistic Vision-Language Models",
                "abstract": "Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable success in classification, retrieval, and generative tasks. For this, VLMs deterministically map images and text descriptions to a joint latent space in which their similarity is assessed using the cosine similarity. However, a deterministic mapping of inputs fails to capture uncertainties over concepts arising from domain shifts when used in downstream tasks. In this work, we propose post-hoc uncertainty estimation in VLMs that does not require additional training. Our method leverages a Bayesian posterior approximation over the last layers in VLMs and analytically quantifies uncertainties over cosine similarities. We demonstrate its effectiveness for uncertainty quantification and support set selection in active learning. Compared to baselines, we obtain improved and well-calibrated predictive uncertainties, interpretable uncertainty estimates, and sample-efficient active learning. Our results show promise for safety-critical applications of large-scale models.",
                "authors": "Anton Baumann, Rui Li, Marcus Klasson, Santeri Mentu, Shyamgopal Karthik, Zeynep Akata, Arno Solin, Martin Trapp",
                "citations": 0
            },
            {
                "title": "Have Large Vision-Language Models Mastered Art History?",
                "abstract": "The emergence of large Vision-Language Models (VLMs) has recently established new baselines in image classification across multiple domains. However, the performance of VLMs in the specific task of artwork classification, particularly art style classification of paintings - a domain traditionally mastered by art historians - has not been explored yet. Artworks pose a unique challenge compared to natural images due to their inherently complex and diverse structures, characterized by variable compositions and styles. Art historians have long studied the unique aspects of artworks, with style prediction being a crucial component of their discipline. This paper investigates whether large VLMs, which integrate visual and textual data, can effectively predict the art historical attributes of paintings. We conduct an in-depth analysis of four VLMs, namely CLIP, LLaVA, OpenFlamingo, and GPT-4o, focusing on zero-shot classification of art style, author and time period using two public benchmarks of artworks. Additionally, we present ArTest, a well-curated test set of artworks, including pivotal paintings studied by art historians.",
                "authors": "Ombretta Strafforello, Derya Soydaner, Michiel Willems, Anne-Sofie Maerten, Stefanie De Winter",
                "citations": 0
            },
            {
                "title": "How to Efficiently Adapt Foundation Models to Remote Sensing for Object Detection?",
                "abstract": "Vision-Language Models (VLMs) trained on millions of images excel at generalization in various domains. However, a significant distribution shift between remote sensing and natural images limits their effectiveness without adaptation. We investigate both the ability of VLMs to adapt to satellite images and the ability of previously adapted VLMs to detect new classes of objects on these images with only few examples. We address this challenge by exploring different parameter-efficient finetuning strategies to adapt to remote sensing the knowledge acquired by these models during their large pre-training. We evaluate these strategies in few-shot settings and scenarios involving training without data constraints. We also show on the DIOR dataset that a GLIP model previously trained on satellite images is able to achieve state-of-the-art performance in the few-shot detection of new classes of objects, without requiring a dedicated training strategy.",
                "authors": "Clément Barbier, B. Abeloos, Stéphane Herbin",
                "citations": 0
            },
            {
                "title": "In-Context Learning Improves Compositional Understanding of Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) have shown remarkable capabilities in a large number of downstream tasks. Nonetheless, compositional image understanding remains a rather difficult task due to the object bias present in training data. In this work, we investigate the reasons for such a lack of capability by performing an extensive bench-marking of compositional understanding in VLMs. We compare contrastive models with generative ones and analyze their differences in architecture, pre-training data, and training tasks and losses. Furthermore, we leverage In-Context Learning (ICL) as a way to improve the ability of VLMs to perform more complex reasoning and understanding given an image. Our extensive experiments demonstrate that our proposed approach outperforms baseline models across multiple compositional understanding datasets.",
                "authors": "Matteo Nulli, Anesa Ibrahimi, Avik Pal, Hoshe Lee, Ivona Najdenkoska",
                "citations": 0
            },
            {
                "title": "MegaCOIN: Enhancing Medium-Grained Color Perception for Vision-Language Models",
                "abstract": "In vision-language models (VLMs), the ability to perceive and interpret color and physical environment is crucial for achieving contextually accurate understanding and interaction. However, despite advances in multimodal modeling, there remains a significant lack of specialized datasets that rigorously evaluate a model's capacity to discern subtle color variations and spatial context -- critical elements for situational comprehension and reliable deployment across real-world applications. Toward that goal, we curate MegaCOIN, a high-quality, human-labeled dataset based on \\emph{real} images with various contextual attributes. MegaCOIN consists of two parts: MegaCOIN-Instruct, which serves as a supervised fine-tuning (SFT) dataset for VLMs; and MegaCOIN-Bench, an annotated test set that can be used as a stand-alone QA dataset. MegaCOIN~provides three annotated features for 220,000 real images: foreground color, background color, and description of an object's physical environment, constituting 660k human annotations. In addition, MegaCOIN can be applied to benchmark domain generalization (DG) algorithms. We explore benchmarking DG methods in the linear probing setup for VLM and show some new insights. Last but not least, we show that VLMs, including GPT-4o, have subpar color recognition capabilities, and fine-tuning with MegaCOIN can result in improved performance on visual evaluation tasks. In certain cases, MegaCOIN fine-tuned small-scale opensource models such as LLaVA and Bunny can outperform closed-source GPT-4o. We hope the utilities of MegaCOIN can shed light on the directions VLMs can improve and provide a more complex platform for domain generalization algorithms.",
                "authors": "Ming-Chang Chiu, Shicheng Wen, Pin-Yu Chen, Xuezhe Ma",
                "citations": 0
            },
            {
                "title": "Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions",
                "abstract": "Recent research increasingly focuses on training vision-language models (VLMs) with long, detailed image captions. However, small-scale VLMs often struggle to balance the richness of these captions with the risk of hallucinating content during fine-tuning. In this paper, we explore how well VLMs adapt to such captions. To quantify caption quality, we propose Decomposed NLI (DNLI), an evaluation framework that breaks down generated captions into individual propositions, assessing each in isolation. This fine-grained analysis reveals a critical balance between capturing descriptive details and preventing hallucinations. Our findings show that simply reducing caption complexity or employing standard data curation techniques does not effectively resolve this issue. To tackle this challenge, we introduce Knowledge Adapted (KnowAda) fine-tuning, a data-centric approach that automatically adapts training data with the model's existing knowledge and visual understanding. KnowAda minimizes hallucinations while preserving high descriptiveness. We validate this approach across several small-scale VLMs (up to 7B parameters) and dense caption datasets, demonstrating that KnowAda effectively balances hallucination reduction and descriptiveness. Our results show that KnowAda outperforms various baselines in both automatic metrics and human evaluations. We will release our code and models.",
                "authors": "Moran Yanuka, Assaf Ben Kish, Yonatan Bitton, Idan Szpektor, Raja Giryes",
                "citations": 0
            },
            {
                "title": "The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) combine visual and textual understanding, rendering them well-suited for diverse tasks like generating image captions and answering visual questions across various domains. However, these capabilities are built upon training on large amount of uncurated data crawled from the web. The latter may include sensitive information that VLMs could memorize and leak, raising significant privacy concerns. In this paper, we assess whether these vulnerabilities exist, focusing on identity leakage. Our study leads to three key findings: (i) VLMs leak identity information, even when the vision-language alignment and the fine-tuning use anonymized data; (ii) context has little influence on identity leakage; (iii) simple, widely used anonymization techniques, like blurring, are not sufficient to address the problem. These findings underscore the urgent need for robust privacy protection strategies when deploying VLMs. Ethical awareness and responsible development practices are essential to mitigate these risks.",
                "authors": "Simone Caldarella, Massimiliano Mancini, Elisa Ricci, Rahaf Aljundi",
                "citations": 0
            },
            {
                "title": "AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models",
                "abstract": "Evaluating the alignment capabilities of large Vision-Language Models (VLMs) is essential for determining their effectiveness as helpful assistants. However, existing benchmarks primarily focus on basic abilities using nonverbal methods, such as yes-no and multiple-choice questions. In this paper, we address this gap by introducing AlignMMBench, a comprehensive alignment benchmark specifically designed for emerging Chinese VLMs. This benchmark is meticulously curated from real-world scenarios and Chinese Internet sources, encompassing thirteen specific tasks across three categories, and includes both single-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite strategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer pairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, a rule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, we report the performance of representative VLMs on AlignMMBench, offering insights into the capabilities and limitations of different VLM architectures. All evaluation codes and data are available on https://alignmmbench.github.io.",
                "authors": "Yuhang Wu, Wenmeng Yu, Yean Cheng, Yan Wang, Xiaohan Zhang, Jiazheng Xu, Ming Ding, Yuxiao Dong",
                "citations": 0
            },
            {
                "title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines",
                "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the user's question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, a novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs.",
                "authors": "Zhixin Zhang, Yiyuan Zhang, Xiaohan Ding, Xiangyu Yue",
                "citations": 0
            },
            {
                "title": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning",
                "abstract": "Recent advancements in vision-language models (VLMs) offer potential for robot task planning, but challenges remain due to VLMs' tendency to generate incorrect action sequences. To address these limitations, we propose VeriGraph, a novel framework that integrates VLMs for robotic planning while verifying action feasibility. VeriGraph employs scene graphs as an intermediate representation, capturing key objects and spatial relationships to improve plan verification and refinement. The system generates a scene graph from input images and uses it to iteratively check and correct action sequences generated by an LLM-based task planner, ensuring constraints are respected and actions are executable. Our approach significantly enhances task completion rates across diverse manipulation scenarios, outperforming baseline methods by 58% for language-based tasks and 30% for image-based tasks.",
                "authors": "Daniel Ekpo, Mara Levy, Saksham Suri, Chuong Huynh, Abhinav Shrivastava",
                "citations": 0
            },
            {
                "title": "Mitigating Hallucination in Visual-Language Models via Re-Balancing Contrastive Decoding",
                "abstract": "Although Visual-Language Models (VLMs) have shown impressive capabilities in tasks like visual question answering and image captioning, they still struggle with hallucinations. Analysis of attention distribution in these models shows that VLMs tend to processing textual tokens rather than visual tokens. This imbalance of attention distribution causes VLMs to favor textual knowledge in the case of multimodal knowledge conflicts, resulting in differences from the image information. In this paper, we propose Re-Balancing Contrastive Decoding (RBD) method, which employs textual and visual branches to recalibrate attention distribution in VLMs. Specifically, the textual branch injects image noise to stimulate the model's dependency on text, thereby reducing textual bias. Concurrently, the visual branch focuses on the selection of significant tokens, refining the attention mechanism to highlight the primary subject. This dual-branch strategy enables the RBD method to diminish textual bias while enhancing visual information. Experimental results demonstrate that our method, RBD, outperforms the existing methods by the CHAIR and POPE metrics, mitigate hallucinations without reducing the model's general capabilities.",
                "authors": "Xiaoyu Liang, Jiayuan Yu, Lianrui Mu, Jiedong Zhuang, Jiaqi Hu, Yuchen Yang, Jiangnan Ye, Lu Lu, Jian Chen, Haoji Hu",
                "citations": 0
            },
            {
                "title": "Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs), such as CLIP, play a foundational role in various cross-modal applications. To fully leverage VLMs' potential in adapting to downstream tasks, context optimization methods like Prompt Tuning are essential. However, one key limitation is the lack of diversity in prompt templates, whether they are hand-crafted or learned through additional modules. This limitation restricts the capabilities of pretrained VLMs and can result in incorrect predictions in downstream tasks. To address this challenge, we propose Context Optimization with Multi-Knowledge Representation (CoKnow), a framework that enhances Prompt Learning for VLMs with rich contextual knowledge. To facilitate CoKnow during inference, we trained lightweight semantic knowledge mappers, which are capable of generating Multi-Knowledge Representation for an input image without requiring additional priors. Experimentally, We conducted extensive experiments on 11 publicly available datasets, demonstrating that CoKnow outperforms a series of previous methods. We will make all resources open-source: https://github.com/EMZucas/CoKnow.",
                "authors": "Enming Zhang, Bingke Zhu, Yingying Chen, Qinghai Miao, Ming Tang, Jinqiao Wang",
                "citations": 0
            },
            {
                "title": "Constructive Apraxia: An Unexpected Limit of Instructible Vision-Language Models and Analog for Human Cognitive Disorders",
                "abstract": "This study reveals an unexpected parallel between instructible vision-language models (VLMs) and human cognitive disorders, specifically constructive apraxia. We tested 25 state-of-the-art VLMs, including GPT-4 Vision, DALL-E 3, and Midjourney v5, on their ability to generate images of the Ponzo illusion, a task that requires basic spatial reasoning and is often used in clinical assessments of constructive apraxia. Remarkably, 24 out of 25 models failed to correctly render two horizontal lines against a perspective background, mirroring the deficits seen in patients with parietal lobe damage. The models consistently misinterpreted spatial instructions, producing tilted or misaligned lines that followed the perspective of the background rather than remaining horizontal. This behavior is strikingly similar to how apraxia patients struggle to copy or construct simple figures despite intact visual perception and motor skills. Our findings suggest that current VLMs, despite their advanced capabilities in other domains, lack fundamental spatial reasoning abilities akin to those impaired in constructive apraxia. This limitation in AI systems provides a novel computational model for studying spatial cognition deficits and highlights a critical area for improvement in VLM architecture and training methodologies.",
                "authors": "David A. Noever, S. M. Noever",
                "citations": 0
            },
            {
                "title": "Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model",
                "abstract": "Most of the current vision-language models (VLMs) for videos struggle to understand videos longer than a few seconds. This is primarily due to the fact that they do not scale to utilizing a large number of frames. In order to address this limitation, we propose Espresso, a novel method that extracts and compresses spatial and temporal information separately. Through extensive evaluations, we show that spatial and temporal compression in Espresso each have a positive impact on the long-form video understanding capabilities; when combined, their positive impact increases. Furthermore, we show that Espresso's performance scales well with more training data, and that Espresso is far more effective than the existing projectors for VLMs in long-form video understanding. Moreover, we devise a more difficult evaluation setting for EgoSchema called\"needle-in-a-haystack\"that multiplies the lengths of the input videos. Espresso achieves SOTA performance on this task, outperforming the SOTA VLMs that have been trained on much more training data.",
                "authors": "Keunwoo Peter Yu, Achal Dave, Rares Ambrus, Jean-Pierre Mercat",
                "citations": 0
            },
            {
                "title": "CROPE: Evaluating In-Context Adaptation of Vision and Language Models to Culture-Specific Concepts",
                "abstract": "As Vision and Language models (VLMs) become accessible across the globe, it is important that they demonstrate cultural knowledge. In this paper, we introduce CROPE, a visual question answering benchmark designed to probe the knowledge of culture-specific concepts and evaluate the capacity for cultural adaptation through contextual information. This allows us to distinguish between parametric knowledge acquired during training and contextual knowledge provided during inference via visual and textual descriptions. Our evaluation of several state-of-the-art open VLMs shows large performance disparities between culture-specific and common concepts in the parametric setting. Moreover, experiments with contextual knowledge indicate that models struggle to effectively utilize multimodal information and bind culture-specific concepts to their depictions. Our findings reveal limitations in the cultural understanding and adaptability of current VLMs that need to be addressed toward more culturally inclusive models.",
                "authors": "Malvina Nikandrou, Georgios Pantazopoulos, Nikolas Vitsakis, Ioannis Konstas, Alessandro Suglia",
                "citations": 0
            },
            {
                "title": "WalkVLM:Aid Visually Impaired People Walking by Vision Language Model",
                "abstract": "Approximately 200 million individuals around the world suffer from varying degrees of visual impairment, making it crucial to leverage AI technology to offer walking assistance for these people. With the recent progress of vision-language models (VLMs), employing VLMs to improve this field has emerged as a popular research topic. However, most existing methods are studied on self-built question-answering datasets, lacking a unified training and testing benchmark for walk guidance. Moreover, in blind walking task, it is necessary to perform real-time streaming video parsing and generate concise yet informative reminders, which poses a great challenge for VLMs that suffer from redundant responses and low inference efficiency. In this paper, we firstly release a diverse, extensive, and unbiased walking awareness dataset, containing 12k video-manual annotation pairs from Europe and Asia to provide a fair training and testing benchmark for blind walking task. Furthermore, a WalkVLM model is proposed, which employs chain of thought for hierarchical planning to generate concise but informative reminders and utilizes temporal-aware adaptive prediction to reduce the temporal redundancy of reminders. Finally, we have established a solid benchmark for blind walking task and verified the advantages of WalkVLM in stream video processing for this task compared to other VLMs. Our dataset and code will be released at anonymous link https://walkvlm2024.github.io.",
                "authors": "Zhiqiang Yuan, Ting Zhang, Ying Deng, Jiapei Zhang, Yeshuang Zhu, Zexi Jia, Jie Zhou, Jinchao Zhang",
                "citations": 0
            },
            {
                "title": "MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4",
                "abstract": "Vision-Language Models (VLMs) have recently seen significant advancements through integrating with Large Language Models (LLMs). The VLMs, which process image and text modalities simultaneously, have demonstrated the ability to learn and understand the interaction between images and texts across various multi-modal tasks. Reverse designing, which could be defined as a complex vision-language task, aims to predict the edits and their parameters, given a source image, an edited version, and an optional high-level textual edit description. This task requires VLMs to comprehend the interplay between the source image, the edited version, and the optional textual context simultaneously, going beyond traditional vision-language tasks. In this paper, we extend and fine-tune MiniGPT-4 for the reverse designing task. Our experiments demonstrate the extensibility of off-the-shelf VLMs, specifically MiniGPT-4, for more complex tasks such as reverse designing. Code is available at this \\href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}",
                "authors": "Vahid Azizi, Fatemeh Koochaki",
                "citations": 0
            },
            {
                "title": "Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities",
                "abstract": "Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models (VLMs) have gained increasing attention, potential ambiguities in these models are still under-explored. To address this issue, we present the COnsistent Multilingual Frame Of Reference Test (COMFORT), an evaluation protocol to systematically assess the spatial reasoning capabilities of VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing some alignment with English conventions in resolving ambiguities, our experiments reveal significant shortcomings of VLMs: notably, the models (1) exhibit poor robustness and consistency, (2) lack the flexibility to accommodate multiple FoRs, and (3) fail to adhere to language-specific or culture-specific conventions in cross-lingual tests, as English tends to dominate other languages. With a growing effort to align vision-language models with human cognitive intuitions, we call for more attention to the ambiguous nature and cross-cultural diversity of spatial reasoning.",
                "authors": "Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, Ziqiao Ma",
                "citations": 0
            },
            {
                "title": "Can Visual Language Models Replace OCR-Based Visual Question Answering Pipelines in Production? A Case Study in Retail",
                "abstract": "Most production-level deployments for Visual Question Answering (VQA) tasks are still build as processing pipelines of independent steps including image pre-processing, object- and text detection, Optical Character Recognition (OCR) and (mostly supervised) object classification. However, the recent advances in vision Foundation Models [25] and Vision Language Models (VLMs) [23] raise the question if these custom trained, multi-step approaches can be replaced with pre-trained, single-step VLMs. This paper analyzes the performance and limits of various VLMs in the context of VQA and OCR [5, 9, 12] tasks in a production-level scenario. Using data from the Retail-786k [10] dataset, we investigate the capabilities of pre-trained VLMs to answer detailed questions about advertised products in images. Our study includes two commercial models, GPT-4V [16] and GPT-4o [17], as well as four open-source models: InternVL [5], LLaVA 1.5 [12], LLaVA-NeXT [13], and CogAgent [9]. Our initial results show, that there is in general no big performance gap between open-source and commercial models. However, we observe a strong task dependent variance in VLM performance: while most models are able to answer questions regarding the product brand and price with high accuracy, they completely fail at the same time to correctly identity the specific product name or discount. This indicates the problem of VLMs to solve fine-grained classification tasks as well to model the more abstract concept of discounts.",
                "authors": "Bianca Lamm, Janis Keuper",
                "citations": 0
            },
            {
                "title": "Towards Zero-shot Point Cloud Anomaly Detection: A Multi-View Projection Framework",
                "abstract": "Detecting anomalies within point clouds is crucial for various industrial applications, but traditional unsupervised methods face challenges due to data acquisition costs, early-stage production constraints, and limited generalization across product categories. To overcome these challenges, we introduce the Multi-View Projection (MVP) framework, leveraging pre-trained Vision-Language Models (VLMs) to detect anomalies. Specifically, MVP projects point cloud data into multi-view depth images, thereby translating point cloud anomaly detection into image anomaly detection. Following zero-shot image anomaly detection methods, pre-trained VLMs are utilized to detect anomalies on these depth images. Given that pre-trained VLMs are not inherently tailored for zero-shot point cloud anomaly detection and may lack specificity, we propose the integration of learnable visual and adaptive text prompting techniques to fine-tune these VLMs, thereby enhancing their detection performance. Extensive experiments on the MVTec 3D-AD and Real3D-AD demonstrate our proposed MVP framework's superior zero-shot anomaly detection performance and the prompting techniques' effectiveness. Real-world evaluations on automotive plastic part inspection further showcase that the proposed method can also be generalized to practical unseen scenarios. The code is available at https://github.com/hustCYQ/MVP-PCLIP.",
                "authors": "Yuqi Cheng, Yunkang Cao, Guoyang Xie, Zhichao Lu, Weiming Shen",
                "citations": 0
            },
            {
                "title": "Vision-Language Models Represent Darker-Skinned Black Individuals as More Homogeneous than Lighter-Skinned Black Individuals",
                "abstract": "Vision-Language Models (VLMs) combine Large Language Model (LLM) capabilities with image processing, enabling tasks like image captioning and text-to-image generation. Yet concerns persist about their potential to amplify human-like biases, including skin tone bias. Skin tone bias, where darker-skinned individuals face more negative stereotyping than lighter-skinned individuals, is well-documented in the social sciences but remains under-explored in Artificial Intelligence (AI), particularly in VLMs. While well-documented in the social sciences, this bias remains under-explored in AI, particularly in VLMs. Using the GAN Face Database, we sampled computer-generated images of Black American men and women, controlling for skin tone variations while keeping other features constant. We then asked VLMs to write stories about these faces and compared the homogeneity of the generated stories. Stories generated by VLMs about darker-skinned Black individuals were more homogeneous than those about lighter-skinned individuals in three of four models, and Black women were consistently represented more homogeneously than Black men across all models. Interaction effects revealed a greater impact of skin tone on women in two VLMs, while the other two showed nonsignificant results, reflecting known stereotyping patterns. These findings underscore the propagation of biases from single-modality AI systems to multimodal models and highlight the need for further research to address intersectional biases in AI.",
                "authors": "Messi H.J. Lee, Soyeon Jeon",
                "citations": 0
            },
            {
                "title": "UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for Diverse Medical Imaging Modalities",
                "abstract": "Vision-Language Models (VLMs) trained via contrastive learning have achieved notable success in natural image tasks. However, their application in the medical domain remains limited due to the scarcity of openly accessible, large-scale medical image-text datasets. Existing medical VLMs either train on closed-source proprietary or relatively small open-source datasets that do not generalize well. Similarly, most models remain specific to a single or limited number of medical imaging domains, again restricting their applicability to other modalities. To address this gap, we introduce UniMed, a large-scale, open-source multi-modal medical dataset comprising over 5.3 million image-text pairs across six diverse imaging modalities: X-ray, CT, MRI, Ultrasound, Pathology, and Fundus. UniMed is developed using a data-collection framework that leverages Large Language Models (LLMs) to transform modality-specific classification datasets into image-text formats while incorporating existing image-text data from the medical domain, facilitating scalable VLM pretraining. Using UniMed, we trained UniMed-CLIP, a unified VLM for six modalities that significantly outperforms existing generalist VLMs and matches modality-specific medical VLMs, achieving notable gains in zero-shot evaluations. For instance, UniMed-CLIP improves over BiomedCLIP (trained on proprietary data) by an absolute gain of +12.61, averaged over 21 datasets, while using 3x less training data. To facilitate future research, we release UniMed dataset, training codes, and models at https://github.com/mbzuai-oryx/UniMed-CLIP.",
                "authors": "Muhammad Uzair Khattak, Shahina K. Kunhimon, Muzammal Naseer, Salman H. Khan, F. Khan",
                "citations": 0
            },
            {
                "title": "Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events",
                "abstract": "The commonsense reasoning capabilities of vision-language models (VLMs), especially in abductive reasoning and defeasible reasoning, remain poorly understood. Most benchmarks focus on typical visual scenarios, making it difficult to discern whether model performance stems from keen perception and reasoning skills, or reliance on pure statistical recall. We argue that by focusing on atypical events in videos, clearer insights can be gained on the core capabilities of VLMs. Explaining and understanding such out-of-distribution events requires models to extend beyond basic pattern recognition and regurgitation of their prior knowledge. To this end, we introduce BlackSwanSuite, a benchmark for evaluating VLMs' ability to reason about unexpected events through abductive and defeasible tasks. Our tasks artificially limit the amount of visual information provided to models while questioning them about hidden unexpected events, or provide new visual information that could change an existing hypothesis about the event. We curate a comprehensive benchmark suite comprising over 3,800 MCQ, 4,900 generative and 6,700 yes/no tasks, spanning 1,655 videos. After extensively evaluating various state-of-the-art VLMs, including GPT-4o and Gemini 1.5 Pro, as well as open-source VLMs such as LLaVA-Video, we find significant performance gaps of up to 32% from humans on these tasks. Our findings reveal key limitations in current VLMs, emphasizing the need for enhanced model architectures and training strategies.",
                "authors": "Aditya Chinchure, Sahithya Ravi, Raymond T. Ng, V. Shwartz, Boyang Li, Leonid Sigal",
                "citations": 0
            },
            {
                "title": "ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting",
                "abstract": "Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. One critical issue is bridging the gap between discrete entities in low-level observations and the abstract concepts required for effective planning. A common solution is building hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language. However, language suffers from the inability to communicate detailed spatial information. We propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from past observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, supported by real-time object tracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to tackle complex tasks that demand spatial reasoning. Experiments in Minecraft show that our approach enables agents to achieve previously unattainable tasks, with a $\\mathbf{76}\\%$ absolute improvement in open-world interaction performance. Codes and demos are now available on the project page: https://craftjarvis.github.io/ROCKET-1.",
                "authors": "Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, Yitao Liang",
                "citations": 0
            },
            {
                "title": "Continual SFT Matches Multimodal RLHF with Negative Supervision",
                "abstract": "Multimodal RLHF usually happens after supervised finetuning (SFT) stage to continually improve vision-language models' (VLMs) comprehension. Conventional wisdom holds its superiority over continual SFT during this preference alignment stage. In this paper, we observe that the inherent value of multimodal RLHF lies in its negative supervision, the logit of the rejected responses. We thus propose a novel negative supervised finetuning (nSFT) approach that fully excavates these information resided. Our nSFT disentangles this negative supervision in RLHF paradigm, and continually aligns VLMs with a simple SFT loss. This is more memory efficient than multimodal RLHF where 2 (e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The effectiveness of nSFT is rigorously proved by comparing it with various multimodal RLHF approaches, across different dataset sources, base VLMs and evaluation metrics. Besides, fruitful of ablations are provided to support our hypothesis. We hope this paper will stimulate further research to properly align large vision language models.",
                "authors": "Ke Zhu, Yu Wang, Yanpeng Sun, Qiang Chen, Jiangjiang Liu, Gang Zhang, Jingdong Wang",
                "citations": 0
            },
            {
                "title": "Attr4Vis: Revisiting Importance of Attribute Classification in Vision-Language Models for Video Recognition",
                "abstract": "Vision-language models (VLMs), pretrained on expansive datasets containing image-text pairs, have exhibited remarkable transferability across a diverse spectrum of visual tasks. The leveraging of knowledge encoded within these potent VLMs holds significant promise for the advancement of effective video recognition models. A fundamental aspect of pretrained VLMs lies in their ability to establish a crucial bridge between the visual and textual domains. In our pioneering work, we introduce the Attr4Vis framework, dedicated to exploring knowledge transfer between Video and Text modalities to bolster video recognition performance. Central to our contributions is the comprehensive revisitation of Text-to-Video classifier initialization, a critical step that refines the initialization process and streamlines the integration of our framework, particularly within existing Vision-Language Models (VLMs). Furthermore, we emphasize the adoption of dense attribute generation techniques, shedding light on their paramount importance in video analysis. By effectively encoding attribute changes over time, these techniques significantly enhance event representation and recognition within videos. In addition, we introduce an innovative Attribute Enrichment Algorithm aimed at enriching set of attributes by large language models (LLMs) like ChatGPT. Through the seamless integration of these components, Attr4Vis attains a state-of-the-art accuracy of 91.5% on the challenging Kinetics-400 dataset using the InternVideo model.",
                "authors": "Alexander Zarichkovyi, Inna V. Stetsenko",
                "citations": 0
            },
            {
                "title": "Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners",
                "abstract": "Prompt tuning, which involves training a small set of parameters, effectively enhances the pre-trained Vision-Language Models (VLMs) to downstream tasks. However, they often come at the cost of flexibility and adaptability when the tuned models are applied to different datasets or domains. In this paper, we explore capturing the task-specific information via meticulous refinement of entire VLMs, with minimal parameter adjustments. When fine-tuning the entire VLMs for specific tasks under limited supervision, overfitting and catastrophic forgetting become the defacto factors. To mitigate these issues, we propose a framework named CLIP-CITE via designing a discriminative visual-text task, further aligning the visual-text semantics in a supervision manner, and integrating knowledge distillation techniques to preserve the gained knowledge. Extensive experimental results under few-shot learning, base-to-new generalization, domain generalization, and cross-domain generalization settings, demonstrate that our method effectively enhances the performance on specific tasks under limited supervision while preserving the versatility of the VLMs on other datasets.",
                "authors": "Mushui Liu, Bozheng Li, Yunlong Yu",
                "citations": 0
            },
            {
                "title": "VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge",
                "abstract": "Generalist vision language models (VLMs) have made significant strides in computer vision, but they fall short in specialized fields like healthcare, where expert knowledge is essential. In traditional computer vision tasks, creative or approximate answers may be acceptable, but in healthcare, precision is paramount.Current large multimodal models like Gemini and GPT-4o are insufficient for medical tasks due to their reliance on memorized internet knowledge rather than the nuanced expertise required in healthcare. VLMs are usually trained in three stages: vision pre-training, vision-language pre-training, and instruction fine-tuning (IFT). IFT has been typically applied using a mixture of generic and healthcare data. In contrast, we propose that for medical VLMs, a fourth stage of specialized IFT is necessary, which focuses on medical data and includes information from domain expert models. Domain expert models developed for medical use are crucial because they are specifically trained for certain clinical tasks, e.g. to detect tumors and classify abnormalities through segmentation and classification, which learn fine-grained features of medical data$-$features that are often too intricate for a VLM to capture effectively especially in radiology. This paper introduces a new framework, VILA-M3, for medical VLMs that utilizes domain knowledge via expert models. Through our experiments, we show an improved state-of-the-art (SOTA) performance with an average improvement of ~9% over the prior SOTA model Med-Gemini and ~6% over models trained on the specific tasks. Our approach emphasizes the importance of domain expertise in creating precise, reliable VLMs for medical applications.",
                "authors": "Vishwesh Nath, Wenqi Li, Dong Yang, A. Myronenko, Mingxin Zheng, Yao Lu, Zhijian Liu, Hongxu Yin, Yee Man Law, Yucheng Tang, Pengfei Guo, Can Zhao, Ziyue Xu, Yufan He, Greg Heinrich, Stephen Aylward, Marc Edgar, Michael Zephyr, Pavlo Molchanov, B. Turkbey, Holger Roth, Daguang Xu",
                "citations": 0
            },
            {
                "title": "Rethinking VLMs and LLMs for Image Classification",
                "abstract": "Visual Language Models (VLMs) are now increasingly being merged with Large Language Models (LLMs) to enable new capabilities, particularly in terms of improved interactivity and open-ended responsiveness. While these are remarkable capabilities, the contribution of LLMs to enhancing the longstanding key problem of classifying an image among a set of choices remains unclear. Through extensive experiments involving seven models, ten visual understanding datasets, and multiple prompt variations per dataset, we find that, for object and scene recognition, VLMs that do not leverage LLMs can achieve better performance than VLMs that do. Yet at the same time, leveraging LLMs can improve performance on tasks requiring reasoning and outside knowledge. In response to these challenges, we propose a pragmatic solution: a lightweight fix involving a relatively small LLM that efficiently routes visual tasks to the most suitable model for the task. The LLM router undergoes training using a dataset constructed from more than 2.5 million examples of pairs of visual task and model accuracy. Our results reveal that this lightweight fix surpasses or matches the accuracy of state-of-the-art alternatives, including GPT-4V and HuggingGPT, while improving cost-effectiveness.",
                "authors": "Avi Cooper, Keizo Kato, Chia-Hsien Shih, Hiroaki Yamane, Kasper Vinken, Kentaro Takemoto, Taro Sunagawa, Hao-Wei Yeh, Jin Yamanaka, Ian Mason, Xavier Boix",
                "citations": 0
            },
            {
                "title": "The Bias of Harmful Label Associations in Vision-Language Models",
                "abstract": "Despite the remarkable performance of foundation vision-language models, the shared representation space for text and vision can also encode harmful label associations detrimental to fairness. While prior work has uncovered bias in vision-language models' (VLMs) classification performance across geography, work has been limited along the important axis of harmful label associations due to a lack of rich, labeled data. In this work, we investigate harmful label associations in the recently released Casual Conversations datasets containing more than 70,000 videos. We study bias in the frequency of harmful label associations across self-provided labels for age, gender, apparent skin tone, and physical adornments across several leading VLMs. We find that VLMs are $4-7$x more likely to harmfully classify individuals with darker skin tones. We also find scaling transformer encoder model size leads to higher confidence in harmful predictions. Finally, we find improvements on standard vision tasks across VLMs does not address disparities in harmful label associations.",
                "authors": "C. Hazirbas, Alicia Sun, Yonathan Efroni, Mark Ibrahim",
                "citations": 0
            },
            {
                "title": "Level Up Your Tutorials: VLMs for Game Tutorials Quality Assessment",
                "abstract": "Designing effective game tutorials is crucial for a smooth learning curve for new players, especially in games with many rules and complex core mechanics. Evaluating the effectiveness of these tutorials usually requires multiple iterations with testers who have no prior knowledge of the game. Recent Vision-Language Models (VLMs) have demonstrated significant capabilities in understanding and interpreting visual content. VLMs can analyze images, provide detailed insights, and answer questions about their content. They can recognize objects, actions, and contexts in visual data, making them valuable tools for various applications, including automated game testing. In this work, we propose an automated game-testing solution to evaluate the quality of game tutorials. Our approach leverages VLMs to analyze frames from video game tutorials, answer relevant questions to simulate human perception, and provide feedback. This feedback is compared with expected results to identify confusing or problematic scenes and highlight potential errors for developers. In addition, we publish complete tutorial videos and annotated frames from different game versions used in our tests. This solution reduces the need for extensive manual testing, especially by speeding up and simplifying the initial development stages of the tutorial to improve the final game experience.",
                "authors": "Daniele Rege Cambrin, Gabriele Scaffidi Militone, Luca Colomba, Giovanni Malnati, D. Apiletti, Paolo Garza",
                "citations": 0
            },
            {
                "title": "A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models",
                "abstract": "Recent advances in deep learning have completely transformed the domain of computational pathology (CPath). More specifically, it has altered the diagnostic workflow of pathologists by integrating foundation models (FMs) and vision-language models (VLMs) in their assessment and decision-making process. The limitations of existing deep learning approaches in CPath can be overcome by FMs through learning a representation space that can be adapted to a wide variety of downstream tasks without explicit supervision. Deploying VLMs allow pathology reports written in natural language be used as rich semantic information sources to improve existing models as well as generate predictions in natural language form. In this survey, a holistic and systematic overview of recent innovations in FMs and VLMs in CPath is presented. Furthermore, the tools, datasets and training schemes for these models are summarized in addition to categorizing them into distinct groups. This extensive survey highlights the current trends in CPath and its possible revolution through the use of FMs and VLMs in the future.",
                "authors": "Dibaloke Chanda, Milan Aryal, Nasim Yahya Soltani, Masoud Ganji",
                "citations": 0
            },
            {
                "title": "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models",
                "abstract": "Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning. Instead of adopting complex graph neural models or heuristic graph-to-text instruction de-sign, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5%-15% compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancement in graph data understanding.",
                "authors": "Qihang Ai, Jiafan Li, Jincheng Dai, Jianwu Zhou, Lemao Liu, Haiyun Jiang, Shuming Shi",
                "citations": 0
            },
            {
                "title": "Active Prompt Learning with Vision-Language Model Priors",
                "abstract": "Vision-language models (VLMs) have demonstrated remarkable zero-shot performance across various classification tasks. Nonetheless, their reliance on hand-crafted text prompts for each task hinders efficient adaptation to new tasks. While prompt learning offers a promising solution, most studies focus on maximizing the utilization of given few-shot labeled datasets, often overlooking the potential of careful data selection strategies, which enable higher accuracy with fewer labeled data. This motivates us to study a budget-efficient active prompt learning framework. Specifically, we introduce a class-guided clustering that leverages the pre-trained image and text encoders of VLMs, thereby enabling our cluster-balanced acquisition function from the initial round of active learning. Furthermore, considering the substantial class-wise variance in confidence exhibited by VLMs, we propose a budget-saving selective querying based on adaptive class-wise thresholds. Extensive experiments in active learning scenarios across nine datasets demonstrate that our method outperforms existing baselines.",
                "authors": "Hoyoung Kim, Seokhee Jin, Changhwan Sung, Jaechang Kim, Jungseul Ok",
                "citations": 0
            },
            {
                "title": "Towards Adversarially Robust Vision-Language Models: Insights from Design Choices and Prompt Formatting Techniques",
                "abstract": "Vision-Language Models (VLMs) have witnessed a surge in both research and real-world applications. However, as they are becoming increasingly prevalent, ensuring their robustness against adversarial attacks is paramount. This work systematically investigates the impact of model design choices on the adversarial robustness of VLMs against image-based attacks. Additionally, we introduce novel, cost-effective approaches to enhance robustness through prompt formatting. By rephrasing questions and suggesting potential adversarial perturbations, we demonstrate substantial improvements in model robustness against strong image-based attacks such as Auto-PGD. Our findings provide important guidelines for developing more robust VLMs, particularly for deployment in safety-critical environments.",
                "authors": "Rishika Bhagwatkar, Shravan Nayak, Reza Bayat, Alexis Roger, Daniel Z Kaplan, P. Bashivan, Irina Rish",
                "citations": 0
            },
            {
                "title": "Skip Tuning: Pre-trained Vision-Language Models are Effective and Efficient Adapters Themselves",
                "abstract": "Prompt tuning (PT) has long been recognized as an effective and efficient paradigm for transferring large pre-trained vision-language models (VLMs) to downstream tasks by learning a tiny set of context vectors. Nevertheless, in this work, we reveal that freezing the parameters of VLMs during learning the context vectors neither facilitates the transferability of pre-trained knowledge nor improves the memory and time efficiency significantly. Upon further investigation, we find that reducing both the length and width of the feature-gradient propagation flows of the full fine-tuning (FT) baseline is key to achieving effective and efficient knowledge transfer. Motivated by this, we propose Skip Tuning, a novel paradigm for adapting VLMs to downstream tasks. Unlike existing PT or adapter-based methods, Skip Tuning applies Layer-wise Skipping (LSkip) and Class-wise Skipping (CSkip) upon the FT baseline without introducing extra context vectors or adapter modules. Extensive experiments across a wide spectrum of benchmarks demonstrate the superior effectiveness and efficiency of our Skip Tuning over both PT and adapter-based methods. Code: https://github.com/Koorye/SkipTuning.",
                "authors": "Shihan Wu, Ji Zhang, Pengpeng Zeng, Lianli Gao, Jingkuan Song, Hengtao Shen",
                "citations": 0
            },
            {
                "title": "Text2Afford: Probing Object Affordance Prediction abilities of Language Models solely from Text",
                "abstract": "We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs).A growing body of literature shows that PTLMs fail inconsistently and non-intuitively, demonstrating a lack of reasoning and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances – Text2Afford, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pre-trained VLMs do not necessarily capture object affordances effectively. Through few-shot fine-tuning, we demonstrate improvement in affordance knowledge in PTLMs and VLMs. Our research contributes a novel dataset for language grounding tasks, and presents insights into LM capabilities, advancing the understanding of object affordances.",
                "authors": "Sayantan Adak, Daivik Agrawal, Animesh Mukherjee, Somak Aditya",
                "citations": 0
            },
            {
                "title": "Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad?",
                "abstract": "Recently, newly developed Vision-Language Models (VLMs), such as OpenAI's GPT-4o, have emerged, seemingly demonstrating advanced reasoning capabilities across text and image modalities. Yet, the depth of these advances in language-guided perception and abstract reasoning remains underexplored, and it is unclear whether these models can truly live up to their ambitious promises. To assess the progress and identify shortcomings, we enter the wonderland of Bongard problems, a set of classical visual reasoning puzzles that require human-like abilities of pattern recognition and abstract reasoning. While VLMs occasionally succeed in identifying discriminative concepts and solving some of the problems, they frequently falter, failing to understand and reason about visual concepts. Surprisingly, even elementary concepts that may seem trivial to humans, such as simple spirals, pose significant challenges. Moreover, even when asked to explicitly focus on and analyze these concepts, they continue to falter, suggesting not only a lack of understanding of these elementary visual concepts but also an inability to generalize to unseen concepts. These observations underscore the current limitations of VLMs, emphasize that a significant gap remains between human-like visual reasoning and machine cognition, and highlight the ongoing need for innovation in this area.",
                "authors": "Antonia Wüst, T. Tobiasch, Lukas Helff, D. Dhami, Constantin A. Rothkopf, Kristian Kersting",
                "citations": 0
            },
            {
                "title": "Interpreting COVID Lateral Flow Tests’ Results with Foundation Models",
                "abstract": "Lateral flow tests (LFTs) enable rapid, low-cost testing for health conditions including Covid, pregnancy, HIV, and malaria. Automated readers of LFT results can yield many benefits including empowering blind people to independently learn about their health and accelerating data entry for large-scale monitoring (e.g., for pandemics such as Covid) by using only a single photograph per LFT test. Accordingly, we explore the abilities of modern foundation vision language models (VLMs) in interpreting such tests. To enable this analysis, we first create a new labeled dataset with hierarchical segmentations of each LFT test and its nested test result window. We call this dataset LFT-Grounding. Next, we benchmark eight modern VLMs in zero-shot settings for analyzing these images. We demonstrate that current VLMs frequently fail to correctly identify the type of LFT test, interpret the test results, locate the nested result window of the LFT tests, and recognize LFT tests when they partially obfuscated. To facilitate community-wide progress towards automated LFT reading, we publicly release our dataset at https://iamstuti.github.io/lft_grounding_foundation_models/",
                "authors": "Stuti Pandey, Josh Myers-Dean, Jarek Reynolds, D. Gurari",
                "citations": 0
            },
            {
                "title": "Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks",
                "abstract": "Gender bias in vision-language models (VLMs) can reinforce harmful stereotypes and discrimination. In this paper, we focus on mitigating gender bias towards vision-language tasks. We identify object hallucination as the essence of gender bias in VLMs. Existing VLMs tend to focus on salient or familiar attributes in images but ignore contextualized nuances. Moreover, most VLMs rely on the co-occurrence between specific objects and gender attributes to infer the ignored features, ultimately resulting in gender bias. We propose GAMA, a task-agnostic generation framework to mitigate gender bias. GAMA consists of two stages: narrative generation and answer inference. During narrative generation, GAMA yields all-sided but gender-obfuscated narratives, which prevents premature concentration on localized image features, especially gender attributes. During answer inference, GAMA integrates the image, generated narrative, and a task-specific question prompt to infer answers for different vision-language tasks. This approach allows the model to rethink gender attributes and answers. We conduct extensive experiments on GAMA, demonstrating its debiasing and generalization ability.",
                "authors": "Yunqi Zhang, Songda Li, Chunyuan Deng, Luyi Wang, Hui Zhao",
                "citations": 0
            },
            {
                "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
                "abstract": "Recent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow. We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream. Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. In contrast, models trained for image and text generation rely on a single token that acts as a narrow gate for the visual information. We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.",
                "authors": "Alessandro Serra, Francesco Ortu, Emanuele Panizon, L. Valeriani, Lorenzo Basile, A. Ansuini, Diego Doimo, Alberto Cazzaniga",
                "citations": 0
            },
            {
                "title": "Do Visual-Language Maps Capture Latent Semantics?",
                "abstract": "Visual-language models (VLMs) have recently been introduced in robotic mapping by using the latent representations, i.e., embeddings, of the VLMs to represent the natural language semantics in the map. The main benefit is moving beyond a small set of human-created labels toward open-vocabulary scene understanding. While there is anecdotal evidence that maps built this way support downstream tasks, such as navigation, rigorous analysis of the quality of the maps using these embeddings is lacking. We investigate two critical properties of map quality: queryability and consistency. The evaluation of queryability addresses the ability to retrieve information from the embeddings. We investigate two aspects of consistency: intra-map consistency and inter-map consistency. Intra-map consistency captures the ability of the embeddings to represent abstract semantic classes, and inter-map consistency captures the generalization properties of the representation. In this paper, we propose a way to analyze the quality of maps created using VLMs, which forms an open-source benchmark to be used when proposing new open-vocabulary map representations. We demonstrate the benchmark by evaluating the maps created by two state-of-the-art methods, VLMaps and OpenScene, using two encoders, LSeg and OpenSeg, using real-world data from the Matterport3D data set. We find that OpenScene outperforms VLMaps with both encoders, and LSeg outperforms OpenSeg with both methods.",
                "authors": "Matti Pekkanen, Tsvetomila Mihaylova, Francesco Verdoja, Ville Kyrki",
                "citations": 0
            },
            {
                "title": "LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models",
                "abstract": "Open-universe 3D layout generation arranges unlabeled 3D assets conditioned on language instruction. Large language models (LLMs) struggle with generating physically plausible 3D scenes and adherence to input instructions, particularly in cluttered scenes. We introduce LayoutVLM, a framework and scene layout representation that exploits the semantic knowledge of Vision-Language Models (VLMs) and supports differentiable optimization to ensure physical plausibility. LayoutVLM employs VLMs to generate two mutually reinforcing representations from visually marked images, and a self-consistent decoding process to improve VLMs spatial planning. Our experiments show that LayoutVLM addresses the limitations of existing LLM and constraint-based approaches, producing physically plausible 3D layouts better aligned with the semantic intent of input language instructions. We also demonstrate that fine-tuning VLMs with the proposed scene layout representation extracted from existing scene datasets can improve performance.",
                "authors": "Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, Jiajun Wu",
                "citations": 0
            },
            {
                "title": "VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and Lexical Alterations",
                "abstract": "Despite their remarkable successes, state-of-the-art language models face challenges in grasping certain important semantic details. This paper introduces the VISLA (Variance and Invariance to Semantic and Lexical Alterations) benchmark, designed to evaluate the semantic and lexical understanding of language models. VISLA presents a 3-way semantic (in)equivalence task with a triplet of sentences associated with an image, to evaluate both vision-language models (VLMs) and unimodal language models (ULMs). An evaluation involving 34 VLMs and 20 ULMs reveals surprising difficulties in distinguishing between lexical and semantic variations. Spatial semantics encoded by language models also appear to be highly sensitive to lexical information. Notably, text encoders of VLMs demonstrate greater sensitivity to semantic and lexical variations than unimodal text encoders. Our contributions include the unification of image-to-text and text-to-text retrieval tasks, an off-the-shelf evaluation without fine-tuning, and assessing LMs' semantic (in)variance in the presence of lexical alterations. The results highlight strengths and weaknesses across diverse vision and unimodal language models, contributing to a deeper understanding of their capabilities. % VISLA enables a rigorous evaluation, shedding light on language models' capabilities in handling semantic and lexical nuances. Data and code will be made available at https://github.com/Sri-Harsha/visla_benchmark.",
                "authors": "Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, E. Milios, Sageev Oore, Hassan Sajjad",
                "citations": 0
            },
            {
                "title": "Revolutionizing Science Education Evaluation Using a Vision Language Model of Effective Assessment and Supervision",
                "abstract": "Implementing the Independent Curriculum in Indonesia presents challenges in educational assessment, especially in efficiently evaluating students' essay responses. This study aims to develop and test an educational evaluation model based on artificial intelligence (AI) technology, especially the vision language model, which can automate and improve the process of assessing student learning outcomes in science. This study explores the potential of Vision Language Models (VLMs) as an innovative solution. This study uses a mixed sequential explanatory method. The subjects in this study were junior high school students. The data collection method used interviews. Data collection instruments with questionnaires. Data analysis techniques used were qualitative, quantitative, descriptive analysis, and inferential statistics. The study results are that integrating VLMs increases the efficiency and objectivity of assessment. This study concludes that VLMs can reduce teacher workload, improve feedback, and show synergy between technology and curriculum reform in the Independent Curriculum era. The implications of this study are very significant for the development of science and technology education; the use of vision language models (Vision-Language Models) in evaluating science education can increase the accuracy and objectivity in assessing student learning outcomes.",
                "authors": "Irfan Ananda Ismail, Mawardi, Qadriati, Munadia Insani, Khairil Arif",
                "citations": 0
            },
            {
                "title": "AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models",
                "abstract": "Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks, particularly targeted adversarial images that manipulate the model to generate harmful content specified by the adversary. Current attack methods rely on predefined target labels to create targeted adversarial attacks, which limits their scalability and applicability for large-scale robustness evaluations. In this paper, we propose AnyAttack , a self-supervised framework that generates targeted adversarial images for VLMs without label supervision, allowing any image to serve as a target for the attack . To address the limitation of existing methods that require label supervision, we introduce a contrastive loss that trains a generator on a large-scale unlabeled image dataset, LAION-400M dataset, for generating targeted adversarial noise. This large-scale pre-training endows our method with powerful transferability across a wide range of VLMs. Extensive experiments on five mainstream open-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks (image-text retrieval, multimodal classification, and image captioning) demonstrate the effectiveness of our attack. Additionally, we successfully transfer AnyAttack to multiple commercial VLMs, including Google’s Gemini, Claude’s Sonnet, and Microsoft’s Copilot. These re-sults reveal an unprecedented risk to VLMs, highlighting the need for effective countermeasures.",
                "authors": "Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Jitao Sang, Dit-Yan Yeung",
                "citations": 0
            },
            {
                "title": "Granular Privacy Control for Geolocation with Vision Language Models",
                "abstract": "Vision Language Models (VLMs) are rapidly advancing in their capability to answer information-seeking questions. As these models are widely deployed in consumer applications, they could lead to new privacy risks due to emergent abilities to identify people in photos, geolocate images, etc. As we demonstrate, somewhat surprisingly, current open-source and proprietary VLMs are very capable image geolocators, making widespread geolocation with VLMs an immediate privacy risk, rather than merely a theoretical future concern. As a first step to address this challenge, we develop a new benchmark, GPTGeoChat, to test the capability of VLMs to moderate geolocation dialogues with users. We collect a set of 1,000 image geolocation conversations between in-house annotators and GPT-4v, which are annotated with the granularity of location information revealed at each turn. Using this new dataset we evaluate the ability of various VLMs to moderate GPT-4v geolocation conversations by determining when too much location information has been revealed. We find that custom fine-tuned models perform on par with prompted API-based models when identifying leaked location information at the country or city level, however fine-tuning on supervised data appears to be needed to accurately moderate finer granularities, such as the name of a restaurant or building.",
                "authors": "Ethan Mendes, Yang Chen, James Hays, Sauvik Das, Wei Xu, Alan Ritter",
                "citations": 0
            },
            {
                "title": "Beyond Visual Understanding: Introducing PARROT-360V for Vision Language Model Benchmarking",
                "abstract": "Current benchmarks for evaluating Vision Language Models (VLMs) often fall short in thoroughly assessing model abilities to understand and process complex visual and textual content. They typically focus on simple tasks that do not require deep reasoning or the integration of multiple data modalities to solve an original problem. To address this gap, we introduce the PARROT-360V Benchmark, a novel and comprehensive benchmark featuring 2487 challenging visual puzzles designed to test VLMs on complex visual reasoning tasks. We evaluated leading models: GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro, using PARROT-360V to assess their capabilities in combining visual clues with language skills to solve tasks in a manner akin to human problem-solving. Our findings reveal a notable performance gap: state-of-the-art models scored between 28 to 56 percentage on our benchmark, significantly lower than their performance on popular benchmarks. This underscores the limitations of current VLMs in handling complex, multi-step reasoning tasks and highlights the need for more robust evaluation frameworks to advance the field.",
                "authors": "Harsha Vardhan Khurdula, Basem Rizk, Indus Khaitan, Janit Anjaria, Aviral Srivastava, Rajvardhan Khaitan",
                "citations": 0
            },
            {
                "title": "VLMs meet UDA: Boosting Transferability of Open Vocabulary Segmentation with Unsupervised Domain Adaptation",
                "abstract": "Segmentation models are typically constrained by the categories defined during training. To address this, researchers have explored two independent approaches: adapting Vision-Language Models (VLMs) and leveraging synthetic data. However, VLMs often struggle with granularity, failing to disentangle fine-grained concepts, while synthetic data-based methods remain limited by the scope of available datasets. This paper proposes enhancing segmentation accuracy across diverse domains by integrating Vision-Language reasoning with key strategies for Unsupervised Domain Adaptation (UDA). First, we improve the fine-grained segmentation capabilities of VLMs through multi-scale contextual data, robust text embeddings with prompt augmentation, and layer-wise fine-tuning in our proposed Foundational-Retaining Open Vocabulary Semantic Segmentation (FROVSS) framework. Next, we incorporate these enhancements into a UDA framework by employing distillation to stabilize training and cross-domain mixed sampling to boost adaptability without compromising generalization. The resulting UDA-FROVSS framework is the first UDA approach to effectively adapt across domains without requiring shared categories.",
                "authors": "Roberto Alcover-Couso, Marcos Escudero-Viñolo, Juan C. Sanmiguel, Jesús Bescós",
                "citations": 0
            },
            {
                "title": "Probing Visual Language Priors in VLMs",
                "abstract": "Despite recent advances in Vision-Language Models (VLMs), many still over-rely on visual language priors present in their training data rather than true visual reasoning. To examine the situation, we introduce ViLP, a visual question answering (VQA) benchmark that pairs each question with three potential answers and three corresponding images: one image whose answer can be inferred from text alone, and two images that demand visual reasoning. By leveraging image generative models, we ensure significant variation in texture, shape, conceptual combinations, hallucinated elements, and proverb-based contexts, making our benchmark images distinctly out-of-distribution. While humans achieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4 achieves only 66.17% on ViLP. To alleviate this, we propose a self-improving framework in which models generate new VQA pairs and images, then apply pixel-level and semantic corruptions to form\"good-bad\"image pairs for self-training. Our training objectives compel VLMs to focus more on actual visual inputs and have demonstrated their effectiveness in enhancing the performance of open-source VLMs, including LLaVA-v1.5 and Cambrian.",
                "authors": "Tiange Luo, Ang Cao, Gunhee Lee, Justin Johnson, Honglak Lee",
                "citations": 0
            },
            {
                "title": "Are VLMs Really Blind",
                "abstract": "Vision Language Models excel in handling a wide range of complex tasks, including Optical Character Recognition (OCR), Visual Question Answering (VQA), and advanced geometric reasoning. However, these models fail to perform well on low-level basic visual tasks which are especially easy for humans. Our goal in this work was to determine if these models are truly\"blind\"to geometric reasoning or if there are ways to enhance their capabilities in this area. Our work presents a novel automatic pipeline designed to extract key information from images in response to specific questions. Instead of just relying on direct VQA, we use question-derived keywords to create a caption that highlights important details in the image related to the question. This caption is then used by a language model to provide a precise answer to the question without requiring external fine-tuning.",
                "authors": "Ayush Singh, Mansi Gupta, Shivank Garg",
                "citations": 0
            },
            {
                "title": "Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models",
                "abstract": "Foundation Vision Language Models (VLMs) exhibit strong capabilities in multi-modal representation learning, comprehension, and reasoning. By injecting action components into the VLMs, Vision-Language-Action Models (VLAs) can be naturally formed and also show promising performance. Existing work has demonstrated the effectiveness and generalization of VLAs in multiple scenarios and tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since existing VLAs differ in their backbones, action-prediction formulations, data distributions, and training recipes. This leads to a missing piece for a systematic understanding of the design choices of VLAs. In this work, we disclose the key factors that significantly influence the performance of VLA and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we need VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets, and toolkits, along with detailed training and evaluation recipes at: robovlms.github.io.",
                "authors": "Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu",
                "citations": 0
            },
            {
                "title": "ScVLM: Enhancing Vision-Language Model for Safety-Critical Event Understanding",
                "abstract": "Accurately identifying, understanding and describing traffic safety-critical events (SCEs), including crashes, tire strikes, and near-crashes, is crucial for advanced driver assistance systems, automated driving systems, and traffic safety. As SCEs are rare events, most general vision-language models (VLMs) have not been trained sufficiently to link SCE videos and narratives, which could lead to hallucinations and missing key safety characteristics. Here, we introduce ScVLM, a novel hybrid methodology that integrates supervised and contrastive learning techniques to classify the severity and types of SCEs, as well as to generate narrative descriptions of SCEs. This approach utilizes classification to enhance VLMs' comprehension of driving videos and improve the rationality of event descriptions. The proposed approach is trained on and evaluated by more than 8,600 SCEs from the Second Strategic Highway Research Program Naturalistic Driving Study dataset, the largest publicly accessible driving dataset with videos and SCE annotations. The results demonstrate the superiority of the proposed approach in generating contextually accurate event descriptions and mitigating VLM hallucinations. The code will be available at https://github.com/datadrivenwheels/ScVLM.",
                "authors": "Liang Shi, Boyu Jiang, Tong Zeng, Feng Guo",
                "citations": 0
            },
            {
                "title": "Adversarial Attacks on Vision-Language Model-Empowered Chatbots in Consumer Electronics",
                "abstract": "Artificial Intelligence-Generated Content (AIGC) technology has revolutionized content creation, distribution, and engagement in the consumer electronics sector, propelling its applications to unprecedented heights. Within this landscape, AIGC-driven conversational agents, exemplified by renowned chatbots like ChatGPT, have gained widespread popularity globally. These advanced conversational agents are instrumental in significantly enhancing user efficiency and overall experience within consumer electronics applications. However, with the increasing integration of vision-language models (VLMs, a representative of AIGC) in consumer electronics, the vulnerability of chatbots to adversarial attacks has become a critical concern. This paper investigates and analyzes the susceptibility of VLMs-empowered chatbots to adversarial manipulation, particularly in the context of consumer electronics applications. The study employs a comprehensive approach, combining vision and language modalities, to explore potential attack vectors and vulnerabilities. Specifically, we designed three adversarial attacks, which all exploited the insufficient alignment of VLMs on multi-modal data to implement effective attacks and make chatbots output harmful content. A series of experiments demonstrate the efficacy of adversarial attacks on three popular chatbot systems, revealing vulnerabilities that may compromise the reliability and security of these systems in real-world scenarios. The findings emphasize the importance of robust defenses against adversarial attacks in VLMs-driven chatbots, urging the development of enhanced security measures to safeguard users and prevent malicious exploitation of consumer electronics. Our data and code are available at https://github.com/yxc0731/VLM-Adversarial-Attacks.",
                "authors": "Yingjia Shang, Ieee Jiawen Kang Senior Member, Ieee M. Shamim Hossain Senior Member, Yi Wu, M. Shamim, Zhijun Liu",
                "citations": 0
            },
            {
                "title": "WaveDN: A Wavelet-based Training-free Zero-shot Enhancement for Vision-Language Models",
                "abstract": "Vision-Language Models (VLMs) built on contrastive learning, such as CLIP, demonstrate great transferability and excel in downstream tasks like zero-shot classification and retrieval. To further enhance the performance of VLMs, existing methods have introduced additional parameter modules or fine-tuned VLMs on downstream datasets. However, these methods often fall short in scenarios where labeled data for downstream tasks is either unavailable or insufficient for fine-tuning, and the training of additional parameter modules may considerably impair the existing transferability of VLMs. To alleviate this issue, we introduce WaveDN, a wavelet-based distribution normalization method that can boost the VLMs’ performance on downstream tasks without parametric modules or labeled data. Initially, wavelet distributions are extracted from the embeddings of the sampled, unlabeled test samples. Subsequently, WaveDN conducts a hierarchical normalization across the wavelet coefficients of all embeddings, thereby incorporating the distributional characteristics of the test data. Finally, the normalized embeddings are reconstructed via inverse wavelet transformation, facilitating the computation of similarity metrics between the samples. Through extensive experiments on two downstream tasks, using a total of 14 datasets covering text-image and text-audio modal data, WaveDN has demonstrated superiority compared to state-of-the-art methods.",
                "authors": "Jiulin Li, Mengyu Yang, Ye Tian, Lanshan Zhang, Yongchun Lu, Jice Liu, Wendong Wang",
                "citations": 0
            },
            {
                "title": "Development and validation of a virtual learning management system usability questionnaire: A case study.",
                "abstract": "BACKGROUND\nThe post-pandemic era has seen a surge in the popularity of Virtual Learning Management Systems (VLMS). However, there is a noticeable lack of tools to measure the usability of these systems. As technology evolves, user needs change, necessitating updated tools for system evaluation.\n\n\nOBJECTIVE\nThis study aims to develop and validate a VLMS usability questionnaire, specifically designed to assess the usability of a university learning management system.\n\n\nMETHODS\nThe VLMS usability tool was systematically developed based on relevant domains identified in existing literature and expert opinions. It was then tested for face validity, content validity, and reliability. In a case study, the tool was distributed among 200 students from a Medical Sciences university who had used the Navid VLMS system.\n\n\nRESULTS\nSemi-structured interviews with experts were analyzed using directed content analysis, resulting in 21 items categorized into four domains: effectiveness, reliability, learnability, and security. The content validity index and ratio were 0.939 and 0.976, respectively. The Intra Class Correlation (ICC) estimates for each section of the questionnaire ranged from 0.8-0.9, indicating high reliability. Cronbach's alpha was 0.97, suggesting excellent internal consistency. The case study results showed that the Navid platform achieved an average usability score of 70.36, with a standard deviation of 10.6, indicating moderate to high usability.\n\n\nCONCLUSIONS\nThe VLMS usability tool is a valid and reliable instrument for assessing the usability of the Navid learning management system. It can be used to improve the usability of the Navid system and serve as a benchmark for assessing the usability of other similar VLMSs.",
                "authors": "Mohamad Sadegh Ghasemi, Raheleh Aghajafari, Jamileh Abolaghasemi, M. K. Danesh, Ehsan Garosi",
                "citations": 0
            },
            {
                "title": "Specialist vision-language models for clinical ophthalmology",
                "abstract": "Clinicians spend a significant amount of time reviewing medical images and transcribing their findings regarding patient diagnosis, referral and treatment in text form. Vision-language models (VLMs), which automatically interpret images and summarize their findings as text, have enormous potential to alleviate clinical workloads and increase patient access to high-quality medical care. While foundational models have stirred considerable interest in the medical community, it is unclear whether their general capabilities translate to real-world clinical utility. In this work, we show that foundation VLMs markedly underperform compared to practicing ophthalmologists on specialist tasks crucial to the care of patients with age-related macular degeneration (AMD). To address this, we initially identified the essential capabilities required for image-based clinical decision-making, and then developed a curriculum to selectively train VLMs in these skills. The resulting model, RetinaVLM, can be instructed to write reports that significantly outperform those written by leading foundation medical VLMs in disease staging (F1 score of 0.63 vs. 0.11) and patient referral (0.67 vs. 0.39), and approaches the diagnostic performance of junior ophthalmologists (who achieve 0.77 and 0.78 on the respective tasks). Furthermore, in a reader study involving two senior ophthalmologists with up to 32 years of experience, RetinaVLM's reports were found to be similarly correct (78.6% vs. 82.1%) and complete (both 78.6%) as reports written by junior ophthalmologists with up to 10 years of experience. These results demonstrate that our curriculum-based approach provides a blueprint for specializing generalist foundation medical VLMs to handle real-world clinical tasks.",
                "authors": "Robbie Holland, Thomas R. P. Taylor, Christopher Holmes, Sophie Riedl, Julia Mai, Maria Patsiamanidi, Dimitra Mitsopoulou, Paul Hager, Philip Müller, H. Scholl, Hrvoje Bogunovi'c, U. Schmidt-Erfurth, Daniel Rueckert, S. Sivaprasad, A. Lotery, M. Menten",
                "citations": 0
            },
            {
                "title": "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement",
                "abstract": "Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between objects. This complexity and diversity in SGG leads to underrepresentation, where parts of triplet labels are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose integrating the pretrained Vision-language Models to enhance representation. However, due to the gap between pretraining and SGG, direct inference of pretrained VLMs on SGG leads to severe bias, which stems from the imbalanced predicates distribution in the pretraining language set. To alleviate the bias, we introduce a novel LM Estimation to approximate the unattainable predicates distribution. Finally, we ensemble the debiased VLMs with SGG models to enhance the representation, where we design a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our training-free method effectively addresses the predicates bias in pretrained VLMs, enhances SGG’s representation, and significantly improve the performance.",
                "authors": "Yuxuan Wang, Xiaoyuan Liu",
                "citations": 0
            },
            {
                "title": "H2OVL-Mississippi Vision Language Models Technical Report",
                "abstract": "Smaller vision-language models (VLMs) are becoming increasingly important for privacy-focused, on-device applications due to their ability to run efficiently on consumer hardware for processing enterprise commercial documents and images. These models require strong language understanding and visual capabilities to enhance human-machine interaction. To address this need, we present H2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs using 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny model with 0.8 billion parameters that specializes in text recognition, achieving state of the art performance on the Text Recognition portion of OCRBench and surpassing much larger models in this area. Additionally, we are releasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use cases, exhibiting highly competitive metrics across various academic benchmarks. Both models build upon our prior work with H2O-Danube language models, extending their capabilities into the visual domain. We release them under the Apache 2.0 license, making VLMs accessible to everyone, democratizing document AI and visual LLMs.",
                "authors": "Shaikat M. Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, SriSatish Ambati",
                "citations": 0
            },
            {
                "title": "Prompt and Prejudice",
                "abstract": "This paper investigates the impact of using first names in Large Language Models (LLMs) and Vision Language Models (VLMs), particularly when prompted with ethical decision-making tasks. We propose an approach that appends first names to ethically annotated text scenarios to reveal demographic biases in model outputs. Our study involves a curated list of more than 300 names representing diverse genders and ethnic backgrounds, tested across thousands of moral scenarios. Following the auditing methodologies from social sciences we propose a detailed analysis involving popular LLMs/VLMs to contribute to the field of responsible AI by emphasizing the importance of recognizing and mitigating biases in these systems. Furthermore, we introduce a novel benchmark, the Pratical Scenarios Benchmark (PSB), designed to assess the presence of biases involving gender or demographic prejudices in everyday decision-making scenarios as well as practical scenarios where an LLM might be used to make sensible decisions (e.g., granting mortgages or insurances). This benchmark allows for a comprehensive comparison of model behaviors across different demographic categories, highlighting the risks and biases that may arise in practical applications of LLMs and VLMs.",
                "authors": "Lorenzo Berlincioni, Luca Cultrera, Federico Becattini, Marco Bertini, A. Bimbo",
                "citations": 0
            },
            {
                "title": "Microscopic Hematological Image Classification with Captions Using Few-Shot Learning in Data-Scarce Environments",
                "abstract": "Recent advancements in vision-language models (VLMs) have opened new possibilities for few-shot learning in medical image analysis. This study explores the application of VLMs for microscopic blood cell image classification using descriptive captions. We investigate the performance of open-source models like CLIP, PLIP, and BiomedCLIP on this task, focusing on prompt engineering techniques and their impact on classification accuracy. Our experiments show that Biomed-CLIP consistently outperforms other models in few-shot learning scenarios, delivering superior results using only 10% of the training data, compared to traditional CNN-based approaches that require 70% to 80% of the training data to achieve similar performance. While BiomedCLIP showed lower accuracy in zero-shot learning, it demonstrated substantial improvements in few-shot settings. By leveraging contextual prompts, our approach effectively classifies both normal and malignant blood cells across various categories. These findings suggest that few-shot learning with VLMs can mitigate data scarcity issues in hematological image analysis, providing a promising methodology for early diagnosis and treatment of hematological disorders.",
                "authors": "Tanviben Patel, Hoda El-Sayed, Md. Kamruzzaman Sarker",
                "citations": 0
            },
            {
                "title": "Jailbreak Large Vision-Language Models Through Multi-Modal Linkage",
                "abstract": "With the significant advancement of Large Vision-Language Models (VLMs), concerns about their potential misuse and abuse have grown rapidly. Previous studies have highlighted VLMs' vulnerability to jailbreak attacks, where carefully crafted inputs can lead the model to produce content that violates ethical and legal standards. However, existing methods struggle against state-of-the-art VLMs like GPT-4o, due to the over-exposure of harmful content and lack of stealthy malicious guidance. In this work, we propose a novel jailbreak attack framework: Multi-Modal Linkage (MML) Attack. Drawing inspiration from cryptography, MML utilizes an encryption-decryption process across text and image modalities to mitigate over-exposure of malicious information. To align the model's output with malicious intent covertly, MML employs a technique called\"evil alignment\", framing the attack within a video game production scenario. Comprehensive experiments demonstrate MML's effectiveness. Specifically, MML jailbreaks GPT-4o with attack success rates of 97.80% on SafeBench, 98.81% on MM-SafeBench and 99.07% on HADES-Dataset. Our code is available at https://github.com/wangyu-ovo/MML",
                "authors": "Yu Wang, Xiaofei Zhou, Yichen Wang, Geyuan Zhang, Tianxing He",
                "citations": 0
            },
            {
                "title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models",
                "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range modeling. Moreover, existing open-source VLMs lack systematic exploration into extending their context length, and commercial models often provide limited details. To tackle this, we aim to establish an effective solution that enhances long context performance of VLMs while preserving their capacities in short context scenarios. Towards this goal, we make the best design choice through extensive experiment settings from data curation to context window extending and utilizing: (1) we analyze data sources and length distributions to construct ETVLM - a data recipe to balance the performance across scenarios; (2) we examine existing position extending methods, identify their limitations and propose M-RoPE++ as an enhanced approach; we also choose to solely instruction-tune the backbone with mixed-source data; (3) we discuss how to better utilize extended context windows and propose hybrid-resolution training. Built on the Qwen-VL series model, we propose Giraffe, which is effectively extended to 128K lengths. Evaluated on extensive long context VLM benchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves state-of-the-art performance among similarly sized open-source long VLMs and is competitive with commercial model GPT-4V. We will open-source the code, data, and models.",
                "authors": "Mukai Li, Lei Li, Shansan Gong, Qi Liu",
                "citations": 0
            },
            {
                "title": "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling",
                "abstract": "This study explores replacing Transformers in Visual Language Models (VLMs) with Mamba, a recent structured state space model (SSM) that demonstrates promising performance in sequence modeling. We test models up to 3B parameters under controlled conditions, showing that Mamba-based VLMs outperforms Transformers-based VLMs in captioning, question answering, and reading comprehension. However, we find that Transformers achieve greater performance in visual grounding and the performance gap widens with scale. We explore two hypotheses to explain this phenomenon: 1) the effect of task-agnostic visual encoding on the updates of the hidden states, and 2) the difficulty in performing visual grounding from the perspective of in-context multimodal retrieval. Our results indicate that a task-aware encoding yields minimal performance gains on grounding, however, Transformers significantly outperform Mamba at in-context multimodal retrieval. Overall, Mamba shows promising performance on tasks where the correct output relies on a summary of the image but struggles when retrieval of explicit information from the context is required.",
                "authors": "Georgios Pantazopoulos, Malvina Nikandrou, Alessandro Suglia, Oliver Lemon, Arash Eshghi",
                "citations": 0
            }
        ]
    }
]