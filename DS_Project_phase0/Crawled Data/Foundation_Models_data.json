[
    {
        "papers": [
            {
                "title": "On the Opportunities and Risks of Foundation Models",
                "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
                "authors": "Rishi Bommasani, Drew A. Hudson, E. Adeli, R. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, E. Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, M. Doumbouya, Esin Durmus, Stefano Ermon, J. Etchemendy, Kawin Ethayarajh, L. Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, S. Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, G. Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, M. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, J. Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, E. Mitchell, Zanele Munyikwa, Suraj Nair, A. Narayan, D. Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, H. Nilforoshan, J. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Papadimitriou, J. Park, C. Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, K. Srinivasan, Alex Tamkin, Rohan Taori, A. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, M. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang",
                "citations": 3601
            },
            {
                "title": "Code Llama: Open Foundation Models for Code",
                "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",
                "authors": "Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, J. Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Cantón Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D'efossez, Jade Copet, F. Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve",
                "citations": 1441
            },
            {
                "title": "Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
                "abstract": "The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (Intern VL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models.",
                "authors": "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai",
                "citations": 450
            },
            {
                "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
                "abstract": "Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.",
                "authors": "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu",
                "citations": 1112
            },
            {
                "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
                "abstract": "ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \\textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \\url{https://github.com/microsoft/visual-chatgpt}.",
                "authors": "Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan",
                "citations": 551
            },
            {
                "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
                "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.",
                "authors": "Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Fanchao Qi, Yao Fu, Maosong Sun, Junxian He",
                "citations": 405
            },
            {
                "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
                "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",
                "authors": "Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-yue Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao",
                "citations": 306
            },
            {
                "title": "LLaMA: Open and Efficient Foundation Language Models",
                "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, M. Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",
                "citations": 9810
            },
            {
                "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
                "authors": "Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, D. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, A. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. Korenev, Punit Singh Koura, M. Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, M. Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom",
                "citations": 8963
            },
            {
                "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
                "abstract": "Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https://github.com/ruixiangcui/AGIEval.",
                "authors": "Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, A. Saied, Weizhu Chen, Nan Duan",
                "citations": 379
            },
            {
                "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine",
                "abstract": "Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities of fine-tuned models. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. We build on a prior study of GPT-4's capabilities on medical challenge benchmarks in the absence of special training. Rather than using simple prompting to highlight the model's out-of-the-box capabilities, we perform a systematic exploration of prompt engineering. We find that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical benchmarks. The prompting methods we explore are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Our experimental design carefully controls for overfitting during the prompt engineering process. We introduce Medprompt, based on a composition of several prompting strategies. With Medprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms leading specialist models such as Med-PaLM 2 by a significant margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27% reduction in error rate on the MedQA dataset over the best methods to date achieved with specialist models and surpasses a score of 90% for the first time. Beyond medical problems, we show the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology.",
                "authors": "Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicoló Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, S. McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, Eric Horvitz",
                "citations": 227
            },
            {
                "title": "Foundation models for generalist medical artificial intelligence",
                "abstract": null,
                "authors": "Michael Moor, Oishi Banerjee, Zahra F H Abad, H. Krumholz, J. Leskovec, E. Topol, P. Rajpurkar",
                "citations": 689
            },
            {
                "title": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
                "abstract": "Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternImage reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like ViTs. The effectiveness of our model is proven on challenging benchmarks including ImageNet, COCO, andADE20K. It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs.",
                "authors": "Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Y. Qiao",
                "citations": 519
            },
            {
                "title": "InternVideo: General Video Foundation Models via Generative and Discriminative Learning",
                "abstract": "The foundation models have recently shown excellent performance on a variety of downstream tasks in computer vision. However, most existing vision foundation models simply focus on image-level pretraining and adpation, which are limited for dynamic and complex video-level understanding tasks. To fill the gap, we present general video foundation models, InternVideo, by taking advantage of both generative and discriminative self-supervised video learning. Specifically, InternVideo efficiently explores masked video modeling and video-language contrastive learning as the pretraining objectives, and selectively coordinates video representations of these two complementary frameworks in a learnable manner to boost various video applications. Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets from extensive tasks including video action recognition/detection, video-language alignment, and open-world video applications. Especially, our methods can obtain 91.1% and 77.2% top-1 accuracy on the challenging Kinetics-400 and Something-Something V2 benchmarks, respectively. All of these results effectively show the generality of our InternVideo for video understanding. The code will be released at https://github.com/OpenGVLab/InternVideo .",
                "authors": "Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, Yu Qiao",
                "citations": 256
            },
            {
                "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
                "abstract": "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.",
                "authors": "Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kaichao Zhang, Cheng Ji, Qi Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, P. Xie, Caiming Xiong, Jian Pei, Philip S. Yu, Lichao Sun Michigan State University, B. University, Lehigh University, M. University, Nanyang Technological University, University of California at San Diego, Duke University, U. Chicago, Salesforce Research",
                "citations": 417
            },
            {
                "title": "A Survey of Hallucination in Large Foundation Models",
                "abstract": "Hallucination in a foundation model (FM) refers to the generation of content that strays from factual reality or includes fabricated information. This survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``Large'' Foundation Models (LFMs). The paper classifies various types of hallucination phenomena that are specific to LFMs and establishes evaluation criteria for assessing the extent of hallucination. It also examines existing strategies for mitigating hallucination in LFMs and discusses potential directions for future research in this area. Essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in LFMs.",
                "authors": "Vipula Rawte, A. Sheth, Amitava Das",
                "citations": 273
            },
            {
                "title": "Multimodal Foundation Models: From Specialists to General-Purpose Assistants",
                "abstract": "This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in computer vision and vision-language multimodal communities who are eager to learn the basics and recent advances in multimodal foundation models.",
                "authors": "Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao",
                "citations": 181
            },
            {
                "title": "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs",
                "abstract": "Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce TaskMatrix.AI as a new AI ecosystem that connects foundation models with millions of APIs for task completion. Unlike most previous work that aimed to improve a single AI model, TaskMatrix.AI focuses more on using existing foundation models (as a brain-like central system) and APIs of other AI models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next.",
                "authors": "Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yangyiwen Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yuntao Wang, Linjun Shou, Ming Gong, Nan Duan",
                "citations": 171
            },
            {
                "title": "Tool Learning with Foundation Models",
                "abstract": "\n Humans possess an extraordinary ability to create and utilize tools. With the advent of foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as\n tool learning with foundation models\n , combines the strengths of tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. This paper presents a systematic investigation and comprehensive review of tool learning. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research and formulate a general framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate generalization in tool learning. Finally, we discuss several open problems that require further investigation, such as ensuring trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges. Overall, we hope this paper could inspire future research in integrating tools with foundation models.\n",
                "authors": "Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Y. Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bo Li, Ziwei Tang, Jing Yi, Yu Zhu, Zhenning Dai, Lan Yan, Xin Cong, Ya-Ting Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, Maosong Sun",
                "citations": 173
            },
            {
                "title": "Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners",
                "abstract": "Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pretraining paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision-generative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by ‘Prompt, Generate, then Cache’. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such collaboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of-the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.",
                "authors": "Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Y. Qiao, Peng Gao",
                "citations": 139
            },
            {
                "title": "The shaky foundations of large language models and foundation models for electronic health records",
                "abstract": null,
                "authors": "Michael Wornow, Yizhe Xu, Rahul Thapa, Birju S. Patel, E. Steinberg, S. Fleming, M. Pfeffer, J. Fries, N. Shah",
                "citations": 139
            },
            {
                "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities",
                "abstract": "Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.",
                "authors": "Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, P. Abbeel, D. Schuurmans",
                "citations": 126
            },
            {
                "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision",
                "abstract": "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs. Project Page: https://q-future.github.io/Q-Bench.",
                "authors": "Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin",
                "citations": 105
            },
            {
                "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
                "abstract": "Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: https://lmexam.com.",
                "authors": "Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou",
                "citations": 105
            },
            {
                "title": "Unmasked Teacher: Towards Training-Efficient Video Foundation Models",
                "abstract": "Video Foundation Models (VFMs) have received limited exploration due to high computational costs and data scarcity. Previous VFMs rely on Image Foundation Models (IFMs), which face challenges in transferring to the video domain. Although VideoMAE has trained a robust ViT from limited data, its low-level reconstruction poses convergence difficulties and conflicts with high-level cross-modal alignment. This paper proposes a training-efficient method for temporal-sensitive VFMs that integrates the benefits of existing methods. To increase data efficiency, we mask out most of the low-semantics video tokens, but selectively align the unmasked tokens with IFM, which serves as the UnMasked Teacher (UMT). By providing semantic guidance, our method enables faster convergence and multi-modal friendliness. With a progressive pre-training framework, our model can handle various tasks including scene-related, temporal-related, and complex video-language understanding. Using only public sources for pre-training in 6 days on 32 A100 GPUs, our scratch-built ViT-L/16 achieves state-of-the-art performances on various video tasks.",
                "authors": "Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, Yu Qiao",
                "citations": 108
            },
            {
                "title": "On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence",
                "abstract": "Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial subdomains including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, these task-agnostic LLMs can outperform task-specific fully-supervised models in a zero-shot or few-shot learning setting. However, on other geospatial tasks, especially tasks that involve multiple data modalities (e.g., POI-based urban function classification, street view image-based urban noise intensity classification, and remote sensing image scene classification), existing foundation models still underperform task-specific models. Based on these observations, we propose that one of the major challenges of developing a FM for GeoAI is to address the multimodality nature of geospatial tasks. After discussing the distinct challenges of each geospatial data modality, we suggest the possibility of a multimodal foundation model which can reason over various types of geospatial data through geospatial alignments. We conclude this paper by discussing the unique risks and challenges to develop such a model for GeoAI.",
                "authors": "Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao Liu, Song Gao, Tianming Liu, G. Cong, Yingjie Hu, Chris Cundy, Ziyuan Li, Rui Zhu, Ni Lao",
                "citations": 102
            },
            {
                "title": "Foundation Models and Fair Use",
                "abstract": "Existing foundation models are trained on copyrighted material. Deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. In the United States and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. However, there is a caveat: If the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. In this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. First, we survey the potential risks of developing and deploying foundation models based on copyrighted content. We review relevant U.S. case law, drawing parallels to existing and potential applications for generating text, source code, and visual art. Experiments confirm that popular foundation models can generate content considerably similar to copyrighted material. Second, we discuss technical mitigations that can help foundation models stay in line with fair use. We argue that more research is needed to align mitigation strategies with the current state of the law. Lastly, we suggest that the law and technical mitigations should co-evolve. For example, coupled with other policy mechanisms, the law could more explicitly consider safe harbors when strong technical tools are used to mitigate infringement harms. This co-evolution may help strike a balance between intellectual property and innovation, which speaks to the original goal of fair use. But we emphasize that the strategies we describe here are not a panacea and more work is needed to develop policies that address the potential harms of foundation models.",
                "authors": "Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, Percy Liang",
                "citations": 95
            },
            {
                "title": "Towards Graph Foundation Models: A Survey and Beyond",
                "abstract": "Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase significant success in natural language processing and several other domains. Meanwhile, the field of graph machine learning is witnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches. The capabilities of foundation models to generalize and adapt motivate graph machine learning researchers to discuss the potential of developing a new graph learning paradigm. This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various graph tasks. Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this new domain. To this end, this article introduces the concept of Graph Foundation Models (GFMs), and offers an exhaustive explanation of their key characteristics and underlying technologies. We proceed to classify the existing work related to GFMs into three distinct categories, based on their dependence on graph neural networks and large language models. In addition to providing a thorough review of the current state of GFMs, this article also outlooks potential avenues for future research in this rapidly evolving domain.",
                "authors": "Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, Chuan Shi",
                "citations": 70
            },
            {
                "title": "How to Index Item IDs for Recommendation Foundation Models",
                "abstract": "Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item as in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text and hallucinated recommendations when deciding which item(s) to recommend, creating LLM-compatible item IDs to uniquely identify each item is essential for recommendation foundation models. In this study, we systematically examine the item ID creation and indexing problem for recommendation foundation models, using P5 as an example of the backbone LLM. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as random indexing, title indexing, and independent indexing. We then propose four simple yet effective solutions, including sequential indexing, collaborative indexing, semantic (content-based) indexing, and hybrid indexing. Our study highlights the significant influence of item indexing methods on the performance of LLM-based recommendation, and our results on real-world datasets validate the effectiveness of our proposed solutions. The research also demonstrates how recent advances on language modeling and traditional IR principles such as indexing can help each other for better learning and inference. Source code and data are available at https://github.com/Wenyueh/LLM-RecSys-ID.",
                "authors": "Wenyue Hua, Shuyuan Xu, Yingqiang Ge, Yongfeng Zhang",
                "citations": 80
            },
            {
                "title": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models",
                "abstract": "The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision encoder with temporal and local perception on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded multimodal perception, HTML comprehension, and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB, we improve over the previous best offline methods by more than 45.8%, even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On the WebShop benchmark, our 3-billion-parameter model achieves superior performance to the existing SoTA, PaLM-540B. Furthermore, WebGUM exhibits strong positive transfer to the real-world planning tasks on the Mind2Web. We also collect 347K high-quality demonstrations using our trained models, 38 times larger than prior work, and make them available to promote future research in this direction.",
                "authors": "Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, S. Gu, Izzeddin Gur",
                "citations": 67
            },
            {
                "title": "Vision-Language Foundation Models as Effective Robot Imitators",
                "abstract": "Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.",
                "authors": "Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chi-Hou Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong",
                "citations": 80
            },
            {
                "title": "Foundation Models in Robotics: Applications, Challenges, and the Future",
                "abstract": "We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models .",
                "authors": "Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, Brian Ichter, Danny Driess, Jiajun Wu, Cewu Lu, Mac Schwager",
                "citations": 83
            },
            {
                "title": "Effective Long-Context Scaling of Foundation Models",
                "abstract": "We present an effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens. Our models are built through continual pretraining from Llama 2 checkpoints with longer text sequences and on a dataset where long texts are upsampled. We perform extensive evaluation using language modeling, synthetic context probing tasks, and a wide range of downstream benchmarks. Across all evaluations, our models achieve consistent improvements on most regular-context tasks and significant improvements on long-context tasks over Llama 2. Moreover, with a cost-effective instruction tuning procedure that is free of expensive annotation, the presented models can already surpass \\texttt{gpt-3.5-turbo-16k}‘s overall performance on long-context benchmarks. Alongside these results, we provide an in-depth analysis on each individual component of our method. We delve into Llama’s position encodings and discuss its key limitation in modeling long data. We examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths – ablation results suggest that having abundant long texts in the pretrain dataset is \\textit{not} the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.",
                "authors": "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oğuz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, Hao Ma",
                "citations": 154
            },
            {
                "title": "LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models",
                "abstract": "Foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, more and more foundation models have become publicly available.However, most of those models exhibit a major deficiency in specialized-domain and specialized-task applications, where the step of domain- and task-aware finetuning is still required to obtain scientific language models. As the number of available foundation models and specialized tasks keeps growing, the job of training scientific language models becomes highly nontrivial. In this paper, we take the first step to address this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the domain- and task-aware finetuning of general foundation models.LMFlow offers a complete finetuning workflow for a foundation model to support specialized training with limited computing resources.Furthermore, it supports continuous pretraining, instruction tuning, parameter-efficient finetuning, alignment tuning, inference acceleration, long context generalization, model customization, and even multimodal finetuning, along with carefully designed and extensible APIs. This toolkit has been thoroughly tested and is available at https://github.com/OptimalScale/LMFlow.",
                "authors": "Shizhe Diao, Rui Pan, Hanze Dong, Kashun Shum, Jipeng Zhang, Wei Xiong, T. Zhang",
                "citations": 59
            },
            {
                "title": "Foundation Models for Generalist Geospatial Artificial Intelligence",
                "abstract": "Significant progress in the development of highly adaptable and reusable Artificial Intelligence (AI) models is expected to have a significant impact on Earth science and remote sensing. Foundation models are pre-trained on large unlabeled datasets through self-supervision, and then fine-tuned for various downstream tasks with small labeled datasets. This paper introduces a first-of-a-kind framework for the efficient pre-training and fine-tuning of foundational models on extensive geospatial data. We have utilized this framework to create Prithvi, a transformer-based geospatial foundational model pre-trained on more than 1TB of multispectral satellite imagery from the Harmonized Landsat-Sentinel 2 (HLS) dataset. Our study demonstrates the efficacy of our framework in successfully fine-tuning Prithvi to a range of Earth observation tasks that have not been tackled by previous work on foundation models involving multi-temporal cloud gap imputation, flood mapping, wildfire scar segmentation, and multi-temporal crop segmentation. Our experiments show that the pre-trained model accelerates the fine-tuning process compared to leveraging randomly initialized weights. In addition, pre-trained Prithvi compares well against the state-of-the-art, e.g., outperforming a conditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%) in the structural similarity index. Finally, due to the limited availability of labeled data in the field of Earth observation, we gradually reduce the quantity of available labeled data for refining the model to evaluate data efficiency and demonstrate that data can be decreased significantly without affecting the model's accuracy. The pre-trained 100 million parameter model and corresponding fine-tuning workflows have been released publicly as open source contributions to the global Earth sciences community through Hugging Face.",
                "authors": "Johannes Jakubik, Sujit Roy, C. Phillips, P. Fraccaro, Denys Godwin, Bianca Zadrozny, D. Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, Daiki Kimura, Naomi Simumba, Linsong Chu, S. K. Mukkavilli, Devyani Lambhate, Kamal Das, Ranjini Bangalore, Dario Oliveira, Michal Muszynski, Kumar Ankur, Muthukumaran Ramasubramanian, I. Gurung, Sam Khallaghi, Hanxi Li, Michael Cecil, Maryam Ahmadi, Fatemeh Kordi, H. Alemohammad, M. Maskey, R. Ganti, Kommy Weldemariam, Rahul Ramachandran",
                "citations": 55
            },
            {
                "title": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models",
                "abstract": "Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, obviating the need for annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal relationships are enforced at both the camera-to-LiDAR and point-to-segment regularization stages, facilitating cross-modal representation learning. iii) Generalizability: Seal enables knowledge transfer in an off-the-shelf manner to downstream tasks involving diverse point clouds, including those from real/synthetic, low/high-resolution, large/small-scale, and clean/corrupted datasets. Extensive experiments conducted on eleven different point cloud datasets showcase the effectiveness and superiority of Seal. Notably, Seal achieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassing random initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU. Moreover, Seal demonstrates significant performance gains over existing methods across 20 different few-shot fine-tuning tasks on all eleven tested point cloud datasets.",
                "authors": "You-Chen Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu",
                "citations": 61
            },
            {
                "title": "Lag-Llama: Towards Foundation Models for Time Series Forecasting",
                "abstract": "Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama , a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen “out-of-distribution” time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws [7] to fit and predict model scaling behavior. The open source code is made available at https://github",
                "authors": "Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Bilos, Hena Ghonia, N. Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas Chapados, Yuriy Nevmyvaka, Irina Rish",
                "citations": 57
            },
            {
                "title": "A Survey of Reasoning with Foundation Models",
                "abstract": "Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, and contribute to the development of AGI.",
                "authors": "Jiankai Sun, Chuanyang Zheng, E. Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, P. Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Jingwei Wen, Xipeng Qiu, Yi-Chen Guo, Hui Xiong, Qun Liu, Zhenguo Li",
                "citations": 46
            },
            {
                "title": "Compositional Foundation Models for Hierarchical Planning",
                "abstract": "To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustrate the efficacy and adaptability of our approach in three different long-horizon table-top manipulation tasks.",
                "authors": "Anurag Ajay, Seung-Jun Han, Yilun Du, Shaung Li, Abhishek Gupta, T. Jaakkola, Josh Tenenbaum, L. Kaelbling, Akash Srivastava, Pulkit Agrawal",
                "citations": 44
            },
            {
                "title": "Towards Geospatial Foundation Models via Continual Pretraining",
                "abstract": "Geospatial technologies are becoming increasingly essential in our world for a wide range of applications, including agriculture, urban planning, and disaster response. To help improve the applicability and performance of deep learning models on these geospatial tasks, various works have begun investigating foundation models for this domain. Researchers have explored two prominent approaches for introducing such models in geospatial applications, but both have drawbacks in terms of limited performance benefit or prohibitive training cost. Therefore, in this work, we propose a novel paradigm for building highly effective geospatial foundation models with minimal resource cost and carbon impact. We first construct a compact yet diverse dataset from multiple sources to promote feature diversity, which we term GeoPile. Then, we investigate the potential of continual pretraining from large-scale ImageNet-22k models and propose a multi-objective continual pretraining paradigm, which leverages the strong representations of ImageNet while simultaneously providing the freedom to learn valuable in-domain features. Our approach outperforms previous state-of-the-art geospatial pretraining methods in an extensive evaluation on seven downstream datasets covering various tasks such as change detection, classification, multi-label classification, semantic segmentation, and super-resolution. Code is available at https://github.com/mmendiet/GFM.",
                "authors": "Mat'ias Mendieta, Boran Han, Xingjian Shi, Yi Zhu, Chen Chen, Mu Li",
                "citations": 44
            },
            {
                "title": "SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding",
                "abstract": "The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that absorbs their expertise. Our method integrates techniques of multitask learning, continual learning, and distillation. Further, it demands significantly less computational cost compared to traditional multi-task training from scratch, and it only needs a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we obtain SAM-CLIP : a unified model that combines the capabilities of SAM and CLIP into a single vision transformer. Compared with deploying SAM and CLIP independently, our merged model, SAM-CLIP, reduces storage and compute costs for inference, making it well-suited for edge device applications. We show that SAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also introduces synergistic functionalities, notably in zero-shot semantic segmentation, where SAM-CLIP establishes new state-of-the-art results on 5 benchmarks. It outperforms previous models that are specifically designed for this task by a large margin, including +6.8% and +5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.",
                "authors": "Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, Hadi Pouransari",
                "citations": 44
            },
            {
                "title": "SLM: Bridge the Thin Gap Between Speech and Text Foundation Models",
                "abstract": "We present a joint Speech and Language Model (SLM), a multitask, multilingual, and dual-modal model that takes advantage of pretrained foundational speech and language models. SLM freezes the pretrained foundation models to maximally preserves their capabilities, and only trains a simple adapter with just 1% (156M) of the foundation models’ parameters. This adaptation not only leads SLM to achieve strong performance on conventional tasks such as automatic speech recognition (ASR) and automatic speech translation (AST), but also unlocks the novel capability of zero-shot instruction-following for more diverse tasks. Given a speech input and a text instruction, SLM is able to perform unseen generation tasks including contextual biasing ASR using real-time context, dialog generation, speech continuation, and question answering. Our approach demonstrates that the representational gap between pretrained speech and language models is narrower than one would expect, and can be bridged by a simple adaptation mechanism. As a result, SLM is not only efficient to train, but also inherits strong capabilities already present in foundation models of different modalities.",
                "authors": "Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Yongqiang Wang, Nanxin Chen, Yu Zhang, H. Soltau, P. Rubenstein, Lukás Zilka, Dian Yu, Zhong Meng, G. Pundak, Nikhil Siddhartha, J. Schalkwyk, Yonghui Wu",
                "citations": 47
            },
            {
                "title": "On the Challenges and Perspectives of Foundation Models for Medical Image Analysis",
                "abstract": "This article discusses the opportunities, applications and future directions of large-scale pretrained models, i.e., foundation models, which promise to significantly improve the analysis of medical images. Medical foundation models have immense potential in solving a wide range of downstream tasks, as they can help to accelerate the development of accurate and robust models, reduce the dependence on large amounts of labeled data, preserve the privacy and confidentiality of patient data. Specifically, we illustrate the \"spectrum\" of medical foundation models, ranging from general imaging models, modality-specific models, to organ/task-specific models, and highlight their challenges, opportunities and applications. We also discuss how foundation models can be leveraged in downstream medical tasks to enhance the accuracy and efficiency of medical image analysis, leading to more precise diagnosis and treatment decisions.",
                "authors": "Shaoting Zhang, Dimitris N. Metaxas",
                "citations": 84
            },
            {
                "title": "A Novel Scenarios Engineering Methodology for Foundation Models in Metaverse",
                "abstract": "Foundation models are used to train a broad system of general data to build adaptations to new bottlenecks. Typically, they contain hundreds of billions of hyperparameters that have been trained with hundreds of gigabytes of data. However, this type of black-box vulnerability places foundation models at risk of data poisoning attacks that are designed to pass on misinformation or purposely introduce machine bias. Moreover, ordinary researchers have not been able to completely participate due to the rise in deployment standards. This study introduces the theoretical framework of scenarios engineering (SE) for building accessible and reliable foundation models in metaverse, namely, “SE-enabled foundation models in metaverse.” Particularly, the research framework comprises a six-layer architecture (infrastructure layer, operation layer, knowledge layer, intelligence layer, management layer, and interaction layer), which can provide controllability, trustworthiness, and interactivity for the foundation models in metaverse. This creates closed-loop, virtual–real, and human–machine environments that provides the best indices and goals for the foundation models, which allows us to fully validate and calibrate the corresponding models. Then, examples of use cases from the automotive industry are listed to provide transparency on the possible use and benefits of our approach. Finally, the open research topics of related frameworks are discussed.",
                "authors": "Xuan Li, Yonglin Tian, Peijun Ye, Haibin Duan, Fei-yue Wang",
                "citations": 71
            },
            {
                "title": "GEO-Bench: Toward Foundation Models for Earth Monitoring",
                "abstract": "Recent progress in self-supervision has shown that pre-training large neural networks on vast amounts of unsupervised data can lead to substantial increases in generalization to downstream tasks. Such models, recently coined foundation models, have been transformational to the field of natural language processing. Variants have also been proposed for image data, but their applicability to remote sensing tasks is limited. To stimulate the development of foundation models for Earth monitoring, we propose a benchmark comprised of six classification and six segmentation tasks, which were carefully curated and adapted to be both relevant to the field and well-suited for model evaluation. We accompany this benchmark with a robust methodology for evaluating models and reporting aggregated results to enable a reliable assessment of progress. Finally, we report results for 20 baselines to gain information about the performance of existing models. We believe that this benchmark will be a driver of progress across a variety of Earth monitoring tasks.",
                "authors": "Alexandre Lacoste, Nils Lehmann, Pau Rodríguez López, Evan D. Sherwin, H. Kerner, Bjorn Lutjens, J. Irvin, David Dao, H. Alemohammad, Alexandre Drouin, Mehmet Gunturkun, Gabriel Huang, David Vázquez, Dava Newman, Y. Bengio, Stefano Ermon, Xiao Xiang Zhu",
                "citations": 36
            },
            {
                "title": "Towards Label-free Scene Understanding by Vision Foundation Models",
                "abstract": "Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks. However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored. In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data. The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain. To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously. In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation. Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments. Our 2D and 3D network achieves label-free semantic segmentation with 28.4\\% and 33.5\\% mIoU on ScanNet, improving 4.7\\% and 7.9\\%, respectively. For nuImages and nuScenes datasets, the performance is 22.1\\% and 26.8\\% with improvements of 3.5\\% and 6.0\\%, respectively. Code is available. (https://github.com/runnanchen/Label-Free-Scene-Understanding).",
                "authors": "Runnan Chen, You-Chen Liu, Lingdong Kong, Nenglun Chen, Xinge Zhu, Yuexin Ma, Tongliang Liu, Wenping Wang",
                "citations": 38
            },
            {
                "title": "Equivariant Similarity for Vision-Language Foundation Models",
                "abstract": "This study explores the concept of equivariance in vision-language foundation models (VLMs), focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. Unlike the existing image-text similarity objective which only categorizes matched pairs as similar and unmatched pairs as dissimilar, equivariance also requires similarity to vary faithfully according to the semantic changes. This allows VLMs to generalize better to nuanced and unseen multimodal compositions. However, modeling equivariance is challenging as the ground truth of semantic change is difficult to collect. For example, given an image-text pair about a dog, it is unclear to what extent the similarity changes when the pixel is changed from dog to cat? To this end, we propose EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we present a new challenging benchmark EqBen. Compared to the existing evaluation sets, EqBen is the first to focus on \"visual-minimal change\". Extensive experiments show the lack of equivariance in current VLMs1 and validate the effectiveness of EqSim2.",
                "authors": "Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, Lijuan Wang",
                "citations": 41
            },
            {
                "title": "Towards Foundation Models for Knowledge Graph Reasoning",
                "abstract": "Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance.",
                "authors": "Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, Zhaocheng Zhu",
                "citations": 31
            },
            {
                "title": "FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models",
                "abstract": "Recent works on generalizable NeRFs have shown promising results on novel view synthesis from single or few images. However, such models have rarely been applied on other downstream tasks beyond synthesis such as semantic understanding and parsing. In this paper, we propose a novel framework named FeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision foundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D pre-trained foundation models to 3D space via neural rendering, and then extract deep features for 3D query points from NeRF MLPs. Consequently, it allows to map 2D images to continuous 3D semantic feature volumes, which can be used for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D semantic keypoint transfer and 2D/3D object part segmentation. Our extensive experiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D semantic feature extractor. Our project page is available at https://jianglongye.com/featurenerf/.",
                "authors": "Jianglong Ye, Naiyan Wang, X. Wang",
                "citations": 32
            },
            {
                "title": "On the Adversarial Robustness of Multi-Modal Foundation Models",
                "abstract": "Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images $\\left({{\\varepsilon _\\infty } = 1/255}\\right)$ in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model. Note: This paper contains fake information to illustrate the outcome of our attacks. It does not reflect the opinion of the authors.",
                "authors": "Christian Schlarmann, Matthias Hein",
                "citations": 58
            },
            {
                "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
                "abstract": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under a shared format. Additionally, we propose a parameter-efficient training method for foundation models, which involves freezing the P5 backbone and fine-tuning lightweight adapters, resulting in improved recommendation performance and increased efficiency in terms of training time and memory usage. Code and data of VIP5 are available at https://github.com/jeykigung/VIP5.",
                "authors": "Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, Yongfeng Zhang",
                "citations": 56
            },
            {
                "title": "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning",
                "abstract": "We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Many models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingually-trained models have not attained “balanced multilingual” capabilities. Our endeavors underscore the need for more generalizable semantic representations and enhanced multilingual contextualization. SeaEval can serve as a launchpad for more thorough investigations and evaluations for multilingual and multicultural scenarios.",
                "authors": "Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, A. Aw, Nancy F. Chen",
                "citations": 49
            },
            {
                "title": "Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting",
                "abstract": "Over the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero-shot and few-shot generalization. However, despite the success of foundation models in modalities such as natural language processing and computer vision, the development of foundation models for time series forecasting has lagged behind. We present Lag-Llama, a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates. Lag-Llama is pretrained on a large corpus of diverse time series data from several domains, and demonstrates strong zero-shot generalization capabilities compared to a wide range of forecasting models on downstream datasets across domains. Moreover, when fine-tuned on relatively small fractions of such previously unseen datasets, Lag-Llama achieves state-of-the-art performance, outperforming prior deep learning approaches, emerging as the best general-purpose model on average. Lag-Llama serves as a strong contender to the current state-of-art in time series forecasting and paves the way for future advancements in foundation models tailored to time series data.",
                "authors": "Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Bilovs, Hena Ghonia, N. Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas Chapados, Yuriy Nevmyvaka, I. Rish",
                "citations": 30
            },
            {
                "title": "Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis",
                "abstract": "Building general-purpose robots that operate seamlessly in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. However, as a community, we have been constraining most robotic systems by designing them for specific tasks, training them on specific datasets, and deploying them within specific environments. These systems require extensively-labeled data and task-specific models. When deployed in real-world scenarios, such systems face several generalization issues and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of general-purpose robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing a generalized formulation of how foundation models are used in robotics, and the fundamental barriers to making generalist robots universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository 2 of resources, including papers reviewed in this survey, as well as related projects and repositories for developing foundation models for robotics.",
                "authors": "Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Varma Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia P. Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Z. Kira, Fei Xia, Yonatan Bisk",
                "citations": 43
            },
            {
                "title": "Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill",
                "abstract": "Zero-shot object navigation is a challenging task for home-assistance robots. This task emphasizes visual grounding, commonsense inference and locomotion abilities, where the first two are inherent in foundation models. But for the locomotion part, most works still depend on map-based planning approaches. The gap between RGB space and map space makes it difficult to directly transfer the knowledge from foundation models to navigation tasks. In this work, we propose a Pixel-guided Navigation skill (PixNav), which bridges the gap between the foundation models and the embodied navigation task. It is straightforward for recent foundation models to indicate an object by pixels, and with pixels as the goal specification, our method becomes a versatile navigation policy towards all different kinds of objects. Besides, our PixNav is a pure RGB-based policy that can reduce the cost of homeassistance robots. Experiments demonstrate the robustness of the PixNav which achieves 80+% success rate in the local path-planning task. To perform long-horizon object navigation, we design an LLM-based planner to utilize the commonsense knowledge between objects and rooms to select the best waypoint. Evaluations across both photorealistic indoor simulators and real-world environments validate the effectiveness of our proposed navigation strategy. More details are accessible via our project website https://sites.google.com/view/pixnav/.",
                "authors": "Wenzhe Cai, Siyuan Huang, Guangran Cheng, Yuxing Long, Peng Gao, Changyin Sun, Hao Dong",
                "citations": 28
            },
            {
                "title": "A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models",
                "abstract": "We study the task of zero-shot vision-and-language navigation (ZS-VLN), a practical yet challenging problem in which an agent learns to navigate following a path described by language instructions without requiring any path-instruction annotation data. Normally, the instructions have complex grammatical structures and often contain various action descriptions (e.g.,\"proceed beyond\",\"depart from\"). How to correctly understand and execute these action demands is a critical problem, and the absence of annotated data makes it even more challenging. Note that a well-educated human being can easily understand path instructions without the need for any special training. In this paper, we propose an action-aware zero-shot VLN method ($A^2$Nav) by exploiting the vision-and-language ability of foundation models. Specifically, the proposed method consists of an instruction parser and an action-aware navigation policy. The instruction parser utilizes the advanced reasoning ability of large language models (e.g., GPT-3) to decompose complex navigation instructions into a sequence of action-specific object navigation sub-tasks. Each sub-task requires the agent to localize the object and navigate to a specific goal position according to the associated action demand. To accomplish these sub-tasks, an action-aware navigation policy is learned from freely collected action-specific datasets that reveal distinct characteristics of each action demand. We use the learned navigation policy for executing sub-tasks sequentially to follow the navigation instruction. Extensive experiments show $A^2$Nav achieves promising ZS-VLN performance and even surpasses the supervised learning methods on R2R-Habitat and RxR-Habitat datasets.",
                "authors": "Peihao Chen, Xinyu Sun, Hongyan Zhi, Runhao Zeng, Thomas H. Li, Gaowen Liu, Mingkui Tan, Chuang Gan",
                "citations": 25
            },
            {
                "title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models",
                "abstract": "Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively. Supplementary materials such as audio samples are provided at our demo website: https://v2a-mapper.github.io/.",
                "authors": "Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, Weidong (Tom) Cai",
                "citations": 24
            },
            {
                "title": "Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models",
                "abstract": "Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in many domains. In this paper, we propose the Federated Foundation Models (FFMs) paradigm, which combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple end-users. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further outline potential future research avenues in FFM, including FFM pre-training, FFM fine-tuning, and federated prompt tuning, which allow the development of more personalized and context-aware models while ensuring data privacy. Moreover, we explore the possibility of continual/lifelong learning in FFMs, as increased computational power at the edge may unlock the potential for optimizing FMs using newly generated private data close to the data source. The proposed FFM concepts offer a flexible and scalable framework for training large language models in a privacy-preserving manner, setting the stage for subsequent advancements in both FM training and federated learning.",
                "authors": "Sixing Yu, J. P. Muñoz, A. Jannesari",
                "citations": 37
            },
            {
                "title": "ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps",
                "abstract": "Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT represents a landmark achievement in this research paradigm, offering hope for general artificial intelligence due to its highly intelligent natural language understanding ability. However, the PHM field lacks a consensus on how to respond to this significant change in the AI field, and a systematic review and roadmap is required to elucidate future development directions. To fill this gap, this paper systematically expounds on the key components and latest developments of LSF-Models. Then, we systematically answered how to build the LSF-Model applicable to PHM tasks and outlined the challenges and future development roadmaps for this research paradigm.",
                "authors": "Yanfang Li, Huan Wang, Muxia Sun",
                "citations": 32
            },
            {
                "title": "Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives",
                "abstract": "Recent decisions by leading AI labs to either open-source their models or to restrict access to their models has sparked debate about whether, and how, increasingly capable AI models should be shared. Open-sourcing in AI typically refers to making model architecture and weights freely and publicly accessible for anyone to modify, study, build on, and use. This offers advantages such as enabling external oversight, accelerating progress, and decentralizing control over AI development and use. However, it also presents a growing potential for misuse and unintended consequences. This paper offers an examination of the risks and benefits of open-sourcing highly capable foundation models. While open-sourcing has historically provided substantial net benefits for most software and AI development processes, we argue that for some highly capable foundation models likely to be developed in the near future, open-sourcing may pose sufficiently extreme risks to outweigh the benefits. In such a case, highly capable foundation models should not be open-sourced, at least not initially. Alternative strategies, including non-open-source model sharing options, are explored. The paper concludes with recommendations for developers, standard-setting bodies, and governments for establishing safe and responsible model sharing practices and preserving open-source benefits where safe.",
                "authors": "Elizabeth Seger, Noemi Dreksler, Richard Moulange, Emily Dardaman, Jonas Schuett, K. Wei, Christoph Winter, Mackenzie Arnold, Anton Korinek, Markus Anderljung, Ben Bucknall, Alan Chan, Eoghan Stafford, Leonie Koessler, Aviv Ovadya, Ben Garfinkel, Emma Bluemke, Michael Aird, Patrick Levermore, Julian Hazell, Abhishek Gupta, Andrew Trask, Ben Cottier, Herbie Bradley, Irene Solaiman, Norman Johnson, Peter Cihon, S. Avin, Stella Biderman",
                "citations": 34
            },
            {
                "title": "A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models",
                "abstract": "—Prompt engineering is a technique that involves augmenting a large pre-trained model with task-speciﬁc hints, known as prompts, to adapt the model to new tasks. Prompts can be created manually as natural language instructions or generated automatically as either natural language instructions or vector representations. Prompt engineering enables the ability to perform predictions based solely on prompts without updating model parameters, and the easier application of large pre-trained models in real-world tasks. In past years, Prompt engineering has been well-studied in natural language processing. Recently, it has also been intensively studied in vision-language modeling. However, there is currently a lack of a systematic overview of prompt engineering on pre-trained vision-language models. This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models ( e.g., Flamingo), image-text matching models ( e.g., CLIP), and text-to-image generation models ( e.g., Stable Diffusion). For each type of model, a brief model summary, prompting methods, prompting-based applications, and the corresponding responsibility and integrity issues are summarized and discussed. Furthermore, the commonalities and differences between prompting on vision-language models, language models, and vision models are also discussed. The challenges, future directions, and research opportunities are summarized to foster future research on this topic.",
                "authors": "Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, Philip H. S. Torr",
                "citations": 105
            },
            {
                "title": "Will Affective Computing Emerge From Foundation Models and General Artificial Intelligence? A First Evaluation of ChatGPT",
                "abstract": "ChatGPT has shown the potential of emerging general artificial intelligence capabilities, as it has demonstrated competent performance across many natural language processing tasks. In this work, we evaluate the capabilities of ChatGPT to perform text classification on three affective computing problems, namely, big-five personality prediction, sentiment analysis, and suicide tendency detection. We utilize three baselines, a robust language model (RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and a simple bag-of-words (BoW) baseline. Results show that the RoBERTa model trained for a specific downstream task generally has a superior performance. On the other hand, ChatGPT provides decent results and is relatively comparable to the Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy data, where the Word2Vec model achieves worse results due to noise. Results indicate that ChatGPT is a good generalist model that is capable of achieving good results across various problems without any specialized training; however, it is not as good as a specialized model for a downstream task.",
                "authors": "Mostafa M. Amin, E. Cambria, Björn Schuller, E. Cambria",
                "citations": 67
            },
            {
                "title": "π-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation",
                "abstract": "Foundation models have achieved great advances in multi-task learning with a unified interface of unimodal and multimodal tasks. However, the potential of such multi-task learners has not been exploited during transfer learning. In this work, we present a universal parameter-efficient transfer learning method, termed Predict-Interpolate Tuning ($\\pi$-Tuning), for vision, language, and vision-language tasks. It aggregates the parameters of lightweight task-specific experts learned from similar tasks to aid the target downstream task. The task similarities are predicted in a unified modality-independent space, yielding a scalable graph to demonstrate task relationships. $\\pi$-Tuning has several appealing benefits. First, it flexibly explores both intra- and inter-modal transferability between similar tasks to improve the accuracy and robustness of transfer learning, especially in data-scarce scenarios. Second, it offers a systematical solution for transfer learning with multi-task prediction-and-then-interpolation, compatible with diverse types of parameter-efficient experts, such as prompt and adapter. Third, an extensive study of task-level mutual benefits on 14 unimodal and 6 multimodal datasets shows that $\\pi$-Tuning surpasses fine-tuning and other parameter-efficient transfer learning methods both in full-shot and low-shot regimes. The task graph also enables an in-depth interpretable analysis of task transferability across modalities. The code will be available at https://github.com/TencentARC/pi-Tuning.",
                "authors": "Chengyue Wu, Teng Wang, Yixiao Ge, Zeyu Lu, Rui-Zhi Zhou, Ping Luo, Ying Shan",
                "citations": 33
            },
            {
                "title": "Q-Instruct: Improving Low-Level Visual Abilities for Multi-Modality Foundation Models",
                "abstract": "Multi-modality large language models (MLLMs), as represented by GPT-4V, have introduced a paradigm shift for visual perception and understanding tasks, that a variety of abilities can be achieved within one foundation model. While current MLLMs demonstrate primary low-level visual abilities from the identification of low-level visual attributes (e.g., clarity, brightness) to the evaluation on image quality, there's still an imperative to further improve the accuracy of MLLMs to substantially alleviate human burdens. To address this, we collect the first dataset consisting of human natural language feedback on low-level vision. Each feedback offers a comprehensive description of an image's low-level visual attributes, culminating in an overall quality assessment. The constructed Q-Pathway dataset includes 58K detailed human feedbacks on 18,973 multi-sourced images with diverse low-level appearance. To ensure MLLMs can adeptly handle diverse queries, we further propose a GPT-participated transformation to convert these feedbacks into a rich set of 200K instruction-response pairs, termed Q-Instruct. Experimental results indicate that the Q-Instruct consistently elevates various low-level visual capabilities across multiple base models. We anticipate that our datasets can pave the way for a future that foundation models can assist humans on low-level visual tasks.",
                "authors": "Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, Geng Xue, Wenxiu Sun, Qiong Yan, Weisi Lin",
                "citations": 49
            },
            {
                "title": "Towards A Unified Agent with Foundation Models",
                "abstract": "Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts.",
                "authors": "Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, N. Heess, Martin A. Riedmiller",
                "citations": 51
            },
            {
                "title": "Adversarial Prompting for Black Box Foundation Models",
                "abstract": "Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to signiﬁcant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce speciﬁc behaviors into the generative process, such as generating images of a particular object or biasing the frequency of speciﬁc letters in the generated text.",
                "authors": "N. Maus, Patrick Chao, Eric Wong, Jacob R. Gardner",
                "citations": 45
            },
            {
                "title": "Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models",
                "abstract": "As autonomous driving technology matures, end-to-end methodologies have emerged as a leading strategy, promising seamless integration from perception to control via deep learning. However, existing systems grapple with challenges such as unexpected open set environments and the complexity of black-box models. At the same time, the evolution of deep learning introduces larger, multimodal foundational models, offering multi-modal visual and textual understanding. In this paper, we harness these multimodal foundation models to enhance the robustness and adaptability of autonomous driving systems. We introduce a method to extract nuanced spatial features from transformers and the incorporation of latent space simulation for improved training and policy debugging. We use pixel/patch-aligned feature descriptors to expand foundational model capabilities to create an end-to-end multimodal driving model, demonstrating unparalleled results in diverse tests. Our solution combines language with visual perception and achieves significantly greater robustness on out-of-distribution situations.",
                "authors": "Tsun-Hsuan Wang, Alaa Maalouf, Wei Xiao, Yutong Ban, Alexander Amini, G. Rosman, S. Karaman, Daniela Rus",
                "citations": 29
            },
            {
                "title": "TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models",
                "abstract": "With the promotion of chatgpt to the public, Large language models indeed showcase remarkable common sense, reasoning, and planning skills, frequently providing insightful guidance. These capabilities hold significant promise for their application in urban traffic management and control. However, LLMs struggle with addressing traffic issues, especially processing numerical data and interacting with simulations, limiting their potential in solving traffic-related challenges. In parallel, specialized traffic foundation models exist but are typically designed for specific tasks with limited input-output interactions. Combining these models with LLMs presents an opportunity to enhance their capacity for tackling complex traffic-related problems and providing insightful suggestions. To bridge this gap, we present TrafficGPT, a fusion of ChatGPT and traffic foundation models. This integration yields the following key enhancements: 1) empowering ChatGPT with the capacity to view, analyze, process traffic data, and provide insightful decision support for urban transportation system management; 2) facilitating the intelligent deconstruction of broad and complex tasks and sequential utilization of traffic foundation models for their gradual completion; 3) aiding human decision-making in traffic control through natural language dialogues; and 4) enabling interactive feedback and solicitation of revised outcomes. By seamlessly intertwining large language model and traffic expertise, TrafficGPT not only advances traffic management but also offers a novel approach to leveraging AI capabilities in this domain. The TrafficGPT demo can be found in https://github.com/lijlansg/TrafficGPT.git.",
                "authors": "Siyao Zhang, Daocheng Fu, Zhao Zhang, Bin Yu, Pinlong Cai",
                "citations": 30
            },
            {
                "title": "Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation",
                "abstract": "In this paper, we first assess and harness various Vision Foundation Models (VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS). Driven by the motivation that Leveraging Stronger pre-trained models and Fewer trainable parameters for Superior generalizability, we introduce a robust fine-tuning approach, namely “Rein”, to parameter-efficiently harness VFMs for DGSS. Built upon a set of trainable tokens, each linked to distinct instances, Rein precisely refines and forwards the feature maps from each layer to the next layer within the backbone. This process produces diverse refinements for different categories within a single image. With fewer trainable parameters, Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full parameter fine-tuning. Extensive experiments across various settings demonstrate that Rein significantly outperforms state-of-the-art methods. Remarkably, with just an extra 1% of trainable parameters within the frozen backbone, Rein achieves a mIoU of 78.4% on the Cityscapes, without accessing any real urban-scene datasets. Code is available at https://github.com/w1oves/Rein.git.",
                "authors": "Zhixiang Wei, Lin Chen, Yi Jin, Xiaoxiao Ma, Tianle Liu, Pengyang Lin, Ben Wang, H. Chen, Jinjin Zheng",
                "citations": 22
            },
            {
                "title": "Towards foundation models of biological image segmentation",
                "abstract": null,
                "authors": "Jun Ma, Bo Wang",
                "citations": 29
            },
            {
                "title": "Foundation Models Meet Visualizations: Challenges and Opportunities",
                "abstract": "Recent studies have indicated that foundation models, such as BERT and GPT, excel at adapting to various downstream tasks. This adaptability has made them a dominant force in building artificial intelligence (AI) systems. Moreover, a new research paradigm has emerged as visualization techniques are incorporated into these models. This study divides these intersections into two research areas: visualization for foundation model (VIS4FM) and foundation model for visualization (FM4VIS). In terms of VIS4FM, we explore the primary role of visualizations in understanding, refining, and evaluating these intricate foundation models. VIS4FM addresses the pressing need for transparency, explainability, fairness, and robustness. Conversely, in terms of FM4VIS, we highlight how foundation models can be used to advance the visualization field itself. The intersection of foundation models with visualizations is promising but also introduces a set of challenges. By highlighting these challenges and promising opportunities, this study aims to provide a starting point for the continued exploration of this research avenue.",
                "authors": "Weikai Yang, Mengchen Liu, Zheng Wang, Shixia Liu",
                "citations": 29
            },
            {
                "title": "Scaling Laws for Sparsely-Connected Foundation Models",
                "abstract": "We explore the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets (i.e.,\"foundation models\"), in both vision and language domains. In this setting, we identify the first scaling law describing the relationship between weight sparsity, number of non-zero parameters, and amount of training data, which we validate empirically across model and data scales; on ViT/JFT-4B and T5/C4. These results allow us to characterize the\"optimal sparsity\", the sparsity level which yields the best performance for a given effective model size and training budget. For a fixed number of non-zero parameters, we identify that the optimal sparsity increases with the amount of data used for training. We also extend our study to different sparsity structures (such as the hardware-friendly n:m pattern) and strategies (such as starting from a pretrained dense model). Our findings shed light on the power and limitations of weight sparsity across various parameter and computational settings, offering both theoretical understanding and practical implications for leveraging sparsity towards computational efficiency improvements.",
                "authors": "Elias Frantar, C. Riquelme, N. Houlsby, Dan Alistarh, Utku Evci",
                "citations": 21
            },
            {
                "title": "EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models",
                "abstract": "While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaptation. Our model and dataset are available via a research data use agreement from the Stanford AIMI Center. Code to reproduce our results are available at our Github repo: https://github.com/som-shahlab/ehrshot-benchmark",
                "authors": "Michael Wornow, Rahul Thapa, E. Steinberg, J. Fries, N. Shah",
                "citations": 20
            },
            {
                "title": "Ecosystem Graphs: The Social Footprint of Foundation Models",
                "abstract": "Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at https://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions as a powerful abstraction and interface for achieving the minimum transparency required to address myriad use cases. Therefore, we envision Ecosystem Graphs will be a community-maintained resource that provides value to stakeholders spanning AI researchers, industry professionals, social scientists, auditors and policymakers.",
                "authors": "Rishi Bommasani, Dilara Soylu, Thomas Liao, Kathleen A. Creel, Percy Liang",
                "citations": 26
            },
            {
                "title": "A Framework and Operational Procedures for Metaverses-Based Industrial Foundation Models",
                "abstract": "Industrial processes are typical cyber–physical–social systems (CPSSs), where the effective management of employees and the efficient control of machines play important roles. Traditional industries heavily rely on human labor and neglect the development of collection–utilization–transmission integrated information loops, thereby leading to high costs and low efficiency in operational procedures. To facilitate the natural interactions and smart operations for humans and machines, industrial foundation models (IFMs) based on metaverses are proposed in this article, serving as the operating systems of industrial parallel machines that provide sustainable data resources and scenarios for management and control experiments. On this basis, IFM comprised of vision foundation models, language foundation models, as well as operational foundation models, are constructed to manage resources in industrial parallel machines and provides comprehensive services for industrial procedures. On the one hand, IFM can efficiently manage various resources including computing power, digital assets, enterprise resources, and platform I/O via the proposed CPSS-based competing, sharing, scheduling, monitoring, allocating, and recovering mechanisms. On the other hand, imaginative intelligence, linguistic intelligence, and algorithmic intelligence can be achieved through vivid visualization of vision foundation models, natural conversations of language foundation models, and smart manipulation of operational foundation models. With the proposed IFM, cyber–physical–social intelligence (CPSI) can be achieved to enhance the efficient management and control of industrial processes.",
                "authors": "Jiangong Wang, Yonglin Tian, Yutong Wang, Jing Yang, Xingxia Wang, Sanjin Wang, Oliver Kwan",
                "citations": 24
            },
            {
                "title": "Foundation Models for Natural Language Processing: Pre-trained Language Models Integrating Media",
                "abstract": null,
                "authors": "G. Paass, Sven Giesselbach",
                "citations": 24
            },
            {
                "title": "Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models",
                "abstract": "Foundation models have achieved remarkable results in 2D and language tasks like image segmentation, object detection, and visual-language understanding. However, their potential to enrich 3D scene representation learning is largely untapped due to the existence of the domain gap. In this work, we propose an innovative methodology called Bridge3D to address this gap by pre-training 3D models using features, semantic masks, and captions sourced from foundation models. Specifically, our method employs semantic masks from foundation models to guide the masking and reconstruction process for the masked autoencoder, enabling more focused attention on foreground representations. Moreover, we bridge the 3D-text gap at the scene level using image captioning foundation models, thereby facilitating scene-level knowledge distillation. We further extend this bridging effort by introducing an innovative object-level knowledge distillation method that harnesses highly accurate object-level masks and semantic text data from foundation models. Our methodology significantly surpasses the performance of existing state-of-the-art methods in 3D object detection and semantic segmentation tasks. For instance, on the ScanNet dataset, Bridge3D improves the baseline by a notable margin of 6.3%. Code will be available at: https://github.com/Zhimin-C/Bridge3D",
                "authors": "Zhimin Chen, Bing Li",
                "citations": 23
            },
            {
                "title": "CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks",
                "abstract": "Distributed training of foundation models, especially large language models (LLMs), is communication-intensive and so has heavily relied on centralized data centers with fast interconnects. Can we train on slow networks and unlock the potential of decentralized infrastructure for foundation models? In this paper, we propose C OCKTAIL SGD, a novel communication-efficient training framework that combines three distinct compression techniques—random sparsification, top-K sparsification, and quantization—to achieve much greater compression than each individual technique alone. We justify the benefit of such a hybrid approach through a theoretical analysis of convergence. Empirically, we show that C OCKTAIL SGD achieves up to 117 × compression in fine-tuning LLMs up to 20 billion parameters without hurting convergence. On a 500Mbps network, C OCKTAIL SGD only incurs ∼ 1 . 2 × slowdown compared with data center networks.",
                "authors": "Jue Wang, Yucheng Lu, Binhang Yuan, Beidi Chen, Percy Liang, Christopher De Sa, Christopher Ré, Ce Zhang",
                "citations": 23
            },
            {
                "title": "Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots",
                "abstract": ": Improving the generalization capabilities of general-purpose robotic agents has long been a signiﬁcant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efﬁciency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efﬁcient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach consists of two distinct steps. First, we introduce a series of foundation models to accurately ground natural language demands across multiple tasks. Second, we develop a Multi-modal Multi-view Policy Model that incorporates inputs such as RGB images, semantic masks, and robot proprioception states to jointly predict precise and executable robot actions. Extensive real-world experiments conducted on a Franka Emika robot arm validate the effectiveness of our proposed paradigm. Real-world demos are shown in YouTube/Bilibili.",
                "authors": "Jiange Yang, Wenhui Tan, Chuhao Jin, Bei Liu, Jianlong Fu, Ruihua Song, Limin Wang",
                "citations": 24
            },
            {
                "title": "The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs",
                "abstract": "The successes of foundation models such as ChatGPT and AlphaFold have spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models' capabilities. We review over 80 foundation models trained on non-imaging EMR data (i.e. clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. In light of these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.",
                "authors": "Michael Wornow, Yizhe Xu, Rahul Thapa, Birju S. Patel, E. Steinberg, S. Fleming, M. Pfeffer, Jason Alan Fries, N. Shah",
                "citations": 25
            },
            {
                "title": "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT",
                "abstract": "ChatGPT has shown the potential of emerging general artificial intelligence capabilities, as it has demonstrated competent performance across many natural language processing tasks. In this work, we evaluate the capabilities of ChatGPT to perform text classification on three affective computing problems, namely, big-five personality prediction, sentiment analysis, and suicide tendency detection. We utilise three baselines, a robust language model (RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and a simple bag-of-words baseline (BoW). Results show that the RoBERTa trained for a specific downstream task generally has a superior performance. On the other hand, ChatGPT provides decent results, and is relatively comparable to the Word2Vec and BoW baselines. ChatGPT further shows robustness against noisy data, where Word2Vec models achieve worse results due to noise. Results indicate that ChatGPT is a good generalist model that is capable of achieving good results across various problems without any specialised training, however, it is not as good as a specialised model for a downstream task.",
                "authors": "Mostafa M. Amin, E. Cambria, Björn Schuller",
                "citations": 64
            },
            {
                "title": "Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation",
                "abstract": "Medical image analysis plays an important role in clinical diagnosis. In this paper, we examine the recent Segment Anything Model (SAM) on medical images, and report both quantitative and qualitative zero-shot segmentation results on nine medical image segmentation benchmarks, covering various imaging modalities, such as optical coherence tomography (OCT), magnetic resonance imaging (MRI), and computed tomography (CT), as well as different applications including dermatology, ophthalmology, and radiology. Those benchmarks are representative and commonly used in model development. Our experimental results indicate that while SAM presents remarkable segmentation performance on images from the general domain, its zero-shot segmentation ability remains restricted for out-of-distribution images, e.g., medical images. In addition, SAM exhibits inconsistent zero-shot segmentation performance across different unseen medical domains. For certain structured targets, e.g., blood vessels, the zero-shot segmentation of SAM completely failed. In contrast, a simple fine-tuning of it with a small amount of data could lead to remarkable improvement of the segmentation quality, showing the great potential and feasibility of using fine-tuned SAM to achieve accurate medical image segmentation for a precision diagnostics. Our study indicates the versatility of generalist vision foundation models on medical imaging, and their great potential to achieve desired performance through fine-turning and eventually address the challenges associated with accessing large and diverse medical datasets in support of clinical diagnostics.",
                "authors": "Peilun Shi, Jianing Qiu, Sai Mu Dalike Abaxi, Hao Wei, F. P. Lo, Wu Yuan",
                "citations": 74
            },
            {
                "title": "Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior",
                "abstract": "Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that-when fine-tuned appropriately-transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behavior across a wide range of downstream examples. We also find that fine-tuning these models yields more performance gains as model size increases, compared to training from scratch on new downstream tasks. These results hold for a broad range of PDE learning tasks. All in all, our results demonstrate the potential of the\"pre-train and fine-tune\"paradigm for SciML problems, demonstrating a path towards building SciML foundation models. We open-source our code for reproducibility.",
                "authors": "Shashank Subramanian, P. Harrington, K. Keutzer, W. Bhimji, D. Morozov, Michael W. Mahoney, A. Gholami",
                "citations": 44
            },
            {
                "title": "Computational Pathology at Health System Scale - Self-Supervised Foundation Models from Three Billion Images",
                "abstract": "Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and DINO algorithms. We evaluated performance on six clinically relevant tasks from three anatomic sites and two institutions: breast cancer detection, inflammatory bowel disease detection, breast cancer estrogen receptor prediction, lung adenocarcinoma EGFR mutation prediction, and lung cancer immunotherapy response prediction. Our results demonstrate that pre-training on pathology data is beneficial for downstream performance compared to pre-training on natural images. Additionally, the DINO algorithm achieved better generalization performance across all tasks tested. The presented results signify a phase change in computational pathology research, paving the way into a new era of more performant models based on large-scale, parallel pre-training at the billion-image scale.",
                "authors": "Gabriele Campanella, Ricky Kwan, Eugene Fluder, Jennifer Zeng, A. Stock, Brandon Veremis, A. Polydorides, Cyrus Hedvat, Adam J. Schoenfeld, Chad M. Vanderbilt, P. Kovatch, Carlos Cordon-Cardo, Thomas J. Fuchs",
                "citations": 22
            },
            {
                "title": "Large-scale Training of Foundation Models for Wearable Biosignals",
                "abstract": "Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch. We curated PPG and ECG datasets from AHMS that include data from ~141K participants spanning ~3 years. Our self-supervised learning framework includes participant level positive pair selection, stochastic augmentation module and a regularized contrastive loss optimized with momentum training, and generalizes well to both PPG and ECG modalities. We show that the pre-trained foundation models readily encode information regarding participants' demographics and health conditions. To the best of our knowledge, this is the first study that builds foundation models using large-scale PPG and ECG data collected via wearable consumer devices $\\unicode{x2013}$ prior works have commonly used smaller-size datasets collected in clinical and experimental settings. We believe PPG and ECG foundation models can enhance future wearable devices by reducing the reliance on labeled data and hold the potential to help the users improve their health.",
                "authors": "Salar Abbaspourazad, Oussama Elachqar, Andrew C. Miller, Saba Emrani, Udhyakumar Nallasamy, Ian Shapiro",
                "citations": 18
            },
            {
                "title": "CHORUS: Foundation Models for Unified Data Discovery and Exploration",
                "abstract": "We apply foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMS) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. We investigate the fundamental characteristics of this approach including generalizability to several foundation models and the impact of non-determinism on the outputs. All in all, this suggests a future direction in which disparate data management tasks can be unified under foundation models.",
                "authors": "Moe Kayali, A. Lykov, Ilias Fountalis, N. Vasiloglou, Dan Olteanu, Dan Suciu",
                "citations": 21
            },
            {
                "title": "Assessing the limits of zero-shot foundation models in single-cell biology",
                "abstract": "The advent and success of foundation models such as GPT has sparked growing interest in their application to single-cell biology. Models like Geneformer and scGPT have emerged with the promise of serving as versatile tools for this specialized field. However, the efficacy of these models, particularly in zero-shot settings where models are not fine-tuned but used without any further training, remains an open question, especially as practical constraints require useful models to function in settings that preclude fine-tuning (e.g., discovery settings where labels are not fully known). This paper presents a rigorous evaluation of the zero-shot performance of these proposed single-cell foundation models. We assess their utility in tasks such as cell type clustering and batch effect correction, and evaluate the generality of their pretraining objectives. Our results indicate that both Geneformer and scGPT exhibit limited reliability in zero-shot settings and often underperform compared to simpler methods. These findings serve as a cautionary note for the deployment of proposed single-cell foundation models and highlight the need for more focused research to realize their potential.2",
                "authors": "Kasia Z. Kedzierska, Lorin Crawford, Ava P. Amini, Alex X. Lu",
                "citations": 22
            },
            {
                "title": "Black Box Adversarial Prompting for Foundation Models",
                "abstract": "Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to significant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.",
                "authors": "N. Maus, Patrick Chao, Eric Wong, Jacob R. Gardner",
                "citations": 42
            },
            {
                "title": "SSL4EO-L: Datasets and Foundation Models for Landsat Imagery",
                "abstract": "The Landsat program is the longest-running Earth observation program in history, with 50+ years of data acquisition by 8 satellites. The multispectral imagery captured by sensors onboard these satellites is critical for a wide range of scientific fields. Despite the increasing popularity of deep learning and remote sensing, the majority of researchers still use decision trees and random forests for Landsat image analysis due to the prevalence of small labeled datasets and lack of foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for Earth Observation for the Landsat family of satellites (including 3 sensors and 2 product levels) and the largest Landsat dataset in history (5M image patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML benchmark datasets for Landsats 4-5 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first foundation models for Landsat imagery using SSL4EO-L and evaluate their performance on multiple semantic segmentation tasks. All datasets and model weights are available via the TorchGeo (https://github.com/microsoft/torchgeo) library, making reproducibility and experimentation easy, and enabling scientific advancements in the burgeoning field of remote sensing for a multitude of downstream applications.",
                "authors": "A. Stewart, Nils Lehmann, I. Corley, Yi Wang, Yi Chang, Nassim Ait Ali Braham, Shradha Sehgal, Caleb Robinson, Arindam Banerjee",
                "citations": 20
            },
            {
                "title": "Grasp-Anything: Large-scale Grasp Dataset from Foundation Models",
                "abstract": "Foundation models such as ChatGPT have made significant strides in robotic tasks due to their universal representation of real-world domains. In this paper, we leverage foundation models to tackle grasp detection, a persistent challenge in robotics with broad industrial applications. Despite numerous grasp datasets, their object diversity remains limited compared to real-world figures. Fortunately, foundation models possess an extensive repository of real-world knowledge, including objects we encounter in our daily lives. As a consequence, a promising solution to the limited representation in previous grasp datasets is to harness the universal knowledge embedded in these foundation models. We present Grasp-Anything, a new large-scale grasp dataset synthesized from foundation models to implement this solution. Grasp-Anything excels in diversity and magnitude, boasting 1M samples with text descriptions and more than 3M objects, surpassing prior datasets. Empirically, we show that Grasp-Anything successfully facilitates zero-shot grasp detection on vision-based tasks and real-world robotic experiments. Our dataset and code are available at https://airvlab.github.io/grasp-anything/.",
                "authors": "An Vuong, Minh N. Vu, Hieu Le, Baoru Huang, B. Huynh, T. Vo, Andreas Kugi, Anh Nguyen",
                "citations": 21
            },
            {
                "title": "Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models",
                "abstract": "Remote sensing imagery has attracted significant attention in recent years due to its instrumental role in global environmental monitoring, land usage monitoring, and more. As image databases grow each year, performing automatic segmentation with deep learning models has gradually become the standard approach for processing the data. Despite the improved performance of current models, certain limitations remain unresolved. Firstly, training deep learning models for segmentation requires per-pixel annotations. Given the large size of datasets, only a small portion is fully annotated and ready for training. Additionally, the high intra-dataset variance in remote sensing data limits the transfer learning ability of such models. Although recently proposed generic segmentation models like SAM have shown promising results in zero-shot instance-level segmentation, adapting them to semantic segmentation is a non-trivial task. To tackle these challenges, we propose a novel method named Text2Seg for remote sensing semantic segmentation. Text2Seg overcomes the dependency on extensive annotations by employing an automatic prompt generation process using different visual foundation models (VFMs), which are trained to understand semantic information in various ways. This approach not only reduces the need for fully annotated datasets but also enhances the model's ability to generalize across diverse datasets. Evaluations on four widely adopted remote sensing datasets demonstrate that Text2Seg significantly improves zero-shot prediction performance compared to the vanilla SAM model, with relative improvements ranging from 31% to 225%. Our code is available at https://github.com/Douglas2Code/Text2Seg.",
                "authors": "Jielu Zhang, Zhongliang Zhou, Gengchen Mai, Lan Mu, Mengxuan Hu, Sheng Li",
                "citations": 38
            },
            {
                "title": "Efficient Domain Adaptation for Speech Foundation Models",
                "abstract": "Foundation models (FMs), that are trained on broad data at scale and are adaptable to a wide range of downstream tasks, have brought large interest in the research community. Benefiting from the diverse data sources such as different modalities, languages and application domains, foundation models have demonstrated strong generalization and knowledge transfer capabilities. In this paper, we present a pioneering study towards building an efficient solution for FM-based speech recognition systems. We adopt the recently developed self-supervised BEST-RQ for pretraining, and extend the joint training strategy JUST Hydra for finetuning using both source and unsuper-vised target domain data. The FM encoder adapter and decoder are then finetuned to the target domain with a small amount of super-vised in-domain data. On a large-scale YouTube and Voice Search task, our method is shown to be both data and model parameter efficient. It achieves the same quality with only 21.6M supervised in-domain data and 130.8M finetuned parameters, compared to the 731.1M model trained from scratch on additional 300M supervised in-domain data.",
                "authors": "Bo Li, DongSeon Hwang, Zhouyuan Huo, Junwen Bai, Guru Prakash, Tara N. Sainath, K. Sim, Yu Zhang, Wei Han, Trevor Strohman, F. Beaufays",
                "citations": 19
            },
            {
                "title": "Revolutionizing Digital Pathology with the Power of Generative Artificial Intelligence and Foundation Models.",
                "abstract": null,
                "authors": "Asim Waqas, Marilyn M. Bui, E. Glassy, I. E. El Naqa, Piotr Borkowski, Andrew A Borkowski, Ghulam Rasool",
                "citations": 33
            },
            {
                "title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
                "abstract": "Large language models excel in many human-language tasks but often falter in highly specialized domains like scholarly astronomy. To bridge this gap, we introduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv. Optimized for traditional causal language modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2, showing marked domain adaptation. Our model generates more insightful and scientifically relevant text completions and embedding extraction than state-of-the-arts foundation models despite having significantly fewer parameters. AstroLLaMA serves as a robust, domain-specific model with broad fine-tuning potential. Its public release aims to spur astronomy-focused research, including automatic paper summarization and conversational agent development.",
                "authors": "Tuan Dung Nguyen, Yuan-Sen Ting, I. Ciucă, Charlie O'Neill, Ze-Chang Sun, Maja Jabłońska, S. Kruk, Ernest Perkowski, Jack William Miller, Jason Li, J. Peek, Kartheik G. Iyer, Tomasz R'o.za'nski, P. Khetarpal, Sharaf Zaman, D. Brodrick, Sergio J. Rodr'iguez M'endez, Thang Bui, Alyssa Goodman, A. Accomazzi, J. P. Naiman, Jesse Cranney, K. Schawinski, UniverseTBD",
                "citations": 15
            },
            {
                "title": "Multimodal Foundation Models Exploit Text to Make Medical Image Predictions",
                "abstract": "Multimodal foundation models have shown compelling but conflicting performance in medical image interpretation. However, the mechanisms by which these models integrate and prioritize different data modalities, including images and text, remain poorly understood. Here, using a diverse collection of 1014 multimodal medical cases, we evaluate the unimodal and multimodal image interpretation abilities of proprietary (GPT-4, Gemini Pro 1.0) and open-source (Llama-3.2-90B, LLaVA-Med-v1.5) multimodal foundational models with and without the use of text descriptions. Across all models, image predictions were largely driven by exploiting text, with accuracy increasing monotonically with the amount of informative text. By contrast, human performance on medical image interpretation did not improve with informative text. Exploitation of text is a double-edged sword; we show that even mild suggestions of an incorrect diagnosis in text diminishes image-based classification, reducing performance dramatically in cases the model could previously answer with images alone. Finally, we conducted a physician evaluation of model performance on long-form medical cases, finding that the provision of images either reduced or had no effect on model performance when text is already highly informative. Our results suggest that multimodal AI models may be useful in medical diagnostic reasoning but that their accuracy is largely driven, for better and worse, by their exploitation of text.",
                "authors": "Thomas A. Buckley, James A. Diao, P. Rajpurkar, Adam Rodman, Arjun K. Manrai",
                "citations": 15
            },
            {
                "title": "AstroCLIP: Cross-Modal Pre-Training for Astronomical Foundation Models",
                "abstract": "We present AstroCLIP, a strategy to facilitate the construction of astronomical foundation models that bridge the gap between diverse observational modalities. We demonstrate that a cross-modal contrastive learning approach between images and optical spectra of galaxies yields highly informative embeddings of both modalities. In particular, we apply our method on multi-band images and optical spectra from the Dark Energy Spectroscopic Instrument (DESI), and show that: (1) these embeddings are well-aligned between modalities and can be used for accurate cross-modal searches, and (2) these embeddings encode valuable physical information about the galaxies in particular redshift and stellar mass that can be used to achieve competitive zeroand fewshot predictions without further finetuning. Additionally, in the process of developing our approach, we also construct a novel, transformer-based model and pretraining approach for processing galaxy spectra.",
                "authors": "François Lanusse, Liam Parker, Siavash Golkar, M. Cranmer, Alberto Bietti, Michael Eickenberg, G. Krawezik, Michael McCabe, Ruben Ohana, Mariel Pettee, Bruno Régaldo-Saint Blancard, Tiberiu Teşileanu, Kyunghyun Cho, Shirley Ho",
                "citations": 17
            },
            {
                "title": "GFM: Building Geospatial Foundation Models via Continual Pretraining",
                "abstract": "Geospatial technologies are becoming increasingly essential in our world for a wide range of applications, including agriculture, urban planning, and disaster response. To help improve the applicability and performance of deep learning models on these geospatial tasks, various works have begun investigating foundation models for this domain. Researchers have explored two prominent approaches for introducing such models in geospatial applications, but both have drawbacks in terms of limited performance beneﬁt or prohibitive training cost. Therefore, in this work, we propose a novel paradigm for building highly effective geospatial foundation models with minimal resource cost and carbon impact. We ﬁrst construct a compact yet diverse dataset from multiple sources to promote feature diversity, which we term GeoPile. Then, we investigate the potential of continual pretraining from large-scale ImageNet-22k models and propose a multi-objective continual pretraining paradigm, which leverages the strong representations of ImageNet while simultaneously providing the freedom to learn valuable in-domain features. Our approach outperforms previous state-of-the-art geospatial pretraining methods in an extensive evaluation on seven downstream datasets covering various tasks such as change detection, classiﬁcation, multi-label classiﬁcation, semantic segmentation, and super-resolution.",
                "authors": "Mat'ias Mendieta, Boran Han, Xingjian Shi, Yi Zhu, Chen Chen, Mu Li",
                "citations": 16
            },
            {
                "title": "Prompt Federated Learning for Weather Forecasting: Toward Foundation Models on Meteorological Data",
                "abstract": "To tackle the global climate challenge, it urgently needs to develop a collaborative platform for comprehensive weather forecasting on large-scale meteorological data. Despite urgency, heterogeneous meteorological sensors across countries and regions, inevitably causing multivariate heterogeneity and data exposure, become the main barrier. This paper develops a foundation model across regions capable of understanding complex meteorological data and providing weather forecasting. To relieve the data exposure concern across regions, a novel federated learning approach has been proposed to collaboratively learn a brand-new spatio-temporal Transformer-based foundation model across participants with heterogeneous meteorological data. Moreover, a novel prompt learning mechanism has been adopted to satisfy low-resourced sensors' communication and computational constraints. The effectiveness of the proposed method has been demonstrated on classical weather forecasting tasks using three meteorological datasets with multivariate time series.",
                "authors": "Shen Chen, Guodong Long, Tao Shen, Jing Jiang",
                "citations": 28
            },
            {
                "title": "Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning",
                "abstract": "We introduce Dataset Grouper, a library to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library facilitates the creation of group-structured versions of existing datasets based on user-specified partitions and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper enables large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work, allowing for federated training of language models with hundreds of millions, and even billions, of parameters. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empirical risk minimization methods at this scale, suggesting their utility in downstream personalization and task-specific adaptation. Dataset Grouper is available at https://github.com/google-research/dataset_grouper.",
                "authors": "Zachary B. Charles, Nicole Mitchell, Krishna Pillutla, Michael Reneer, Zachary Garrett",
                "citations": 18
            },
            {
                "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models",
                "abstract": "Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like\"left\"can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.",
                "authors": "Joy Hsu, Jiayuan Mao, J. B. Tenenbaum, Jiajun Wu",
                "citations": 16
            },
            {
                "title": "Batched Low-Rank Adaptation of Foundation Models",
                "abstract": "Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and a multilingual speech recognition task across 6 languages.",
                "authors": "Yeming Wen, Swarat Chaudhuri",
                "citations": 16
            },
            {
                "title": "A Deep Dive into Single-Cell RNA Sequencing Foundation Models",
                "abstract": "Large-scale foundation models, which are pre-trained on massive, unlabeled datasets and subsequently fine-tuned on specific tasks, have recently achieved unparalleled success on a wide array of applications, including in healthcare and biology. In this paper, we explore two foundation models recently developed for single-cell RNA sequencing data, scBERT and scGPT. Focusing on the fine-tuning task of cell type annotation, we explore the relative performance of pre-trained models compared to a simple baseline, L1-regularized logistic regression, including in the few-shot setting. We perform ablation studies to understand whether pretraining improves model performance and to better understand the difficulty of the pre-training task in scBERT. Finally, using scBERT as an example, we demonstrate the potential sensitivity of fine-tuning to hyperparameter settings and parameter initializations. Taken together, our results highlight the importance of rigorously testing foundation models against well established baselines, establishing challenging fine-tuning tasks on which to benchmark foundation models, and performing deep introspection into the embeddings learned by the model in order to more effectively harness these models to transform single-cell data analysis. Code is available at https://github.com/clinicalml/sc-foundation-eval.",
                "authors": "Rebecca Boiarsky, Nalini M. Singh, Alejandro Buendia, Gad Getz, David Sontag",
                "citations": 15
            },
            {
                "title": "DIME-FM : DIstilling Multimodal and Efficient Foundation Models",
                "abstract": "Large Vision-Language Foundation Models (VLFM), such as CLIP, ALIGN and Florence, are trained on large-scale datasets of image-caption pairs and achieve superior transferability and robustness on downstream tasks, but they are difficult to use in many practical applications due to their large size, high latency and fixed architectures. Unfortunately, recent work shows training a small custom VLFM for resource-limited applications is currently very difficult using public and smaller-scale data. In this paper, we introduce a new distillation mechanism (DIME-FM) that allows us to transfer the knowledge contained in large VLFMs to smaller, customized foundation models using a relatively small amount of inexpensive, unpaired images and sentences. We transfer the knowledge from the pre-trained CLIP-ViT-L/14 model to a ViT-B/32 model, with only 40M public images and 28.4M unpaired public sentences. The resulting model \"Distill-ViT-B/32\" rivals the CLIP-ViT-B/32 model pre-trained on its private WiT dataset (400M image-text pairs): Distill-ViT-B/32 achieves similar results in terms of zero-shot and linear-probing performance on both Ima-geNet and the ELEVATER (20 image classification tasks) benchmarks. It also displays comparable robustness when evaluated on five datasets with natural distribution shifts from ImageNet. Please refer to our project page for code and more details.",
                "authors": "Ximeng Sun, Pengchuan Zhang, Peizhao Zhang, Hardik Shah, Kate Saenko, Xide Xia",
                "citations": 17
            },
            {
                "title": "Robot Learning in the Era of Foundation Models: A Survey",
                "abstract": "The proliferation of Large Language Models (LLMs) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence (AI). Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application. However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework. In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios. Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and AI alignment, etc.",
                "authors": "Xuan Xiao, Jiahang Liu, Zhipeng Wang, Yanmin Zhou, Yong Qi, Qian Cheng, Bin He, Shuo Jiang",
                "citations": 17
            },
            {
                "title": "MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models",
                "abstract": "Foundation models have shown outstanding performance and generalization capabilities across domains. Since most studies on foundation models mainly focus on the pretraining phase, a naive strategy to minimize a single task-specific loss is adopted for fine-tuning. However, such fine-tuning methods do not fully leverage other losses that are potentially beneficial for the target task. Therefore, we propose MEta Loss TRansformer (MELTR), a plug-in module that automatically and non-linearly combines various loss functions to aid learning the target task via auxiliary learning. We formulate the auxiliary learning as a bi-level optimization problem and present an efficient optimization algorithm based on Approximate Implicit Differentiation (AID). For evaluation, we apply our framework to various video foundation models (UniVL, Violet and All-in-one), and show significant performance gain on all four downstream tasks: text-to-video retrieval, video question answering, video captioning, and multimodal sentiment analysis. Our qualitative analyses demonstrate that MELTR adequately 'transforms' individual loss functions and 'melts' them into an effective unified loss. Code is available at https://github.com/mlvlab/MELTR.",
                "authors": "Dohwan Ko, Joon-Young Choi, Hyeong Kyu Choi, Kyoung-Woon On, Byungseok Roh, Hyunwoo J. Kim",
                "citations": 14
            },
            {
                "title": "Foundation Models in Healthcare: Opportunities, Risks & Strategies Forward",
                "abstract": "Foundation models (FMs) are a new paradigm in AI. First pretrained on broad data at immense scale and subsequently adapted to more specific tasks, they achieve high performances and unlock powerful new capabilities to be leveraged in many domains, including healthcare. This SIG will bring together researchers and practitioners within the CHI community interested in such emerging technology and healthcare. Drawing attention to the rapid evolution of these models and proposals for their wide-spread adoption, we aim to demonstrate their strengths whilst simultaneously highlighting deficiencies and limitations that give raise to ethical and societal concerns. In particular, we will invite the community to actively debate how the field of HCI – with its research frameworks and methods – can help address some of these existing challenges and mitigate risks to ensure the safe and ethical use of the end-product; a requirement to realize many of the ambitious visions for how these models can positively transform healthcare delivery. This conversation will benefit from a diversity of voices, critical perspectives, and open debate, which are necessary to bring about the right norms and best practices, and to identify a path forward in devising responsible approaches to future FM design and use in healthcare.",
                "authors": "Anja Thieme, A. Nori, M. Ghassemi, Rishi Bommasani, T. Andersen, E. Luger",
                "citations": 14
            },
            {
                "title": "Are Natural Domain Foundation Models Useful for Medical Image Classification?",
                "abstract": "The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely Sam, Seem, Dinov2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. Dinov2 consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.",
                "authors": "Joana Palés Huix, Adithya Raju Ganeshan, Johan Fredin Haslum, Magnus Söderberg, Christos Matsoukas, Kevin Smith",
                "citations": 14
            },
            {
                "title": "FORGE: Pre-Training Open Foundation Models for Science",
                "abstract": "Large language models (LLMs) are poised to revolutionize the way we conduct scientific research. However, both model complexity and pre-training cost are impeding effective adoption for the wider science community. Identifying suitable scientific use cases, finding the optimal balance between model and data sizes, and scaling up model training are among the most pressing issues that need to be addressed. In this study, we provide practical solutions for building and using LLM-based foundation models targeting scientific research use cases. We present an end-to-end examination of the effectiveness of LLMs in scientific research, including their scaling behavior and computational requirements on Frontier, the first Exascale supercomputer. We have also developed for release to the scientific community a suite of open foundation models called FORGE with up to 26B parameters using 257B tokens from over 200M scientific articles, with performance either on par or superior to other state-of-the-art comparable models. We have demonstrated the use and effectiveness of FORGE on scientific downstream tasks. Our research establishes best practices that can be applied across various fields to take advantage of LLMs for scientific discovery.",
                "authors": "Junqi Yin, Sajal Dash, Feiyi Wang, M. Shankar",
                "citations": 14
            },
            {
                "title": "Few-Shot Panoptic Segmentation With Foundation Models",
                "abstract": "Current state-of-the-art methods for panoptic segmentation require an immense amount of annotated training data that is both arduous and expensive to obtain posing a significant challenge for their widespread adoption. Concurrently, recent breakthroughs in visual representation learning have sparked a paradigm shift leading to the advent of large foundation models that can be trained with completely unlabeled images. In this work, we propose to leverage such task-agnostic image features to enable few-shot panoptic segmentation by presenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). In detail, our method combines a DINOv2 backbone with lightweight network heads for semantic segmentation and boundary estimation. We show that our approach, albeit being trained with only ten annotated images, predicts high-quality pseudo-labels that can be used with any existing panoptic segmentation method. Notably, we demonstrate that SPINO achieves competitive results compared to fully supervised baselines while using less than 0.3% of the ground truth labels, paving the way for learning complex visual recognition tasks leveraging foundation models. To illustrate its general applicability, we further deploy SPINO on real-world robotic vision systems for both outdoor and indoor environments. To foster future research, we make the code and trained models publicly available at http://spino.cs.uni-freiburg.de.",
                "authors": "Markus Kappeler, Kürsat Petek, Niclas Vodisch, Wolfram Burgard, Abhinav Valada",
                "citations": 14
            },
            {
                "title": "Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models",
                "abstract": "Foundation models like CLIP allow zero-shot transfer on various tasks without additional training data. Yet, the zero-shot performance is less competitive than a fully supervised one. Thus, to enhance the performance, fine-tuning and ensembling are also commonly adopted to better fit the downstream tasks. However, we argue that such prior work has overlooked the inherent biases in foundation models. Due to the highly imbalanced Web-scale training set, these foundation models are inevitably skewed toward frequent semantics, and thus the subsequent fine-tuning or ensembling is still biased. In this study, we systematically examine the biases in foundation models and demonstrate the efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that bias estimation in foundation models is challenging, as most pre-train data cannot be explicitly accessed like in traditional long-tailed classification tasks. To this end, GLA has an optimization-based bias estimation approach for debiasing foundation models. As our work resolves a fundamental flaw in the pre-training, the proposed GLA demonstrates significant improvements across a diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on long-tailed classification. Codes are in \\url{https://github.com/BeierZhu/GLA}.",
                "authors": "Beier Zhu, Kaihua Tang, Qianru Sun, Hanwang Zhang",
                "citations": 14
            },
            {
                "title": "Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models (Vision Paper)",
                "abstract": "In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for research directions and preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models and advocate for the development of privacy-preserving and secure GeoAI foundation models.",
                "authors": "Jinmeng Rao, Song Gao, Gengchen Mai, Krzysztof Janowicz",
                "citations": 14
            },
            {
                "title": "Market Concentration Implications of Foundation Models",
                "abstract": "We analyze the structure of the market for foundation models, i.e., large AI models such as those that power ChatGPT and that are adaptable to downstream uses, and we examine the implications for competition policy and regulation. We observe that the most capable models will have a tendency towards natural monopoly and may have potentially vast markets. This calls for a two-pronged regulatory response: (i) Antitrust authorities need to ensure the contestability of the market by tackling strategic behavior, in particular by ensuring that monopolies do not propagate vertically to downstream uses, and (ii) given the diminished potential for market discipline, there is a role for regulators to ensure that the most capable models meet sufficient quality standards (including safety, privacy, non-discrimination, reliability and interoperability standards) to maximally contribute to social welfare. Regulators should also ensure a level regulatory playing field between AI and non-AI applications in all sectors of the economy. For models that are behind the frontier, we expect competition to be quite intense, implying a more limited role for competition policy, although a role for regulation remains.",
                "authors": "Jai Vipra, Anton Korinek",
                "citations": 14
            },
            {
                "title": "AI Foundation Models for Weather and Climate: Applications, Design, and Implementation",
                "abstract": "Machine learning and deep learning methods have been widely explored in understanding the chaotic behavior of the atmosphere and furthering weather forecasting. There has been increasing interest from technology companies, government institutions, and meteorological agencies in building digital twins of the Earth. Recent approaches using transformers, physics-informed machine learning, and graph neural networks have demonstrated state-of-the-art performance on relatively narrow spatiotemporal scales and specific tasks. With the recent success of generative artificial intelligence (AI) using pre-trained transformers for language modeling and vision with prompt engineering and fine-tuning, we are now moving towards generalizable AI. In particular, we are witnessing the rise of AI foundation models that can perform competitively on multiple domain-specific downstream tasks. Despite this progress, we are still in the nascent stages of a generalizable AI model for global Earth system models, regional climate models, and mesoscale weather models. Here, we review current state-of-the-art AI approaches, primarily from transformer and operator learning literature in the context of meteorology. We provide our perspective on criteria for success towards a family of foundation models for nowcasting and forecasting weather and climate predictions. We also discuss how such models can perform competitively on downstream tasks such as downscaling (super-resolution), identifying conditions conducive to the occurrence of wildfires, and predicting consequential meteorological phenomena across various spatiotemporal scales such as hurricanes and atmospheric rivers. In particular, we examine current AI methodologies and contend they have matured enough to design and implement a weather foundation model.",
                "authors": "S. K. Mukkavilli, D. S. Civitarese, J. Schmude, Johannes Jakubik, Anne Jones, Nam Nguyen, C. Phillips, Sujit Roy, Shraddha Singh, Campbell Watson, R. Ganti, Hendrik F. Hamann, U. Nair, Rahul Ramachandran, Kommy Weldemariam",
                "citations": 14
            },
            {
                "title": "Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",
                "abstract": "Foundation models or pre-trained models have substantially improved the performance of various language, vision, and vision-language understanding tasks. However, existing foundation models can only perform the best in one type of tasks, namely language, vision, or vision-language. It is still an open question whether it is possible to construct a foundation model performing the best for all the understanding tasks, which we call a general foundation model. In this paper, we propose a new general foundation model, X-FM (the X-Foundation Model). X-FM has one language encoder, one vision encoder, and one fusion encoder, as well as a new training method. The training method includes two new techniques for learning X-FM from text, image, and image-text pair data. One is to stop gradients from the vision-language training when learning the language encoder. The other is to leverage the vision-language training to guide the learning of the vision encoder. Extensive experiments on benchmark datasets show that X-FM can significantly outperform existing general foundation models and perform better than or comparable to existing foundation models specifically for language, vision, or vision-language understanding. Code and pre-trained models are released at https://github.com/zhangxinsong-nlp/XFM.",
                "authors": "Xinsong Zhang, Yan Zeng, Jipeng Zhang, Hang Li",
                "citations": 13
            },
            {
                "title": "3D Open-vocabulary Segmentation with Foundation Models",
                "abstract": "Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.",
                "authors": "Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El-Saddik, C. Theobalt, Eric P. Xing, Shijian Lu",
                "citations": 13
            },
            {
                "title": "Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models",
                "abstract": "Human-object interaction (HOI) detection aims to comprehend the intricate relationships between humans and objects, predicting $$ triplets, and serving as the foundation for numerous computer vision tasks. The complexity and diversity of human-object interactions in the real world, however, pose significant challenges for both annotation and recognition, particularly in recognizing interactions within an open world context. This study explores the universal interaction recognition in an open-world setting through the use of Vision-Language (VL) foundation models and large language models (LLMs). The proposed method is dubbed as \\emph{\\textbf{UniHOI}}. We conduct a deep analysis of the three hierarchical features inherent in visual HOI detectors and propose a method for high-level relation extraction aimed at VL foundation models, which we call HO prompt-based learning. Our design includes an HO Prompt-guided Decoder (HOPD), facilitates the association of high-level relation representations in the foundation model with various HO pairs within the image. Furthermore, we utilize a LLM (\\emph{i.e.} GPT) for interaction interpretation, generating a richer linguistic understanding for complex HOIs. For open-category interaction recognition, our method supports either of two input types: interaction phrase or interpretive sentence. Our efficient architecture design and learning methods effectively unleash the potential of the VL foundation models and LLMs, allowing UniHOI to surpass all existing methods with a substantial margin, under both supervised and zero-shot settings. The code and pre-trained weights are available at: \\url{https://github.com/Caoyichao/UniHOI}.",
                "authors": "Yichao Cao, Qingfei Tang, Xiu Su, Chen Song, Shan You, Xiaobo Lu, Chang Xu",
                "citations": 13
            },
            {
                "title": "One-Shot Open Affordance Learning with Foundation Models",
                "abstract": "We introduce One-shot Open Affordance Learning (OOAL), where a model is trained with just one example per base object category, but is expected to identify novel objects and affordances. While vision-language models excel at recognizing novel objects and scenes, they often struggle to understand finer levels of granularity such as affordances. To handle this issue, we conduct a comprehensive analysis of existing foundation models, to explore their inherent understanding of affordances and assess the potential for data-limited affordance learning. We then propose a vision-language framework with simple and effective designs that boost the alignment between visual features and affordance text embeddings. Experiments on two affordance segmentation benchmarks show that the proposed method outperforms state-of-the-art models with less than 1% of the full training data, and exhibits reasonable generalization capability on unseen objects and affordances. Project page: https://reagan1311.github.io/ooal.",
                "authors": "Gen Li, Deqing Sun, Laura Sevilla-Lara, Varun Jampani",
                "citations": 13
            },
            {
                "title": "Large Foundation Models for Power Systems",
                "abstract": "Foundation models, such as Large Language Models (LLMs), can respond to a wide range of format-free queries without any task-specific data collection or model training, creating various research and application opportunities for the modeling and operation of large-scale power systems. In this paper, we outline how such large foundation model such as GPT-4 are developed, and discuss how they can be leveraged in challenging power and energy system tasks. We first investigate the potential of existing foundation models by validating their performance on four representative tasks across power system domains, including the optimal power flow (OPF), electric vehicle (EV) scheduling, knowledge retrieval for power engineering technical reports, and situation awareness. Our results indicate strong capabilities of such foundation models on boosting the efficiency and reliability of power system operational pipelines. We also provide suggestions and projections on future deployment of foundation models in power system applications.",
                "authors": "Chenghao Huang, Siyang Li, Ruohong Liu, Hao Wang, Yize Chen",
                "citations": 11
            },
            {
                "title": "RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data Lakes",
                "abstract": "Can foundation models (such as ChatGPT) clean your data? In this proposal, we demonstrate that indeed ChatGPT can assist in data cleaning by suggesting corrections for specific cells in a data table (scenario 1). However, ChatGPT may struggle with datasets it has never encountered before (e.g., local enterprise data) or when the user requires an explanation of the source of the suggested clean values. To address these issues, we developed a retrieval-based method that complements ChatGPT’s power with a user-provided data lake. The data lake is first indexed, we then retrieve the top-𝑘 relevant tuples to the user’s query tuple and finally leverage Chat-GPT to infer the correct value (scenario 2). Nevertheless, sharing enterprise data with ChatGPT, an externally hosted model, might not be feasible for privacy reasons. To assist with this scenario, we developed a custom RoBERTa-based foundation model that can be locally deployed. By fine-tuning it on a small number of examples, it can effectively make value inferences based on the retrieved tuples (scenario 3). Our proposed system, RetClean , seamlessly supports all three scenarios and provides a user-friendly GUI that enables the VLDB audience to explore and experiment with the system.",
                "authors": "M. Ahmad, Z. Naeem, M. Eltabakh, M. Ouzzani, N. Tang",
                "citations": 11
            },
            {
                "title": "Applications of Large Scale Foundation Models for Autonomous Driving",
                "abstract": "Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007, autonomous driving has been the most active field of AI applications. Recently powered by large language models (LLMs), chat systems, such as chatGPT and PaLM, emerge and rapidly become a promising direction to achieve artificial general intelligence (AGI) in natural language processing (NLP). There comes a natural thinking that we could employ these abilities to reformulate autonomous driving. By combining LLM with foundation models, it is possible to utilize the human knowledge, commonsense and reasoning to rebuild autonomous driving systems from the current long-tailed AI dilemma. In this paper, we investigate the techniques of foundation models and LLMs applied for autonomous driving, categorized as simulation, world model, data annotation and planning or E2E solutions etc.",
                "authors": "Yu Huang, Yue Chen, Zhu Li",
                "citations": 10
            },
            {
                "title": "Geo-Foundation Models: Reality, Gaps and Opportunities",
                "abstract": "With the recent rapid advances of revolutionary AI models such as ChatGPT, foundation models have become a main topic for the discussion of future AI. Despite the excitement, the success is still limited to specific types of tasks. Particularly, ChatGPT and similar foundation models have unique characteristics that are difficult to replicate for most geospatial tasks. This paper envisions several major challenges and opportunities in the creation of geospatial foundation (geo-foundation) models, as well as potential future adoption scenarios. We also expect that a major success story is necessary for geo-foundation models to take off in the long term.",
                "authors": "Yiqun Xie, Zhaonan Wang, Gengchen Mai, Yanhua Li, Xiaowei Jia, Song Gao, Shaowen Wang",
                "citations": 11
            },
            {
                "title": "Federated Prompt Learning for Weather Foundation Models on Devices",
                "abstract": "On-device intelligence for weather forecasting uses local deep learning models to analyze weather patterns without centralized cloud computing, holds significance for supporting human activates. Federated Learning is a promising solution for such forecasting by enabling collaborative model training without sharing raw data. However, it faces three main challenges that hinder its reliability: (1) data heterogeneity among devices due to geographic differences; (2) data homogeneity within individual devices and (3) communication overload from sending large model parameters for collaboration. To address these challenges, this paper propose Federated Prompt learning for Weather Foundation Models on Devices (FedPoD), which enables devices to obtain highly customized models while maintaining communication efficiency. Concretely, our Adaptive Prompt Tuning leverages lightweight prompts guide frozen foundation model to generate more precise predictions, also conducts prompt-based multi-level communication to encourage multi-source knowledge fusion and regulate optimization. Additionally, Dynamic Graph Modeling constructs graphs from prompts, prioritizing collaborative training among devices with similar data distributions to against heterogeneity. Extensive experiments demonstrates FedPoD leads the performance among state-of-the-art baselines across various setting in real-world on-device weather forecasting datasets.",
                "authors": "Shen Chen, Guodong Long, Tao Shen, Tianyi Zhou, Jing Jiang",
                "citations": 9
            },
            {
                "title": "Integrating Visual Foundation Models for Enhanced Robot Manipulation and Motion Planning: A Layered Approach",
                "abstract": "This paper presents a novel layered framework that integrates visual foundation models to improve robot manipulation tasks and motion planning. The framework consists of five layers: Perception, Cognition, Planning, Execution, and Learning. Using visual foundation models, we enhance the robot's perception of its environment, enabling more efficient task understanding and accurate motion planning. This approach allows for real-time adjustments and continual learning, leading to significant improvements in task execution. Experimental results demonstrate the effectiveness of the proposed framework in various robot manipulation tasks and motion planning scenarios, highlighting its potential for practical deployment in dynamic environments.",
                "authors": "Chenguang Yang, Peng Zhou, Jiaming Qi",
                "citations": 9
            },
            {
                "title": "Foundation models such as ChatGPT through the prism of the UNESCO Recommendation on the Ethics of Artificial Intelligence",
                "abstract": "The release into the public domain and massive growth in the user base of artificial intelligence (AI) foundation models for text, images, and audio is fuelling debate about the risks they pose to work, education, scientific research, and democracy, as well as their potential negative impacts on cultural diversity and cross-cultural interactions, among other areas. Foundation models are AI systems that are characterized by the use of very large machine learning models trained on massive unlabelled data sets using considerable compute resources. Examples include large language models (LLMs) such as the GPT series and Bard, and image generator tools such as DALL·E 2 and Stable Diffusion. This discussion paper focuses on a widely used foundation model, ChatGPT, as a case study, but many of the points below are applicable to other LLMs and foundation models more broadly. UNESCO Catno: 0000385629",
                "authors": "",
                "citations": 7
            },
            {
                "title": "Adapting Vision Foundation Models for Plant Phenotyping",
                "abstract": "Foundation models are large models pre-trained on tremendous amount of data. They can be typically adapted to diverse downstream tasks with minimal effort. However, as foundation models are usually pre-trained on images or texts sourced from the Internet, their performance in specialized domains, such as plant phenotyping, comes into question. In addition, fully fine-tuning foundation models is time-consuming and requires high computational power. This paper investigates the efficient adaptation of foundation models for plant phenotyping settings and tasks. We perform extensive experiments on fine-tuning three foundation models, MAE, DINO, and DINOv2 on three essential plant phenotyping tasks: leaf counting, instance segmentation, and disease classification. In particular, the pretrained backbones are kept frozen, while two distinct fine-tuning methods are evaluated, namely adapter tuning (using LoRA) and decoder tuning. The experimental results show that a foundation model can be efficiently adapted to multiple plant phenotyping tasks, yielding similar performance as the state-of-the-art (SoTA) models specifically designed or trained for each task. Despite exhibiting great transferability over different tasks, the fine-tuned foundation models perform slightly worse than the SoTA task-specific models in some scenarios, which requires further investigation.",
                "authors": "Feng Chen, M. Giuffrida, S. Tsaftaris",
                "citations": 7
            },
            {
                "title": "Open World Object Detection in the Era of Foundation Models",
                "abstract": "Object detection is integral to a bevy of real-world applications, from robotics to medical image analysis. To be used reliably in such applications, models must be capable of handling unexpected - or novel - objects. The open world object detection (OWD) paradigm addresses this challenge by enabling models to detect unknown objects and learn discovered ones incrementally. However, OWD method development is hindered due to the stringent benchmark and task definitions. These definitions effectively prohibit foundation models. Here, we aim to relax these definitions and investigate the utilization of pre-trained foundation models in OWD. First, we show that existing benchmarks are insufficient in evaluating methods that utilize foundation models, as even naive integration methods nearly saturate these benchmarks. This result motivated us to curate a new and challenging benchmark for these models. Therefore, we introduce a new benchmark that includes five real-world application-driven datasets, including challenging domains such as aerial and surgical images, and establish baselines. We exploit the inherent connection between classes in application-driven datasets and introduce a novel method, Foundation Object detection Model for the Open world, or FOMO, which identifies unknown objects based on their shared attributes with the base known objects. FOMO has ~3x unknown object mAP compared to baselines on our benchmark. However, our results indicate a significant place for improvement - suggesting a great research opportunity in further scaling object detection methods to real-world domains. Our code and benchmark are available at https://orrzohar.github.io/projects/fomo/.",
                "authors": "O. Zohar, Alejandro Lozano, Shelly Goel, Serena Yeung, Kuan-Chieh Wang",
                "citations": 8
            },
            {
                "title": "Universal Domain Adaptation from Foundation Models",
                "abstract": "Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target data distillation on the CLIP model, and achieves consistent improvement over the baseline across all the UniDA benchmarks. Our studies are under a newly proposed evaluation metric of universal classification rate (UCR), which is threshold- and ratio-free and addresses the threshold-sensitive issue encountered when using the existing H-score metric.",
                "authors": "Bin Deng, K. Jia",
                "citations": 7
            },
            {
                "title": "Can Foundation Models Wrangle Your Data?",
                "abstract": "Foundation Models (FMs) are models trained on large corpora of data that, at very large scale, can generalize to new tasks without any task-specific finetuning. As these models continue to grow in size, innovations continue to push the boundaries of what these models can do on language and image tasks. This paper aims to understand an underexplored area of FMs: classical data tasks like cleaning and integration. As a proof-of-concept, we cast five data cleaning and integration tasks as prompting tasks and evaluate the performance of FMs on these tasks. We find that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks. We identify specific research challenges and opportunities that these models present, including challenges with private and domain specific data, and opportunities to make data management systems more accessible to non-experts. We make our code and experiments publicly available at: https://github.com/HazyResearch/fm_data_tasks.",
                "authors": "A. Narayan, Ines Chami, Laurel J. Orr, Christopher R'e",
                "citations": 165
            },
            {
                "title": "Multimodal Foundation Models For Echocardiogram Interpretation",
                "abstract": "Multimodal deep learning foundation models can learn the relationship between images and text. In the context of medical imaging, mapping images to language concepts reflects the clinical task of diagnostic image interpretation, however current general-purpose foundation models do not perform well in this context because their training corpus have limited medical text and images. To address this challenge and account for the range of cardiac physiology, we leverage 1,032,975 cardiac ultrasound videos and corresponding expert interpretations to develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP displays strong zero-shot (not explicitly trained) performance in cardiac function assessment (external validation left ventricular ejection fraction mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and artificial heart valves). We also developed a long-context variant (EchoCLIP-R) with a custom echocardiography report text tokenizer which can accurately identify unique patients across multiple videos (AUC of 0.86), identify clinical changes such as orthotopic heart transplants (AUC of 0.79) or cardiac surgery (AUC 0.77), and enable robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These emergent capabilities can be used for preliminary assessment and summarization of echocardiographic findings.",
                "authors": "M. Christensen, Milos Vukadinovic, N. Yuan, David Ouyang",
                "citations": 7
            },
            {
                "title": "Backdoor Attacks to Pre-trained Unified Foundation Models",
                "abstract": "The rise of pre-trained unified foundation models breaks down the barriers between different modalities and tasks, providing comprehensive support to users with unified architectures. However, the backdoor attack on pre-trained models poses a serious threat to their security. Previous research on backdoor attacks has been limited to uni-modal tasks or single tasks across modalities, making it inapplicable to unified foundation models. In this paper, we make proof-of-concept level research on the backdoor attack for pre-trained unified foundation models. Through preliminary experiments on NLP and CV classification tasks, we reveal the vulnerability of these models and suggest future research directions for enhancing the attack approach.",
                "authors": "Zenghui Yuan, Yixin Liu, Kai Zhang, Pan Zhou, Lichao Sun",
                "citations": 8
            },
            {
                "title": "Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation",
                "abstract": null,
                "authors": "Julio Silva-Rodr'iguez, J. Dolz, Ismail Ben Ayed",
                "citations": 8
            },
            {
                "title": "Investigating the Emergent Audio Classification Ability of ASR Foundation Models",
                "abstract": "Text and vision foundation models can perform many tasks in a zero-shot setting, a desirable property that enables these systems to be applied in general and low-resource settings. There has been far less work, however, on the zero-shot abilities of ASR foundation models, with these systems typically fine-tuned to specific tasks or constrained to applications that match their training criterion and data annotation. In this work we investigate the ability of Whisper and MMS, ASR foundation models trained primarily for speech recognition, to perform zero-shot audio classification. We use simple template-based text prompts at the decoder and use the resulting decoding probabilities to generate zero-shot predictions. Without training the model on extra data or adding any new parameters, we demonstrate that Whisper shows promising zero-shot classification performance on a range of 8 audio-classification datasets, outperforming the accuracy of existing state-of-the-art zero-shot baselines by an average of 9%. One important step to unlock the emergent ability is debiasing, where a simple unsupervised reweighting method of the class probabilities yields consistent significant performance gains. We further show that performance increases with model size, implying that as ASR foundation models scale up, they may exhibit improved zero-shot performance.",
                "authors": "Rao Ma, Adian Liusie, M. Gales, K. Knill",
                "citations": 6
            },
            {
                "title": "Granite Foundation Models",
                "abstract": "—We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint, testing and evaluation, socio-technical harms and mitigations, and usage policies.",
                "authors": "Ibm Research",
                "citations": 5
            },
            {
                "title": "Comparing Foundation Models using Data Kernels",
                "abstract": "Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models, known as foundation models, which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves evaluating them with aggregate metrics on various benchmark datasets. This method of model comparison is heavily dependent on the chosen evaluation metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a methodology for directly comparing the embedding space geometry of foundation models, which facilitates model comparison without the need for an explicit evaluation metric. Our methodology is grounded in random graph theory and enables valid hypothesis testing of embedding similarity on a per-datum basis. Further, we demonstrate how our methodology can be extended to facilitate population level model comparison. In particular, we show how our framework can induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics. We remark on the utility of this population level model comparison as a first step towards a taxonomic science of foundation models.",
                "authors": "Brandon Duderstadt, Hayden S. Helm, Carey E. Priebe",
                "citations": 5
            },
            {
                "title": "Foundation Models for Mining 5.0: Challenges, Frameworks, and Opportunities",
                "abstract": "In recent years, there has been a widespread interest in autonomous driving, with researchers primarily focusing on issues such as edge computing, constrained scenarios, and model generalization. ChatGPT is a large-scale natural language model based on the Generative Pre-trained Transformer algorithm. It has gained popularity worldwide and is commonly referred to as a \"large model\" or \"foundation model\" due to its high parameter count (over 100M). These models possess human-like understanding capabilities, enabling them to address complex challenges through self-learning and unifying multiple subtasks. While these models are primarily designed for natural language processing (NLP) tasks, their effectiveness has also been proven in the visual and multi-modal domains. Therefore, foundation models hold promise for contributing to the development of autonomous driving technology. Autonomous driving in mining areas is considered an edge scenario, and the reliable capabilities of foundation models make them highly beneficial in such complex and harsh working environments. This article aims to explore the characteristics of mining areas, the features of foundation models, as well as the framework and key technologies related to large models in mining. It provides technical guidance for the implementation of next-generation (mining 5.0) foundation models in unmanned mines.",
                "authors": "Yuchen Li, Siyu Teng, Lingxi Li, Zhe Xuanyuan, Long Chen",
                "citations": 5
            },
            {
                "title": "Foundation models and the privatization of public knowledge",
                "abstract": null,
                "authors": "Fabian Ferrari, José van Dijck, Antal van den Bosch",
                "citations": 4
            },
            {
                "title": "Danish Foundation Models",
                "abstract": "Large language models, sometimes referred to as foundation models, have transformed multiple fields of research. However, smaller languages risk falling behind due to high training costs and small incentives for large companies to train these models. To combat this, the Danish Foundation Models project seeks to provide and maintain open, well-documented, and high-quality foundation models for the Danish language. This is achieved through broad cooperation with public and private institutions, to ensure high data quality and applicability of the trained models. We present the motivation of the project, the current status, and future perspectives.",
                "authors": "K. Enevoldsen, Lasse Hansen, Dan S. Nielsen, R. A. F. Egebæk, Soren V. Holm, Martin C. Nielsen, M. Bernstorff, Rasmus Larsen, Peter B. Jorgensen, Malte Højmark-Bertelsen, P. B. Vahlstrup, Per Moldrup-Dalum, Kristoffer L. Nielbo",
                "citations": 2
            },
            {
                "title": "DeFACT in ManuVerse for Parallel Manufacturing: Foundation Models and Parallel Workers in Smart Factories",
                "abstract": "In cyber–physical–social systems, smart manufacturing has to overcome challenges, such as uncertainty, diversity, complexity in modeling, long-delayed responses to market changes, and human engineer dependency. DeFACT is a framework of parallel manufacturing in ManuVerse where the Decentralized Autonomous Organization-based interactions between parallel workers consisting of robotic, digital, and human workers are elaborated to transform from professional division to real-virtual division. In DeFACT, human workers are only responsible for 5% physical and mental work that is complex and creative, and the robotic and digital workers can take care of the rest. The perceptual and cognitive intelligence of digital workers are intensified by a manufacturing foundation model (MF-PC), where calibration and certification (C&C), and verification and validation (V&V) guarantee not only the accuracy of task models, but also the interpretability and controllability of feature learning. As a case study, the workflow of customized shoes of SANBODY Technology Company is illustrated to show how DeFACT breaks the time and space constraints, avoids production waste caused by aesthetic discrepancies with consumers, and truly realizes flexible manufacturing.",
                "authors": "Jing Yang, Shimeng Li, Xiaoxi Wang, Jingwei Lu, Huaiyu Wu, Xiao Wang",
                "citations": 17
            },
            {
                "title": "Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey",
                "abstract": "As artificial intelligence (AI) continues to rapidly evolve, the realm of Earth and atmospheric sciences is increasingly adopting data-driven models, powered by progressive developments in deep learning (DL). Specifically, DL techniques are extensively utilized to decode the chaotic and nonlinear aspects of Earth systems, and to address climate challenges via understanding weather and climate data. Cutting-edge performance on specific tasks within narrower spatio-temporal scales has been achieved recently through DL. The rise of large models, specifically large language models (LLMs), has enabled fine-tuning processes that yield remarkable outcomes across various downstream tasks, thereby propelling the advancement of general AI. However, we are still navigating the initial stages of crafting general AI for weather and climate. In this survey, we offer an exhaustive, timely overview of state-of-the-art AI methodologies specifically engineered for weather and climate data, with a special focus on time series and text data. Our primary coverage encompasses four critical aspects: types of weather and climate data, principal model architectures, model scopes and applications, and datasets for weather and climate. Furthermore, in relation to the creation and application of foundation models for weather and climate data understanding, we delve into the field's prevailing challenges, offer crucial insights, and propose detailed avenues for future research. This comprehensive approach equips practitioners with the requisite knowledge to make substantial progress in this domain. Our survey encapsulates the most recent breakthroughs in research on large, data-driven models for weather and climate data understanding, emphasizing robust foundations, current advancements, practical applications, crucial resources, and prospective research opportunities.",
                "authors": "Shengchao Chen, Guodong Long, Jing Jiang, Dikai Liu, Chengqi Zhang",
                "citations": 16
            },
            {
                "title": "Federated Generative Learning with Foundation Models",
                "abstract": "Existing approaches in Federated Learning (FL) mainly focus on sending model parameters or gradients from clients to a server. However, these methods are plagued by significant inefficiency, privacy, and security concerns. Thanks to the emerging foundation generative models, we propose a novel federated learning framework, namely Federated Generative Learning. In this framework, each client can create text embeddings that are tailored to their local data, and send embeddings to the server. Then the informative training data can be synthesized remotely on the server using foundation generative models with these embeddings, which can benefit FL tasks. Our proposed framework offers several advantages, including increased communication efficiency, robustness to data heterogeneity, substantial performance improvements, and enhanced privacy protection. We validate these benefits through extensive experiments conducted on 12 datasets. For example, on the ImageNet100 dataset with a highly skewed data distribution, our method outperforms FedAvg by 12% in a single communication round, compared to FedAvg's performance over 200 communication rounds. We have released the code for all experiments conducted in this study.",
                "authors": "J. Zhang, Xiaohua Qi, Bo-Lu Zhao",
                "citations": 14
            },
            {
                "title": "Panel: Multimodal Large Foundation Models",
                "abstract": "The surprisingly fluent predictive performance of LLM (Large Language Models) as well as the high-quality photo-realistic rendering of Diffusion Models has heralded a new beginning in the area of Generative AI. Such kinds of deep learning based models with billions of parameters and pre-trained on massive-scale data-sets are also called Large Foundation Models (LFM). These models not only have caught the public imagination but also have led to an unprecedented surge in interest towards the applications of these models. Instead of the previous approach of developing AI models for specific tasks, more and more researchers are developing large task-agnostic models pre-trained on massive data, which can then be adapted to a variety of downstream tasks via fine-tuning, fewshot learning, or zero-shot learning. Some examples are ChatGPT, LLaMA, GPT-4, Flamingo, MidJourney, Stable-Diffusion and DALLE. Some of them can handle text (e.g., ChatGPT, LLaMA) while some others (e.g., GPT-4 and Flamingo) can utilize multimodal data and can hence be considered Multimodal Large Foundation Models (MLFM). Several recent studies have shown that when adapted to specific tasks (e.g., visual question answering), the foundation models can often surpass the performance of state-of-the-art, fully supervised AI models. However, applying foundation models to specialized domain tasks (e.g., medical diagnosis, financial recommendation etc.) raises many ethical issues (e.g., privacy, model bias or hallucinations). The panel members will discuss the emerging trends in the development and use of large multimodal foundation models. Some of the issues to be discussed are: Research issues in going from LLM to MLFM Behaviour of MLFM Application Potential of MLFM Trust issues in MLFM Limitations of MLFM Societal, Legal and Regulatory issues of MLFM Promising future research in MLFM This panel will bring together several leading experts from universities, research institutions, and industry who will discuss and debate together with the audience. We invite everybody to participate and contribute towards this important and promising research direction.",
                "authors": "Mohan S. Kankanhalli, M. Worring",
                "citations": 0
            },
            {
                "title": "Decentralized Training of Foundation Models in Heterogeneous Environments",
                "abstract": "Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational\"tasklets\"in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8X faster than prior state-of-the-art training systems (Megatron).",
                "authors": "Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy Liang, Christopher Ré, Ce Zhang",
                "citations": 75
            },
            {
                "title": "Leveraging language foundation models for human mobility forecasting",
                "abstract": "In this paper, we propose a novel pipeline that leverages language foundation models for temporal sequential pattern mining, such as for human mobility forecasting tasks. For example, in the task of predicting Place-of-Interest (POI) customer flows, typically the number of visits is extracted from historical logs, and only the numerical data are used to predict visitor flows. In this research, we perform the forecasting task directly on the natural language input that includes all kinds of information such as numerical values and contextual semantic information. Specific prompts are introduced to transform numerical temporal sequences into sentences so that existing language models can be directly applied. We design an AuxMobLCast pipeline for predicting the number of visitors in each POI, integrating an auxiliary POI category classification task with the encoder-decoder architecture. This research provides empirical evidence of the effectiveness of the proposed AuxMobLCast pipeline to discover sequential patterns in mobility forecasting tasks. The results, evaluated on three real-world datasets, demonstrate that pre-trained language foundation models also have good performance in forecasting temporal sequences. This study could provide visionary insights and lead to new research directions for predicting human mobility.",
                "authors": "Hao Xue, Bhanu Prakash Voutharoja, Flora D. Salim",
                "citations": 52
            },
            {
                "title": "ChemBERTa-2: Towards Chemical Foundation Models",
                "abstract": "Large pretrained models such as GPT-3 have had tremendous impact on modern natural language processing by leveraging self-supervised learning to learn salient representations that can be used to readily finetune on a wide variety of downstream tasks. We investigate the possibility of transferring such advances to molecular machine learning by building a chemical foundation model, ChemBERTa-2, using the language of SMILES. While labeled data for molecular prediction tasks is typically scarce, libraries of SMILES strings are readily available. In this work, we build upon ChemBERTa by optimizing the pretraining process. We compare multi-task and self-supervised pretraining by varying hyperparameters and pretraining dataset size, up to 77M compounds from PubChem. To our knowledge, the 77M set constitutes one of the largest datasets used for molecular pretraining to date. We find that with these pretraining improvements, we are competitive with existing state-of-the-art architectures on the MoleculeNet benchmark suite. We analyze the degree to which improvements in pretraining translate to improvement on downstream tasks.",
                "authors": "Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar",
                "citations": 105
            },
            {
                "title": "Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models",
                "abstract": "A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. We call the resulting models self-destructing models, inspired by mechanisms that prevent adversaries from using tools for harmful purposes. We present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, which we call meta-learned adversarial censoring (MLAC). In a small-scale experiment, we show MLAC can largely prevent a BERT-style model from being re-purposed to perform gender identification without harming the model’s ability to perform profession classification.",
                "authors": "E. Mitchell, Peter Henderson, Christopher D. Manning, Dan Jurafsky, Chelsea Finn",
                "citations": 42
            },
            {
                "title": "Continual Learning with Foundation Models: An Empirical Study of Latent Replay",
                "abstract": "Rapid development of large-scale pre-training has resulted in foundation models that can act as effective feature extractors on a variety of downstream tasks and domains. Motivated by this, we study the efficacy of pre-trained vision models as a foundation for downstream continual learning (CL) scenarios. Our goal is twofold. First, we want to understand the compute-accuracy trade-off between CL in the raw-data space and in the latent space of pre-trained encoders. Second, we investigate how the characteristics of the encoder, the pre-training algorithm and data, as well as of the resulting latent space affect CL performance. For this, we compare the efficacy of various pre-trained models in large-scale benchmarking scenarios with a vanilla replay setting applied in the latent and in the raw-data space. Notably, this study shows how transfer, forgetting, task similarity and learning are dependent on the input data characteristics and not necessarily on the CL algorithms. First, we show that under some circumstances reasonable CL performance can readily be achieved with a non-parametric classifier at negligible compute. We then show how models pre-trained on broader data result in better performance for various replay sizes. We explain this with representational similarity and transfer properties of these representations. Finally, we show the effectiveness of self-supervised pre-training for downstream domains that are out-of-distribution as compared to the pre-training domain. We point out and validate several research directions that can further increase the efficacy of latent CL including representation ensembling. The diverse set of datasets used in this study can serve as a compute-efficient playground for further CL research. The codebase is available under https://github.com/oleksost/latent_CL.",
                "authors": "O. Ostapenko, Timothée Lesort, P. Rodr'iguez, Md Rifat Arefin, Arthur Douillard, I. Rish, Laurent Charlin",
                "citations": 39
            },
            {
                "title": "@ CREPE: Can Vision-Language Foundation Models Reason Compositionally?",
                "abstract": "A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, we find that-across 7 architectures trained with 4 algorithms on massive datasets-they struggle at compositionality. To arrive at this conclusion, we introduce a new compositionality evaluation benchmark, @ CREPE, which measures two important aspects of compositionality identified by cognitive science literature: system-aticity and productivity. To measure systematicity, CREPE consists of a test dataset containing over 370K image-text pairs and three different seen-unseen splits. The three splits are designed to test models trained on three popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also generate 325K, 316K, and 309K hard negative captions for a subset of the pairs. To test productivity, CREPE contains 17 K image-text pairs with nine different complexities plus 278K hard negative captions with atomic, swapping and negation foils. The datasets are generated by repurposing the Visual Genome scene graphs and region descriptions and applying handcrafted templates and GPT-3. For systematicity, we find that model performance decreases consistently when novel compositions dominate the retrieval set, with Recall@1 dropping by up to 9%. For productivity, models' retrieval success decays as complexity increases, frequently nearing random chance at high complexity. These results hold regardless of model and training dataset size.",
                "authors": "Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, Ranjay Krishna",
                "citations": 100
            },
            {
                "title": "Assemble Foundation Models for Automatic Code Summarization",
                "abstract": "Automatic code summarization is beneficial to software development and maintenance since it reduces the burden of manual tasks. Currently, artificial intelligence is undergoing a paradigm shift. The foundation models pretrained on massive data and finetuned to downstream tasks surpass specially customized models. This trend inspired us to consider reusing foundation models instead of learning from scratch. Based on this, we propose a flexible and robust approach for automatic code summarization based on neural networks. We assemble available foundation models, such as CodeBERT and GPT-2, into a single model named AdaMo. Moreover, we utilize Gaussian noise as the simulation of contextual information to optimize the latent representation. Furthermore, we introduce two adaptive schemes from the perspective of knowledge transfer, namely continuous pretraining and intermediate finetuning, and design intermediate stage tasks for general sequence-to-sequence learning. Finally, we evaluate AdaMo against a benchmark dataset for code summarization, by comparing it with state-of-the-art models.",
                "authors": "Jian Gu, P. Salza, H. Gall",
                "citations": 31
            },
            {
                "title": "EHR foundation models improve robustness in the presence of temporal distribution shift",
                "abstract": null,
                "authors": "L. Guo, E. Steinberg, S. Fleming, J. Posada, J. Lemmon, S. Pfohl, N. Shah, J. Fries, L. Sung",
                "citations": 30
            },
            {
                "title": "Merak: An Efficient Distributed DNN Training Framework With Automated 3D Parallelism for Giant Foundation Models",
                "abstract": "Foundation models are in the process of becoming the dominant deep learning technology. Pretraining a foundation model is always time-consuming due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the pretraining process is extremely memory- and communication-intensive. These challenges make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism, and tensor model parallelism, to achieve high training efficiency. However, current 3D parallelism frameworks still encounter two issues: i) they are not transparent to model developers, requiring manual model modification to parallelize training, and ii) their utilization of computation resources, GPU memory, and network bandwidth is insufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automatically deploys 3D parallelism with an automatic model partitioner, which includes a graph-sharding algorithm and proxy node-based model graph. Merak also offers a non-intrusive API to scale out foundation model training with minimal code modification. In addition, we design a high-performance 3D parallel runtime engine that employs several techniques to exploit available training resources, including a shifted critical path pipeline schedule that increases computation utilization, stage-aware recomputation that makes use of idle worker memory, and sub-pipelined tensor model parallelism that overlaps communication and computation. Experiments on 64 GPUs demonstrate Merak's capability to speed up training performance over state-of-the-art 3D parallelism frameworks of models with 1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42, 1.39, 1.43, and 1.61×, respectively.",
                "authors": "Zhiquan Lai, Shengwei Li, Xudong Tang, Ke-shi Ge, Weijie Liu, Yabo Duan, Linbo Qiao, Dongsheng Li",
                "citations": 32
            },
            {
                "title": "Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?",
                "abstract": "Task specification is at the core of programming autonomous robots. A low-effort modality for task specification is critical for engagement of non-expert end-users and ultimate adoption of personalized robot agents. A widely studied approach to task specification is through goals, using either compact state vectors or goal images from the same robot scene. The former is hard to interpret for non-experts and necessitates detailed state estimation and scene understanding. The latter requires the generation of desired goal image, which often requires a human to complete the task, defeating the purpose of having autonomous robots. In this work, we explore alternate and more general forms of goal specification that are expected to be easier for humans to specify and use such as images obtained from the internet, hand sketches that provide a visual description of the desired task, or simple language descriptions. As a preliminary step towards this, we investigate the capabilities of large scale pre-trained models (foundation models) for zero-shot goal specification, and find promising results in a collection of simulated robot manipulation tasks and real-world datasets.",
                "authors": "Yuchen Cui, S. Niekum, Abhi Gupta, Vikash Kumar, A. Rajeswaran",
                "citations": 66
            },
            {
                "title": "Exploring Parameter-Efficient Fine-Tuning to Enable Foundation Models in Federated Learning",
                "abstract": "Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown to be effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters, known as the \"Foundation Models.\" In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fine-tuning in federated learning and thus introduce a new framework: FedPEFT. Specifically, we systemically evaluate the performance of FedPEFT across a variety of client stability, data distribution, and differential privacy settings. By only locally tuning and globally sharing a small portion of the model weights, significant reductions in the total communication overhead can be achieved while maintaining competitive or even better performance in a wide range of federated learning scenarios, providing insight into a new paradigm for practical and effective federated systems.",
                "authors": "Guangyu Sun, Mat'ias Mendieta, Taojiannan Yang, Chen Chen",
                "citations": 14
            },
            {
                "title": "Can Foundation Models Talk Causality?",
                "abstract": "Foundation models are subject to an ongoing heated debate, leaving open the question of progress towards AGI and dividing the community into two camps: the ones who see the arguably impressive results as evidence to the scaling hypothesis, and the others who are worried about the lack of interpretability and reasoning capabilities. By investigating to which extent causal representations might be captured by these large scale language models, we make a humble efforts towards resolving the ongoing philosophical conflicts.",
                "authors": "Moritz Willig, M. Zecevic, D. Dhami, K. Kersting",
                "citations": 21
            },
            {
                "title": "NamedMask: Distilling Segmenters from Complementary Foundation Models",
                "abstract": "The goal of this work is to segment and name regions of images without access to pixel-level labels during training. To tackle this task, we construct segmenters by distilling the complementary strengths of two foundation models. The first, CLIP [26], exhibits the ability to assign names to image content but lacks an accessible representation of object structure. The second, DINO [5], captures the spatial extent of objects but has no knowledge of object names. Our method, termed NamedMask, begins by using CLIP to construct category-specific archives of images. These images are pseudo-labelled with a category-agnostic salient object detector bootstrapped from DINO, then refined by category-specific segmenters using the CLIP archive labels. Thanks to the high quality of the refined masks, we show that a standard segmentation architecture trained on these archives with appropriate data augmentation achieves impressive semantic segmentation abilities for both single-object and multi-object images. As a result, our proposed NamedMask performs favourably against a range of prior work on five benchmarks including the VOC2012, COCO and large-scale ImageNet-S datasets.",
                "authors": "Gyungin Shin, Weidi Xie, Samuel Albanie",
                "citations": 21
            },
            {
                "title": "Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned",
                "abstract": "Foundation models pre-trained on large corpora demonstrate significant gains across many natural language processing tasks and domains e.g., law, healthcare, education, etc. However, only limited efforts have investigated the opportunities and limitations of applying these powerful models to science and security applications. In this work, we develop foundation models of scientific knowledge for chemistry to augment scientists with the advanced ability to perceive and reason at scale previously unimagined. Specifically, we build large-scale (1.47B parameter) general-purpose models for chemistry that can be effectively used to perform a wide range of in-domain and out-of-domain tasks. Evaluating these models in a zero-shot setting, we analyze the effect of model and data scaling, knowledge depth, and temporality on model performance in context of model training efficiency. Our novel findings demonstrate that (1) model size significantly contributes to the task performance when evaluated in a zero-shot setting; (2) data quality (aka diversity) affects model performance more than data quantity; (3) similarly, unlike previous work, temporal order of the documents in the corpus boosts model performance only for specific tasks, e.g., SciQ; and (4) models pre-trained from scratch perform better on in-domain tasks than those tuned from general-purpose models like Open AI’s GPT-2.",
                "authors": "Sameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, Robin Cosbey, M. Glenski, Svitlana Volkova",
                "citations": 17
            },
            {
                "title": "Quantifying Uncertainty in Foundation Models via Ensembles",
                "abstract": "As large pre-trained foundation models begin to proliferate and have increasing impact in real-world applications, it is of utmost importance to guarantee that these models are trustworthy and reliable. An important capability towards this is for models to know what they don’t know models that are capable of quantifying uncertainty about their own outputs (potentially on inputs very different from what they were trained on). Uncertainty quantification can give guidance on when to trust the model. It can also lead to new capabilities, such as active learning (Settles, 2009) or exploration behavior in decision-making agents (Baranes & Oudeyer, 2013) driven by the goal of reducing uncertainty.",
                "authors": "Meiqi Sun, Wilson Yan, P. Abbeel, Igor Mordatch",
                "citations": 16
            },
            {
                "title": "FETA: Towards Specializing Foundation Models for Expert Task Applications",
                "abstract": "Foundation Models (FMs) have demonstrated unprecedented capabilities including zero-shot learning, high fidelity data synthesis, and out of domain generalization. However, as we show in this paper, FMs still have poor out-of-the-box performance on expert tasks (e.g. retrieval of car manuals technical illustrations from language queries), data for which is either unseen or belonging to a long-tail part of the data distribution of the huge datasets used for FM pre-training. This underlines the necessity to explicitly evaluate and finetune FMs on such expert tasks, arguably ones that appear the most in practical real-world applications. In this paper, we propose a first of its kind FETA benchmark built around the task of teaching FMs to understand technical documentation, via learning to match their graphical illustrations to corresponding language descriptions. Our FETA benchmark focuses on text-to-image and image-to-text retrieval in public car manuals and sales catalogue brochures. FETA is equipped with a procedure for completely automatic annotation extraction (code would be released upon acceptance), allowing easy extension of FETA to more documentation types and application domains in the future. Our automatic annotation leads to an automated performance metric shown to be consistent with metrics computed on human-curated annotations (also released). We provide multiple baselines and analysis of popular FMs on FETA leading to several interesting findings that we believe would be very valuable to the FM community, paving the way towards real-world application of FMs for practical expert tasks currently 'overlooked' by standard benchmarks focusing on common objects.",
                "authors": "Amit Alfassy, Assaf Arbelle, Oshri Halimi, Sivan Harary, Roei Herzig, Eli Schwartz, Rameswar Panda, Michele Dolfi, Christoph Auer, Kate Saenko, P. Staar, R. Feris, Leonid Karlinsky",
                "citations": 18
            },
            {
                "title": "Differentially Private Bias-Term Fine-tuning of Foundation Models",
                "abstract": "We study the problem of differentially private (DP) fine-tuning of large pre-trained models -- a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the efficiency of the standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture), parameter efficient (only training about 0.1% of the parameters), and computation efficient (almost removing the overhead caused by DP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiT is 2~30X faster and uses 2~8X less memory than DP full fine-tuning, even faster than the standard full fine-tuning. This amazing efficiency enables us to conduct DP fine-tuning on language and vision tasks with long-sequence texts and high-resolution images, which were computationally difficult using existing methods. We open-source our code at FastDP (https://github.com/awslabs/fast-differential-privacy).",
                "authors": "Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, G. Karypis",
                "citations": 43
            },
            {
                "title": "On the power of foundation models",
                "abstract": "With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be seen as a new type of generalization theorem, showing that the foundation model can generate unseen objects from the target category (e.g., images) using the structural information from the source category (e.g., texts). Along the way, we provide a categorical framework for supervised and self-supervised learning, which might be of independent interest.",
                "authors": "Yang Yuan",
                "citations": 29
            },
            {
                "title": "Risk of Bias in Chest Radiography Deep Learning Foundation Models",
                "abstract": "Purpose To analyze a recently published chest radiography foundation model for the presence of biases that could lead to subgroup performance disparities across biologic sex and race. Materials and Methods This Health Insurance Portability and Accountability Act–compliant retrospective study used 127 118 chest radiographs from 42 884 patients (mean age, 63 years ± 17 [SD]; 23 623 male, 19 261 female) from the CheXpert dataset that were collected between October 2002 and July 2017. To determine the presence of bias in features generated by a chest radiography foundation model and baseline deep learning model, dimensionality reduction methods together with two-sample Kolmogorov–Smirnov tests were used to detect distribution shifts across sex and race. A comprehensive disease detection performance analysis was then performed to associate any biases in the features to specific disparities in classification performance across patient subgroups. Results Ten of 12 pairwise comparisons across biologic sex and race showed statistically significant differences in the studied foundation model, compared with four significant tests in the baseline model. Significant differences were found between male and female (P < .001) and Asian and Black (P < .001) patients in the feature projections that primarily capture disease. Compared with average model performance across all subgroups, classification performance on the “no finding” label decreased between 6.8% and 7.8% for female patients, and performance in detecting “pleural effusion” decreased between 10.7% and 11.6% for Black patients. Conclusion The studied chest radiography foundation model demonstrated racial and sex-related bias, which led to disparate performance across patient subgroups; thus, this model may be unsafe for clinical applications. Keywords: Conventional Radiography, Computer Application-Detection/Diagnosis, Chest Radiography, Bias, Foundation Models Supplemental material is available for this article. Published under a CC BY 4.0 license. See also commentary by Czum and Parr in this issue.",
                "authors": "B. Glocker, Charles Jones, Mélanie Roschewitz, S. Winzeck",
                "citations": 28
            },
            {
                "title": "On the Opportunities and Risks of Foundation Models for Natural Language Processing in Radiology.",
                "abstract": null,
                "authors": "Walter F. Wiggins, Ali S. Tejani",
                "citations": 30
            },
            {
                "title": "Foundation Models for Transportation Intelligence: ITS Convergence in TransVerse",
                "abstract": "Smart cities are our aspiration for a better life where transportation intelligence is indispensable. Recent technological advances in intelligent transportation systems have opened up new possibilities for smart mobility in smart cities. Here we present TengYun, a transportation foundation model designed and developed with parallel learning and federated intelligence for our transportation metaverse called TransVerse. TengYun enables decentralized/distributed autonomous organizations with decentralized/ distributed operations, as well as various federated technologies, from federated security, federated control, federated management, federated services, to federated ecology for transportation intelligence in smart cities. An example for a federation of transportation transformers is discussed for illustrating the operating procedure of TengYun.",
                "authors": "Chen Zhao, Xingyuan Dai, Yisheng Lv, Yonglin Tian, Yuhai Ren, Fei Wang, Fei Wang",
                "citations": 18
            },
            {
                "title": "Rethinking data-driven networking with foundation models: challenges and opportunities",
                "abstract": "Foundational models have caused a paradigm shift in the way artificial intelligence (AI) systems are built. They have had a major impact in natural language processing (NLP), and several other domains, not only reducing the amount of required labeled data or even eliminating the need for it, but also significantly improving performance on a wide range of tasks. We argue foundation models can have a similar profound impact on network traffic analysis, and management. More specifically, we show that network data shares several of the properties that are behind the success of foundational models in linguistics. For example, network data contains rich semantic content, and several of the networking tasks (e.g., traffic classification, generation of protocol implementations from specification text, anomaly detection) can find similar counterparts in NLP (e.g., sentiment analysis, translation from natural language to code, out-of-distribution). However, network settings also present unique characteristics and challenges that must be overcome. Our contribution is in highlighting the opportunities and challenges at the intersection of foundation models and networking.",
                "authors": "Franck Le, M. Srivatsa, R. Ganti, Vyas Sekar",
                "citations": 11
            },
            {
                "title": "Towards Galaxy Foundation Models with Hybrid Contrastive Learning",
                "abstract": "New astronomical tasks are often related to earlier tasks for which labels have already been collected. We adapt the contrastive framework BYOL to leverage those labels as a pretraining task while also enforcing augmentation invariance. For large-scale pretraining, we introduce GZ-Evo v0.1, a set of 96.5M volunteer responses for 552k galaxy images plus a further 1.34M comparable unlabelled galaxies. Most of the 206 GZ-Evo answers are unknown for any given galaxy, and so our pretraining task uses a Dirichlet loss that naturally handles unknown answers. GZ-Evo pretraining, with or without hybrid learning, improves on direct training even with plentiful downstream labels (+4% accuracy with 44k labels). Our hybrid pretraining/contrastive method further improves downstream accuracy vs. pretraining or contrastive learning, especially in the low-label transfer regime (+6% accuracy with 750 labels).",
                "authors": "Mike Walmsley, I. V. Slijepcevic, Micah Bowles, A. Scaife",
                "citations": 14
            },
            {
                "title": "Risk of Bias in Chest X-ray Foundation Models",
                "abstract": "Foundation models are considered a breakthrough in all applications of AI, promising robust and reusable mechanisms for feature extraction, alleviating the need for large amounts of high quality annotated training data for task-specific prediction models. However, foundation models may potentially encode and even reinforce existing biases present in historic datasets. Given the limited ability to scrutinize foundation models, it remains unclear whether the opportunities outweigh the risks in safety critical applications such as clinical decision making. In our statistical bias analysis of a recently published, and publicly accessible chest X-ray foundation model, we found reasons for concern as the model seems to encode protected characteristics including biological sex and racial identity. When used for the downstream application of disease detection, we observed substantial degradation of performance of the foundation model compared to a standard model with specific disparities in protected subgroups. While research into foundation models for healthcare applications is in an early stage, we hope to raise awareness of the risks by highlighting the importance of conducting thorough bias and subgroup performance analyses.",
                "authors": "B. Glocker, Charles Jones, Mélanie Bernhardt, S. Winzeck",
                "citations": 8
            },
            {
                "title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models",
                "abstract": "Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \\Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.",
                "authors": "Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan",
                "citations": 95
            },
            {
                "title": "Multimodal foundation models are better simulators of the human brain",
                "abstract": "Multimodal learning, especially large-scale multimodal pre-training, has developed rapidly over the past few years and led to the greatest advances in artificial intelligence (AI). Despite its effectiveness, understanding the underlying mechanism of multimodal pre-training models still remains a grand challenge. Revealing the explainability of such models is likely to enable breakthroughs of novel learning paradigms in the AI field. To this end, given the multimodal nature of the human brain, we propose to explore the explainability of multimodal learning models with the aid of non-invasive brain imaging technologies such as functional magnetic resonance imaging (fMRI). Concretely, we first present a newly-designed multimodal foundation model pre-trained on 15 million image-text pairs, which has shown strong multimodal understanding and generalization abilities in a variety of cognitive downstream tasks. Further, from the perspective of neural encoding (based on our foundation model), we find that both visual and lingual encoders trained multimodally are more brain-like compared with unimodal ones. Particularly, we identify a number of brain regions where multimodally-trained encoders demonstrate better neural encoding performance. This is consistent with the findings in existing studies on exploring brain multi-sensory integration. Therefore, we believe that multimodal foundation models are more suitable tools for neuroscientists to study the multimodal signal processing mechanisms in the human brain. Our findings also demonstrate the potential of multimodal foundation models as ideal computational simulators to promote both AI-for-brain and brain-for-AI research.",
                "authors": "Haoyu Lu, Qiongyi Zhou, Nanyi Fei, Zhiwu Lu, Mingyu Ding, Jingyuan Wen, Changde Du, Xin Zhao, Haoran Sun, Huiguang He, J. Wen",
                "citations": 12
            },
            {
                "title": "A Case for Business Process-Specific Foundation Models",
                "abstract": "The inception of large language models has helped advance state-of-the-art performance on numerous natural language tasks. This has also opened the door for the development of foundation models for other domains and data modalities such as images, code, and music. In this paper, we argue that business process data representations have unique characteristics that warrant the development of a new class of foundation models to handle tasks like process mining, optimization, and decision making. These models should also tackle the unique challenges of applying AI to business processes which include data scarcity, multi-modal representations, domain specific terminology, and privacy concerns.",
                "authors": "Yara Rizk, P. Venkateswaran, Vatche Isahagian, Vinod Muthusamy",
                "citations": 7
            },
            {
                "title": "Foundation models in brief: A historical, socio-technical focus",
                "abstract": "Foundation models can be disruptive for future AI development by scaling up deep learning in terms of model size and training data's breadth and size. These models achieve state-of-the-art performance (often through further adaptation) on a variety of tasks in domains such as natural language processing and computer vision. Foundational models exhibit a novel {emergent behavior}: {In-context learning} enables users to provide a query and a few examples from which a model derives an answer without being trained on such queries. Additionally, {homogenization} of models might replace a myriad of task-specific models with fewer very large models controlled by few corporations leading to a shift in power and control over AI. This paper provides a short introduction to foundation models. It contributes by crafting a crisp distinction between foundation models and prior deep learning models, providing a history of machine learning leading to foundation models, elaborating more on socio-technical aspects, i.e., organizational issues and end-user interaction, and a discussion of future research.",
                "authors": "Johannes Schneider",
                "citations": 7
            },
            {
                "title": "Toward Foundation Models for Earth Monitoring: Proposal for a Climate Change Benchmark",
                "abstract": "Recent progress in self-supervision shows that pre-training large neural networks on vast amounts of unsupervised data can lead to impressive increases in generalisation for downstream tasks. Such models, recently coined as foundation models, have been transformational to the field of natural language processing. While similar models have also been trained on large corpuses of images, they are not well suited for remote sensing data. To stimulate the development of foundation models for Earth monitoring, we propose to develop a new benchmark comprised of a variety of downstream tasks related to climate change. We believe that this can lead to substantial improvements in many existing applications and facilitate the development of new applications. This proposal is also a call for collaboration with the aim of developing a better evaluation process to mitigate potential downsides of foundation models for Earth monitoring.",
                "authors": "Alexandre Lacoste, Evan D. Sherwin, H. Kerner, H. Alemohammad, Bjorn Lutjens, J. Irvin, David Dao, Alex Chang, Mehmet Gunturkun, Alexandre Drouin, Pau Rodríguez López, David Vázquez",
                "citations": 26
            },
            {
                "title": "Risks of AI Foundation Models in Education",
                "abstract": "If the authors of a recent Stanford report (Bommasani et al., 2021) on the opportunities and risks of\"foundation models\"are to be believed, these models represent a paradigm shift for AI and for the domains in which they will supposedly be used, including education. Although the name is new (and contested (Field, 2021)), the term describes existing types of algorithmic models that are\"trained on broad data at scale\"and\"fine-tuned\"(i.e., adapted) for particular downstream tasks, and is intended to encompass large language models such as BERT or GPT-3 and computer vision models such as CLIP. Such technologies have the potential for harm broadly speaking (e.g., Bender et al., 2021), but their use in the educational domain is particularly fraught, despite the potential benefits for learners claimed by the authors. In section 3.3 of the Stanford report, Malik et al. argue that achieving the goal of providing education for all learners requires more efficient computational approaches that can rapidly scale across educational domains and across educational contexts, for which they argue foundation models are uniquely well-suited. However, evidence suggests that not only are foundation models not likely to achieve the stated benefits for learners, but their use may also introduce new risks for harm.",
                "authors": "Su Lin Blodgett, Michael A. Madaio",
                "citations": 13
            },
            {
                "title": "Quantifying Interlinguality In Foundation Models",
                "abstract": "Large multilingual language models show re-001 markable zero-shot cross-lingual transfer per-002 formance on a range of tasks. Follow-up 003 works hypothesized that these models inter-004 nally project representations of different lan-005 guages into a shared interlingual space. How-006 ever, they produced contradictory results. In 007 this paper, we correct the prior work claiming 008 that \"BERT is not an Interlingua\" and show 009 that with the proper choice of sentence repre-010 sentation different languages actually do con-011 verge to a shared space in such language mod-012 els. Lastly, we apply our own advice and quan-013 tify interlinguality across six pretrained mod-014 els and 378 language pairs. 1 015",
                "authors": "",
                "citations": 0
            },
            {
                "title": "10 Security and Privacy Problems in Large Foundation Models",
                "abstract": "Foundation models--such as GPT, CLIP, and DINO--have achieved revolutionary progress in the past several years and are commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised learning is adopted to pre-train a foundation model using a large amount of unlabeled data. A pre-trained foundation model is like an ``operating system'' of the AI ecosystem. Specifically, a foundation model can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on foundation models mainly focused on pre-training a better foundation model to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained foundation model leads to a single point of failure for the AI ecosystem. In this book chapter, we discuss 10 basic security and privacy problems for the pre-trained foundation models, including six confidentiality problems, three integrity problems, and one availability problem. For each problem, we discuss potential opportunities and challenges. We hope our book chapter will inspire future research on the security and privacy of foundation models.",
                "authors": "Jinyuan Jia, Hongbin Liu, N. Gong",
                "citations": 7
            },
            {
                "title": "Effect of foundation models on seismic response of arch dams",
                "abstract": null,
                "authors": "Ai-Yun Jin, Jianwen Pan, Jin‐Ting Wang, Chong Zhang",
                "citations": 22
            },
            {
                "title": "Foundation and large language models: fundamentals, challenges, opportunities, and social impacts",
                "abstract": null,
                "authors": "Devon Myers, Rami Mohawesh, Venkata Ishwarya Chellaboina, Anantha Lakshmi Sathvik, Praveen Venkatesh, Yi-Hui Ho, Hanna Henshaw, Muna Alhawawreh, David Berdik, Yaser Jararweh",
                "citations": 34
            },
            {
                "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
                "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat",
                "authors": "Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, O. Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Xudong Han, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, A. Jackson, Preslav Nakov, Timothy Baldwin, Eric P. Xing",
                "citations": 30
            },
            {
                "title": "PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models – Federated Learning in Age of Foundation Model",
                "abstract": "Quick global aggregation of effective distributed parameters is crucial to federated learning (FL), which requires adequate bandwidth for parameters communication and sufficient user data for local training. Otherwise, FL may cost excessive training time for convergence and produce inaccurate models. In this paper, we propose a brand-new FL framework, PromptFL, that replaces the federated model training with the federated prompt training, i.e., let federated participants train prompts instead of a shared model, to simultaneously achieve the efficient global aggregation and local training on insufficient data by exploiting the power of foundation models (FM) in a distributed way. PromptFL ships an off-the-shelf FM, i.e., CLIP, to distributed clients who would cooperatively train shared soft prompts based on very few local data. Since PromptFL only needs to update the prompts instead of the whole model, both the local training and the global aggregation can be significantly accelerated. And FM trained over large scale data can provide strong adaptation capability to distributed users tasks with the trained soft prompts. We empirically analyze the PromptFL via extensive experiments, and show its superiority in terms of system feasibility, user privacy, and performance.",
                "authors": "Tao Guo, Song Guo, Junxiao Wang, Xueyang Tang, Wenchao Xu",
                "citations": 87
            },
            {
                "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
                "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.",
                "authors": "Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, T. Zhang",
                "citations": 321
            },
            {
                "title": "Predicting resilient modulus of flexible pavement foundation using extreme gradient boosting based optimised models",
                "abstract": "ABSTRACT Resilient modulus ($M_R$MR) plays the most critical role in the evaluation and design of flexible pavement foundations. $M_R$MR is utilised as the principal parameter for representing stiffness and behaviour of flexible pavement foundation in experimental and semi-empirical approaches. To determine $M_R$MR, cyclic triaxial compressive experiments under different confining pressures and deviatoric stresses are needed. However, such experiments are costly and time-consuming. In the present study, an extreme gradient boosting-based ($XGB$XGB) model is presented for predicting the resilient modulus of flexible pavement foundations. The model is optimised using four different optimisation methods (particle swarm optimisation ($PSO$PSO), social spider optimisation ($SSO$SSO), sine cosine algorithm ($SCA$SCA), and multi-verse optimisation ($MVO$MVO)) and a database collected from previously published technical literature. The outcomes present that all developed designs have good workability in estimating the $M_R$MR of flexible pavement foundation, but the $PSO-XGB$PSO−XGB models have the best prediction accuracy considering both training and testing datasets.",
                "authors": "Reza Sarkhani Benemaran, M. Esmaeili‐Falak, A. Javadi",
                "citations": 86
            },
            {
                "title": "SpectralGPT: Spectral Remote Sensing Foundation Model",
                "abstract": "The foundation model has recently garnered significant attention due to its potential to revolutionize the field of visual representation learning in a self-supervised manner. While most foundation models are tailored to effectively process RGB images for various visual tasks, there is a noticeable gap in research focused on spectral data, which offers valuable information for scene understanding, especially in remote sensing (RS) applications. To fill this gap, we created for the first time a universal RS foundation model, named SpectralGPT, which is purpose-built to handle spectral RS images using a novel 3D generative pretrained transformer (GPT). Compared to existing foundation models, SpectralGPT 1) accommodates input images with varying sizes, resolutions, time series, and regions in a progressive training fashion, enabling full utilization of extensive RS Big Data; 2) leverages 3D token generation for spatial-spectral coupling; 3) captures spectrally sequential patterns via multi-target reconstruction; and 4) trains on one million spectral RS images, yielding models with over 600 million parameters. Our evaluation highlights significant performance improvements with pretrained SpectralGPT models, signifying substantial potential in advancing spectral RS Big Data applications within the field of geoscience across four downstream tasks: single/multi-label scene classification, semantic segmentation, and change detection.",
                "authors": "D. Hong, Bing Zhang, Xuyang Li, Yuxuan Li, Chenyu Li, Jing Yao, N. Yokoya, Hao Li, Pedram Ghamisi, Xiuping Jia, Antonio J. Plaza, Paolo Gamba, J. Benediktsson, J. Chanussot",
                "citations": 275
            },
            {
                "title": "A foundation model for generalizable disease detection from retinal images",
                "abstract": null,
                "authors": "Yukun Zhou, Mark A. Chia, Siegfried K. Wagner, Murat S. Ayhan, Dominic J. Williamson, R. Struyven, Timing Liu, Moucheng Xu, Mateo G. Lozano, Peter Woodward-Court, Y. Kihara, Naomi John E. J. Thomas Tariq Paul Graeme Panagiotis Den Allen Gallacher Littlejohns Aslam Bishop Black Ser, N. Allen, John Gallacher, T. Littlejohns, Tariq Aslam, P. Bishop, Graeme Black, P. Sergouniotis, Denize Atan, Andrew D. Dick, Cathy Williams, S. Barman, J. Barrett, S. Mackie, Tasanee Braithwaite, R. Carare, Sarah Ennis, Jane Gibson, A. Lotery, Jay Self, U. Chakravarthy, Ruth E. Hogg, E. Paterson, J. Woodside, T. Peto, G. McKay, Bernadette Mcguinness, P. Foster, Konstantinos Balaskas, A. Khawaja, N. Pontikos, J. Rahi, G. Lascaratos, Praveen J. Patel, Michelle Chan, S. Chua, A. Day, Parul Desai, Catherine A Egan, Marcus Fruttiger, D. Garway-Heath, A. Hardcastle, S. P. T. Khaw, T. Moore, S. Sivaprasad, N. Strouthidis, D. Thomas, A. Tufail, Ananth C. Viswanathan, B. Dhillon, T. Macgillivray, C. Sudlow, V. Vitart, Alexander Doney, E. Trucco, Jeremy A. Guggeinheim, James E. Morgan, C. Hammond, Katie M. Williams, P. Hysi, Simon Harding, Yalin Zheng, R. Luben, P. Luthert, Zihan Sun, Martin McKibbin, Eoin O'sullivan, R. Oram, Mike Weedon, Chris G. Owen, A. Rudnicka, N. Sattar, David Steel, Irene Stratton, Robyn Tapp, M. Yates, A. Petzold, S. Madhusudhan, A. Altmann, Aaron Y. Lee, E. Topol, A. Denniston, Daniel C Alexander, P. Keane",
                "citations": 228
            },
            {
                "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
                "authors": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang",
                "citations": 908
            },
            {
                "title": "ClimaX: A foundation model for weather and climate",
                "abstract": "Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings. ClimaX extends the Transformer architecture with novel encoding and aggregation blocks that allow effective use of available compute while maintaining general utility. ClimaX is pre-trained with a self-supervised learning objective on climate datasets derived from CMIP6. The pre-trained ClimaX can then be fine-tuned to address a breadth of climate and weather tasks, including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining. Compared to existing data-driven baselines, we show that this generality in ClimaX results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets. The source code is available at https://github.com/microsoft/ClimaX.",
                "authors": "Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K. Gupta, Aditya Grover",
                "citations": 205
            },
            {
                "title": "Holistic Evaluation of Language Models",
                "abstract": "Language models (LMs) like GPT‐3, PaLM, and ChatGPT are the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of LMs. LMs can serve many purposes and their behavior should satisfy many desiderata. To navigate the vast space of potential scenarios and metrics, we taxonomize the space and select representative subsets. We evaluate models on 16 core scenarios and 7 metrics, exposing important trade‐offs. We supplement our core evaluation with seven targeted evaluations to deeply analyze specific aspects (including world knowledge, reasoning, regurgitation of copyrighted content, and generation of disinformation). We benchmark 30 LMs, from OpenAI, Microsoft, Google, Meta, Cohere, AI21 Labs, and others. Prior to HELM, models were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: all 30 models are now benchmarked under the same standardized conditions. Our evaluation surfaces 25 top‐level findings. For full transparency, we release all raw model prompts and completions publicly. HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/.",
                "authors": "Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, D. Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R'e, Diana Acosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, O. Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, S. Ganguli, Tatsunori Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda",
                "citations": 777
            },
            {
                "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
                "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.",
                "authors": "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Zhang, Feiyan Huang",
                "citations": 753
            },
            {
                "title": "RingMo: A Remote Sensing Foundation Model With Masked Image Modeling",
                "abstract": "Deep learning approaches have contributed to the rapid development of remote sensing (RS) image interpretation. The most widely used training paradigm is to use ImageNet pretrained models to process RS data for specified tasks. However, there are issues such as domain gap between natural and RS scenes and the poor generalization capacity of RS models. It makes sense to develop a foundation model with general RS feature representation. Since a large amount of unlabeled data is available, the self-supervised method has more development significance than the fully supervised method in RS. However, most of the current self-supervised methods use contrastive learning, whose performance is sensitive to data augmentation, additional information, and selection of positive and negative pairs. In this article, we leverage the benefits of generative self-supervised learning (SSL) for RS images and propose an RS foundation model framework called RingMo, which consists of two parts. First, a large-scale dataset is constructed by collecting two million RS images from satellite and aerial platforms, covering multiple scenes and objects around the world. Second, we propose an RS foundation model training method designed for dense and small objects in complicated RS scenes. We show that the foundation model trained on our dataset with RingMo method achieves state-of-the-art (SOTA) on eight datasets across four downstream tasks, demonstrating the effectiveness of the proposed framework. Through in-depth exploration, we believe it is time for RS researchers to embrace generative SSL and leverage its general representation capabilities to speed up the development of RS applications.",
                "authors": "Xian Sun, Peijin Wang, Wanxuan Lu, Zicong Zhu, Xiaonan Lu, Qi He, Junxi Li, Xuee Rong, Zhujun Yang, Hao Chang, Qinglin He, Guang Yang, Ruiping Wang, Jiwen Lu, Kun Fu",
                "citations": 156
            },
            {
                "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
                "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.",
                "authors": "Zhiliang Peng, Wenhui Wang, Li Dong, Y. Hao, Shaohan Huang, Shuming Ma, Furu Wei",
                "citations": 561
            },
            {
                "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
                "abstract": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",
                "authors": "Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, X. Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, Qingsong Wen",
                "citations": 221
            },
            {
                "title": "Large Scale Foundation Model on Single-cell Transcriptomics",
                "abstract": "Large-scale pretrained models have become foundation models leading to breakthroughs in natural language processing and related fields. Developing foundation models in life science for deciphering the “languages” of cells and facilitating biomedical research is promising yet challenging. We developed a large-scale pretrained model scFoundation with 100M parameters for this purpose. scFoundation was trained on over 50 million human single-cell transcriptomics data, which contain high-throughput observations on the complex molecular features in all known types of cells. scFoundation is currently the largest model in terms of the size of trainable parameters, dimensionality of genes and the number of cells used in the pre-training. Experiments showed that scFoundation can serve as a foundation model for single-cell transcriptomics and achieve state-of-the-art performances in a diverse array of downstream tasks, such as gene expression enhancement, tissue drug response prediction, single-cell drug response classification, and single-cell perturbation prediction.",
                "authors": "Minsheng Hao, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Leo T. Song, Xuegong Zhang",
                "citations": 99
            },
            {
                "title": "RemoteCLIP: A Vision Language Foundation Model for Remote Sensing",
                "abstract": "General-purpose foundation models have led to recent breakthroughs in artificial intelligence (AI). In remote sensing, self-supervised learning (SSL) and masked image modeling (MIM) have been adopted to build foundation models. However, these models primarily learn low-level features and require annotated data for fine-tuning. Moreover, they are inapplicable for retrieval and zero-shot applications due to the lack of language understanding. To address these limitations, we propose RemoteCLIP, the first vision-language foundation model for remote sensing that aims to learn robust visual features with rich semantics and aligned text embeddings for seamless downstream application. To address the scarcity of pretraining data, we leverage data scaling which converts heterogeneous annotations into a unified image-caption data format based on box-to-caption (B2C) and mask-to-box (M2B) conversion. By further incorporating unmanned aerial vehicle (UAV) imagery, we produce a $12\\times $ larger pretraining dataset than the combination of all available datasets. RemoteCLIP can be applied to a variety of downstream tasks, including zero-shot image classification, linear probing, k-NN classification, few-shot classification, image-text retrieval, and object counting in remote sensing images. Evaluation of 16 datasets, including a newly introduced RemoteCount benchmark to test the object counting ability, shows that RemoteCLIP consistently outperforms baseline foundation models across different model scales. Impressively, RemoteCLIP beats the state-of-the-art (SOTA) method by 9.14% mean recall on the RSITMD dataset and 8.92% on the RSICD dataset. For zero-shot classification, our RemoteCLIP outperforms the contrastive language image pretraining (CLIP) baseline by up to 6.39% average accuracy on 12 downstream datasets.",
                "authors": "F. Liu, Delong Chen, Zhan-Rong Guan, Xiaocong Zhou, Jiale Zhu, Jun Zhou",
                "citations": 104
            },
            {
                "title": "ViNT: A Foundation Model for Visual Navigation",
                "abstract": "General-purpose pre-trained models (\"foundation models\") have enabled practitioners to produce generalizable solutions for individual machine learning problems with datasets that are significantly smaller than those required for learning from scratch. Such models are typically trained on large and diverse datasets with weak supervision, consuming much more training data than is available for any individual downstream application. In this paper, we describe the Visual Navigation Transformer (ViNT), a foundation model that aims to bring the success of general-purpose pre-trained models to vision-based robotic navigation. ViNT is trained with a general goal-reaching objective that can be used with any navigation dataset, and employs a flexible Transformer-based architecture to learn navigational affordances and enable efficient adaptation to a variety of downstream navigational tasks. ViNT is trained on a number of existing navigation datasets, comprising hundreds of hours of robotic navigation from a variety of different robotic platforms, and exhibits positive transfer, outperforming specialist models trained on singular datasets. ViNT can be augmented with diffusion-based subgoal proposals to explore novel environments, and can solve kilometer-scale navigation problems when equipped with long-range heuristics. ViNT can also be adapted to novel task specifications with a technique inspired by prompt-tuning, where the goal encoder is replaced by an encoding of another task modality (e.g., GPS waypoints or routing commands) embedded into the same space of goal tokens. This flexibility and ability to accommodate a variety of downstream problem domains establishes ViNT as an effective foundation model for mobile robotics. For videos, code, and model checkpoints, see our project page at https://visualnav-transformer.github.io.",
                "authors": "Dhruv Shah, A. Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin Black, N. Hirose, S. Levine",
                "citations": 100
            },
            {
                "title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome",
                "abstract": "Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates $36$ distinct datasets across $9$ tasks, with input lengths ranging from $70$ to $10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with $21 \\times$ fewer parameters and approximately $92 \\times$ less GPU time in pre-training.",
                "authors": "Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, R. Davuluri, Han Liu",
                "citations": 119
            },
            {
                "title": "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks",
                "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
                "authors": "Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith",
                "citations": 2179
            },
            {
                "title": "scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI",
                "abstract": "Generative pre-trained models have achieved remarkable success in various domains such as natural language processing and computer vision. Specifically, the combination of large-scale diverse datasets and pre-trained transformers has emerged as a promising approach for developing foundation models. Drawing parallels between linguistic constructs and cellular biology — where texts comprise words, similarly, cells are defined by genes — our study probes the applicability of foundation models to advance cellular biology and genetics research. Utilizing the burgeoning single-cell sequencing data, we have pioneered the construction of a foundation model for single-cell biology, scGPT, which is based on generative pre-trained transformer across a repository of over 33 million cells. Our findings illustrate that scGPT, a generative pre-trained transformer, effectively distills critical biological insights concerning genes and cells. Through the further adaptation of transfer learning, scGPT can be optimized to achieve superior performance across diverse downstream applications. This includes tasks such as cell-type annotation, multi-batch integration, multi-omic integration, genetic perturbation prediction, and gene network inference. The scGPT codebase is publicly available at https://github.com/bowang-lab/scGPT.",
                "authors": "Haotian Cui, Chloe X. Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Bo Wang",
                "citations": 71
            },
            {
                "title": "Florence: A New Foundation Model for Computer Vision",
                "abstract": "Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",
                "authors": "Lu Yuan, Dongdong Chen, Yi-Ling Chen, N. Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang",
                "citations": 801
            },
            {
                "title": "A Survey on Multimodal Large Language Models for Autonomous Driving",
                "abstract": "With the emergence of Large Language Models (LLMs) and Vision Foundation Models (VFMs), multimodal AI systems benefiting from large models have the potential to equally perceive the real world, make decisions, and control tools as humans. In recent months, LLMs have shown widespread attention in autonomous driving and map systems. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors to apply in LLM driving systems. In this paper, we present a systematic investigation in this field. We first introduce the background of Multimodal Large Language Models (MLLMs), the multimodal models development using LLMs, and the history of autonomous driving. Then, we overview existing MLLM tools for driving, transportation, and map systems together with existing datasets and benchmarks. Moreover, we summarized the works in The 1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD), which is the first workshop of its kind regarding LLMs in autonomous driving. To further promote the development of this field, we also discuss several important problems regarding using MLLMs in autonomous driving systems that need to be solved by both academia and industry.",
                "authors": "Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, Tianren Gao, Erlong Li, Kun Tang, Zhipeng Cao, Tongxi Zhou, Ao Liu, Xinrui Yan, Shuqi Mei, Jianguo Cao, Ziran Wang, Chao Zheng",
                "citations": 164
            },
            {
                "title": "CogVLM: Visual Expert for Pretrained Language Models",
                "abstract": "We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at https://github.com/THUDM/CogVLM.",
                "authors": "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang",
                "citations": 345
            },
            {
                "title": "Towards Generalist Foundation Model for Radiology",
                "abstract": "In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM. We consider the construction of foundational models from three perspectives, namely, dataset construction, model design, and thorough evaluation. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases. To the best of our knowledge, this is the first large-scale, high-quality, medical visual-language dataset, with both 2D and 3D scans; (ii), we propose an architecture that enables visually conditioned generative pre-training, i.e., allowing for integration of text input with 2D or 3D medical scans, and generate responses for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently fine-tuned on the domain-specific dataset, which is a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs, termed as RadMD; (iii), we propose a new evaluation benchmark, RadBench, that comprises five tasks, including modality recognition, disease diagnosis, visual question answering, report generation and rationale diagnosis, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems. We conduct both automatic and human evaluation on RadBench, in both cases, RadFM outperforms existing multi-modal foundation models, that are publicaly accessible, including Openflamingo, MedFlamingo, MedVInT and GPT-4V. Additionally, we also adapt RadFM for different public benchmarks, surpassing existing SOTAs on diverse datasets. All codes, data, and model checkpoint will all be made publicly available to promote further research and development in the field.",
                "authors": "Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie",
                "citations": 110
            },
            {
                "title": "Retentive Network: A Successor to Transformer for Large Language Models",
                "abstract": "In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.",
                "authors": "Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei",
                "citations": 228
            },
            {
                "title": "A decoder-only foundation model for time-series forecasting",
                "abstract": "Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.",
                "authors": "Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou",
                "citations": 123
            },
            {
                "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
                "abstract": "Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio",
                "authors": "Dongchao Yang, Jinchuan Tian, Xuejiao Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao, Helen Meng",
                "citations": 90
            },
            {
                "title": "A generalist vision-language foundation model for diverse biomedical tasks.",
                "abstract": null,
                "authors": "Kai Zhang, Jun Yu, Zhilin Yan, Yixin Liu, Eashan Adhikarla, S. Fu, Xun Chen, Chen Chen, Yuyin Zhou, Xiang Li, Lifang He, B. Davison, Quanzheng Li, Yong Chen, Hongfang Liu, Lichao Sun",
                "citations": 87
            },
            {
                "title": "Skywork: A More Open Bilingual Foundation Model",
                "abstract": "In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves \\emph{state of the art} performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",
                "authors": "Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, X. Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou",
                "citations": 83
            },
            {
                "title": "SpectralGPT: Spectral Foundation Model",
                "abstract": "—The foundation model has recently garnered significant attention due to its potential to revolutionize the field of visual representation learning in a self-supervised manner. While most foundation models are tailored to effectively process RGB images for various visual tasks, there is a noticeable gap in research focused on spectral data, which offers valuable information for scene understanding, especially in remote sensing (RS) applications. To fill this gap, we created for the first time a universal RS foundation model, named SpectralGPT, which is purpose-built to handle spectral RS images using a novel 3D generative pretrained transformer (GPT). Compared to existing foundation models, SpectralGPT 1) accommodates input images with varying sizes, resolutions, time series, and regions in a progressive training fashion, enabling full utilization of extensive RS big data; 2) leverages 3D token generation for spatial-spectral coupling; 3) captures spectrally sequential patterns via",
                "authors": "D. Hong, Bing Zhang, Xuyang Li, Yuxuan Li, Chenyu Li, Jing Yao, N. Yokoya, Hao Li, Pedram Ghamisi, Xiuping Jia, Antonio J. Plaza, Paolo Gamba, J. Benediktsson, J. Chanussot",
                "citations": 61
            },
            {
                "title": "A Billion-scale Foundation Model for Remote Sensing Images",
                "abstract": "As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation model in the remote sensing field. Furthermore, we propose an effective method for scaling up and fine-tuning a vision transformer in the remote sensing field. To evaluate general performance in downstream tasks, we employed the DOTA v2.0 and DIOR-R benchmark datasets for rotated object detection, and the Potsdam and LoveDA datasets for semantic segmentation. Experimental results demonstrated that, across all benchmark datasets and downstream tasks, the performance of the foundation models and data efficiency improved as the number of parameters increased. Moreover, our models achieve the state-of-the-art performance on several datasets including DIOR-R, Postdam, and LoveDA.",
                "authors": "Keumgang Cha, Junghoon Seo, Taekyung Lee",
                "citations": 45
            },
            {
                "title": "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
                "abstract": "Recent advances in Foundation Models such as Large Language Models (LLMs) have propelled them to the forefront of Recommender Systems (RS). Despite their utility, there is a growing concern that LLMs might inadvertently perpetuate societal stereotypes, resulting in unfair recommendations. Since fairness is critical for RS as many users take it for decision-making and demand fulfillment, this paper focuses on user-side fairness for LLM-based recommendation where the users may require a recommender system to be fair on specific sensitive features such as gender or age. In this paper, we dive into the extent of unfairness exhibited by LLM-based recommender models based on both T5 and LLaMA backbones, and discuss appropriate methods for promoting equitable treatment of users in LLM-based recommendation models. We introduce a novel Counterfactually-Fair-Prompt (CFP) method towards Unbiased Foundation mOdels (UFO) for fairness-aware LLM-based recommendation. Experiments are conducted on two real-world datasets, MovieLens-1M and Insurance, and compared with both matching-based and sequential-based fairness-aware recommendation models. Results show that CFP achieves better recommendation performance with a high level of fairness.",
                "authors": "Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, Yongfeng Zhang",
                "citations": 40
            },
            {
                "title": "Sustainable business models for inclusive growth: Towards a conceptual foundation of inclusive business",
                "abstract": null,
                "authors": "G. Schoneveld",
                "citations": 69
            },
            {
                "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
                "abstract": "Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vision-text, audio-text, and multi-modal video-text tasks (retrieval, captioning and QA). Extensive experiments have been conducted to demonstrate the effectiveness of our proposed VAST-27M corpus and VAST foundation model. VAST achieves 22 new state-of-the-art results on various cross-modality benchmarks. Code, model and dataset will be released at https://github.com/TXH-mercury/VAST.",
                "authors": "Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Ming-Ting Sun, Xinxin Zhu, J. Liu",
                "citations": 68
            },
            {
                "title": "Text-guided Foundation Model Adaptation for Pathological Image Classification",
                "abstract": "The recent surge of foundation models in computer vision and natural language processing opens up perspectives in utilizing multi-modal clinical data to train large models with strong generalizability. Yet pathological image datasets often lack biomedical text annotation and enrichment. Guiding data-efficient image diagnosis from the use of biomedical text knowledge becomes a substantial interest. In this paper, we propose to Connect Image and Text Embeddings (CITE) to enhance pathological image classification. CITE injects text insights gained from language models pre-trained with a broad range of biomedical texts, leading to adapt foundation models towards pathological image understanding. Through extensive experiments on the PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE achieves leading performance compared with various baselines especially when training data is scarce. CITE offers insights into leveraging in-domain text knowledge to reinforce data-efficient pathological image classification. Code is available at https://github.com/Yunkun-Zhang/CITE.",
                "authors": "Yunkun Zhang, Jinglei Gao, Mu Zhou, Xiaosong Wang, Y. Qiao, Shaoting Zhang, Dequan Wang",
                "citations": 32
            },
            {
                "title": "MolFM: A Multimodal Molecular Foundation Model",
                "abstract": "Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures local and global molecular knowledge by minimizing the distance in the feature space between different modalities of the same molecule, as well as molecules sharing similar structures or functions. MolFM achieves state-of-the-art performance on various downstream tasks. On cross-modal retrieval, MolFM outperforms existing models with 12.13% and 5.04% absolute gains under the zero-shot and fine-tuning settings, respectively. Furthermore, qualitative analysis showcases MolFM's implicit ability to provide grounding from molecular substructures and knowledge graphs. Code and models are available on https://github.com/BioFM/OpenBioMed.",
                "authors": "Yi Luo, Kai Yang, Massimo Hong, Xingyi Liu, Zaiqing Nie",
                "citations": 31
            },
            {
                "title": "The Foundation Model Transparency Index",
                "abstract": "Foundation models have rapidly permeated society, catalyzing a wave of generative AI applications spanning enterprise and consumer-facing contexts. While the societal impact of foundation models is growing, transparency is on the decline, mirroring the opacity that has plagued past digital technologies (e.g. social media). Reversing this trend is essential: transparency is a vital precondition for public accountability, scientific innovation, and effective governance. To assess the transparency of the foundation model ecosystem and help improve transparency over time, we introduce the Foundation Model Transparency Index. The Foundation Model Transparency Index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). We score 10 major foundation model developers (e.g. OpenAI, Google, Meta) against the 100 indicators to assess their transparency. To facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. GPT-4 for OpenAI, PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. Overall, the Foundation Model Transparency Index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.",
                "authors": "Rishi Bommasani, Kevin Klyman, S. Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, Percy Liang",
                "citations": 48
            },
            {
                "title": "Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model",
                "abstract": "The Segment Anything Model (SAM) is a recently developed large model for general-purpose segmentation for computer vision tasks. SAM was trained using 11 million images with over 1 billion masks and can produce segmentation results for a wide range of objects in natural scene images. SAM can be viewed as a general perception model for segmentation (partitioning images into semantically meaningful regions). Thus, how to utilize such a large foundation model for medical image segmentation is an emerging research target. This paper shows that although SAM does not immediately give high-quality segmentation for medical image data, its generated masks, features, and stability scores are useful for building and training better medical image segmentation models. In particular, we demonstrate how to use SAM to augment image input for commonly-used medical image segmentation models (e.g., U-Net). Experiments on three segmentation tasks show the effectiveness of our proposed SAMAug method. The code is available at \\url{https://github.com/yizhezhang2000/SAMAug}.",
                "authors": "Yizhe Zhang, Tao Zhou, Peixian Liang, Da Chen",
                "citations": 66
            },
            {
                "title": "Virchow: A Million-Slide Digital Pathology Foundation Model",
                "abstract": "The use of artificial intelligence to enable precision medicine and decision support systems through the analysis of pathology images has the potential to revolutionize the diagnosis and treatment of cancer. Such applications will depend on models' abilities to capture the diverse patterns observed in pathology images. To address this challenge, we present Virchow, a foundation model for computational pathology. Using self-supervised learning empowered by the DINOv2 algorithm, Virchow is a vision transformer model with 632 million parameters trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue and specimen types, which is orders of magnitude more data than previous works. The Virchow model enables the development of a pan-cancer detection system with 0.949 overall specimen-level AUC across 17 different cancer types, while also achieving 0.937 AUC on 7 rare cancer types. The Virchow model sets the state-of-the-art on the internal and external image tile level benchmarks and slide level biomarker prediction tasks. The gains in performance highlight the importance of training on massive pathology image datasets, suggesting scaling up the data and network architecture can improve the accuracy for many high-impact computational pathology applications where limited amounts of training data are available.",
                "authors": "Eugene Vorontsov, A. Bozkurt, Adam Casson, George Shaikovski, Michal Zelechowski, Siqi Liu, Eric Zimmermann, Philippe Mathieu, Alexander van Eck, Donghun Lee, Julian Viret, Eric Robert, Yi Kan Wang, Jeremy D. Kun, Matthew C. H. Le, Jan H Bernhard, R. Godrich, Gerard Oakley, Ewan Millar, Matthew G Hanna, J. Retamero, William A. Moye, Razik Yousfi, Christopher Kanan, D. Klimstra, B. Rothrock, Thomas J. Fuchs",
                "citations": 51
            },
            {
                "title": "Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train",
                "abstract": "Foundation models have exhibited remarkable success in various applications, such as disease diagnosis and text report generation. To date, a foundation model for endoscopic video analysis is still lacking. In this paper, we propose Endo-FM, a foundation model specifically developed using massive endoscopic video data. First, we build a video transformer, which captures both local and global long-range dependencies across spatial and temporal dimensions. Second, we pre-train our transformer model using global and local views via a self-supervised manner, aiming to make it robust to spatial-temporal variations and discriminative across different scenes. To develop the foundation model, we construct a large-scale endoscopy video dataset by combining 9 publicly available datasets and a privately collected dataset from Baoshan Branch of Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K video clips with up to 5 million frames, encompassing various protocols, target organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a given downstream task via fine-tuning by serving as the backbone. With experiments on 3 different types of downstream tasks, including classification, segmentation, and detection, our Endo-FM surpasses the current state-of-the-art (SOTA) self-supervised pre-training and adapter-based transfer learning methods by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6% Dice, and 9.9% F1 for classification, segmentation, and detection). Code, datasets, and models are released at https://github.com/med-air/Endo-FM.",
                "authors": "Zhao Wang, Chang Liu, Shaoting Zhang, Q. Dou",
                "citations": 38
            },
            {
                "title": "scGPT: Towards Building a Foundation Model for Single-Cell 1 Multi-omics Using Generative AI",
                "abstract": "10 Generative pre-trained models have achieved remarkable success in various domains such as nat-11 ural language processing and computer vision. Specifically, the combination of large-scale diverse 12 datasets and pre-trained transformers has emerged as a promising approach for developing founda-13 tion models. While texts are made up of words, cells can be characterized by genes. This analogy 14 inspires us to explore the potential of foundation models for cell and gene biology. By leveraging the",
                "authors": "Haotian Cui, Chloe X. Wang, Hassaan Maan, Bo Wang, C. E. D. Masked-Attention",
                "citations": 30
            },
            {
                "title": "RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model",
                "abstract": "Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we tried several Parameter-Efficient Fine-Tuning methods on RS5M to implement the DFM. Experimental results show that our proposed dataset is highly effective for various tasks, improving upon the baseline by 8% „ 16% in zero-shot classification tasks, and obtaining good results in both Vision-Language Retrieval and Semantic Localization tasks. https",
                "authors": "Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin",
                "citations": 30
            },
            {
                "title": "A Variational Perspective on Solving Inverse Problems with Diffusion Models",
                "abstract": "Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-Diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-to-noise-ratio (SNR). Our approach provides a new variational perspective for solving inverse problems with diffusion models, allowing us to formulate sampling as stochastic optimization, where one can simply apply off-the-shelf solvers with lightweight iterates. Our experiments for image restoration tasks such as inpainting and superresolution demonstrate the strengths of our method compared with state-of-the-art sampling-based diffusion models.",
                "authors": "M. Mardani, Jiaming Song, J. Kautz, Arash Vahdat",
                "citations": 92
            },
            {
                "title": "Advancing Plain Vision Transformer Toward Remote Sensing Foundation Model",
                "abstract": "Large-scale vision foundation models have made significant progress in visual tasks on natural images, with vision transformers (ViTs) being the primary choice due to their good scalability and representation ability. However, large-scale models in remote sensing (RS) have not yet been sufficiently explored. In this article, we resort to plain ViTs with about 100 million parameters and make the first attempt to propose large vision models tailored to RS tasks and investigate how such large models perform. To handle the large sizes and objects of arbitrary orientations in RS images, we propose a new rotated varied-size window attention to replace the original full attention in transformers, which can significantly reduce the computational cost and memory footprint while learning better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks show the superiority of our model over all state-of-the-art models, achieving 81.24% mean average precision (mAP) on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also show competitive performance compared to existing advanced methods. Further experiments show the advantages of our models in terms of computational complexity and data efficiency in transferring. The code and models will be released at https://github.com/ViTAE-Transformer/Remote-Sensing-RVSA.",
                "authors": "Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du, Dacheng Tao, L. Zhang",
                "citations": 186
            },
            {
                "title": "A New Learning Paradigm for Foundation Model-Based Remote-Sensing Change Detection",
                "abstract": "Change detection (CD) is a critical task to observe and analyze dynamic processes of land cover. Although numerous deep-learning (DL)-based CD models have performed excellently, their further performance improvements are constrained by the limited knowledge extracted from the given labeled data. On the other hand, the foundation models that emerged recently contain a huge amount of knowledge by scaling up across data modalities and proxy tasks. In this article, we propose a bi-temporal adapter network (BAN), which is a universal foundation model-based CD adaptation framework aiming to extract the knowledge of foundation models for CD. The proposed BAN contains three parts, that is, the frozen foundation model (e.g., CLIP), bi-temporal adapter branch (Bi-TAB), and bridging modules between them. Specifically, BAN extracts general features through a frozen foundation model, which are then selected, aligned, and injected into Bi-TAB via the bridging modules. Bi-TAB is designed as a model-agnostic concept to extract task/domain-specific features, which can be either an existing arbitrary CD model or some hand-crafted stacked blocks. Beyond current customized models, BAN is the first extensive attempt to adapt the foundation model to the CD task. Experimental results show the effectiveness of our BAN in improving the performance of existing CD methods (e.g., up to 4.08% IoU improvement) with only a few additional learnable parameters. More importantly, these successful practices show us the potential of foundation models for remote-sensing (RS) CD. The code is available at https://github.com/likyoo/BAN and will be supported in our Open-CD.",
                "authors": "Kaiyu Li, Xiangyong Cao, Deyu Meng",
                "citations": 32
            },
            {
                "title": "Towards a Visual-Language Foundation Model for Computational Pathology",
                "abstract": "The accelerated adoption of digital pathology and advances in deep learning have enabled the development of powerful models for various pathology tasks across a diverse array of diseases and patient cohorts. However, model training is often difficult due to label scarcity in the medical domain and the model's usage is limited by the specific task and disease for which it is trained. Additionally, most models in histopathology leverage only image data, a stark contrast to how humans teach each other and reason about histopathologic entities. We introduce CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13 diverse benchmarks, CONCH can be transferred to a wide range of downstream tasks involving either or both histopathology images and text, achieving state-of-the-art performance on histology image classification, segmentation, captioning, text-to-image and image-to-text retrieval. CONCH represents a substantial leap over concurrent visual-language pretrained systems for histopathology, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.",
                "authors": "Ming Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, Andrew Zhang, L. Le, G. Gerber, A. Parwani, Faisal Mahmood",
                "citations": 39
            },
            {
                "title": "Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability",
                "abstract": "Causal abstraction provides a theoretical foundation for mechanistic interpretability, the field concerned with providing intelligible algorithms that are faithful simplifications of the known, but opaque low-level details of black box AI models. Our contributions are (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old mechanisms to new mechanisms), (2) providing a flexible, yet precise formalization for the core concepts of modular features, polysemantic neurons, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability methodologies in the common language of causal abstraction, namely activation and path patching, causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure, sparse autoencoders, differential binary masking, distributed alignment search, and activation steering.",
                "authors": "Atticus Geiger, D. Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang, Aryaman Arora, Zhengxuan Wu, Noah D. Goodman, Christopher Potts, Thomas F. Icard",
                "citations": 39
            },
            {
                "title": "Text-to-Image Diffusion Models are Zero-Shot Classifiers",
                "abstract": "The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data. However, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks. We investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers. The key idea is using a diffusion model's ability to denoise a noised image given a text description of a label as a proxy for that label's likelihood. We apply our method to Stable Diffusion and Imagen, using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's zero-shot abilities. They perform competitively with CLIP on a wide range of zero-shot image classification datasets. Additionally, they achieve state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot. Although generative pre-training is prevalent in NLP, visual foundation models often use other methods such as contrastive learning. Based on our findings, we argue that generative pre-training should be explored as a compelling alternative for vision-language tasks.",
                "authors": "Kevin Clark, P. Jaini",
                "citations": 81
            },
            {
                "title": "Debiasing Vision-Language Models via Biased Prompts",
                "abstract": "Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.",
                "authors": "Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, A. Torralba, S. Jegelka",
                "citations": 76
            },
            {
                "title": "Ignore Previous Prompt: Attack Techniques For Language Models",
                "abstract": "Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PromptInject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject is available at https://github.com/agencyenterprise/PromptInject.",
                "authors": "Fábio Perez, Ian Ribeiro",
                "citations": 318
            },
            {
                "title": "A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4",
                "abstract": "Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI GPT-3 model, and the popularity of LLMs is increasing exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GPT-3 family large language models.",
                "authors": "Katikapalli Subramanyam Kalyan",
                "citations": 140
            },
            {
                "title": "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models",
                "abstract": "The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better visual dog classifier by reading about dogs and listening to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for vision-language adaptation. Furthermore, we show that our approach can benefit existing methods such as prefix tuning, adapters, and classifier ensembling. Finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification. Project site at link.",
                "authors": "Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, Deva Ramana",
                "citations": 71
            },
            {
                "title": "A Survey on Model Compression for Large Language Models",
                "abstract": "Abstract Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.",
                "authors": "Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang",
                "citations": 132
            },
            {
                "title": "Large AI Models in Health Informatics: Applications, Challenges, and the Future",
                "abstract": "Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which can reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A prime example is ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our lives. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multi-modal data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents a comprehensive review of large AI models, from background to their applications. We identify seven key sectors in which large AI models are applicable and might have substantial influence, including: 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics. We examine their challenges, followed by a critical discussion about potential future directions and pitfalls of large AI models in transforming the field of health informatics.",
                "authors": "Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, Rui Zhang, Yinzhao Dong, K. Lam, F. P. Lo, Bo Xiao, Wu Yuan, Dong Xu, Benny P. L. Lo",
                "citations": 95
            },
            {
                "title": "A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering",
                "abstract": "The Segment Anything Model (SAM), developed by Meta AI Research, represents a significant breakthrough in computer vision, offering a robust framework for image and video segmentation. This survey provides a comprehensive exploration of the SAM family, including SAM and SAM 2, highlighting their advancements in granularity and contextual understanding. Our study demonstrates SAM's versatility across a wide range of applications while identifying areas where improvements are needed, particularly in scenarios requiring high granularity and in the absence of explicit prompts. By mapping the evolution and capabilities of SAM models, we offer insights into their strengths and limitations and suggest future research directions, including domain-specific adaptations and enhanced memory and propagation mechanisms. We believe that this survey comprehensively covers the breadth of SAM's applications and challenges, setting the stage for ongoing advancements in segmentation technology.",
                "authors": "Chaoning Zhang, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin, François Rameau, S. Bae, Choong-Seon Hong",
                "citations": 54
            },
            {
                "title": "Data Portraits: Recording Foundation Model Training Data",
                "abstract": "Foundation models are trained on increasingly immense and opaque datasets. Even while these models are now key in AI system building, it can be difficult to answer the straightforward question: has the model already encountered a given example during training? We therefore propose a widespread adoption of Data Portraits: artifacts that record training data and allow for downstream inspection. First we outline the properties of such an artifact and discuss how existing solutions can be used to increase transparency. We then propose and implement a solution based on data sketching, stressing fast and space efficient querying. Using our tools, we document a popular language modeling corpus (The Pile) and a recently released code modeling dataset (The Stack). We show that our solution enables answering questions about test set leakage and model plagiarism. Our tool is lightweight and fast, costing only 3% of the dataset size in overhead. We release a live interface of our tools at https://dataportraits.org/ and call on dataset and model creators to release Data Portraits as a complement to current documentation practices.",
                "authors": "Marc Marone, Benjamin Van Durme",
                "citations": 28
            },
            {
                "title": "FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning",
                "abstract": "Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning framework tailored to heterogeneous multi-modal FL, called Federated Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the client local updates and applying Mutual Knowledge Distillation (MKD) for an efficient knowledge transfer. FedDAT is the first approach that enables an efficient distributed finetuning of foundation models for a variety of heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we conduct extensive experiments on four multi-modality FL benchmarks with different types of data heterogeneity, where FedDAT substantially outperforms the existing centralized PEFT methods adapted for FL.",
                "authors": "Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, Volker Tresp",
                "citations": 23
            },
            {
                "title": "An RNA foundation model enables discovery of disease mechanisms and candidate therapeutics",
                "abstract": "Accurately modeling and predicting RNA biology has been a long-standing challenge, bearing significant clinical ramifications for variant interpretation and the formulation of tailored therapeutics. We describe a foundation model for RNA biology, “BigRNA”, which was trained on thousands of genome-matched datasets to predict tissue-specific RNA expression, splicing, microRNA sites, and RNA binding protein specificity from DNA sequence. Unlike approaches that are restricted to missense variants, BigRNA can identify pathogenic non-coding variant effects across diverse mechanisms, including polyadenylation, exon skipping and intron retention. BigRNA accurately predicted the effects of steric blocking oligonucleotides (SBOs) on increasing the expression of 4 out of 4 genes, and on splicing for 18 out of 18 exons across 14 genes, including those involved in Wilson disease and spinal muscular atrophy. We anticipate that BigRNA and foundation models like it will have widespread applications in the field of personalized RNA therapeutics.",
                "authors": "Albi Celaj, Alice Jiexin Gao, Tammy T. Y. Lau, Erle M. Holgersen, Alston Lo, Varun Lodaya, Christopher B. Cole, R. Denroche, Carl Spickett, Omar Wagih, Pedro O. Pinheiro, Parth Vora, P. Mohammadi-Shemirani, Steve Chan, Zach Nussbaum, Xi Zhang, Helen Zhu, Easwaran Ramamurthy, Bhargav Kanuparthi, Michael Iacocca, Diane Ly, Ken Kron, Marta Verby, Kahlin Cheung-Ong, Zvi Shalev, Brandon Vaz, Sakshi Bhargava, Farhan Yusuf, Sharon Samuel, Sabriyeh Alibai, Zahra Baghestani, Xinwen He, Kirsten Krastel, Oladipo Oladapo, Amrudha Mohan, Arathi Shanavas, Magdalena Bugno, Jovanka J Bogojeski, F. Schmitges, Carolyn Kim, Solomon Grant, Rachana Jayaraman, Tehmina Masud, A. Deshwar, Shreshth Gandhi, Brendan J. Frey",
                "citations": 23
            },
            {
                "title": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images",
                "abstract": "Generating differentially private (DP) synthetic data that closely resembles the original private data is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are only accessible via their inference APIs. However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the API provider. In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its initial promise on synthetic images. Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methods without any model training. For example, on CIFAR10 (with ImageNet as the public data), we achieve FID<= 7.9 with privacy cost {\\epsilon} = 0.67, significantly improving the previous SOTA from {\\epsilon} = 32. We further demonstrate the promise of applying PE on large foundation models such as Stable Diffusion to tackle challenging private datasets with a small number of high-resolution images. The code and data are released at https://github.com/microsoft/DPSDA.",
                "authors": "Zi-Han Lin, Sivakanth Gopi, Janardhan Kulkarni, Harsha Nori, S. Yekhanin",
                "citations": 21
            },
            {
                "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
                "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
                "authors": "Andy Zeng, Adrian S. Wong, Stefan Welker, K. Choromanski, F. Tombari, Aveek Purohit, M. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Peter R. Florence",
                "citations": 523
            },
            {
                "title": "Animal Models for Stem Cell-Based Pulp Regeneration: Foundation for Human Clinical Applications.",
                "abstract": "IMPACT STATEMENT\nAnimal models are essential for tissue regeneration studies. This review summarizes and discusses the small and large animal models, including mouse, ferret, dog, and miniswine that have been utilized to experiment and to demonstrate stem cell-mediated dental pulp tissue regeneration. We describe the models based on the location where the tissue regeneration is tested-either ectopic, semiorthotopic, or orthotopic. Developing and utilizing optimal animal models for both mechanistic and translational studies of pulp regeneration are of critical importance to advance this field.",
                "authors": "M. Nakashima, K. Iohara, M. Bottino, A. Fouad, J. Nör, G. T. Huang",
                "citations": 54
            },
            {
                "title": "PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm",
                "abstract": "In contrast to numerous NLP and 2D vision foundational models, learning a 3D foundational model poses considerably greater challenges. This is primarily due to the inherent data variability and diversity of downstream tasks. In this paper, we introduce a novel universal 3D pre-training framework designed to facilitate the acquisition of efficient 3D representation, thereby establishing a pathway to 3D foundational models. Considering that informative 3D features should encode rich geometry and appearance cues that can be utilized to render realistic images, we propose to learn 3D representations by differentiable neural rendering. We train a 3D backbone with a devised volumetric neural renderer by comparing the rendered with the real images. Notably, our approach seamlessly integrates the learned 3D encoder into various downstream tasks. These tasks encompass not only high-level challenges such as 3D detection and segmentation but also low-level objectives like 3D reconstruction and image synthesis, spanning both indoor and outdoor scenarios. Besides, we also illustrate the capability of pre-training a 2D backbone using the proposed methodology, surpassing conventional pre-training methods by a large margin. For the first time, PonderV2 achieves state-of-the-art performance on 11 indoor and outdoor benchmarks, implying its effectiveness. Code and models are available at https://github.com/OpenGVLab/PonderV2.",
                "authors": "Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha Zhang, Xianglong He, Tong He, Hengshuang Zhao, Chunhua Shen, Yu Qiao, Wanli Ouyang",
                "citations": 31
            },
            {
                "title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization",
                "abstract": "Large language models (LLMs) have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pre-trained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce. Meanwhile, we equip K2 with the abilities of using tools to be a naive geoscience aide. Experiments conducted on the GeoBench demonstrate the effectiveness of our approach and datasets on geoscience knowledge understanding and utilization.We open-source all the training data and K2 model checkpoints at https://github.com/davendw49/k2",
                "authors": "Cheng Deng, Tianhang Zhang, Zhongmou He, Yi Xu, Qiyuan Chen, Yuanyuan Shi, Le Zhou, Luoyi Fu, Weinan Zhang, Xinbing Wang, Cheng Zhou, Zhouhan Lin, Junxian He",
                "citations": 43
            },
            {
                "title": "PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology",
                "abstract": "As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We open-source our dataset, as well as a comprehensive toolkit for extensive pathology data collection and preprocessing at https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology.",
                "authors": "Yuxuan Sun, Chenglu Zhu, S. Zheng, Kai Zhang, Zhongyi Shui, Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong Zhang, Ruojia Zhao, Xinheng Lyu, Lin Yang",
                "citations": 28
            },
            {
                "title": "Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI",
                "abstract": null,
                "authors": "Mahyar Abbasian, Elahe Khatibi, Iman Azimi, David Oniani, Zahra Shakeri Hossein Abad, Alexander Thieme, Ram Sriram, Zhongqi Yang, Yanshan Wang, Bryant Lin, Olivier Gevaert, Li-Jia Li, Ramesh C. Jain, Amir M. Rahmani",
                "citations": 41
            },
            {
                "title": "Human Preference Score: Better Aligning Text-to-image Models with Human Preference",
                "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human preferences. Our experiments show that HPS outperforms CLIP in predicting human choices and has good generalization capability toward images generated from other models. By tuning Stable Diffusion with the guidance of HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/alignsd-web/.",
                "authors": "Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li",
                "citations": 94
            },
            {
                "title": "Better Aligning Text-to-Image Models with Human Preference",
                "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human aesthetic preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classiﬁer with the collected dataset and derive a Human Preference Score (HPS) based on the classiﬁer. Using the HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human aesthetic preferences. Our experiments show that the HPS outperforms CLIP in predicting human choices and has good generalization capability towards images generated from other models. By tuning Stable Diffusion with the guidance of the HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/align sd web/.",
                "authors": "Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li",
                "citations": 87
            },
            {
                "title": "Industry Foundation Classes: A Standardized Data Model for the Vendor-Neutral Exchange of Digital Building Models",
                "abstract": null,
                "authors": "A. Borrmann, J. Beetz, C. Koch, T. Liebich, S. Muhic",
                "citations": 56
            },
            {
                "title": "A Recipe for Watermarking Diffusion Models",
                "abstract": "Diffusion models (DMs) have demonstrated advantageous potential on generative tasks. Widespread interest exists in incorporating DMs into downstream applications, such as producing or editing photorealistic images. However, practical deployment and unprecedented power of DMs raise legal issues, including copyright protection and monitoring of generated content. In this regard, watermarking has been a proven solution for copyright protection and content monitoring, but it is underexplored in the DMs literature. Specifically, DMs generate samples from longer tracks and may have newly designed multimodal structures, necessitating the modification of conventional watermarking pipelines. To this end, we conduct comprehensive analyses and derive a recipe for efficiently watermarking state-of-the-art DMs (e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe is straightforward but involves empirically ablated implementation details, providing a foundation for future research on watermarking DMs. The code is available at https://github.com/yunqing-me/WatermarkDM.",
                "authors": "Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, Min Lin",
                "citations": 86
            },
            {
                "title": "A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language",
                "abstract": "Although artificial intelligence (AI) has made significant progress in understanding molecules in a wide range of fields, existing models generally acquire the single cognitive ability from the single molecular modality. Since the hierarchy of molecular knowledge is profound, even humans learn from different modalities including both intuitive diagrams and professional texts to assist their understanding. Inspired by this, we propose a molecular multimodal foundation model which is pretrained from molecular graphs and their semantically related textual data (crawled from published Scientific Citation Index papers) via contrastive learning. This AI model represents a critical attempt that directly bridges molecular graphs and natural language. Importantly, through capturing the specific and complementary information of the two modalities, our proposed model can better grasp molecular expertise. Experimental results show that our model not only exhibits promising performance in cross-modal tasks such as cross-modal retrieval and molecule caption, but also enhances molecular property prediction and possesses capability to generate meaningful molecular graphs from natural language descriptions. We believe that our model would have a broad impact on AI-empowered fields across disciplines such as biology, chemistry, materials, environment, and medicine, among others.",
                "authors": "Bing Su, Dazhao Du, Zhao-Qing Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Haoran Sun, Zhiwu Lu, Ji-rong Wen",
                "citations": 94
            },
            {
                "title": "DiffusionSat: A Generative Foundation Model for Satellite Imagery",
                "abstract": "Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction. Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets. As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, superresolution given multi-spectral inputs and in-painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale generative foundation model for satellite imagery. The project website can be found here: https://samar-khanna.github.io/DiffusionSat/",
                "authors": "Samar Khanna, Patrick Liu, Linqi Zhou, Chenlin Meng, Robin Rombach, Marshall Burke, David B. Lobell, Stefano Ermon",
                "citations": 34
            },
            {
                "title": "AnyPredict: Foundation Model for Tabular Prediction",
                "abstract": "Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated signiﬁcant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains. This paper proposes a method for building training data at scale for tabular prediction foundation models ( AnyPredict ) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a “learn, annotate, and audit” pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular dataset in the domain without ﬁne-tuning, resulting in signiﬁcant improvements over supervised baselines: it reaches an average ranking of 1.57 and 1.00 on 7 patient outcome prediction datasets and 3 trial outcome prediction datasets, respectively. In addition, AnyPredict exhibits impressive zero-shot performances: it outperforms supervised XGBoost models by 8 . 9% and 17 . 2% on average in two prediction tasks, respectively.",
                "authors": "Zifeng Wang, Chufan Gao, Cao Xiao, Jimeng Sun",
                "citations": 20
            },
            {
                "title": "The New Zealand Community Fault Model – version 1.0: an improved geological foundation for seismic hazard modelling",
                "abstract": "ABSTRACT The New Zealand Community Fault Model (NZ CFM) is a publicly available representation of New Zealand fault zones that have the potential to produce damaging earthquakes. Compiled through collaborative engagement between New Zealand earthquake-science experts, this first edition (version 1.0) of the NZ CFM builds upon previous compilations of earthquake-source active fault models with the addition of new and modified information. Developed primarily to support an update of the New Zealand National Seismic Hazard Model, the NZ CFM comprises two principal components. The first dataset is a two-dimensional map representation of the surface traces of 880 generalised fault zones. Each fault zone is assigned specific geometric and kinematic attributes, including uncertainties, supplemented with a subjective quality ranking focused primarily on the confidence in assigned slip rates. The second component is a three-dimensional representation of the fault zones as triangulated mesh surfaces that are projected down-dip from the two-dimensional mapped traces to a geophysically-defined maximum fault rupture depth. This article summarises the compilation and parameterisation of the NZ CFM, along with background on its relation to predecessor datasets, and forward applications to probabilistic seismic hazard assessment and physics-based earthquake models currently being developed for Aotearoa New Zealand.",
                "authors": "H. Seebeck, R. Dissen, N. Litchfield, P. Barnes, A. Nicol, R. Langridge, D. Barrell, P. Villamor, S. Ellis, M. Rattenbury, Stephen Bannister, M. Gerstenberger, F. Ghisetti, R. Sutherland, H. Hirschberg, J. Fraser, S. Nodder, M. Stirling, Jade Humphrey, K. J. Bland, A. Howell, J. Mountjoy, V. Moon, T. Stahl, F. Spinardi, D. Townsend, K. Clark, I. Hamling, S. Cox, W. D. de Lange, P. Wopereis, M. Johnston, R. Morgenstern, G. Coffey, J. Eccles, T. Little, B. Fry, J. Griffin, John Townend, N. Mortimer, S. Alcaraz, C. Massiot, J. Rowland, James Muirhead, P. Upton, Julie Lee",
                "citations": 35
            },
            {
                "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text",
                "abstract": "Evaluation practices in natural language generation (NLG) have many known flaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural generation models have improved to the point where their outputs can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their findings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for evaluation research and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 generation papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.",
                "authors": "Sebastian Gehrmann, Elizabeth Clark, Thibault Sellam",
                "citations": 169
            },
            {
                "title": "Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence",
                "abstract": "Nowadays, foundation models become one of fundamental infrastructures in artificial intelligence, paving ways to the general intelligence. However, the reality presents two urgent challenges: existing foundation models are dominated by the English-language community; users are often given limited resources and thus cannot always use foundation models. To support the development of the Chinese-language community, we introduce an open-source project, called Fengshenbang, which leads by the research center for Cognitive Computing and Natural Language (CCNL). Our project has comprehensive capabilities, including large pre-trained models, user-friendly APIs, benchmarks, datasets, and others. We wrap all these in three sub-projects: the Fengshenbang Model, the Fengshen Framework, and the Fengshen Benchmark. An open-source roadmap, Fengshenbang, aims to re-evaluate the open-source community of Chinese pre-trained large-scale models, prompting the development of the entire Chinese large-scale model community. We also want to build a user-centered open-source ecosystem to allow individuals to access the desired models to match their computing resources. Furthermore, we invite companies, colleges, and research institutions to collaborate with us to build the large-scale open-source model-based ecosystem. We hope that this project will be the foundation of Chinese cognitive intelligence.",
                "authors": "Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yan-Ze Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting-Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhong Zeng, Chong-An Chen, Ruyi Gan, Jiaxing Zhang",
                "citations": 99
            },
            {
                "title": "A Real-world Dataset and Benchmark For Foundation Model Adaptation in Medical Image Classification",
                "abstract": null,
                "authors": "Dequan Wang, Xiaosong Wang, Lilong Wang, Mengzhang Li, Q. Da, Xiaoqiang Liu, Xiangyu Gao, Jun Shen, Junjun He, Tian Shen, Qi Duan, Jie Zhao, Kang Li, Y. Qiao, Shaoting Zhang",
                "citations": 19
            },
            {
                "title": "In-Context Learning Unlocked for Diffusion Models",
                "abstract": "We present Prompt Diffusion, a framework for enabling in-context learning in diffusion-based generative models. Given a pair of task-specific example images, such as depth from/to image and scribble from/to image, and a text guidance, our model automatically understands the underlying task and performs the same task on a new query image following the text guidance. To achieve this, we propose a vision-language prompt that can model a wide range of vision-language tasks and a diffusion model that takes it as input. The diffusion model is trained jointly over six different tasks using these prompts. The resulting Prompt Diffusion model is the first diffusion-based vision-language foundation model capable of in-context learning. It demonstrates high-quality in-context generation on the trained tasks and generalizes effectively to new, unseen vision tasks with their respective prompts. Our model also shows compelling text-guided image editing results. Our framework aims to facilitate research into in-context learning for computer vision. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Prompt-Diffusion.",
                "authors": "Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou",
                "citations": 63
            },
            {
                "title": "Joint Foundation Model Caching and Inference of Generative AI Services for Edge Intelligence",
                "abstract": "With the rapid development of artificial general intelligence (AGI), various multimedia services based on pretrained foundation models (PFMs) need to be effectively deployed. With edge servers that have cloud-level computing power, edge intelligence can extend the capabilities of AGI to mobile edge networks. However, compared with cloud data centers, resource-limited edge servers can only cache and execute a small number of PFMs, which typically consist of billions of parameters and require intensive computing power and GPU memory during inference. To address this challenge, in this paper, we propose a joint foundation model caching and inference framework that aims to balance the tradeoff among inference latency, accuracy, and resource consumption by managing cached PFMs and user requests efficiently during the provisioning of generative AI services. Specifically, considering the in-context learning ability of PFMs, a new metric named the Age of Context (AoC), is proposed to model the freshness and relevance between examples in past demonstrations and current service requests. Based on the AoC, we propose a least context caching algorithm to manage cached PFMs at edge servers with historical prompts and inference results. The numerical results demonstrate that the proposed algorithm can reduce system costs compared with existing baselines by effectively utilizing contextual information.",
                "authors": "Minrui Xu, D. Niyato, Hongliang Zhang, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han",
                "citations": 16
            },
            {
                "title": "Foundation Model Assisted Weakly Supervised Semantic Segmentation",
                "abstract": "This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAMbased seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014. Our code will be released upon acceptance.",
                "authors": "Xiaobo Yang, Xiaojin Gong",
                "citations": 16
            },
            {
                "title": "Building Transportation Foundation Model via Generative Graph Transformer",
                "abstract": "In recent years, researchers have made notable advancements in various disciplines using large-scale foundation models. However, foundation models in the transportation system have not received adequate attention. To address this gap, we propose the Generative Graph Transformer (GGT), a transportation foundation model (TFM) that leverages graph structure and dynamic graph generation algorithms. The primary objective of our TFM is to capture participant behavior and interaction in the transportation system, at various scales, and establish a large-scale neural network to comprehend the entire system. The GGT-based TFM can overcom challenges of structural complexity and model accuracy in conventional traffic models. This approach holds promise for addressing complex traffic issues by utilizing up-to-date real traffic data. To demonstrate the capabilities of GGT, a simulation experiment was conducted.",
                "authors": "Xuhong Wang, Ding Wang, Liang Chen, Yilun Lin",
                "citations": 15
            },
            {
                "title": "Insect-Foundation: A Foundation Model and Large-Scale 1M Dataset for Visual Insect Understanding",
                "abstract": "In precision agriculture, the detection and recognition of insects play an essential role in the ability of crops to grow healthy and produce a high-quality yield. The current machine vision model requires a large volume of data to achieve high performance. However, there are approximately 5.5 million different insect species in the world. None of the existing insect datasets can cover even a fraction of them due to varying geographic locations and acquisition costs. In this paper, we introduce a novel “Insect-1M” dataset, a game-changing resource poised to revolutionize insect-related foundation model training. Covering a vast spectrum of insect species, our dataset, including 1 million images with dense identification labels of taxonomy hierarchy and insect descriptions, offers a panoramic view of entomology, enabling foundation models to comprehend visual and semantic information about insects like never before. Then, to efficiently establish an Insect Foundation Model, we develop a micro-feature self-supervised learning method with a Patch-wise Relevant Attention mechanism capable of discerning the subtle differences among insect images. In addition, we introduce Description Consistency loss to improve micro-feature modeling via insect descriptions. Through our experiments, we illustrate the effectiveness of our proposed approach in insect modeling and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks. Our Insect Foundation Model and Dataset promise to empower the next generation of insect-related vision models, bringing them closer to the ultimate goal of precision agriculture.",
                "authors": "Hoang-Quan Nguyen, Thanh-Dat Truong, Xuan-Bac Nguyen, Ashley Dowling, Xin Li, Khoa Luu",
                "citations": 15
            },
            {
                "title": "GenePT: A Simple But Effective Foundation Model for Genes and Cells Built From ChatGPT",
                "abstract": "There has been significant recent progress in leveraging large-scale gene expression data to develop foundation models for single-cell biology. Models such as Geneformer and scGPT implicitly learn gene and cellular functions from the gene expression profiles of millions of cells, which requires extensive data curation and resource-intensive training. Here we explore a much simpler alternative by leveraging ChatGPT embeddings of genes based on literature. Our proposal, GenePT, uses NCBI text descriptions of individual genes with GPT-3.5 to generate gene embeddings. From there, GenePT generates single-cell embeddings in two ways: (i) by averaging the gene embeddings, weighted by each gene’s expression level; or (ii) by creating a sentence embedding for each cell, using gene names ordered by the expression level. Without the need for dataset curation and additional pretraining, GenePT is efficient and easy to use. On many downstream tasks used to evaluate recent single-cell foundation models — e.g., classifying gene properties and cell types — GenePT achieves comparable, and often better, performance than Geneformer and other models. GenePT demonstrates that large language model embedding of literature is a simple and effective path for biological foundation models.",
                "authors": "Yiqun T. Chen, James Zou",
                "citations": 14
            },
            {
                "title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
                "abstract": "Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",
                "authors": "Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, J. Leskovec",
                "citations": 188
            },
            {
                "title": "Transformer models: an introduction and catalog",
                "abstract": "In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovations in Transformer models. Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT).",
                "authors": "X. Amatriain",
                "citations": 33
            },
            {
                "title": "FoundPose: Unseen Object Pose Estimation with Foundation Features",
                "abstract": "We propose FoundPose, a model-based method for 6D pose estimation of unseen objects from a single RGB image. The method can quickly onboard new objects using their 3D models without requiring any object- or task-specific training. In contrast, existing methods typically pre-train on large-scale, task-specific datasets in order to generalize to new objects and to bridge the image-to-model domain gap. We demonstrate that such generalization capabilities can be observed in a recent vision foundation model trained in a self-supervised manner. Specifically, our method estimates the object pose from image-to-model 2D-3D correspondences, which are established by matching patch descriptors from the recent DINOv2 model between the image and pre-rendered object templates. We find that reliable correspondences can be established by kNN matching of patch descriptors from an intermediate DINOv2 layer. Such descriptors carry stronger positional information than descriptors from the last layer, and we show their importance when semantic information is ambiguous due to object symmetries or a lack of texture. To avoid establishing correspondences against all object templates, we develop an efficient template retrieval approach that integrates the patch descriptors into the bag-of-words representation and can promptly propose a handful of similarly looking templates. Additionally, we apply featuremetric alignment to compensate for discrepancies in the 2D-3D correspondences caused by coarse patch sampling. The resulting method noticeably outperforms existing RGB methods for refinement-free pose estimation on the standard BOP benchmark with seven diverse datasets and can be seamlessly combined with an existing render-and-compare refinement method to achieve RGB-only state-of-the-art results. Project page: evinpinar.github.io/foundpose.",
                "authors": "Evin Pınar Örnek, Yann Labb'e, Bugra Tekin, Lingni Ma, Cem Keskin, Christian Forster, Tomás Hodan",
                "citations": 21
            },
            {
                "title": "A Foundation Model for Music Informatics",
                "abstract": "This paper investigates foundation models tailored for music informatics, a domain currently challenged by the scarcity of labeled data and generalization issues. To this end, we conduct an in-depth comparative study among various foundation model variants, examining key determinants such as model architectures, tokenization methods, temporal resolution, data, and model scalability. This research aims to bridge the existing knowledge gap by elucidating how these individual factors contribute to the success of foundation models in music informatics. Employing a careful evaluation frame-work, we assess the performance of these models across diverse downstream tasks in music information retrieval, with a particular focus on frame-level and sequence-level classification. Our results reveal that our model demonstrates robust performance, surpassing existing models in specific key metrics. These findings contribute to the understanding of self-supervised learning in music informatics and pave the way for developing more effective and versatile foundation models in the field. A pretrained version of our model is publicly available to foster reproducibility and future research.",
                "authors": "Minz Won, Yun-Ning Hung, Duc Le",
                "citations": 13
            },
            {
                "title": "Mobile Foundation Model as Firmware",
                "abstract": "In the current AI era, mobile devices such as smartphones are tasked with executing a myriad of deep neural networks (DNNs) locally. It presents a complex landscape, as these models are highly fragmented in terms of architecture, operators, and implementations. Such fragmentation poses significant challenges to the co-optimization of hardware, systems, and algorithms for efficient and scalable mobile AI. Inspired by the recent groundbreaking progress in large foundation models, this work introduces a novel paradigm for mobile AI, where mobile OS and hardware jointly manage a foundation model that is capable of serving a wide array of mobile AI tasks. This foundation model functions akin to firmware, unmodifiable by apps or the OS, exposed as a system service to Apps. They can invoke this foundation model through a small, offline fine-tuned \"adapter\" for various downstream tasks. We propose a tangible design of this vision called M4, and prototype it from publicly available pre-trained models. To assess its capability, we also build a comprehensive benchmark consisting of 38 mobile AI tasks and 50 datasets, spanning 5 multimodal inputs. Extensive experiments demonstrate M4's remarkable results: it achieves comparable accuracy in 85% of tasks, offers enhanced scalability regarding storage and memory, and has much simpler operations. In broader terms, this work paves a new way towards efficient and scalable mobile AI in the post-LLM era.",
                "authors": "Jinliang Yuan, Chenchen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling Zhang, Xiang Li, Di Zhang, Hanzi Mei, Xianqing Jia, Shangguang Wang, Mengwei Xu",
                "citations": 13
            },
            {
                "title": "A Coupled Model for Dam Foundation Seepage Behavior Monitoring and Forecasting Based on Variational Mode Decomposition and Improved Temporal Convolutional Network",
                "abstract": "Grasping the change behavior of dam foundation seepage pressure is of great significance for ensuring the safety of concrete dams. Because of the environmental complexity of the dam location, the prototypical seepage pressure data are easy to be contaminated by noise, which brings challenges to accurate prediction. Traditional denoising methods will lose the detailed characteristics of the objects, resulting in prediction models with limited flexibility and prediction accuracy. To address these problems, the prototypical data with noise are denoised using the variational mode decomposition (VMD)-wavelet packet denoising method. Then, an improved temporal convolutional network (ITCN) model is built for dam foundation seepage pressure data prediction. A hysteresis experiment is carried out to optimize the model structure by correlating the receptive field size of the ITCN model with the hysteresis of the dam foundation seepage pressure. Finally, the optimal ITCN dam foundation seepage pressure prediction model of each measurement point is obtained after the training. Three state-of-the-art methods in dam seepage monitoring are used as benchmark methods to compare the prediction performance of the proposed method. Four evaluation indicators are introduced to quantitatively evaluate and compare the prediction performance of the proposed method. The experimental results prove that the proposed method achieves high prediction accuracy flexibility. The indicator values of the ITCN model are only 50%–90% of those of LSTM and RNN models and 15%–40% of those of the stepwise regression model, and the values are all small.",
                "authors": "Yantao Zhu, Zhiduan Zhang, C. Gu, Yangtao Li, Kang Zhang, Mingxia Xie",
                "citations": 20
            },
            {
                "title": "An Unified Search and Recommendation Foundation Model for Cold-Start Scenario",
                "abstract": "In modern commercial search engines and recommendation systems, data from multiple domains is available to jointly train the multi-domain model. Traditional methods train multi-domain models in the multi-task setting, with shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of features, labels, and sample distributions of individual tasks. With the development of large language models, LLM can extract global domain-invariant text features that serve both search and recommendation tasks. We propose a novel framework called S&R Multi-Domain Foundation, which uses LLM to extract domain invariant features, and Aspect Gating Fusion to merge the ID feature, domain invariant text features and task-specific heterogeneous sparse features to obtain the representations of query and item. Additionally, samples from multiple search and recommendation scenarios are trained jointly with Domain Adaptive Multi-Task module to obtain the multi-domain foundation model. We apply the S&R Multi-Domain foundation model to cold start scenarios in the pretrain-finetune manner, which achieves better performance than other SOTA transfer learning methods. The S&R Multi-Domain Foundation model has been successfully deployed in Alipay Mobile Application's online services, such as content query recommendation and service card recommendation, etc.",
                "authors": "Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, Guannan Zhang",
                "citations": 20
            },
            {
                "title": "Toward a Foundation Model for Time Series Data",
                "abstract": "A foundation model is a machine learning model trained on a large and diverse set of data, typically using self-supervised learning-based pre-training techniques, that can be adapted to various downstream tasks. However, current research on time series pre-training has predominantly focused on models trained exclusively on data from a single domain. As a result, these models possess domain-specific knowledge that may not be easily transferable to time series from other domains. In this paper, we aim to develop an effective time series foundation model by leveraging unlabeled samples from multiple domains. To achieve this, we repurposed the publicly available UCR Archive and evaluated four existing self-supervised learning-based pre-training methods, along with a novel method, on the datasets. We tested these methods using four popular neural network architectures for time series to understand how the pre-training methods interact with different network designs. Our experimental results show that pre-training improves downstream classification tasks by enhancing the convergence of the fine-tuning process. Furthermore, we found that the proposed pre-training method, when combined with the Transformer, outperforms the alternatives. The proposed method outperforms or achieves equal performance compared to the second best method in ~93% of downstream tasks.",
                "authors": "Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng, Yujie Fan, Audrey Der, Vivian Lai, Zhongfang Zhuang, Junpeng Wang, Liang Wang, Wei Zhang",
                "citations": 21
            },
            {
                "title": "Retrieval-based Language Models and Applications",
                "abstract": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.",
                "authors": "Akari Asai, Sewon Min, Zexuan Zhong, Danqi Chen",
                "citations": 64
            },
            {
                "title": "Similarity of Neural Network Models: A Survey of Functional and Representational Measures",
                "abstract": "Measuring similarity of neural networks to understand and improve their behavior has become an issue of great importance and research interest. In this survey, we provide a comprehensive overview of two complementary perspectives of measuring neural network similarity: (i) representational similarity, which considers how activations of intermediate layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties of and relationships between these measures, and point to open research problems. We hope our work lays a foundation for more systematic research on the properties and applicability of similarity measures for neural network models.",
                "authors": "Max Klabunde, Tobias Schumacher, M. Strohmaier, Florian Lemmerich",
                "citations": 54
            },
            {
                "title": "Composite Backdoor Attacks Against Large Language Models",
                "abstract": "Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered Rate (FTR) below $2.06\\%$ and negligible model accuracy degradation. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.",
                "authors": "Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang",
                "citations": 30
            },
            {
                "title": "Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision",
                "abstract": "Foundation models, large-scale, pre-trained deep-learning models adapted to a wide range of downstream tasks have gained significant interest lately in various deep-learning problems undergoing a paradigm shift with the rise of these models. Trained on large-scale dataset to bridge the gap between different modalities, foundation models facilitate contextual reasoning, generalization, and prompt capabilities at test time. The predictions of these models can be adjusted for new tasks by augmenting the model input with task-specific hints called prompts without requiring extensive labeled data and retraining. Capitalizing on the advances in computer vision, medical imaging has also marked a growing interest in these models. To assist researchers in navigating this direction, this survey intends to provide a comprehensive overview of foundation models in the domain of medical imaging. Specifically, we initiate our exploration by providing an exposition of the fundamental concepts forming the basis of foundation models. Subsequently, we offer a methodical taxonomy of foundation models within the medical domain, proposing a classification system primarily structured around training strategies, while also incorporating additional facets such as application domains, imaging modalities, specific organs of interest, and the algorithms integral to these models. Furthermore, we emphasize the practical use case of some selected approaches and then discuss the opportunities, applications, and future directions of these large-scale pre-trained models, for analyzing medical images. In the same vein, we address the prevailing challenges and research pathways associated with foundational models in medical imaging. These encompass the areas of interpretability, data management, computational requirements, and the nuanced issue of contextual comprehension.",
                "authors": "Bobby Azad, Reza Azad, Sania Eskandari, Afshin Bozorgpour, A. Kazerouni, I. Rekik, D. Merhof",
                "citations": 37
            },
            {
                "title": "LLark: A Multimodal Foundation Model for Music",
                "abstract": "Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLARK, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLARK, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model’s responses in captioning and reasoning tasks. LLARK is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https: //github.com/spotify-research/llark .",
                "authors": "Josh Gardner, Simon Durand, Daniel Stoller, Rachel M. Bittner",
                "citations": 21
            },
            {
                "title": "Seismic Foundation Model (SFM): a next generation deep learning model in geophysics",
                "abstract": "While computer science has seen remarkable advancements in foundation models, they remain underexplored in geoscience. Addressing this gap, we introduce a workflow to develop geophysical foundation models, including data preparation, model pre-training, and adaption to downstream tasks. From 192 globally collected 3-D seismic volumes, we create a carefully curated dataset of 2,286,422 2-D seismic images. Fully using these unlabeled images, we employ the self-supervised learning to pre-train a Transformer-based Seismic Foundation Model (SFM) for producing all-purpose seismic features that work across various tasks and surveys. Through experiments on seismic facies classification, geobody identification, interpolation, denoising, and inversion, our pre-trained model demonstrates versatility, generalization, scalability, and superior performance over baseline models. In conclusion, we provide a foundation model and vast dataset to advance AI in geophysics, addressing challenges (poor generalization, lacking labels, and repetitive training for task-specified models) of applying AI in geophysics and paving the way for future innovations in geoscience.",
                "authors": "Hanlin Sheng, Xinming Wu, Xu Si, Jintao Li, Sibo Zhang, X. Duan",
                "citations": 10
            },
            {
                "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving",
                "abstract": "Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their\"black box\"nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about \\textit{Large Language Models for Autonomous Driving (LLM4AD)}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.",
                "authors": "Zhenjie Yang, Xiaosong Jia, Hongyang Li, Junchi Yan",
                "citations": 57
            },
            {
                "title": "A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture",
                "abstract": "Large language model (LLM) based chatbots, such as ChatGPT, have attracted huge interest in foundation models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. However, the architecture design of foundation model based systems has not yet been systematically explored. There is limited understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and system design options. Our taxonomy comprises three categories: the pretraining and adaptation of foundation models, the architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy can serve as concrete guidance for designing foundation model based systems.CCS CONCEPTS•Software and its engineering → Software design engineering;•Computer systems organization → Architectures;•Computing methodologies → Artificial intelligence.",
                "authors": "Qinghua Lu, Liming Zhu, Xiwei Xu, Yue Liu, Zhenchang Xing, Jon Whittle",
                "citations": 9
            },
            {
                "title": "Foundation Model for Material Science",
                "abstract": "Foundation models (FMs) are achieving remarkable successes to realize complex downstream tasks in domains including natural language and visions. In this paper, we propose building an FM for material science, which is trained with massive data across a wide variety of material domains and data modalities. Nowadays machine learning models play key roles in material discovery, particularly for property prediction and structure generation. However, those models have been independently developed to address only specific tasks without sharing more global knowledge. Development of an FM for material science will enable overarching modeling across material domains and data modalities by sharing their feature representations. We discuss fundamental challenges and required technologies to build an FM from the aspects of data preparation, model development, and downstream tasks.",
                "authors": "Seiji Takeda, Akihiro Kishimoto, Lisa Hamada, D. Nakano, John Smith",
                "citations": 9
            },
            {
                "title": "RoentGen: Vision-Language Foundation Model for Chest X-ray Generation",
                "abstract": "Multimodal models trained on large natural image-text pair datasets have exhibited astounding abilities in generating high-quality images. Medical imaging data is fundamentally different to natural images, and the language used to succinctly capture relevant details in medical data uses a different, narrow but semantically rich, domain-specific vocabulary. Not surprisingly, multi-modal models trained on natural image-text pairs do not tend to generalize well to the medical domain. Developing generative imaging models faithfully representing medical concepts while providing compositional diversity could mitigate the existing paucity of high-quality, annotated medical imaging datasets. In this work, we develop a strategy to overcome the large natural-medical distributional shift by adapting a pre-trained latent diffusion model on a corpus of publicly available chest x-rays (CXR) and their corresponding radiology (text) reports. We investigate the model's ability to generate high-fidelity, diverse synthetic CXR conditioned on text prompts. We assess the model outputs quantitatively using image quality metrics, and evaluate image quality and text-image alignment by human domain experts. We present evidence that the resulting model (RoentGen) is able to create visually convincing, diverse synthetic CXR images, and that the output can be controlled to a new extent by using free-form text prompts including radiology-specific language. Fine-tuning this model on a fixed training set and using it as a data augmentation method, we measure a 5% improvement of a classifier trained jointly on synthetic and real images, and a 3% improvement when trained on a larger but purely synthetic training set. Finally, we observe that this fine-tuning distills in-domain knowledge in the text-encoder and can improve its representation capabilities of certain diseases like pneumothorax by 25%.",
                "authors": "P. Chambon, Christian Blüthgen, Jean-Benoit Delbrouck, Rogier van der Sluijs, M. Polacin, Juan Manuel Zambrano Chaves, T. Abraham, Shivanshu Purohit, C. Langlotz, Akshay Chaudhari",
                "citations": 85
            },
            {
                "title": "Uncover This Tech Term: Foundation Model",
                "abstract": "The foundation model (FM) is a family of machine artificial intelligence (AI) models that are generally trained by self-supervised learning using a large volume of unannotated dataset and can be adapted to various downstream tasks [1]. The most well-known examples of FMs are large language models (LLMs), such as ChatGPT [2]. Similar to ChatGPT, LLMs typically consist of billions of parameters and are designed to perform various natural language tasks. An LLM is initially pretrained to predict next words that follow a given input text (referred to as ‘pretext task’), through which the LLM learns the semantics and structure of languages. With subsequent fine-tuning by human feedback, the LLM then acquires capabilities to generate natural and plausible responses to a wide range of queries (referred to as ‘downstream tasks’). Training for the pretext task is typically achieved through self-supervised learning, using a massive unannotated Uncover This Tech Term: Foundation Model Kyu-Hwan Jung Department of Medical Device Management and Research, Samsung Advanced Institute for Health Sciences and Technology, Sungkyunkwan University, Seoul, Republic of Korea Dataset Science Research Institute, Research Institute for Future Medicine, Samsung Medical Center, Seoul, Republic of Korea",
                "authors": "K. Jung",
                "citations": 18
            },
            {
                "title": "Contrastive Adapters for Foundation Model Group Robustness",
                "abstract": "While large pretrained foundation models (FMs) have shown remarkable zero-shot classification robustness to dataset-level distribution shifts, their robustness to subpopulation or group shifts is relatively underexplored. We study this problem, and find that FMs such as CLIP may not be robust to various group shifts. Across 9 robustness benchmarks, zero-shot classification with their embeddings results in gaps of up to 80.7 percentage points (pp) between average and worst-group accuracy. Unfortunately, existing methods to improve robustness require retraining, which can be prohibitively expensive on large foundation models. We also find that efficient ways to improve model inference (e.g., via adapters, lightweight networks with FM embeddings as inputs) do not consistently improve and can sometimes hurt group robustness compared to zero-shot (e.g., increasing the accuracy gap by 50.1 pp on CelebA). We thus develop an adapter training strategy to effectively and efficiently improve FM group robustness. Our motivating observation is that while poor robustness results from groups in the same class being embedded far apart in the foundation model\"embedding space,\"standard adapter training may not bring these points closer together. We thus propose contrastive adapting, which trains adapters with contrastive learning to bring sample embeddings close to both their ground-truth class embeddings and other sample embeddings in the same class. Across the 9 benchmarks, our approach consistently improves group robustness, raising worst-group accuracy by 8.5 to 56.0 pp over zero-shot. Our approach is also efficient, doing so without any FM finetuning and only a fixed set of frozen FM embeddings. On benchmarks such as Waterbirds and CelebA, this leads to worst-group accuracy comparable to state-of-the-art methods that retrain entire models, while only training $\\leq$1% of the model parameters.",
                "authors": "Michael Zhang, Christopher R'e",
                "citations": 49
            },
            {
                "title": "Towards a foundation model for geospatial artificial intelligence (vision paper)",
                "abstract": "Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have yet to see an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges for developing multimodal foundation models for GeoAI. We first show the advantages of this idea by testing the performance of existing Large pre-trained Language Models (LLMs) (e.g. GPT-2 and GPT-3) on two geospatial semantics tasks. Results indicate that these task-agnostic LLMs can outperform task-specific fully-supervised models on both tasks with 2--9% improvement in a few-shot learning setting. However, we also show the limitations of these existing foundation models given the multimodality nature of GeoAI, especially when dealing with geometries in conjunction with other modalities. So we discuss the possibility of a multimodal foundation model which can reason over various types of geospatial data through geospatial alignments. We conclude this paper by discussing the unique risks and challenges to develop such model for GeoAI.",
                "authors": "Gengchen Mai, Chris Cundy, Kristy Choi, Yingjie Hu, Ni Lao, Stefano Ermon",
                "citations": 56
            },
            {
                "title": "A Foundation Model for Cell Segmentation",
                "abstract": "Cells are a fundamental unit of biological organization, and identifying them in imaging data – cell segmentation – is a critical task for various cellular imaging experiments. While deep learning methods have led to substantial progress on this problem, most models in use are specialist models that work well for specific domains. Methods that have learned the general notion of “what is a cell” and can identify them across different domains of cellular imaging data have proven elusive. In this work, we present CellSAM, a foundation model for cell segmentation that generalizes across diverse cellular imaging data. CellSAM builds on top of the Segment Anything Model (SAM) by developing a prompt engineering approach for mask generation. We train an object detector, CellFinder, to automatically detect cells and prompt SAM to generate segmentations. We show that this approach allows a single model to achieve human-level performance for segmenting images of mammalian cells (in tissues and cell culture), yeast, and bacteria collected across various imaging modalities. We show that CellSAM has strong zero-shot performance and can be improved with a few examples via few-shot learning. We also show that CellSAM can unify bioimaging analysis workflows such as spatial transcriptomics and cell tracking. A deployed version of CellSAM is available at https://cellsam.deepcell.org/.",
                "authors": "Uriah Israel, Markus Marks, Rohit Dilip, Qilin Li, Changhua Yu, Emily Laubscher, Shenyi Li, Morgan Schwartz, Elora Pradhan, Ada Ates, Martin Abt, Caitlin Brown, Edward Pao, Alexander Pearson-Goulart, Pietro Perona, Georgia Gkioxari, Ross Barnowski, Yisong Yue, D. V. Valen",
                "citations": 17
            },
            {
                "title": "A Perspective on the Theoretical Foundation of Dual Process Models",
                "abstract": null,
                "authors": "Gordon Pennycook",
                "citations": 62
            },
            {
                "title": "When is a Foundation Model a Foundation Model",
                "abstract": "Recently, several studies have reported on the fine-tuning of foundation models for image-text modeling in the field of medicine, utilizing images from online data sources such as Twitter and PubMed. Foundation models are large, deep artificial neural networks capable of learning the context of a specific domain through training on exceptionally extensive datasets. Through validation, we have observed that the representations generated by such models exhibit inferior performance in retrieval tasks within digital pathology when compared to those generated by significantly smaller, conventional deep networks.",
                "authors": "Saghir Alfasly, Peyman Nejat, S. Hemati, Jibran A. Khan, Isaiah Lahr, Areej Alsaafin, Abubakr Shafique, N. Comfere, Dennis Murphree, Chady Meroueh, Saba Yasir, Aaron Mangold, Lisa Boardman, Vijay H. Shah, Joaquin J. Garcia, H. Tizhoosh",
                "citations": 5
            },
            {
                "title": "Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting",
                "abstract": "In this work, we propose \\texttt{TimeGrad}, an autoregressive model for multivariate probabilistic time series forecasting which samples from the data distribution at each time step by estimating its gradient. To this end, we use diffusion probabilistic models, a class of latent variable models closely connected to score matching and energy-based methods. Our model learns gradients by optimizing a variational bound on the data likelihood and at inference time converts white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling. We demonstrate experimentally that the proposed autoregressive denoising diffusion model is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions. We hope that this method is a useful tool for practitioners and lays the foundation for future research in this area.",
                "authors": "Kashif Rasul, C. Seward, Ingmar Schuster, Roland Vollgraf",
                "citations": 248
            },
            {
                "title": "Enhancing Knowledge Graph Construction Using Large Language Models",
                "abstract": "The growing trend of Large Language Models (LLM) development has attracted significant attention, with models for various applications emerging consistently. However, the combined application of Large Language Models with semantic technologies for reasoning and inference is still a challenging task. This paper analyzes how the current advances in foundational LLM, like ChatGPT, can be compared with the specialized pretrained models, like REBEL, for joint entity and relation extraction. To evaluate this approach, we conducted several experiments using sustainability-related text as our use case. We created pipelines for the automatic creation of Knowledge Graphs from raw texts, and our findings indicate that using advanced LLM models can improve the accuracy of the process of creating these graphs from unstructured text. Furthermore, we explored the potential of automatic ontology creation using foundation LLM models, which resulted in even more relevant and accurate knowledge graphs.",
                "authors": "Milena Trajanoska, Riste Stojanov, D. Trajanov",
                "citations": 41
            },
            {
                "title": "Hyperuricemia, Acute and Chronic Kidney Disease, Hypertension, and Cardiovascular Disease: Report of a Scientific Workshop Organized by the National Kidney Foundation.",
                "abstract": "Urate is a cause of gout, kidney stones, and acute kidney injury from tumor lysis syndrome, but its relationship to kidney disease, cardiovascular disease, and diabetes remains controversial. A scientific workshop organized by the National Kidney Foundation was held in September 2016 to review current evidence. Cell culture studies and animal models suggest that elevated serum urate concentrations can contribute to kidney disease, hypertension, and metabolic syndrome. Epidemiologic evidence also supports elevated serum urate concentrations as a risk factor for the development of kidney disease, hypertension, and diabetes, but differences in methodologies and inpacts on serum urate concentrations by even subtle changes in kidney function render conclusions uncertain. Mendelian randomization studies generally do not support a causal role of serum urate in kidney disease, hypertension, or diabetes, although interpretation is complicated by nonhomogeneous populations, a failure to consider environmental interactions, and a lack of understanding of how the genetic polymorphisms affect biological mechanisms related to urate. Although several small clinical trials suggest benefits of urate-lowering therapies on kidney function, blood pressure, and insulin resistance, others have been negative, with many trials having design limitations and insufficient power. Thus, whether uric acid has a causal role in kidney and cardiovascular diseases requires further study.",
                "authors": "Richard J. Johnson, G. Bakris, C. Borghi, M. Chonchol, D. Feldman, M. Lanaspa, T. Merriman, O. Moe, D. Mount, L. G. Sánchez Lozada, E. Stahl, D. Weiner, G. Chertow",
                "citations": 435
            },
            {
                "title": "Towards an astronomical foundation model for stars with a Transformer-based model",
                "abstract": "Rapid strides are currently being made in the field of artificial intelligence using Transformer-based models like Large Language Models (LLMs). The potential of these methods for creating a single, large, versatile model in astronomy has not yet been explored. In this work, we propose a framework for data-driven astronomy that uses the same core techniques and architecture as used by LLMs. Using a variety of observations and labels of stars as an example, we build a Transformer-based model and train it in a self-supervised manner with cross-survey data sets to perform a variety of inference tasks. In particular, we demonstrate that a $\\textit{single}$ model can perform both discriminative and generative tasks even if the model was not trained or fine-tuned to do any specific task. For example, on the discriminative task of deriving stellar parameters from Gaia XP spectra, we achieve an accuracy of 47 K in $T_\\mathrm{eff}$, 0.11 dex in $\\log{g}$, and 0.07 dex in $[\\mathrm{M/H}]$, outperforming an expert $\\texttt{XGBoost}$ model in the same setting. But the same model can also generate XP spectra from stellar parameters, inpaint unobserved spectral regions, extract empirical stellar loci, and even determine the interstellar extinction curve. Our framework demonstrates that building and training a $\\textit{single}$ foundation model without fine-tuning using data and parameters from multiple surveys to predict unmeasured observations and parameters is well within reach. Such\"Large Astronomy Models\"trained on large quantities of observational data will play a large role in the analysis of current and future large surveys.",
                "authors": "Henry W. Leung, J. Bovy",
                "citations": 13
            },
            {
                "title": "nach0: multimodal natural and chemical languages foundation model",
                "abstract": "Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder–decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups.",
                "authors": "M. Livne, Z. Miftahutdinov, E. Tutubalina, Maksim Kuznetsov, Daniil Polykovskiy, Annika Brundyn, Aastha Jhunjhunwala, Anthony B Costa, A. Aliper, A. Zhavoronkov",
                "citations": 11
            },
            {
                "title": "Analyzing mixing systems using a new generation of Bayesian tracer mixing models",
                "abstract": "The ongoing evolution of tracer mixing models has resulted in a confusing array of software tools that differ in terms of data inputs, model assumptions, and associated analytic products. Here we introduce MixSIAR, an inclusive, rich, and flexible Bayesian tracer (e.g., stable isotope) mixing model framework implemented as an open-source R package. Using MixSIAR as a foundation, we provide guidance for the implementation of mixing model analyses. We begin by outlining the practical differences between mixture data error structure formulations and relate these error structures to common mixing model study designs in ecology. Because Bayesian mixing models afford the option to specify informative priors on source proportion contributions, we outline methods for establishing prior distributions and discuss the influence of prior specification on model outputs. We also discuss the options available for source data inputs (raw data versus summary statistics) and provide guidance for combining sources. We then describe a key advantage of MixSIAR over previous mixing model software—the ability to include fixed and random effects as covariates explaining variability in mixture proportions and calculate relative support for multiple models via information criteria. We present a case study of Alligator mississippiensis diet partitioning to demonstrate the power of this approach. Finally, we conclude with a discussion of limitations to mixing model applications. Through MixSIAR, we have consolidated the disparate array of mixing model tools into a single platform, diversified the set of available parameterizations, and provided developers a platform upon which to continue improving mixing model analyses in the future.",
                "authors": "Brian C. Stock, A. Jackson, E. Ward, A. Parnell, D. Phillips, B. Semmens",
                "citations": 815
            },
            {
                "title": "M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems",
                "abstract": "Industrial recommender systems have been growing increasingly complex, may involve \\emph{diverse domains} such as e-commerce products and user-generated contents, and can comprise \\emph{a myriad of tasks} such as retrieval, ranking, explanation generation, and even AI-assisted content production. The mainstream approach so far is to develop individual algorithms for each domain and each task. In this paper, we explore the possibility of developing a unified foundation model to support \\emph{open-ended domains and tasks} in an industrial recommender system, which may reduce the demand on downstream settings' data and can minimize the carbon footprint by avoiding training a separate model from scratch for every task. Deriving a unified foundation is challenging due to (i) the potentially unlimited set of downstream domains and tasks, and (ii) the real-world systems' emphasis on computational efficiency. We thus build our foundation upon M6, an existing large-scale industrial pretrained language model similar to GPT-3 and T5, and leverage M6's pretrained ability for sample-efficient downstream adaptation, by representing user behavior data as plain texts and converting the tasks to either language understanding or generation. To deal with a tight hardware budget, we propose an improved version of prompt tuning that outperforms fine-tuning with negligible 1\\% task-specific parameters, and employ techniques such as late interaction, early exiting, parameter sharing, and pruning to further reduce the inference time and the model size. We demonstrate the foundation model's versatility on a wide range of tasks such as retrieval, ranking, zero-shot recommendation, explanation generation, personalized content creation, and conversational recommendation, and manage to deploy it on both cloud servers and mobile devices.",
                "authors": "Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang",
                "citations": 162
            },
            {
                "title": "Language Models are General-Purpose Interfaces",
                "abstract": "Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",
                "authors": "Y. Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei",
                "citations": 93
            },
            {
                "title": "Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains",
                "abstract": "Multi-modal foundation models are typically trained on millions of pairs of natural images and text captions, frequently obtained through web-crawling approaches. Although such models depict excellent generative capabilities, they do not typically generalize well to specific domains such as medical images that have fundamentally shifted distributions compared to natural images. Building generative models for medical images that faithfully depict clinical context may help alleviate the paucity of healthcare datasets. Thus, in this study, we seek to research and expand the representational capabilities of large pretrained foundation models to medical concepts, specifically for leveraging the Stable Diffusion model to generate domain specific images found in medical imaging. We explore the sub-components of the Stable Diffusion pipeline (the variational autoencoder, the U-Net and the text-encoder) to fine-tune the model to generate medical images. We benchmark the efficacy of these efforts using quantitative image quality metrics and qualitative radiologist-driven evaluations that accurately represent the clinical content of conditional text prompts. Our best-performing model improves upon the stable diffusion baseline and can be conditioned to insert a realistic-looking abnormality on a synthetic radiology image, while maintaining a 95% accuracy on a classifier trained to detect the abnormality.",
                "authors": "P. Chambon, Christian Blüthgen, C. Langlotz, Akshay Chaudhari",
                "citations": 94
            },
            {
                "title": "Phenomenography as a Foundation for Mixed Models Research",
                "abstract": "Phenomenography is a methodological paradigm, which emphasizes personal conceptions as a necessary construct to understand the relationship between the physical events that people experience and the personal meanings that they derive from those experiences. This perspective provides a useful framework for mixed methodology research, because its ontology provides both equal legitimacy to objective and subjective phenomena and an integrated paradigm within which one can jointly engage quantitative and qualitative methods. We examine several instances of mixed methods research from the literature that utilize a phenomenographic perspective and identify implications for further development of mixed research strategies.",
                "authors": "David F. Feldon, Colby Tofel-Grehl",
                "citations": 32
            },
            {
                "title": "Heterogeneity within and among co-occurring foundation species increases biodiversity",
                "abstract": null,
                "authors": "M. Thomsen, A. Altieri, C. Angelini, M. Bishop, F. Bulleri, Roxanne Farhan, Viktoria M. M. Frühling, P. Gribben, Seamus B. Harrison, Qiang He, Moritz Klinghardt, Joachim Langeneck, Brendan S. Lanham, Luca Mondardini, Yannick Mulders, Semonn Oleksyn, Aaron P. Ramus, D. Schiel, Tristan Schneider, A. Siciliano, B. Silliman, D. Smale, Paul M. South, T. Wernberg, Stacy Zhang, G. Zotz",
                "citations": 43
            },
            {
                "title": "Business Model Innovation for Sustainability: Towards a Unified Perspective for Creation of Sustainable Business Models",
                "abstract": "Business model innovation has seen a recent surge in academic research and business practice. Changes to business models are recognized as a fundamental approach to realize innovations for sustainability. However, little is known about the successful adoption of sustainable business models (SBMs). The purpose of this paper is to develop a unified theoretical perspective for understanding business model innovations that lead to better organizational economic, environmental and social performance. The paper examines bodies of literature on business model innovation, sustainability innovation, networks theory, stakeholder theory and product–service systems. We develop five propositions that support the creation of SBMs in a unified perspective, which lays a foundation to support organizations in investigating and experimenting with alternative new business models. This article contributes to the emerging field of SBMs, which embed economic, environmental and social flows of value that are created, delivered and captured in a value network. It highlights gaps for addressing the challenges of business model innovation for sustainability and suggests avenues for future research. © 2017 The Authors. Business Strategy and the Environment published by ERP Environment and John Wiley & Sons Ltd",
                "authors": "S. Evans, Doroteya Vladimirova, M. Holgado, Kirsten van Fossen, Miying Yang, Elisabete A. Silva, C. Y. Barlow",
                "citations": 840
            },
            {
                "title": "Solving inverse problems using data-driven models",
                "abstract": "Recent research in inverse problems seeks to develop a mathematically coherent foundation for combining data-driven models, and in particular those based on deep learning, with domain-specific knowledge contained in physical–analytical models. The focus is on solving ill-posed inverse problems that are at the core of many challenging applications in the natural sciences, medicine and life sciences, as well as in engineering and industrial applications. This survey paper aims to give an account of some of the main contributions in data-driven inverse problems.",
                "authors": "S. Arridge, P. Maass, O. Öktem, C. Schönlieb",
                "citations": 518
            },
            {
                "title": "ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation",
                "abstract": "—Although no speciﬁc domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for body pose estimation tasks. In this paper, we show the surprisingly good properties of plain vision transformers for body pose estimation from various aspects, namely simplicity in model structure, scalability in model size, ﬂexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model dubbed ViTPose . Speciﬁcally, ViTPose employs the plain and non-hierarchical vision transformer as an encoder to encode features and a lightweight decoder to decode body keypoints in either a top-down or a bottom-up manner. It can be scaled up from about 20M to 1B parameters by taking the advantage of the scalable model capacity and high parallelism of the vision transformer, setting a new Pareto front for throughput and performance. Besides, ViTPose is very ﬂexible regarding the attention type, input resolution, and pre-training and ﬁne-tuning strategy. Based on the ﬂexibility, a novel ViTPose+ model is proposed to deal with heterogeneous body keypoint categories in different types of body pose estimation tasks via knowledge factorization, i.e ., adopting task-agnostic and task-speciﬁc feed-forward networks in the transformer. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our ViTPose model outperforms representative methods on the challenging MS COCO Human Keypoint Detection benchmark at both top-down and bottom-up settings. Speciﬁcally, our largest single model ViTPose-G with 1B parameters sets a new record on the MS COCO test set without model ensemble. Furthermore, our ViTPose+ model achieves state-of-the-art performance simultaneously on a series of body pose estimation tasks",
                "authors": "Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao",
                "citations": 27
            },
            {
                "title": "Information theory: A foundation for complexity science",
                "abstract": "Modeling and inference are central to most areas of science and especially to evolving and complex systems. Critically, the information we have is often uncertain and insufficient, resulting in an underdetermined inference problem; multiple inferences, models, and theories are consistent with available information. Information theory (in particular, the maximum information entropy formalism) provides a way to deal with such complexity. It has been applied to numerous problems, within and across many disciplines, over the last few decades. In this perspective, we review the historical development of this procedure, provide an overview of the many applications of maximum entropy and its extensions to complex systems, and discuss in more detail some recent advances in constructing comprehensive theory based on this inference procedure. We also discuss efforts at the frontier of information-theoretic inference: application to complex dynamic systems with time-varying constraints, such as highly disturbed ecosystems or rapidly changing economies.",
                "authors": "Amos Golan, John Harte",
                "citations": 25
            },
            {
                "title": "Numerical Study on the Deformation of Tunnels by Excavation of Foundation Pit Adjacent to the Subway",
                "abstract": "The excavation of the foundation pit will cause changes in the soil stress field around the foundation pit, and that may have adverse effects on the adjacent subway tunnels. In this paper, a complex deep foundation pit excavated in different sections is taken as the research object, and the support structure of the complex foundation pit project is introduced, which accumulates experience in the selection of support structure for similar projects. The finite element models are established by MIDAS/GTS software to evaluate the influence of excavation in different sections of the foundation pit on the tunnel deformation, and the accuracy of the finite element calculation results is verified by comparing the monitoring data. The results show that: The horizontal deformation of the subway tunnel is generally smaller than the vertical deformation. Tunnel monitoring should focus more on the development of the vertical deformation of the tunnel. The maximum vertical deformation and horizontal deformation of the tunnel are both smaller than the local specification limits, and the excavation of the foundation pit in this project has little influence on the deformation of the subway tunnel.",
                "authors": "Xiang Zhao, Han-Lin Wang, Zhongwei Li, Guoliang Dai, Ziwei Yin, Shuning Cao, Junlong Zhou",
                "citations": 22
            },
            {
                "title": "Blockchain Disruption and Decentralized Finance: The Rise of Decentralized Business Models",
                "abstract": "Abstract Blockchain technology can reduce transaction costs, generate distributed trust, and empower decentralized platforms, potentially becoming a new foundation for decentralized business models. In the financial industry, blockchain technology allows for the rise of decentralized financial services, which tend to be more decentralized, innovative, interoperable, borderless, and transparent. Empowered by blockchain technology, decentralized financial services have the potential to broaden financial inclusion, facilitate open access, encourage permissionless innovation, and create new opportunities for entrepreneurs and innovators. In this article, we assess the benefits of decentralized finance, identify existing business models, and evaluate potential challenges and limits. As a new area of financial technology, decentralized finance may reshape the structure of modern finance and create a new landscape for entrepreneurship and innovation, showcasing the promises and challenges of decentralized business models.",
                "authors": "Yuanchun Chen, C. Bellavitis",
                "citations": 422
            },
            {
                "title": "Hybrid ELM and MARS-Based Prediction Model for Bearing Capacity of Shallow Foundation",
                "abstract": "The nature of soil varies horizontally as well as vertically, owing to the process of the formation of soil. Thus, ensuring the safe design of geotechnical structures has been a major challenge. In shallow foundations, conducting field tests is expensive and time-consuming and often conducted on significantly scaled-down models. Empirical models, too, have been found to be the least reliable in the literature. The study proposes AI-based techniques to predict the bearing capacity of a shallow foundation, simulated using the datasets obtained in experiments conducted in different laboratories in the literature. The results of the ELM-EO and ELM-PSO hybrid models are compared with that of the ELM and MARS models. The performance of the models is analyzed and compared with each other using various performance parameters. The models are graded to each other using rank analysis and the visual interpretations are provided using error matrices and REC curves. ELM-EO is concluded to be the best performing model (R2 and RMSE equal to 0.995 and 0.01, respectively, in the testing phase), closely followed by ELM-PSO, MARS, and ELM. The performance of MARS is better than ELM (R2 equals 0.97 and 0.5, respectively, in the testing phase); however, hybridization greatly enhances the performance of the ELM and the hybrid models perform better than MARS. The paper concludes that AI-based models are robust and hybridization of regression models with optimization techniques should be encouraged in further research. Sensitivity analysis suggests that all the input parameters have a significant influence on the output, with friction angle being the highest.",
                "authors": "Manish Kumar, Vinay Kumar, Rahul Biswas, P. Samui, Mosbeh R. Kaloop, Majed Alzara, A. Yosri",
                "citations": 20
            },
            {
                "title": "Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization",
                "abstract": "Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks. Empirically, it improves the state of the art on the reference DomainBed benchmark for out-of-distribution generalization. Looking forward, this work contributes to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to reliably update machine learning models. Our code is released: https://github.com/facebookresearch/ModelRatatouille.",
                "authors": "Alexandre Ram'e, Kartik Ahuja, Jianyu Zhang, M. Cord, L. Bottou, David Lopez-Paz",
                "citations": 63
            },
            {
                "title": "Deflections, stresses and free vibration studies of FG-CNT reinforced sandwich plates resting on Pasternak elastic foundation",
                "abstract": "The present study covenants with the static and free vibration behavior of nanocomposite sandwich plates \nreinforced by carbon nanotubes resting on Pasternak elastic foundation. Uniformly distributed (UD-CNT) and functionally \ngraded (FG-CNT) distributions of aligned carbon nanotube are considered for two types of sandwich plates such as, the face sheet reinforced and homogeneous core and the homogeneous face sheet and reinforced core. Based on the first shear deformation theory (FSDT), the Hamilton\\'s principle is employed to derive the mathematical models. The obtained solutions are numerically validated by comparison with some available cases in the literature. The elastic foundation model is assumed as one parameter Winkler - Pasternak foundation. A parametric study is conducted to study the effects of aspect ratios, foundation \nparameters, carbon nanotube volume fraction, types of reinforcement, core-to-face sheet thickness ratio and types of loads acting on the bending and free vibration analyses. It is explicitly shown that the (FG-CNT) face sheet reinforced sandwich plate has a high resistance against deflections compared to other types of reinforcement. It is also revealed that the reduction in the dimensionless natural frequency is most pronounced in core reinforced sandwich plate.",
                "authors": "Noureddine Bendenia, M. Zidour, A. A. Bousahla, F. Bourada, A. Tounsi, K. H. Benrahou, E. A. Bedia, S. R. Mahmoud, A. Tounsi",
                "citations": 159
            },
            {
                "title": "MolE: a molecular foundation model for drug discovery",
                "abstract": "Models that accurately predict properties based on chemical structure are valuable tools in drug discovery. However, for many properties, public and private training sets are typically small, and it is difficult for the models to generalize well outside of the training data. Recently, large language models have addressed this problem by using self-supervised pretraining on large unlabeled datasets, followed by fine-tuning on smaller, labeled datasets. In this paper, we report MolE, a molecular foundation model that adapts the DeBERTa architecture to be used on molecular graphs together with a two-step pretraining strategy. The first step of pretraining is a self-supervised approach focused on learning chemical structures, and the second step is a massive multi-task approach to learn biological information. We show that fine-tuning pretrained MolE achieves state-of-the-art results on 9 of the 22 ADMET tasks included in the Therapeutic Data Commons.",
                "authors": "Oscar M'endez-Lucio, C. Nicolaou, Berton A. Earnshaw",
                "citations": 12
            },
            {
                "title": "Measurements and models of electric fields in the in vivo human brain during transcranial electric stimulation",
                "abstract": "Transcranial electric stimulation aims to stimulate the brain by applying weak electrical currents at the scalp. However, the magnitude and spatial distribution of electric fields in the human brain are unknown. We measured electric potentials intracranially in ten epilepsy patients and estimated electric fields across the entire brain by leveraging calibrated current-flow models. When stimulating at 2 mA, cortical electric fields reach 0.8 V/m, the lower limit of effectiveness in animal studies. When individual whole-head anatomy is considered, the predicted electric field magnitudes correlate with the recorded values in cortical (r = 0.86) and depth (r = 0.88) electrodes. Accurate models require adjustment of tissue conductivity values reported in the literature, but accuracy is not improved when incorporating white matter anisotropy or different skull compartments. This is the first study to validate and calibrate current-flow models with in vivo intracranial recordings in humans, providing a solid foundation to target stimulation and interpret clinical trials. DOI: http://dx.doi.org/10.7554/eLife.18834.001",
                "authors": "Yu Huang, Anli Liu, Belen Lafon, Daniel Friedman, Michael Dayan, Xiuyuan Wang, M. Bikson, W. Doyle, O. Devinsky, Lucas C. Parra",
                "citations": 450
            },
            {
                "title": "Rome Foundation Working Team Report on Post-Infection Irritable Bowel Syndrome.",
                "abstract": "BACKGROUND & AIMS\nThe existence of postinfection irritable bowel syndrome (PI-IBS) has been substantiated by epidemiology studies conducted in diverse geographic and clinical settings. However, the available evidence has not been well summarized, and there is little guidance for diagnosis and treatment of PI-IBS. The ROME Foundation has produced a working team report to summarize the available evidence on the pathophysiology of PI-IBS and provide guidance for diagnosis and treatment, based on findings reported in the literature and clinical experience.\n\n\nMETHODS\nThe working team conducted an evidence-based review of publication databases for articles describing the clinical features (diagnosis), pathophysiology (intestinal sensorimotor function, microbiota, immune dysregulation, barrier dysfunction, enteroendocrine pathways, and genetics), and animal models of PI-IBS. We used a Delphi-based consensus system to create guidelines for management of PI-IBS and a developed treatment algorithm based on published findings and experiences of team members.\n\n\nRESULTS\nPI-IBS develops in about 10% of patients with infectious enteritis. Risk factors include female sex, younger age, psychological distress during or before acute gastroenteritis, and severity of the acute episode. The pathogenesis of PI-PBS appears to involve changes in the intestinal microbiome as well as epithelial, serotonergic, and immune system factors. However, these mechanisms are incompletely understood. There are no evidence-based, effective pharmacologic strategies for treatment of PI-IBS. We provide a consensus-based treatment algorithm, based on clinical presentation and potential disease mechanisms.\n\n\nCONCLUSIONS\nBased on a systematic review of the literature and team experience, we summarize the clinical features, pathophysiology (from animal models and human studies), and progression of PI-IBS. Based on these findings, we present an algorithm for diagnosis and treatment of PI-IBS based on team consensus. We also propose areas for future investigation.",
                "authors": "G. Barbara, M. Grover, P. Bercik, M. Corsetti, U. Ghoshal, L. Ohman, M. Rajilić-Stojanović",
                "citations": 171
            },
            {
                "title": "Reliability Analysis of Pile Foundation Using Soft Computing Techniques: A Comparative Study",
                "abstract": "Uncertainty and variability are inherent to pile design and consequently, there have been considerable researches in quantifying the reliability or probability of failure of structures. This paper aims at examining and comparing the applicability and adaptability of Minimax Probability Machine Regression (MPMR), Emotional Neural Network (ENN), Group Method of Data Handling (GMDH), and Adaptive Neuro-Fuzzy Inference System (ANFIS) in the reliability analysis of pile embedded in cohesionless soil and proposes an AI-based prediction method for bearing capacity of pile foundation. To ascertain the homogeneity and distribution of the datasets, Mann–Whitney U (M–W) and Anderson–Darling (AD) tests are carried out, respectively. The performance of the developed soft computing models is ascertained using various statistical parameters. A comparative study is implemented among reliability indices of the proposed models by employing First Order Second Moment Method (FOSM). The results of FOSM showed that the ANFIS approach outperformed other models for reliability analysis of bearing capacity of pile and ENN is the worst performing model. The value of R2 for all the developed models is close to 1. The best RMSE value is achieved for the training phase of the ANFIS model (0 in training and 2.13 in testing) and the poorest for the ENN (2.03 in training and 31.24 in testing) model. Based on the experimental results of reliability indices, the developed ANFIS model is found to be very close to that computed from the original data.",
                "authors": "Manish Kumar, A. Bardhan, P. Samui, J. Hu, Mosbeh R. Kaloop",
                "citations": 37
            },
            {
                "title": "Analytical design models for geotechnical seismic isolation systems",
                "abstract": null,
                "authors": "H. Tsang",
                "citations": 28
            },
            {
                "title": "The ModelSEED Biochemistry Database for the integration of metabolic annotations and the reconstruction, comparison and analysis of metabolic models for plants, fungi and microbes",
                "abstract": "Abstract For over 10 years, ModelSEED has been a primary resource for the construction of draft genome-scale metabolic models based on annotated microbial or plant genomes. Now being released, the biochemistry database serves as the foundation of biochemical data underlying ModelSEED and KBase. The biochemistry database embodies several properties that, taken together, distinguish it from other published biochemistry resources by: (i) including compartmentalization, transport reactions, charged molecules and proton balancing on reactions; (ii) being extensible by the user community, with all data stored in GitHub; and (iii) design as a biochemical ‘Rosetta Stone’ to facilitate comparison and integration of annotations from many different tools and databases. The database was constructed by combining chemical data from many resources, applying standard transformations, identifying redundancies and computing thermodynamic properties. The ModelSEED biochemistry is continually tested using flux balance analysis to ensure the biochemical network is modeling-ready and capable of simulating diverse phenotypes. Ontologies can be designed to aid in comparing and reconciling metabolic reconstructions that differ in how they represent various metabolic pathways. ModelSEED now includes 33,978 compounds and 36,645 reactions, available as a set of extensible files on GitHub, and available to search at https://modelseed.org/biochem and KBase.",
                "authors": "S. Seaver, Filipe Liu, Qizh Zhang, James G. Jeffryes, José P. Faria, Janaka N. Edirisinghe, Michael B. Mundy, N. Chia, E. Noor, M. Beber, A. Best, M. DeJongh, Jeffrey A. Kimbrel, P. D’haeseleer, S. McCorkle, Jay R. Bolton, Erik Pearson, S. Canon, E. Wood-Charlson, R. Cottingham, A. Arkin, C. Henry",
                "citations": 146
            },
            {
                "title": "Visual Object Tracking for Unmanned Aerial Vehicles: A Benchmark and New Motion Models",
                "abstract": "\n \n Despite recent advances in the visual tracking community, most studies so far have focused on the observation model. As another important component in the tracking system, the motion model is much less well-explored especially for some extreme scenarios. In this paper, we consider one such scenario in which the camera is mounted on an unmanned aerial vehicle (UAV) or drone. We build a benchmark dataset of high diversity, consisting of 70 videos captured by drone cameras. To address the challenging issue of severe camera motion, we devise simple baselines to model the camera motion by geometric transformation based on background feature points. An extensive comparison of recent state-of-the-art trackers and their motion model variants on our drone tracking dataset validates both the necessity of the dataset and the effectiveness of the proposed methods. Our aim for this work is to lay the foundation for further research in the UAV tracking area.\n \n",
                "authors": "Siyi Li, D. Yeung",
                "citations": 312
            },
            {
                "title": "Deep CNN models-based ensemble approach to driver drowsiness detection",
                "abstract": null,
                "authors": "M. Dua, Shakshi, Ritu Singla, Saumya Raj, A. Jangra",
                "citations": 137
            },
            {
                "title": "Dynamic behaviour of bidirectional functionally graded sandwich beams under a moving mass with partial foundation supporting effect",
                "abstract": null,
                "authors": "A. Vu, Ngoc Anh Thi Le, D. Nguyen",
                "citations": 25
            },
            {
                "title": "Egocentric Network Analysis: Foundations, Methods, and Models",
                "abstract": "Egocentric network analysis is used widely across the social sciences, especially in anthropology, political science, economics, and sociology, and is increasingly being employed in communications, informatics, and business and marketing studies. Egocentric network analysis requires a unique set of data collection and analysis skills that overlap only minimally with other network methodologies. However, until now there has been no single reference for conceptualizing, collecting, and analyzing egocentric social network data. This comprehensive guide to study design, data collection, and analysis brings together the state of knowledge with the most effective research tools to guide newcomers to this field. It is illustrated with many engaging examples and graphics and assumes no prior knowledge. Covering the entire research process in a logical sequence, from conceptualizing research questions to interpreting findings, this volume provides a solid foundation for researchers at any stage of their career to learn and apply ego network methods.",
                "authors": "B. Perry, B. Pescosolido, S. Borgatti",
                "citations": 234
            },
            {
                "title": "A review on the properties, preparation, models and stability of hybrid nanofluids to optimize energy consumption",
                "abstract": null,
                "authors": "Hamed Eshgarf, Rasool Kalbasi, A. Maleki, Mostafa Safdari Shadloo, A. Karimipour",
                "citations": 142
            },
            {
                "title": "Coordination Dynamics: A Foundation for Understanding Social Behavior",
                "abstract": "Humans’ interactions with each other or with socially competent machines exhibit lawful coordination patterns at multiple levels of description. According to Coordination Dynamics, such laws specify the flow of coordination states produced by functional synergies of elements (e.g., cells, body parts, brain areas, people…) that are temporarily organized as single, coherent units. These coordinative structures or synergies may be mathematically characterized as informationally coupled self-organizing dynamical systems (Coordination Dynamics). In this paper, we start from a simple foundation, an elemental model system for social interactions, whose behavior has been captured in the Haken-Kelso-Bunz (HKB) model. We follow a tried and tested scientific method that tightly interweaves experimental neurobehavioral studies and mathematical models. We use this method to further develop a body of empirical research that advances the theory toward more generalized forms. In concordance with this interdisciplinary spirit, the present paper is written both as an overview of relevant advances and as an introduction to its mathematical underpinnings. We demonstrate HKB’s evolution in the context of social coordination along several directions, with its applicability growing to increasingly complex scenarios. In particular, we show that accommodating for symmetry breaking in intrinsic dynamics and coupling, multiscale generalization and adaptation are principal evolutions. We conclude that a general framework for social coordination dynamics is on the horizon, in which models support experiments with hypothesis generation and mechanistic insights.",
                "authors": "E. Tognoli, Mengsen Zhang, A. Fuchs, C. Beetle, J. Kelso",
                "citations": 49
            },
            {
                "title": "Shallow Foundation Settlement Quantification: Application of Hybridized Adaptive Neuro-Fuzzy Inference System Model",
                "abstract": "Settlement simulating in cohesion materials is a crucial issue due to complexity of cohesion soil texture. This research emphasis on the implementation of newly developed machine learning models called hybridized Adaptive Neuro-Fuzzy Inference System (ANFIS) with Particle Swarm Optimization (PSO) algorithm, Ant Colony optimizer (ACO), Differential Evolution (DE), and Genetic Algorithm (GA) as efficient approaches to predict settlement of shallow foundation over cohesion soil properties. The width of footing (B), pressure of footing (qa), geometry of footing (L/B), count of SPT blow (N), and ratio of footing embedment (Df/B) are considered as predictive variables. Nonhomogeneity and inconsistency of employed dataset is a major concern during prediction modeling. Hence, two different modeling scenarios (i) preprocessed dataset (PP) and (ii) nonprocessed (initial) dataset (NP) were inspected. To assess the accuracy of the applied hybrid models and standalone one, multiple statistical metrics were computed and analyzed over the training and testing phases. Results indicated ANFIS-PSO model exhibited an accurate and reliable prediction data intelligent and had the highest predictability performance against all employed models. In addition, results demonstrated that data preprocessing is highly essential to be performed prior to building the predictive models. Overall, ANFIS-PSO model showed a robust machine learning for settlement prediction.",
                "authors": "M. Mohammed, A. Sharafati, N. Al‐Ansari, Z. Yaseen",
                "citations": 46
            },
            {
                "title": "PERFICT: A Re‐imagined foundation for predictive ecology",
                "abstract": "Abstract Making predictions from ecological models—and comparing them to data—offers a coherent approach to evaluate model quality, regardless of model complexity or modelling paradigm. To date, our ability to use predictions for developing, validating, updating, integrating and applying models across scientific disciplines while influencing management decisions, policies, and the public has been hampered by disparate perspectives on prediction and inadequately integrated approaches. We present an updated foundation for Predictive Ecology based on seven principles applied to ecological modelling: make frequent Predictions, Evaluate models, make models Reusable, Freely accessible and Interoperable, built within Continuous workflows that are routinely Tested (PERFICT). We outline some benefits of working with these principles: accelerating science; linking with data science; and improving science‐policy integration.",
                "authors": "Eliot J. B. McIntire, A. Chubaty, S. Cumming, D. Andison, Ceres Barros, C. Boisvenue, S. Haché, Yong Luo, T. Micheletti, F. Stewart",
                "citations": 20
            },
            {
                "title": "Development of a standardized histopathology scoring system for intervertebral disc degeneration in rat models: An initiative of the ORS spine section",
                "abstract": "Rats are a widely accepted preclinical model for evaluating intervertebral disc (IVD) degeneration and regeneration. IVD morphology is commonly assessed using histology, which forms the foundation for quantifying the state of IVD degeneration. IVD degeneration severity is evaluated using different grading systems that focus on distinct degenerative features. A standard grading system would facilitate more accurate comparison across laboratories and more robust comparisons of different models and interventions.",
                "authors": "Alon Lai, J. Gansau, S. Gullbrand, J. Crowley, Carla Cunha, S. Dudli, J. Engiles, M. Fusellier, R. Gonçalves, Daisuke Nakashima, Jeffrey O. Okewunmi, M. Pelletier, S. Presciutti, Jordy Schol, Yoshiki Takeoka, Sidong Yang, T. Yurube, Yejia Zhang, J. Iatridis",
                "citations": 56
            },
            {
                "title": "Building a better foundation: improving root-trait measurements to understand and model plant and ecosystem processes.",
                "abstract": "Trait-based approaches provide a useful framework to investigate plant strategies for resource acquisition, growth, and competition, as well as plant impacts on ecosystem processes. Despite significant progress capturing trait variation within and among stems and leaves, identification of trait syndromes within fine-root systems and between fine roots and other plant organs is limited. Here we discuss three underappreciated areas where focused measurements of fine-root traits can make significant contributions to ecosystem science. These include assessment of spatiotemporal variation in fine-root traits, integration of mycorrhizal fungi into fine-root-trait frameworks, and the need for improved scaling of traits measured on individual roots to ecosystem-level processes. Progress in each of these areas is providing opportunities to revisit how below-ground processes are represented in terrestrial biosphere models. Targeted measurements of fine-root traits with clear linkages to ecosystem processes and plant responses to environmental change are strongly needed to reduce empirical and model uncertainties. Further identifying how and when suites of root and whole-plant traits are coordinated or decoupled will ultimately provide a powerful tool for modeling plant form and function at local and global scales.",
                "authors": "M. L. McCormack, Dali Guo, C. Iversen, Weile Chen, D. Eissenstat, Christopher W. Fernandez, Le Li, Chengen Ma, Zeqing Ma, Hendrik Poorter, P. Reich, M. Zadworny, A. Zanne",
                "citations": 170
            },
            {
                "title": "A theoretical foundation for multi-scale regular vegetation patterns",
                "abstract": null,
                "authors": "C. Tarnita, J. A. Bonachela, Efrat Sheffer, Jennifer A. Guyton, Tyler C. Coverdale, R. Long, R. Pringle",
                "citations": 157
            },
            {
                "title": "Application of optimized grey discrete Verhulst–BP neural network model in settlement prediction of foundation pit",
                "abstract": null,
                "authors": "Chuang Zhang, Jianzhong Li, Yong He",
                "citations": 53
            },
            {
                "title": "Integrating Behavioral Theories in Agent-Based Models for Agricultural Drought Risk Assessments",
                "abstract": "Improving assessments of droughts risk for smallholder farmers requires a better understanding of the interaction between individual adaptation decisions and drought risk. Agent-based modeling is increasingly used to capture the interaction between individual decision-making and the environment. In this paper, we provide a review of drought risk agent-based models with a focus on behavioral rules. This review leads to the conclusion that human decision rules in existing drought risk agent-based models are often based on ad hoc assumptions without a solid theoretical and empirical foundation. Subsequently, we review behavioral economic and psychological theories to provide a clear overview of theories that can improve the theoretical foundation of smallholder farmer behavior and we review empirical parameterization, calibration, and validation methods of those theories. Based on these reviews, we provide a conceptual framework that can give guidance for the integration of behavioral theories in agent-based models. We conclude with an agenda to guide future research in this field.",
                "authors": "Teun Schrieks, W. Botzen, M. Wens, T. Haer, J. Aerts",
                "citations": 31
            },
            {
                "title": "Seismic settlement of a strip foundation resting on a dry sand",
                "abstract": null,
                "authors": "Saif Alzabeebee",
                "citations": 28
            },
            {
                "title": "A Formal Foundation for Secure Remote Execution of Enclaves",
                "abstract": "Recent proposals for trusted hardware platforms, such as Intel SGX and the MIT Sanctum processor, offer compelling security features but lack formal guarantees. We introduce a verification methodology based on a trusted abstract platform (TAP), a formalization of idealized enclave platforms along with a parameterized adversary. We also formalize the notion of secure remote execution and present machine-checked proofs showing that the TAP satisfies the three key security properties that entail secure remote execution: integrity, confidentiality and secure measurement. We then present machine-checked proofs showing that SGX and Sanctum are refinements of the TAP under certain parameterizations of the adversary, demonstrating that these systems implement secure enclaves for the stated adversary models.",
                "authors": "P. Subramanyan, Rohit Sinha, Ilia A. Lebedev, S. Devadas, S. Seshia",
                "citations": 134
            },
            {
                "title": "Towards a formal foundation of intermittent computing",
                "abstract": "Intermittently powered devices enable new applications in harsh or inaccessible environments, such as space or in-body implants, but also introduce problems in programmability and correctness. Researchers have developed programming models to ensure that programs make progress and do not produce erroneous results due to memory inconsistencies caused by intermittent executions. As the technology has matured, more and more features are added to intermittently powered devices, such as I/O. Prior work has shown that all existing intermittent execution models have problems with repeated device or sensor inputs (RIO). RIOs could leave intermittent executions in an inconsistent state. Such problems and the proliferation of existing intermittent execution models necessitate a formal foundation for intermittent computing. In this paper, we formalize intermittent execution models, their correctness properties with respect to memory consistency and inputs, and identify the invariants needed to prove systems correct. We prove equivalence between several existing intermittent systems. To address RIO problems, we define an algorithm for identifying variables affected by RIOs that need to be restored after reboot and prove the algorithm correct. Finally, we implement the algorithm in a novel intermittent runtime system that is correct with respect to input operations and evaluate its performance.",
                "authors": "Milijana Surbatovich, Limin Jia, Brandon Lucia",
                "citations": 27
            },
            {
                "title": "Comparison of Time Series Methods and Machine Learning Algorithms for Forecasting Taiwan Blood Services Foundation's Blood Supply",
                "abstract": "Purpose The uncertainty in supply and the short shelf life of blood products have led to a substantial outdating of the collected donor blood. On the other hand, hospitals and blood centers experience severe blood shortage due to the very limited donor population. Therefore, the necessity to forecast the blood supply to minimize outdating as well as shortage is obvious. This study aims to efficiently forecast the supply of blood components at blood centers. Methods Two different types of forecasting techniques, time series and machine learning algorithms, are developed and the best performing method for the given case study is determined. Under the time series, we consider the Autoregressive (AUTOREG), Autoregressive Moving Average (ARMA), Autoregressive Integrated Moving Average (ARIMA), Seasonal ARIMA, Seasonal Exponential Smoothing Method (ESM), and Holt-Winters models. Artificial neural network (ANN) and multiple regression are considered under the machine learning algorithms. Results We leverage five years worth of historical blood supply data from the Taiwan Blood Services Foundation (TBSF) to conduct our study. On comparing the different techniques, we found that time series forecasting methods yield better results than machine learning algorithms. More specifically, the least value of the error measures is observed in seasonal ESM and ARIMA models. Conclusions The models developed can act as a decision support system to administrators and pathologists at blood banks, blood donation centers, and hospitals to determine their inventory policy based on the estimated future blood supply. The forecasting models developed in this study can help healthcare managers to manage blood inventory control more efficiently, thus reducing blood shortage and blood wastage.",
                "authors": "Han M. Shih, S. Rajendran",
                "citations": 51
            },
            {
                "title": "A Survey on Reinforcement Learning Models and Algorithms for Traffic Signal Control",
                "abstract": "Traffic congestion has become a vexing and complex issue in many urban areas. Of particular interest are the intersections where traffic bottlenecks are known to occur despite being traditionally signalized. Reinforcement learning (RL), which is an artificial intelligence approach, has been adopted in traffic signal control for monitoring and ameliorating traffic congestion. RL enables autonomous decision makers (e.g., traffic signal controllers) to observe, learn, and select the optimal action (e.g., determining the appropriate traffic phase and its timing) to manage traffic such that system performance is improved. This article reviews various RL models and algorithms applied to traffic signal control in the aspects of the representations of the RL model (i.e., state, action, and reward), performance measures, and complexity to establish a foundation for further investigation in this research field. Open issues are presented toward the end of this article to discover new research areas with the objective to spark new interest in this research field.",
                "authors": "K. Yau, Junaid Qadir, H. L. Khoo, Mee Hong Ling, P. Komisarczuk",
                "citations": 177
            },
            {
                "title": "Geographic scenario: a possible foundation for further development of virtual geographic environments",
                "abstract": "ABSTRACT It has been two decades since virtual geographic environments (VGEs) were initially proposed. While relevant theories and technologies are evolving, data organization models have always been the foundation of VGE development, and they require further exploration. Based on the comprehensive consideration of the characteristics of VGEs, geographic scene is proposed to organize geographic information and data. We empirically find that geographic scene provides a suitable organization schema to support geo-visualization, geo-simulation, and geo-collaboration. To systematically investigate the concept and method of geographic scene, Geographic Scenario is proposed as the theory on developing geographic scene, and corresponding key issues of the Geographic Scenario are illustrated in this article. Prospects of the proposed method are discussed with the hope of informing future studies of VGEs.",
                "authors": "Guonian Lü, Min Chen, Linwang Yuan, Liangchen Zhou, Y. Wen, Mingguang Wu, B. Hu, Zhaoyuan Yu, S. Yue, Y. Sheng",
                "citations": 62
            },
            {
                "title": "Stablecoins 2.0: Economic Foundations and Risk-based Models",
                "abstract": "Stablecoins are one of the most widely capitalized type of cryptocurrency. However, their risks vary significantly according to their design and are often poorly understood. We seek to provide a sound foundation for stablecoin theory, with a risk-based functional characterization of the economic structure of stablecoins. First, we match existing economic models to the disparate set of custodial systems. Next, we characterize the unique risks that emerge in non-custodial stablecoins and develop a model framework that unifies existing models from economics and computer science. We further discuss how this modeling framework is applicable to a wide array of cryptoeconomic systems, including cross-chain protocols, collateralized lending, and decentralized exchanges. These unique risks yield unanswered research questions that will form the crux of research in decentralized finance going forward.",
                "authors": "Ariah Klages-Mundt, D. Harz, L. Gudgeon, Jun-You Liu, Andreea Minca",
                "citations": 65
            },
            {
                "title": "Buckling of carbon nanotube reinforced composite plates supported by Kerr foundation using Hamilton’s energy principle",
                "abstract": "This paper investigates the buckling behavior of carbon nanotube-reinforced composite plates supported by Kerr foundation model. In this foundation elastic of Kerr consisting of two spring layers interconnected by a shearing layer. The plates are reinforced by single-walled carbon nanotubes with four types of distributions of uniaxially aligned reinforcement material. The analytical equations are derived and the exact solutions for buckling analyses of such type\\'s plates are obtained. The mathematical models provided, and the present solutions are numerically validated by comparison with some available results in the literature. Effect of various reinforced plates parameters such as aspect ratios, volume fraction, types of reinforcement, parameters constant factors of Kerr foundation and plate thickness on the buckling analyses of carbon nanotube-reinforced composite plates are studied and discussed.",
                "authors": "Ammar Boulal, T. Bensattalah, Abdelkader Karas, M. Zidour, H. Heireche, E. A. Bedia",
                "citations": 23
            },
            {
                "title": "Discrete and Continuum Models for Complex Metamaterials",
                "abstract": "Bringing together contributions on a diverse range of topics, this text explores the relationship between discrete and continuum mechanics as a tool to model new and complex metamaterials. Providing a comprehensive bibliography and historical review of the field, it covers mechanical, acoustic, and pantographic metamaterials, discusses naive model theory and Lagrangian discrete models, and their applications, and presents methods for pantographic structures and variational methods for multidisciplinary modeling and computation. The relationship between discrete and continuous models is discussed from both mathematical and engineering viewpoints, making the text ideal for those interested in the foundation of mechanics and computational applications, and innovative viewpoints on the use of discrete systems to model metamaterials are presented for those who want to go deeper into the field. An ideal text for graduate students and researchers interested in continuum approaches to the study of modern materials, in mechanical engineering, civil engineering, applied mathematics, physics, and materials science.",
                "authors": "Misra",
                "citations": 58
            },
            {
                "title": "Wasserstein Learning of Deep Generative Point Process Models",
                "abstract": "Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.",
                "authors": "Shuai Xiao, Mehrdad Farajtabar, X. Ye, Junchi Yan, Xiaokang Yang, Le Song, H. Zha",
                "citations": 163
            },
            {
                "title": "Neuromodulatory Systems and Their Interactions: A Review of Models, Theories, and Experiments",
                "abstract": "Neuromodulatory systems, including the noradrenergic, serotonergic, dopaminergic, and cholinergic systems, track environmental signals, such as risks, rewards, novelty, effort, and social cooperation. These systems provide a foundation for cognitive function in higher organisms; attention, emotion, goal-directed behavior, and decision-making derive from the interaction between the neuromodulatory systems and brain areas, such as the amygdala, frontal cortex, hippocampus, and sensory cortices. Given their strong influence on behavior and cognition, these systems also play a key role in disease states and are the primary target of many current treatment strategies. The fact that these systems interact with each other either directly or indirectly, however, makes it difficult to understand how a failure in one or more systems can lead to a particular symptom or pathology. In this review, we explore experimental evidence, as well as focus on computational and theoretical models of neuromodulation. Better understanding of neuromodulatory systems may lead to the development of novel treatment strategies for a number of brain disorders.",
                "authors": "Michael C. Avery, J. Krichmar",
                "citations": 163
            },
            {
                "title": "Prediction of bearing capacity of thin-walled foundation: a simulation approach",
                "abstract": null,
                "authors": "E. Momeni, D. J. Armaghani, S. A. Fatemi, R. Nazir",
                "citations": 53
            },
            {
                "title": "Neutrino event generators: foundation, status and future",
                "abstract": "Neutrino event generators are an essential tool needed for the extraction of neutrino mixing parameters, the mass hierarchy and a CP violating phase from long-baseline experiments. In this article I first describe the theoretical basis and the approximations needed to get to present-days generators. I also discuss the strengths and limitations of theoretical models used to describe semi-inclusive neutrino-nucleus reactions. I then confront present day's generators with this theoretical basis by detailed discussions of the various reaction processes. Finally, as examples I then show for various experiments results of the generator GiBUU for lepton semi-inclusive cross sections as well as particle spectra. I also discuss features of these cross sections in terms of the various reaction components, with predictions for DUNE. Finally, I argue for the need for a new neutrino generator that respects our present-day knowledge of both nuclear theory and nuclear reactions and is as much state-of-the-art as the experimental equipment. I outline some necessary requirements for such a new generator.",
                "authors": "U. Mosel",
                "citations": 49
            },
            {
                "title": "Models of Care Delivery for Children With Medical Complexity",
                "abstract": "We compare models of care for CMC and provide a conceptual foundation for improvements to care for this population. Children with medical complexity (CMC) are a subset of children and youth with special health care needs with high resource use and health care costs. Novel care delivery models in which care coordination and other services to CMC are provided are a focus of national and local health care and policy initiatives. Current models of care for CMC can be grouped into 3 main categories: (1) primary care–centered models, (2) consultative- or comanagement-centered models, and (3) episode-based models. Each model has unique advantages and disadvantages. Evaluations of these models have demonstrated positive outcomes, but most studies have limited generalizability for broader populations of CMC. A lack of standardized outcomes and population definitions for CMC hinders assessment of the comparative effectiveness of different models of care and identification of which components of the models lead to positive outcomes. Ongoing challenges include inadequate support for family caregivers and threats to the sustainability of models of care. Collaboration among key stakeholders (patients, families, providers, payers, and policy makers) is needed to address the gaps in care and create best practice guidelines to ensure the delivery of high-value care for CMC.",
                "authors": "Elisabeth Pordes, J. Gordon, L. Sanders, E. Cohen",
                "citations": 95
            },
            {
                "title": "Foundation-Based Cleft Care in Developing Countries",
                "abstract": "Background: Cleft deformities of the lip and palate affect nearly one in 500 to 700 births, and lead to increased morbidity and mortality if untreated. Nevertheless, significant global disparities in access to timely and appropriate care still exist. The relatively basic infrastructure required to surgically correct these deformities and large unmet disease burden have resulted in a significant number of foundation-based cleft care initiatives focused on developing countries. In this study, the authors evaluate the peer-reviewed literature generated by these foundations in an attempt to assess their clinical, scientific, educational, and economic impact. Methods: A comprehensive review of the literature was performed using key search terms, and the level of evidence of identified articles was determined. Data were then analyzed to determine the different models of foundation-based cleft care in developing countries, and their clinical, scientific, educational, and economic impact. Results: A total of 244 articles were identified through the authors’ search and reviewed. Foundation-based cleft care initiatives in developing countries have significantly contributed to a better understanding of disease epidemiology, barriers to care, safety considerations, complications and outcomes, and international and local cleft surgery education. The cleft care center model is more cost-effective than the surgical mission model and provides more sustainable care. Conclusions: Foundation-based cleft care prevents significant morbidity in developing countries and has provided valuable resources for capacity building. The surgical mission model should be considered as a transitory conduit for establishing the more effective and sustainable cleft care center model of care.",
                "authors": "Rami S. Kantar, Michael J. Cammarata, William J Rifkin, J. Diaz-Siso, U. Hamdan, Roberto L. Flores",
                "citations": 34
            },
            {
                "title": "Business models as service strategy",
                "abstract": null,
                "authors": "H. Wieland, Nathaniel N. Hartmann, S. Vargo",
                "citations": 141
            },
            {
                "title": "Humanized mouse models of immunological diseases and precision medicine",
                "abstract": null,
                "authors": "L. Shultz, J. Keck, Lisa M. Burzenski, Sonal Jangalwe, Shantashri Vaidya, D. Greiner, M. Brehm",
                "citations": 77
            },
            {
                "title": "Effect of foundation type and modelling on dynamic response and fatigue of offshore wind turbines",
                "abstract": "Correspondence Amir M. Kaynia, Norwegian University of Science and Technology (NTNU), Trondheim, Norway. Email:amir.kaynia@ntnu.no Abstract This paper presents dynamic response and fatigue analyses of several bottom-mounted offshore wind turbine (OWT) models, simulated in the aero-hydro-servo-elastic simulation tool FAST. The distinction between the models is the foundations, which are modelled with different methods, concepts, and dimensions. The US National Renewable Energy Laboratory has developed a 5-MW reference turbine supported on a monopile, the NREL 5MW, which was used as a reference model in this paper. The paper presents the implementation and comparison of two different foundation modeling methods, referred to as the simplified apparent fixity method and the improved apparent fixity method. Furthermore, sensitivity analyses of different monopile dimensions were performed, followed by sensitivity analyses of suction caisson foundations of different dimensions. The final part of the paper presents fatigue analyses for the foundation models considered in this study subjected to 17 load cases. Fatigue damage, fatigue life, and damage equivalent loads were calculated, as well as the relative fatigue contribution from each load case.",
                "authors": "I. B. Løken, A. Kaynia",
                "citations": 30
            },
            {
                "title": "General Principles for the Validation of Proarrhythmia Risk Prediction Models: An Extension of the CiPA In Silico Strategy",
                "abstract": "This white paper presents principles for validating proarrhythmia risk prediction models for regulatory use as discussed at the In Silico Breakout Session of a Cardiac Safety Research Consortium/Health and Environmental Sciences Institute/US Food and Drug Administration–sponsored Think Tank Meeting on May 22, 2018. The meeting was convened to evaluate the progress in the development of a new cardiac safety paradigm, the Comprehensive in Vitro Proarrhythmia Assay (CiPA). The opinions regarding these principles reflect the collective views of those who participated in the discussion of this topic both at and after the breakout session. Although primarily discussed in the context of in silico models, these principles describe the interface between experimental input and model‐based interpretation and are intended to be general enough to be applied to other types of nonclinical models for proarrhythmia assessment. This document was developed with the intention of providing a foundation for more consistency and harmonization in developing and validating different models for proarrhythmia risk prediction using the example of the CiPA paradigm.",
                "authors": "Zhihua Li, Gary R. Mirams, T. Yoshinaga, Bradley J. Ridder, Xiaomei Han, Janell Chen, N. Stockbridge, Todd A. Wisialowski, B. Damiano, S. Severi, P. Morissette, P. Kowey, M. Holbrook, Godfrey L. Smith, R. Rasmusson, Michael Liu, Z. Song, Z. Qu, D. Leishman, J. Steidl-Nichols, B. Rodríguez, A. Bueno-Orovio, Xiaoxia Zhou, Elisa Passini, A. Edwards, S. Morotti, H. Ni, E. Grandi, C. Clancy, J. Vandenberg, A. Hill, Mikiko Nakamura, T. Singer, L. Polonchuk, A. Greiter‐Wilke, Ken Wang, S. Nave, Aaron M. Fullerton, E. Sobie, M. Paci, Flora Musuamba Tshinanu, D. Strauss",
                "citations": 69
            },
            {
                "title": "Segment Anything",
                "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643.",
                "authors": "A. Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloé Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, A. Berg, Wan-Yen Lo, Piotr Dollár, Ross B. Girshick",
                "citations": 4980
            },
            {
                "title": "Design theory: a foundation of a new paradigm for design science and engineering",
                "abstract": null,
                "authors": "A. Hatchuel, P. Le Masson, Y. Reich, E. Subrahmanian",
                "citations": 67
            },
            {
                "title": "Size-dependent vibrations of a micro beam conveying fluid and resting on an elastic foundation",
                "abstract": "In this study, fluid conveying continuous media was considered as micro beam. Unlike the classical beam theory, the effects of shear stress on micro-structure's dynamic behavior not negligible. Therefore, modified couple stress theory (MCST) were used to see the effects of being micro-sized. By using Hamilton's principle, the nonlinear equations of motion for the fluid conveying micro beam were obtained. Micro beam was considered as resting on an elastic foundation. The obtained equations of motion were became independence from material and geometric structure by nondimensionalization. Approximate solutions of the system were achieved with using the multiple time scales method (a perturbation method). The effects of micro-structure, spring constant, the occupancy rate of micro beam, the fluid velocity on natural frequency and solutions were researched. MCST compared with classical beam theory and showed that beam models that based on classical beam theory are not capable of describing the size effects. Comparisons of classical beam theory and MCST were showed in graphics and these graphics also proved that obtained mathematical model suitable for describe the behavior of normal sized beams.",
                "authors": "S. Kural, E. Özkaya",
                "citations": 50
            },
            {
                "title": "Direct finite element method for nonlinear analysis of semi‐unbounded dam–water–foundation rock systems",
                "abstract": "A direct finite element method is presented for nonlinear earthquake analysis of interacting dam–water–foundation rock systems. The analysis procedure applies viscous damper absorbing boundaries to truncate the semi‐unbounded fluid and foundation‐rock domains and specifies at these boundaries effective earthquake forces determined from the design ground motion defined at a control point on the free surface. The analysis procedure is validated numerically by computing the frequency response functions and transient response of an idealized dam–water–foundation rock system and comparing with results from the substructure method. Because the analysis procedure is applicable to nonlinear systems, it allows for modeling of concrete cracking, as well as sliding and separation at construction joints, lift joints, and at concrete–rock interfaces. Implementation of the procedure is facilitated by commercial finite element software with nonlinear material models that permit modeling of viscous damper boundaries and specification of effective earthquake forces at these boundaries. Copyright © 2017 John Wiley & Sons, Ltd.",
                "authors": "A. Løkke, A. Chopra",
                "citations": 56
            },
            {
                "title": "Linear Models and Time-Series Analysis",
                "abstract": "Linear Models and Time-Series Analysis: Regression, ANOVA, ARMA and GARCH sets a strong foundation, in terms of distribution theory, for the linear model (regression and ANOVA), univariate time series analysis (ARMAX and GARCH), and some multivariate models associated primarily with modeling financial asset returns (copula-based structures and the discrete mixed normal and Laplace). It builds on the author's previous book, Fundamental Statistical Inference: A Computational Approach, which introduced the major concepts of statistical inference. Attention is explicitly paid to application and numeric computation, with examples of Matlab code throughout. The code offers a framework for discussion and illustration of numerics, and shows the mapping from theory to computation.",
                "authors": "Marc S. Paolella",
                "citations": 78
            },
            {
                "title": "Reliability Analysis of Pile Foundation Using ELM and MARS",
                "abstract": null,
                "authors": "Manish Kumar, P. Samui",
                "citations": 39
            },
            {
                "title": "Analysis of Load Sharing Response and Prediction of Interaction Behaviour in Piled Raft Foundation",
                "abstract": null,
                "authors": "Plaban Deb, S. Pal",
                "citations": 21
            },
            {
                "title": "Mixtures of Experts Models",
                "abstract": "Mixtures of experts models provide a framework in which covariates may be included in mixture models. This is achieved by modelling the parameters of the mixture model as functions of the concomitant covariates. Given their mixture model foundation, mixtures of experts models possess a diverse range of analytic uses, from clustering observations to capturing parameter heterogeneity in cross-sectional data. This chapter focuses on delineating the mixture of experts modelling framework and demonstrates the utility and flexibility of mixtures of experts models as an analytic tool.",
                "authors": "I. C. Gormley, Sylvia Fruhwirth-Schnatter",
                "citations": 64
            },
            {
                "title": "A parametric study on the vertical pullout capacity of suction caisson foundation in cohesive soil",
                "abstract": null,
                "authors": "Suchit Kumar Patel, Baleshwar Singh",
                "citations": 30
            },
            {
                "title": "Laboratory Models of Treatment Relapse and Mitigation Techniques",
                "abstract": "Behavioral treatments arranging differential reinforcement effectively treat severe problem behavior while interventions are underway. However, many events challenge these treatments clinically, thereby producing relapse of problem behavior. This paper reviews six laboratory models of treatment relapse for their relevance to understanding the processes underlying treatment relapse, including resurgence, reinstatement, rapid reacquisition, disinhibition, and spontaneous recovery. In addition, we also discuss clinical examples resembling these models and accompanying effects, as well as studies examining the combined effects of these laboratory models. Finally, we describe approaches developed in laboratory studies to mitigate treatment relapse using a variety of approaches involving both antecedent and consequence-based interventions. This research provides a foundation from which basic and clinical researchers can collaborate to establish more durable treatments for problem behavior.",
                "authors": "Stephanie N. Wathen, Christopher A. Podlesnik",
                "citations": 64
            },
            {
                "title": "Nonlinear vibrations of a rectangular hyperelastic membrane resting on a nonlinear elastic foundation",
                "abstract": null,
                "authors": "R. Soares, P. Gonçalves",
                "citations": 28
            },
            {
                "title": "Seismic responses of bridges with rocking column‐foundation: A dimensionless regression analysis",
                "abstract": "Rocking column‐foundation system is a new design concept for bridges that can reduce overall seismic damage, minimize construction and repair time, and achieve lower cost in general. However, such system involves complex dynamic responses due to impacts and highly nonlinear rocking behavior. This study presents a dimensionless regression analysis to estimate the rocking and shaking responses of the flexible column‐foundation system under near‐fault ground motions. First, the transient drift and rocking responses of the system are solved numerically using previously established analytical models. Subsequently, the peak column drifts and uplift angles are derived as functions of ground motion characteristics and the geometric and dynamic parameters of column‐foundation system in regressed dimensionless forms. The proposed response models are further examined by validating against the numerical simulations for several as‐built bridge cases. It is shown that the proposed model not only physically quantifies the influences of prominent parameters, but also consistently reflects the complex dynamics of the system. The seismic demands of rocking column‐foundation system can be realistically predicted directly from structural and ground motion characteristics. This can significantly benefit the design of bridges incorporating this new design concept.",
                "authors": "Jian Zhang, Yazhou Xie, Gang Wu",
                "citations": 29
            },
            {
                "title": "Longitudinal varying elastic foundation effects on vibration behavior of axially graded nanobeams via nonlocal strain gradient elasticity theory",
                "abstract": "ABSTRACT In this research, vibration characteristics of axially functionally graded nanobeams resting on variable elastic foundation are investigated based on nonlocal strain gradient theory. This nonclassical nanobeam model contains a length scale parameter to explore the influence of strain gradients and also a nonlocal parameter to study the long-range interactions between the particles. The present model can degenerate into the classical models if the material length scale parameter and the nonlocal stress field parameter are both taken to be zero. Elastic foundation consists of two layers: a Winkler layer with variable stiffness and a Pasternak layer with constant stiffness. Linear, parabolic and sinusoidal variations of Winkler foundation in longitudinal direction are considered. Material properties are graded axially via a power-law distribution scheme. Hamilton's principle is employed to derive the governing equations that are solved applying a Galerkin-based solution for different boundary edges. Comparison study is also performed to verify the present formulation with those of previous papers. Results are presented to investigate the influences of the nonlocal and length scale parameters, various material compositions, elastic foundation parameters, type of foundation and various boundary conditions on the vibration frequencies of AFG nanobeams in detail.",
                "authors": "F. Ebrahimi, M. Barati",
                "citations": 27
            },
            {
                "title": "Animal Models of Zika Virus Infection during Pregnancy",
                "abstract": "Zika virus (ZIKV) emerged suddenly in the Americas in 2015 and was associated with a widespread outbreak of microcephaly and other severe congenital abnormalities in infants born to mothers infected during pregnancy. Vertical transmission of ZIKV in humans was confirmed when viral RNA was detected in fetal and placental tissues, and this outcome has been recapitulated experimentally in animals. Unlike other flaviviruses, ZIKV is both arthropod- and sexually-transmitted, and has a broad tissue tropism in humans, including multiple tissues of the reproductive tract. The threats posed by ZIKV have prompted the development of multiple in vivo models to better understand the pathogenesis of ZIKV, particularly during pregnancy. Here, we review the progress on animal models of ZIKV infection during pregnancy. These studies have generated a foundation of insights into the biology of ZIKV, and provide a means for evaluating vaccines and therapeutics.",
                "authors": "Elizabeth A. Caine, B. W. Jagger, M. Diamond",
                "citations": 57
            },
            {
                "title": "Elastic and inelastic buckling of square and skew FGM plates with cutout resting on elastic foundation using isoparametric spline finite strip method",
                "abstract": null,
                "authors": "Mojtaba Gholami Shahrestani, M. Azhari, H. Foroughi",
                "citations": 28
            },
            {
                "title": "Sustainability-Oriented Business Model Assessment—A Conceptual Foundation",
                "abstract": null,
                "authors": "Florian Lüdeke‐Freund, Birte Freudenreich, S. Schaltegger, Iolanda Saviuc, M. Stock",
                "citations": 43
            },
            {
                "title": "DINOv2: Learning Robust Visual Features without Supervision",
                "abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.",
                "authors": "M. Oquab, Timothée Darcet, Théo Moutakanni, Huy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ Howes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao Xu, H. Jégou, J. Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski",
                "citations": 2009
            },
            {
                "title": "Free Vibrational Response of Single-Layered Graphene Sheets Embedded in an Elastic Matrix using Different Nonlocal Plate Models",
                "abstract": "In this paper, the small scale effects are incorporated into the free vibration analysis of single-layered graphene sheets (SLGSs) embedded in an elastic medium. To this end, Eringen’s nonlocal elasticity continuum are applied to the different types of plate theory namely as the classical plate theory (CLPT), first order shear deformation theory (FSDT), and higher order shear deformation theory (HSDT). Winkler and Pasternak foundation models used to simulate the surrounding elastic medium are compared with each other. Explicit expressions are derived to calculate the natural frequencies of square SLGSs corresponding to each type of nonlocal plate model. Selected numerical results are given to indicate the influence of the nonlocal parameter, Winkler and Pasternak elastic moduli, mode number, and the side length of SLGSs in detail. Also, comparison is made between the vibrational responses of SLGSs obtained through different nonlocal plate theories. It is found that the elastic foundation and value of nonlocal parameter have quite significant effects on the natural frequencies of SLGSs and these effects are influenced by mode number as well as side length. DOI: http://dx.doi.org/10.5755/j01.mech.23.5.14883",
                "authors": "B. Safaei, A. Fattahi",
                "citations": 44
            },
            {
                "title": "A Systems Approach to Understanding the Philosophical Foundation of Marketing Studies",
                "abstract": null,
                "authors": "B. Tronvoll, S. Barile, F. Caputo",
                "citations": 30
            },
            {
                "title": "Chemical element transport in stellar evolution models",
                "abstract": "Stellar evolution computations provide the foundation of several methods applied to study the evolutionary properties of stars and stellar populations, both Galactic and extragalactic. The accuracy of the results obtained with these techniques is linked to the accuracy of the stellar models, and in this context the correct treatment of the transport of chemical elements is crucial. Unfortunately, in many respects calculations of the evolution of the chemical abundance profiles in stars are still affected by sometimes sizable uncertainties. Here, we review the various mechanisms of element transport included in the current generation of stellar evolution calculations, how they are implemented, the free parameters and uncertainties involved, the impact on the models and the observational constraints.",
                "authors": "M. Salaris, S. Cassisi",
                "citations": 57
            },
            {
                "title": "Seismic performance of buildings with structural and foundation rocking in centrifuge testing",
                "abstract": "Rocking motion, established in either the superstructure in the form of a 2‐point stepping mechanism (structural rocking) or resulting from rotational motion of the foundation on the soil (foundation rocking), is considered an effective, low‐cost base isolation technique. This paper unifies for the first time the 2 types of rocking motion under a common experimental campaign, so that on the one hand, structural rocking can be examined under the influence of soil and on the other, foundation rocking can be examined under the influence of a linear elastic superstructure. Two building models, designed to rock above or below their foundation level so that they can reproduce structural and foundation rocking respectively, were tested side by side in a centrifuge. The models were placed on a dry sandbed and subjected to a sequence of earthquake motions. The range of rocking amplitude that is required for base isolation was quantified. Overall, it is shown that the relative density of sand does not influence structural rocking, while for foundation rocking, the change from dense to loose sand can affect the time‐frequency response significantly and lead to a more predictable behaviour.",
                "authors": "I. Pelekis, G. Madabhushi, M. DeJong",
                "citations": 22
            },
            {
                "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
                "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
                "authors": "Albert Gu, Tri Dao",
                "citations": 1484
            },
            {
                "title": "Optimization problems for elastic contact models with unilateral constraints",
                "abstract": null,
                "authors": "M. Sofonea, Yi-bin Xiao, Maxime Couderc",
                "citations": 56
            },
            {
                "title": "Dynamic Response of Saturated Soil - Foundation System Acted upon by Vibration",
                "abstract": "In this study, the response and behavior of machine foundations resting on dry and saturated sand was investigated experimentally. In order to investigate the response of soil and footing to steady state dynamic loading, a physical model was manufactured to simulate steady state harmonic load at different operating frequencies. Total of 84 physical models were performed. The footing parameters are related to the size of the rectangular footing and depth of embedment. Two sizes of rectangular steel model footing were tested at the surface and at 50 mm depth below model surface. Meanwhile the investigated parameters of the soil condition include dry and saturated sand for two relative densities 30% and 80%. The response of the footing was elaborated by measuring the amplitude of displacement by the vibration meter. The response of the soil to dynamic loading includes measuring the stresses inside the soil using piezoelectric sensors as well as measuring the excess pore water pressure using pore water pressure transducers. It was concluded that the maximum displacement amplitude response of the foundation resting on dry sand models is more than that on the saturated sand by about 5.0–10%. The maximum displacement amplitude of footing is reduced to half when the size of footing is doubled for dry and saturated sand. The final settlement (St) of the foundation increases with increasing the amplitude of dynamic force, operating frequency and degree of saturation. Meanwhile, it is reduced with increasing the relative density of sand, modulus of elasticity, and embedding inside soils. The excess pore water pressure increases with increasing the relative density of the sand, the amplitude of dynamic loading and the operating frequency. In contrast, the rate of dissipation of the excess pore water pressure during dynamic loading is more in the case of loose sand.",
                "authors": "M. Fattah, Mosa J. Al-Mosawi, A. F. Al-Ameri",
                "citations": 36
            },
            {
                "title": "Teaching Einsteinian physics at schools: part 1, models and analogies for relativity",
                "abstract": "The Einstein-First project aims to change the paradigm of school science teaching through the introduction of modern Einsteinian concepts of space and time, gravity and quanta at an early age. These concepts are rarely taught to school students despite their central importance to modern science and technology. The key to implementing the Einstein-First curriculum is the development of appropriate models and analogies. This paper is the first part of a three-paper series. It presents the conceptual foundation of our approach, based on simple physical models and analogies, followed by a detailed description of the models and analogies used to teach concepts of general and special relativity. Two accompanying papers address the teaching of quantum physics (Part 2) and research outcomes (Part 3).",
                "authors": "T. Kaur, David Blair, John Moschilla, Warren B. Stannard, M. Zadnik",
                "citations": 53
            },
            {
                "title": "When a Foundation Crumbles: Forecasting Forest Dynamics Following the Decline of the Foundation Species Tsuga Canadensis",
                "abstract": "In the forests of northeastern North America, invasive insects and pathogens are causing major declines in some tree species and a subsequent reorganization of associated forest communities. Using observations and experiments to investigate the consequences of such declines are hampered because trees are long-lived. Simulation models can provide a means to forecast possible futures based on different scenarios of tree species decline, death, and removal. Such modeling is particularly urgent for species such as eastern hemlock (Tsuga canadensis), a foundation species in many northeastern forest regions that is declining due to the hemlock woolly adelgid (Adelges tsugae). Here, we used an individual-based forest simulator, SORTIE-ND, to forecast changes in forest communities in Central Massachusetts over the next 200 yr under a range of scenarios: a no-adelgid, status quo scenario; partial resistance of hemlock to the adelgid; adelgid irruption and total hemlock decline over 25 yr, adelgid irruption and salvage logging of hemlock trees; and two scenarios of preemptive logging of hemlock and hemlock/white pine. We applied the model to six study plots comprising a range of initial species mixtures, abundances, and levels of hemlock dominance. Simulations indicated that eastern white pine, and to a lesser extent black birch and American beech, would gain most in relative abundance and basal area following hemlock decline. The relative dominance of these species depended on initial conditions and the amount of hemlock mortality, and their combined effect on neighborhood-scale community dynamics. Simulated outcomes were little different whether hemlock died out gradually due to the adelgid or disappeared rapidly following logging. However, if eastern hemlock were to become partially resistant to the adelgid, hemlock would be able to retain its dominance despite substantial losses of basal area. Our modeling highlights the complexities associated with secondary forest succession due to ongoing hemlock decline and loss. We emphasize the need both for a precautionary approach in deciding between management intervention or simply doing nothing in these declining hemlock forests, and for clear aims and understanding regarding desired community- and ecosystem-level outcomes.",
                "authors": "B. Case, H. Buckley, A. B. Plotkin, D. Orwig, A. Ellison",
                "citations": 27
            },
            {
                "title": "Rangeland Systems: Foundation for a Conceptual Framework",
                "abstract": null,
                "authors": "D. Briske",
                "citations": 33
            },
            {
                "title": "Improving the foundation and practice of reliability engineering",
                "abstract": "Reliability engineering is today a well-established field, accounting for many scientific journals and conferences, educational programmes and courses, academic positions and societies. There are also many standards which guide the practice of reliability engineering, and every year a number of scientific papers are published which address reliability engineering issues. Yet the area faces many challenges, in particular when addressing systems characterised by large uncertainties, and accurate prediction models are not easily established. We see alternative analysis perspectives being advocated, with varying degrees of theoretical justification. This article argues that there is potential for improvements to be made in terms of both theoretical frameworks and the practice of reliability engineering to meet these challenges and guide reliability engineers and decision-makers. Examples relate to the understanding and treatment of uncertainties, and the use of ideas and methods from risk management. Clear recommendations are provided on how to obtain such improvements.",
                "authors": "T. Aven",
                "citations": 29
            },
            {
                "title": "Agent-based modelling as a foundation for big data",
                "abstract": "In this article, we propose a process-based definition of big data, as opposed to the size- and technology-based definitions. We argue that big data should be perceived as a continuous, unstructured and unprocessed dynamics of primitives, rather than as points (snapshots) or summaries (aggregates) of an underlying phenomenon. Given this, we show that big data can be generated through agent-based models but not by equation-based models. Though statistical and machine learning tools can be used to analyse big data, they do not constitute a big data-generation mechanism. Furthermore, agent-based models can aid in evaluating the quality (interpreted as information aggregation efficiency) of big data. Based on this, we argue that agent-based modelling can serve as a possible foundation for big data. We substantiate this interpretation through some pioneering studies from the 1980s on swarm intelligence and several prototypical agent-based models developed around the 2000s.",
                "authors": "Shu-Heng Chen, Ragupathy Venkatachalam",
                "citations": 23
            },
            {
                "title": "General Multi-Fidelity Framework for Training Artificial Neural Networks With Computational Models",
                "abstract": "DFG German Research Foundation − Projektnummer 192346071; SFB 986; Projektnummer 257981274.",
                "authors": "R. Aydin, F. Braeu, C. Cyron",
                "citations": 30
            },
            {
                "title": "Investigation of the dynamic behaviour of a storage tank with different foundation types focusing on the soil‐foundation‐structure interactions using centrifuge model tests",
                "abstract": "This paper proposes a dynamic centrifuge model test method for the accurate simulation of the behaviours of a liquid storage tank with different types of foundations during earthquakes. The method can be used to determine the actual stress conditions of a prototype storage‐tank structure. It was used in the present study to investigate the soil‐foundation‐structure interactions of a simplified storage tank under two different earthquake motions, which were simulated using a shaking table installed in a centrifuge basket. Three different types of foundations were considered, namely, a shallow foundation, a slab on the surface of the ground connected to piles and a slab with disconnected piles. The test results were organised to compare the ground surface and foundation motions, the slab of foundation and top of structure motions and the horizontal and vertical motions of the slab, respectively. These were used to establish the complex dynamic behaviours of tank models with different foundations. The effects of soil–foundation–structure interaction with three foundation conditions and two different earthquake motions are focused and some important factors, that should be considered for future designs are also discussed in this research. Copyright © 2017 John Wiley & Sons, Ltd.",
                "authors": "Heon-Joon Park, J. Ha, S. Kwon, Moon-Gyo Lee, Dong‐Soo Kim",
                "citations": 20
            },
            {
                "title": "VideoChat: Chat-Centric Video Understanding",
                "abstract": "In this paper, we initiate an attempt of developing an end-to-end chat-centric video understanding system, coined as VideoChat. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we build a video-centric instruction dataset, composed of thousands of videos associated with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and captures causal relationships, providing a valuable asset for training our chat-centric video understanding system. Preliminary qualitative experiments demonstrate the potential of our system across a broad spectrum of video applications, which could serve as a simple prototype system for future research on chat-centric video understanding. Access our code and data at https://github.com/OpenGVLab/Ask-Anything",
                "authors": "Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao",
                "citations": 387
            },
            {
                "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
                "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.",
                "authors": "Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, Tao Gui",
                "citations": 616
            },
            {
                "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
                "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
                "authors": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, C. McLeavey, I. Sutskever",
                "citations": 2607
            },
            {
                "title": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
                "abstract": "Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-training on a mixed labeled dataset. Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner.",
                "authors": "Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, Yu Qiao",
                "citations": 259
            },
            {
                "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
                "abstract": "Large language models (LLMs) have notably accelerated progress towards artificial general intelligence (AGI), with their impressive zero-shot capacity for user-tailored tasks, endowing them with immense potential across a range of applications. However, in the field of computer vision, despite the availability of numerous powerful vision foundation models (VFMs), they are still restricted to tasks in a pre-defined form, struggling to match the open-ended task capabilities of LLMs. In this work, we present an LLM-based framework for vision-centric tasks, termed VisionLLM. This framework provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-centric tasks with language tasks that can be flexibly defined and managed using language instructions. An LLM-based decoder can then make appropriate predictions based on these instructions for open-ended tasks. Extensive experiments show that the proposed VisionLLM can achieve different levels of task customization through language instructions, from fine-grained object-level to coarse-grained task-level customization, all with good results. It's noteworthy that, with a generalist LLM-based framework, our model can achieve over 60\\% mAP on COCO, on par with detection-specific models. We hope this model can set a new baseline for generalist vision and language models. The demo shall be released based on https://github.com/OpenGVLab/InternGPT. The code shall be released at https://github.com/OpenGVLab/VisionLLM.",
                "authors": "Wen Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Y. Qiao, Jifeng Dai",
                "citations": 376
            },
            {
                "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
                "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
                "authors": "Lorenz Kuhn, Y. Gal, Sebastian Farquhar",
                "citations": 185
            },
            {
                "title": "Adversarial Diffusion Distillation",
                "abstract": "We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1-4 steps while maintaining high image quality. We use score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal in combination with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps. Our analyses show that our model clearly outperforms existing few-step methods (GANs, Latent Consistency Models) in a single step and reaches the performance of state-of-the-art diffusion models (SDXL) in only four steps. ADD is the first method to unlock single-step, real-time image synthesis with foundation models. Code and weights available under https://github.com/Stability-AI/generative-models and https://huggingface.co/stabilityai/ .",
                "authors": "Axel Sauer, Dominik Lorenz, A. Blattmann, Robin Rombach",
                "citations": 218
            },
            {
                "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
                "abstract": "In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",
                "authors": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou",
                "citations": 504
            },
            {
                "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
                "abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level, an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics for simple adaptation to novel tasks without updating pretrained model weights. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 17 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on all 8 datasets on average by +9 accuracy points.",
                "authors": "Eric D Nguyen, Michael Poli, Marjan Faizi, A. Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton M. Rabideau, Stefano Massaroli, Y. Bengio, Stefano Ermon, S. Baccus, Christopher Ré",
                "citations": 151
            },
            {
                "title": "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
                "abstract": "Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific pre-defined vision tasks. Yet, existing systems can only handle videos with very few frames. For long videos, the computation complexity, memory cost, and long-term temporal connection impose additional challenges. Taking advantage of the Atkinson-Shiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination with our specially designed memory mechanism, we propose the MovieChat to overcome these challenges. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video and 14K manual annotations for validation of the effectiveness of our method. The code, models and data can be found in https://reself.github.io/MovieChat.",
                "authors": "Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tianbo Ye, Yang Lu, Jenq-Neng Hwang, Gaoang Wang",
                "citations": 147
            },
            {
                "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
                "abstract": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.",
                "authors": "Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, A. Awadallah",
                "citations": 236
            },
            {
                "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
                "abstract": "We launch EVA, a vision-centric foundation model to Explore the limits of Visual representation at scAle using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVIS dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models.",
                "authors": "Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen Sun, Ledell Yu Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao",
                "citations": 562
            },
            {
                "title": "ConceptFusion: Open-set Multimodal 3D Mapping",
                "abstract": "Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts. We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today's foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping. For more information, visit our project page https://concept-fusion.github.io or watch our 5-minute explainer video https://www.youtube.com/watch?v=rkXgws8fiDs",
                "authors": "Krishna Murthy Jatavallabhula, Ali Kuwajerwala, Qiao Gu, Mohd. Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Varma Keetha, A. Tewari, J. Tenenbaum, Celso M. de Melo, M. Krishna, L. Paull, F. Shkurti, A. Torralba",
                "citations": 179
            },
            {
                "title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective",
                "abstract": "ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat to foundation models. Moreover, ChatGPT shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. Finally, we present in-depth discussions of possible research directions.",
                "authors": "Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Weirong Ye, Xiubo Geng, Binxing Jiao, Yue Zhang, Xingxu Xie",
                "citations": 200
            },
            {
                "title": "Recognize Anything: A Strong Image Tagging Model",
                "abstract": "We present the Recognize Anything Model (RAM): a strong foundation model for image tagging. RAM makes a substantial step for foundation models in computer vision, demonstrating the zero-shot ability to recognize any common category with high accuracy. By leveraging large-scale image-text pairs for training instead of manual annotations, RAM introduces a new paradigm for image tagging.The development of RAM comprises four key steps. Firstly, annotation-free image tags are obtained at scale through automatic text semantic parsing. Subsequently, a preliminary model is trained for automatic annotation by unifying the captioning and tagging tasks, supervised by the original texts and parsed tags, respectively. Thirdly, a data engine is employed to generate additional annotations and clean incorrect ones. Lastly, the model is retrained with the processed data and fine-tuned using a smaller but higher-quality dataset.We evaluate the tagging capability of RAM on numerous benchmarks and observe an impressive zero-shot performance, which significantly outperforms CLIP and BLIP. Remarkably, RAM even surpasses fully supervised models and exhibits a competitive performance compared with the Google tagging API. We have released RAM at https://recognize-anything.github.io/ to foster the advancement of foundation models in computer vision.",
                "authors": "Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Siyi Liu, Yandong Guo, Lei Zhang",
                "citations": 160
            },
            {
                "title": "End-to-End Autonomous Driving: Challenges and Frontiers",
                "abstract": "The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework.",
                "authors": "Li Chen, Peng Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, Hongyang Li",
                "citations": 160
            },
            {
                "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
                "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving 16 AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Code can be found in https://github.com/AIGC-Audio/AudioGPT",
                "authors": "Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, Shinji Watanabe",
                "citations": 155
            },
            {
                "title": "Hyperledger fabric: a distributed operating system for permissioned blockchains",
                "abstract": "Fabric is a modular and extensible open-source system for deploying and operating permissioned blockchains and one of the Hyperledger projects hosted by the Linux Foundation (www.hyperledger.org). Fabric is the first truly extensible blockchain system for running distributed applications. It supports modular consensus protocols, which allows the system to be tailored to particular use cases and trust models. Fabric is also the first blockchain system that runs distributed applications written in standard, general-purpose programming languages, without systemic dependency on a native cryptocurrency. This stands in sharp contrast to existing block-chain platforms that require \"smart-contracts\" to be written in domain-specific languages or rely on a cryptocurrency. Fabric realizes the permissioned model using a portable notion of membership, which may be integrated with industry-standard identity management. To support such flexibility, Fabric introduces an entirely novel blockchain design and revamps the way blockchains cope with non-determinism, resource exhaustion, and performance attacks. This paper describes Fabric, its architecture, the rationale behind various design decisions, its most prominent implementation aspects, as well as its distributed application programming model. We further evaluate Fabric by implementing and benchmarking a Bitcoin-inspired digital currency. We show that Fabric achieves end-to-end throughput of more than 3500 transactions per second in certain popular deployment configurations, with sub-second latency, scaling well to over 100 peers.",
                "authors": "Elli Androulaki, Artem Barger, V. Bortnikov, C. Cachin, K. Christidis, Angelo De Caro, David Enyeart, Christopher Ferris, Gennady Laventman, Yacov Manevich, S. Muralidharan, Chet Murthy, Binh Nguyen, Manish Sethi, Gari Singh, Keith A. Smith, A. Sorniotti, C. Stathakopoulou, M. Vukolic, S. Cocco, Jason Yellick",
                "citations": 3468
            },
            {
                "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
                "abstract": "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https://aka.ms/BLURB.",
                "authors": "Yu Gu, Robert Tinn, Hao Cheng, Michael R. Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon",
                "citations": 1522
            },
            {
                "title": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning",
                "abstract": "For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. To explore the full scope of our experiments and results, we encourage readers to visit our project webpage.",
                "authors": "Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Ramalingam Chellappa, Chuang Gan, C. D. Melo, J. B. Tenenbaum, Antonio Torralba, F. Shkurti, L. Paull",
                "citations": 118
            },
            {
                "title": "SVIT: Scaling up Visual Instruction Tuning",
                "abstract": "Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.",
                "authors": "Bo Zhao, Boya Wu, Tiejun Huang",
                "citations": 106
            },
            {
                "title": "Clinical-grade computational pathology using weakly supervised deep learning on whole slide images",
                "abstract": null,
                "authors": "Gabriele Campanella, M. Hanna, Luke Geneslaw, Allen P. Miraflor, Vitor Werneck Krauss Silva, K. Busam, E. Brogi, V. Reuter, D. Klimstra, Thomas J. Fuchs",
                "citations": 1736
            },
            {
                "title": "Certified Robustness to Adversarial Examples with Differential Privacy",
                "abstract": "Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google’s Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism, that provides a rigorous, generic, and flexible foundation for defense.",
                "authors": "Mathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel J. Hsu, S. Jana",
                "citations": 886
            },
            {
                "title": "Language Modeling Is Compression",
                "abstract": "It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.",
                "authors": "Gr'egoire Del'etang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Wenliang Kevin Li, Matthew Aitchison, Laurent Orseau, Marcus Hutter, J. Veness",
                "citations": 98
            },
            {
                "title": "FLAVA: A Foundational Language And Vision Alignment Model",
                "abstract": "State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a “foundation”, that targets all modalities at once-a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.",
                "authors": "Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela",
                "citations": 592
            },
            {
                "title": "PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents",
                "abstract": "Foundation models trained on large-scale dataset gain a recent surge in CV and NLP. In contrast, development in biomedical domain lags far behind due to data scarcity. To address this issue, we build and release PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, which is 8 times larger than before. PMC-OA covers diverse modalities or diseases, with majority of the image-caption samples aligned at finer-grained level, i.e., subfigure and subcaption. While pretraining a CLIP-style model on PMC-OA, our model named PMC-CLIP achieves state-of-the-art results on various downstream tasks, including image-text retrieval on ROCO, MedMNIST image classification, Medical VQA, i.e. +8.1% R@10 on image-text retrieval, +3.9% accuracy on image classification.",
                "authors": "Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie",
                "citations": 93
            },
            {
                "title": "Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields",
                "abstract": "3D scene representations have gained immense popularity in recent years. Methods that use Neural Radiance fields are versatile for traditional tasks such as novel view synthesis. In recent times, some work has emerged that aims to extend the functionality of NeRF beyond view synthesis, for semantically aware tasks such as editing and segmentation using 3D feature field distillation from 2D foundation models. However, these methods have two major limitations: (a) they are limited by the rendering speed of NeRF pipelines, and (b) implicitly represented feature fields suffer from continuity artifacts reducing feature quality. Recently, 3D Gaussian Splatting has shown state-of-the-art performance on real-time radiance field rendering. In this work, we go one step further: in addition to radiance field rendering, we enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D foundation model distillation. This translation is not straightforward: naively incorporating feature fields in the 3DGS framework encounters significant challenges, notably the disparities in spatial resolution and channel consistency between RGB images and feature maps. We propose architectural and training changes to efficiently avert this problem. Our proposed method is general, and our experiments showcase novel view semantic segmentation, language-guided editing and segment anything through learning feature fields from state-of-the-art 2D foundation models such as SAM and CLIP-LSeg. Across experiments, our distillation method is able to provide comparable or better results, while being significantly faster to both train and render. Additionally, to the best of our knowledge, we are the first method to enable point and bounding-box prompting for radiance field manipulation, by leveraging the SAM model. Project website at: https://feature-3dgs.github.io/.",
                "authors": "Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, A. Kadambi",
                "citations": 91
            },
            {
                "title": "Human-Timescale Adaptation in an Open-Ended Task Space",
                "abstract": "Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.",
                "authors": "Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal M. P. Behbahani, Avishkar Bhoopchand, N. Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreyaan Pathak, Nicolas Perez Nieves, Nemanja Rakicevic, Tim Rocktäschel, Yannick Schroecker, Jakub Sygnowski, K. Tuyls, Sarah York, Alexander Zacherl, Lei M. Zhang",
                "citations": 96
            },
            {
                "title": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild",
                "abstract": "Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions, we augment pretrained text-to-image diffusion models and introduce a task-aware HyperNet to modulate the diffusion models, enabling the adaptation to different C2I tasks simultaneously. Trained on nine unique C2I tasks, UniControl demonstrates impressive zero-shot generation abilities with unseen visual conditions. Experimental results show that UniControl often surpasses the performance of single-task-controlled methods of comparable model sizes. This control versatility positions UniControl as a significant advancement in the realm of controllable visual generation.",
                "authors": "Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Haiquan Wang, Juan Carlos Niebles, Caiming Xiong, S. Savarese, Stefano Ermon, Yun Fu, Ran Xu",
                "citations": 85
            },
            {
                "title": "SAM-Adapter: Adapting Segment Anything in Underperformed Scenes",
                "abstract": "The emergence of large models, also known as foundation models, has brought significant advancements to AI research. One such model is Segment Anything (SAM), which is designed for image segmentation tasks. However, as with other foundation models, our experimental findings suggest that SAM may fail or perform poorly in certain segmentation tasks, such as shadow detection and camouflaged object detection (concealed object detection). This study first paves the way for applying the large pre-trained image segmentation model SAM to these downstream tasks, even in situations where SAM performs poorly. Rather than fine-tuning the SAM network, we propose SAM-Adapter, which incorporates domain-specific information or visual prompts into the segmentation network by using simple yet effective adapters. By integrating task-specific knowledge with general knowledge learnt by the large model, SAM-Adapter can significantly elevate the performance of SAM in challenging tasks as shown in extensive experiments. We can even outperform task-specific network models and achieve state-of-the-art performance in the task we tested: camouflaged object detection, shadow detection. Our code of adapting SAM in downstream applications have been released publicly at https://github.com/tianrun-chen/SAM-Adapter-PyTorch/ and has benefited many researchers. We believe our work opens up opportunities for utilizing SAM in downstream tasks, with potential applications in various fields, including medical image processing, agriculture, remote sensing, and more.",
                "authors": "Tianrun Chen, Lanyun Zhu, Chao Ding, Runlong Cao, Yan Wang, Shangzhan Zhang, Zejian Li, Lingyun Sun, Ying-Dong Zang, Papa Mao",
                "citations": 90
            },
            {
                "title": "Segment anything in medical images",
                "abstract": null,
                "authors": "Jun Ma, Yuting He, Feifei Li, Li-Jun Han, Chenyu You, Bo Wang",
                "citations": 267
            },
            {
                "title": "SLiM 3: Forward Genetic Simulations Beyond the Wright–Fisher Model",
                "abstract": "With the desire to model population genetic processes under increasingly realistic scenarios, forward genetic simulations have become a critical part of the toolbox of modern evolutionary biology. The SLiM forward genetic simulation framework is one of the most powerful and widely used tools in this area. However, its foundation in the Wright–Fisher model has been found to pose an obstacle to implementing many types of models; it is difficult to adapt the Wright–Fisher model, with its many assumptions, to modeling ecologically realistic scenarios such as explicit space, overlapping generations, individual variation in reproduction, density-dependent population regulation, individual variation in dispersal or migration, local extinction and recolonization, mating between subpopulations, age structure, fitness-based survival and hard selection, emergent sex ratios, and so forth. In response to this need, we here introduce SLiM 3, which contains two key advancements aimed at abolishing these limitations. First, the new non-Wright–Fisher or “nonWF” model type provides a much more flexible foundation that allows the easy implementation of all of the above scenarios and many more. Second, SLiM 3 adds support for continuous space, including spatial interactions and spatial maps of environmental variables. We provide a conceptual overview of these new features, and present several example models to illustrate their use. These two key features allow SLiM 3 models to go beyond the Wright–Fisher model, opening up new horizons for forward genetic modeling.",
                "authors": "Benjamin C. Haller, Philipp W. Messer",
                "citations": 669
            },
            {
                "title": "Deep Learning for Health Informatics",
                "abstract": "With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.",
                "authors": "D. Ravì, Charence Wong, F. Deligianni, M. Berthelot, Javier Andreu-Perez, Benny P. L. Lo, Guang-Zhong Yang",
                "citations": 1419
            },
            {
                "title": "SAM struggles in concealed scenes — empirical study on “Segment Anything”",
                "abstract": null,
                "authors": "Ge-Peng Ji, Deng-Ping Fan, Peng Xu, Ming-Ming Cheng, Bowen Zhou, L. Gool",
                "citations": 86
            },
            {
                "title": "Knowledge-enhanced visual-language pre-training on chest radiology images",
                "abstract": null,
                "authors": "Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie",
                "citations": 83
            },
            {
                "title": "Power of data in quantum machine learning",
                "abstract": null,
                "authors": "Hsin-Yuan Huang, M. Broughton, M. Mohseni, R. Babbush, S. Boixo, H. Neven, J. McClean",
                "citations": 541
            },
            {
                "title": "Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation",
                "abstract": "Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.",
                "authors": "Bokui (William) Shen, Ge Yang, Alan Yu, J. Wong, L. Kaelbling, Phillip Isola",
                "citations": 70
            },
            {
                "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model",
                "abstract": "Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs.",
                "authors": "Siyuan Huang, Zhengkai Jiang, Hao Dong, Y. Qiao, Peng Gao, Hongsheng Li",
                "citations": 78
            },
            {
                "title": "Entrepreneurship and Economic Growth",
                "abstract": null,
                "authors": "P. Petrakis, Dionysis G. Valsamis, K. Kafka",
                "citations": 657
            },
            {
                "title": "Valley: Video Assistant with Large Language model Enhanced abilitY",
                "abstract": "Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced abilitY. The Valley consists of a LLM, a temporal modeling module, a visual encoder, and a simple projection module designed to bridge visual and textual modes. To empower Valley with video comprehension and instruction-following capabilities, we construct a video instruction dataset and adopt a two-stage tuning procedure to train it. Specifically, we employ ChatGPT to facilitate the construction of task-oriented conversation data encompassing various tasks, including multi-shot captions, long video descriptions, action recognition, causal relationship inference, etc. Subsequently, we adopt a pre-training-then-instructions-tuned pipeline to align visual and textual modalities and improve the instruction-following capability of Valley. Qualitative experiments demonstrate that Valley has the potential to function as a highly effective video assistant that can make complex video understanding scenarios easy.",
                "authors": "Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Ming-Hui Qiu, Pengcheng Lu, Tao Wang, Zhongyu Wei",
                "citations": 139
            },
            {
                "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
                "abstract": "We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/",
                "authors": "D. Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Y. Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David C. Minnen, David A. Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming Yang, Xuan Yang, Bryan Seybold, Lu Jiang",
                "citations": 152
            },
            {
                "title": "Transportation 5.0: The DAO to Safe, Secure, and Sustainable Intelligent Transportation Systems",
                "abstract": "In 2014, IEEE Intelligent Transportation Systems Society established a Technical Committee on Transportation 5.0 with the mission of promoting and transforming the deployment of advanced and innovative technologies, especially Artificial Intelligence in transportation. This paper briefly summarizes our main research and findings over the last decade. Transportation Foundation Models, Transportation Scenarios Engineering, and Transportation Operating Systems have been identified as the main directions for the research and development of next-generation intelligent transportation systems.",
                "authors": "Fei Wang, Yilun Lin, Petros A. Ioannou, L. Vlacic, Xiaoming Liu, A. Eskandarian, Yisheng Lv, X. Na, D. Cebon, Jiaqi Ma, Lingxi Li, Cristina Olaverri-Monreal",
                "citations": 64
            },
            {
                "title": "Segment Any Anomaly without Training via Hybrid Prompt Regularization",
                "abstract": "We present a novel framework, i.e., Segment Any Anomaly + (SAA+), for zero-shot anomaly segmentation with hybrid prompt regularization to improve the adaptability of modern foundation models. Existing anomaly segmentation models typically rely on domain-specific fine-tuning, limiting their generalization across countless anomaly patterns. In this work, inspired by the great zero-shot generalization ability of foundation models like Segment Anything, we first explore their assembly to leverage diverse multi-modal prior knowledge for anomaly localization. For non-parameter foundation model adaptation to anomaly segmentation, we further introduce hybrid prompts derived from domain expert knowledge and target image context as regularization. Our proposed SAA+ model achieves state-of-the-art performance on several anomaly segmentation benchmarks, including VisA, MVTec-AD, MTD, and KSDD2, in the zero-shot setting. We will release the code at \\href{https://github.com/caoyunkang/Segment-Any-Anomaly}{https://github.com/caoyunkang/Segment-Any-Anomaly}.",
                "authors": "Yunkang Cao, Xiaohao Xu, Chen Sun, Y. Cheng, Zongwei Du, Liang Gao, Weiming Shen",
                "citations": 65
            },
            {
                "title": "A Comprehensive Survey on Segment Anything Model for Vision and Beyond",
                "abstract": "Artificial intelligence (AI) is evolving towards artificial general intelligence, which refers to the ability of an AI system to perform a wide range of tasks and exhibit a level of intelligence similar to that of a human being. This is in contrast to narrow or specialized AI, which is designed to perform specific tasks with a high degree of efficiency. Therefore, it is urgent to design a general class of models, which we term foundation models, trained on broad data that can be adapted to various downstream tasks. The recently proposed segment anything model (SAM) has made significant progress in breaking the boundaries of segmentation, greatly promoting the development of foundation models for computer vision. To fully comprehend SAM, we conduct a survey study. As the first to comprehensively review the progress of segmenting anything task for vision and beyond based on the foundation model of SAM, this work focuses on its applications to various tasks and data types by discussing its historical development, recent progress, and profound impact on broad applications. We first introduce the background and terminology for foundation models including SAM, as well as state-of-the-art methods contemporaneous with SAM that are significant for segmenting anything task. Then, we analyze and summarize the advantages and limitations of SAM across various image processing applications, including software scenes, real-world scenes, and complex scenes. Importantly, many insights are drawn to guide future research to develop more versatile foundation models and improve the architecture of SAM. We also summarize massive other amazing applications of SAM in vision and beyond. Finally, we maintain a continuously updated paper list and an open-source project summary for foundation model SAM at \\href{https://github.com/liliu-avril/Awesome-Segment-Anything}{\\color{magenta}{here}}.",
                "authors": "Chunhui Zhang, Li Liu, Yawen Cui, Guanjie Huang, Weilin Lin, Yiqian Yang, Yuehong Hu",
                "citations": 65
            },
            {
                "title": "Caption Anything: Interactive Image Description with Diverse Multimodal Controls",
                "abstract": "Controllable image captioning is an emerging multimodal topic that aims to describe the image with natural language following human purpose, $\\textit{e.g.}$, looking at the specified regions or telling in a particular text style. State-of-the-art methods are trained on annotated pairs of input controls and output captions. However, the scarcity of such well-annotated multimodal data largely limits their usability and scalability for interactive AI systems. Leveraging unimodal instruction-following foundation models is a promising alternative that benefits from broader sources of data. In this paper, we present Caption AnyThing (CAT), a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality. Powered by Segment Anything Model (SAM) and ChatGPT, we unify the visual and language prompts into a modularized framework, enabling the flexible combination between different controls. Extensive case studies demonstrate the user intention alignment capabilities of our framework, shedding light on effective user interaction modeling in vision-language applications. Our code is publicly available at https://github.com/ttengwang/Caption-Anything.",
                "authors": "Teng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao, Ying Shan, Feng Zheng",
                "citations": 67
            },
            {
                "title": "Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching",
                "abstract": "Powered by large-scale pre-training, vision foundation models exhibit significant potential in open-world image understanding. However, unlike large language models that excel at directly tackling various language tasks, vision foundation models require a task-specific model structure followed by fine-tuning on specific tasks. In this work, we present Matcher, a novel perception paradigm that utilizes off-the-shelf vision foundation models to address various perception tasks. Matcher can segment anything by using an in-context example without training. Additionally, we design three effective components within the Matcher framework to collaborate with these foundation models and unleash their full potential in diverse perception tasks. Matcher demonstrates impressive generalization performance across various segmentation tasks, all without training. For example, it achieves 52.7% mIoU on COCO-20$^i$ with one example, surpassing the state-of-the-art specialist model by 1.6%. In addition, Matcher achieves 33.0% mIoU on the proposed LVIS-92$^i$ for one-shot semantic segmentation, outperforming the state-of-the-art generalist model by 14.4%. Our visualization results further showcase the open-world generality and flexibility of Matcher when applied to images in the wild. Our code can be found at https://github.com/aim-uofa/Matcher.",
                "authors": "Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, Chunhua Shen",
                "citations": 60
            },
            {
                "title": "Language is All a Graph Needs",
                "abstract": "The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data like images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, languages, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is available at https://github.com/agiresearch/InstructGLM.",
                "authors": "Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang",
                "citations": 126
            },
            {
                "title": "Visual Prompt Multi-Modal Tracking",
                "abstract": "Visible-modal object tracking gives rise to a series of downstream multi-modal tracking tributaries. To inherit the powerful representations of the foundation model, a natural modus operandi for multi-modal tracking is full fine-tuning on the RGB-based parameters. Albeit effective, this manner is not optimal due to the scarcity of downstream data and poor transferability, etc. In this paper, inspired by the recent success of the prompt learning in language models, we develop Visual Prompt multi-modal Tracking (ViPT), which learns the modal-relevant prompts to adapt the frozen pre-trained foundation model to various downstream multi-modal tracking tasks. ViPT finds a better way to stimulate the knowledge of the RGB-based model that is pre-trained at scale, meanwhile only introducing a few trainable parameters (less than 1% of model parameters). ViPT outperforms the full fine-tuning paradigm on multiple downstream tracking tasks including RGB+Depth, RGB+Thermal, and RGB+Event tracking. Extensive experiments show the potential of visual prompt learning for multi-modal tracking, and ViPT can achieve state-of-the-art performance while satisfying parameter efficiency. Code and models are available at https://github.com/jiawen-zhu/ViPT.",
                "authors": "Jiawen Zhu, Simiao Lai, Xin Chen, D. Wang, Huchuan Lu",
                "citations": 116
            },
            {
                "title": "BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine",
                "abstract": "Foundation models (FMs) have exhibited remarkable performance across a wide range of downstream tasks in many domains. Nevertheless, general-purpose FMs often face challenges when confronted with domain-specific problems, due to their limited access to the proprietary training data in a particular domain. In biomedicine, there are various biological modalities, such as molecules, proteins, and cells, which are encoded by the language of life and exhibit significant modality gaps with human natural language. In this paper, we introduce BioMedGPT, an open multimodal generative pre-trained transformer (GPT) for biomedicine, to bridge the gap between the language of life and human natural language. BioMedGPT allows users to easily ``communicate'' with diverse biological modalities through free text, which is the first of its kind. BioMedGPT aligns different biological modalities with natural language via a large generative language model, namely, BioMedGPT-LM. We publish BioMedGPT-10B, which unifies the feature spaces of molecules, proteins, and natural language via encoding and alignment. Through fine-tuning, BioMedGPT-10B outperforms or is on par with human and significantly larger general-purpose foundation models on the biomedical QA task. It also demonstrates promising performance in the molecule QA and protein QA tasks, which could greatly accelerate the discovery of new drugs and therapeutic targets. In addition, BioMedGPT-LM-7B is the first large generative language model based on Llama2 in the biomedical domain, therefore is commercial friendly. Both BioMedGPT-10B and BioMedGPT-LM-7B are open-sourced to the research community. In addition, we publish the datasets that are meticulously curated for the alignment of multi-modalities, i.e., PubChemQA and UniProtQA. All the models, codes, and datasets are available at \\url{https://github.com/PharMolix/OpenBioMed}.",
                "authors": "Yi Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, Zaiqing Nie",
                "citations": 59
            },
            {
                "title": "mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections",
                "abstract": "Large-scale pre-trained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from inefficiency and linguistic signal overwhelmed by long visual sequences in cross-modal alignment. To address both problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections.mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, including image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability on vision-language and video-language tasks. The code and pre-trained models are available at https://github.com/alibaba/AliceMind",
                "authors": "Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng-da Cao, Ji Zhang, Songfang Huang, Feiran Huang, Jingren Zhou, Luo Si",
                "citations": 185
            },
            {
                "title": "Offsite-Tuning: Transfer Learning without Full Model",
                "abstract": "Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion-parameter foundation models to downstream data without access to the full model. In offsite-tuning, the model owner sends a light-weight adapter and a lossy compressed emulator to the data owner, who then fine-tunes the adapter on the downstream data with the emulator's assistance. The fine-tuned adapter is then returned to the model owner, who plugs it into the full model to create an adapted foundation model. Offsite-tuning preserves both parties' privacy and is computationally more efficient than the existing fine-tuning methods that require access to the full model weights. We demonstrate the effectiveness of offsite-tuning on various large language and vision foundation models. Offsite-tuning can achieve comparable accuracy as full model fine-tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is available at https://github.com/mit-han-lab/offsite-tuning.",
                "authors": "Guangxuan Xiao, Ji Lin, Song Han",
                "citations": 55
            },
            {
                "title": "VanillaNet: the Power of Minimalism in Deep Learning",
                "abstract": "At the heart of foundation models is the philosophy of\"more is different\", exemplified by the astonishing success in computer vision and natural language processing. However, the challenges of optimization and inherent complexity of transformer models call for a paradigm shift towards simplicity. In this study, we introduce VanillaNet, a neural network architecture that embraces elegance in design. By avoiding high depth, shortcuts, and intricate operations like self-attention, VanillaNet is refreshingly concise yet remarkably powerful. Each layer is carefully crafted to be compact and straightforward, with nonlinear activation functions pruned after training to restore the original architecture. VanillaNet overcomes the challenges of inherent complexity, making it ideal for resource-constrained environments. Its easy-to-understand and highly simplified architecture opens new possibilities for efficient deployment. Extensive experimentation demonstrates that VanillaNet delivers performance on par with renowned deep neural networks and vision transformers, showcasing the power of minimalism in deep learning. This visionary journey of VanillaNet has significant potential to redefine the landscape and challenge the status quo of foundation model, setting a new path for elegant and effective model design. Pre-trained models and codes are available at https://github.com/huawei-noah/VanillaNet and https://gitee.com/mindspore/models/tree/master/research/cv/vanillanet.",
                "authors": "Hanting Chen, Yunhe Wang, Jianyuan Guo, Dacheng Tao",
                "citations": 54
            },
            {
                "title": "SGPT: GPT Sentence Embeddings for Semantic Search",
                "abstract": "Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.",
                "authors": "Niklas Muennighoff",
                "citations": 152
            },
            {
                "title": "Differentially Private Generative Adversarial Network",
                "abstract": "Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.",
                "authors": "Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, Jiayu Zhou",
                "citations": 462
            },
            {
                "title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends",
                "abstract": "This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: ($i$) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and ($iii$) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.",
                "authors": "Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao",
                "citations": 141
            },
            {
                "title": "The effectiveness of MAE pre-pretraining for billion-scale pretraining",
                "abstract": "This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on Food-101 (96.2%). Our study reveals that model initialization plays a significant role, even for web-scale pretraining with billions of images.",
                "authors": "Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron B. Adcock, Armand Joulin, Piotr Doll'ar, Christoph Feichtenhofer, Ross B. Girshick, Rohit Girdhar, Ishan Misra",
                "citations": 47
            },
            {
                "title": "Weakly Supervised 3D Open-vocabulary Segmentation",
                "abstract": "Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \\url{https://github.com/Kunhao-Liu/3D-OVS}.",
                "authors": "Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, A. E. Saddik, C. Theobalt, Eric P. Xing, Shijian Lu",
                "citations": 48
            },
            {
                "title": "PLA: Language-Driven Open-Vocabulary 3D Scene Understanding",
                "abstract": "Open-vocabulary scene understanding aims to localize and recognize unseen categories beyond the annotated label space. The recent breakthrough of 2D open-vocabulary perception is largely driven by Internet-scale paired image-text data with rich vocabulary concepts. However, this success cannot be directly transferred to 3D scenarios due to the inaccessibility of large-scale 3D-text pairs. To this end, we propose to distill knowledge encoded in pretrained vision-language (VL) foundation models through captioning multi-view images from 3D, which allows explicitly associating 3D and semantic-rich captions. Further, to foster coarse-to-fine visual-semantic representation learning from captions, we design hierarchical 3D-caption pairs, leveraging geometric constraints between 3D scenes and multi-view images. Finally, by employing contrastive learning, the model learns language-aware embeddings that connect 3D and text for open-vocabulary tasks. Our method not only remarkably outperforms baseline methods by 25.8% ~ 44.7% hIoU and 14.5% ~ 50.4% hAP50 in open-vocabulary semantic and instance segmentation, but also shows robust transferability on challenging zero-shot domain transfer tasks. See the project website at https://dingry.github.io/projects/PLA.",
                "authors": "Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, Xiaojuan Qi",
                "citations": 123
            },
            {
                "title": "TimeGPT-1",
                "abstract": "In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.",
                "authors": "Azul Garza, Cristian Challu, Max Mergenthaler-Canseco",
                "citations": 71
            },
            {
                "title": "Deep Model Fusion: A Survey",
                "abstract": "Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1)\"Mode connectivity\", which connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion; (2)\"Alignment\"matches units between neural networks to create better conditions for fusion; (3)\"Weight average\", a classical model fusion method, averages the weights of multiple models to obtain more accurate results closer to the optimal solution; (4)\"Ensemble learning\"combines the outputs of diverse models, which is a foundational technique for improving the accuracy and robustness of the final model. In addition, we analyze the challenges faced by deep model fusion and propose possible research directions for model fusion in the future. Our review is helpful in deeply understanding the correlation between different model fusion methods and practical application methods, which can enlighten the research in the field of deep model fusion.",
                "authors": "Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, Li Shen",
                "citations": 42
            },
            {
                "title": "How Segment Anything Model (SAM) Boost Medical Image Segmentation?",
                "abstract": "Due to the flexibility of prompting, foundation models have become the dominant force in the domains of natural language processing and image generation. With the recent introduction of the Segment Anything Model (SAM), the prompt-driven paradigm has entered the realm of image segmentation, bringing with a range of previously unexplored capabilities. However, it remains unclear whether it can be applicable to medical image segmentation due to the significant differences between natural images and medical images. In this work, we summarize recent efforts to extend the success of SAM to medical image segmentation tasks, including both empirical benchmarking and methodological adaptations, and discuss potential future directions for SAM in medical image segmentation. Although directly applying SAM to medical image segmentation cannot obtain satisfying performance on multi-modal and multi-target medical datasets, many insights are drawn to guide future research to develop foundation models for medical image analysis. We also set up a continuously updated paper list and open-source project summary to boost the research on this topic at https://github.com/YichiZhang98/SAM4MIS.",
                "authors": "Yichi Zhang, Rushi Jiao",
                "citations": 38
            },
            {
                "title": "Functional and Biomimetic Materials for Engineering of the Three-Dimensional Cell Microenvironment.",
                "abstract": "The cell microenvironment has emerged as a key determinant of cell behavior and function in development, physiology, and pathophysiology. The extracellular matrix (ECM) within the cell microenvironment serves not only as a structural foundation for cells but also as a source of three-dimensional (3D) biochemical and biophysical cues that trigger and regulate cell behaviors. Increasing evidence suggests that the 3D character of the microenvironment is required for development of many critical cell responses observed in vivo, fueling a surge in the development of functional and biomimetic materials for engineering the 3D cell microenvironment. Progress in the design of such materials has improved control of cell behaviors in 3D and advanced the fields of tissue regeneration, in vitro tissue models, large-scale cell differentiation, immunotherapy, and gene therapy. However, the field is still in its infancy, and discoveries about the nature of cell-microenvironment interactions continue to overturn much early progress in the field. Key challenges continue to be dissecting the roles of chemistry, structure, mechanics, and electrophysiology in the cell microenvironment, and understanding and harnessing the roles of periodicity and drift in these factors. This review encapsulates where recent advances appear to leave the ever-shifting state of the art, and it highlights areas in which substantial potential and uncertainty remain.",
                "authors": "Guoyou Huang, Fei Li, Xin Zhao, Yufei Ma, Yuhui Li, Min Lin, Guorui Jin, T. Lu, G. Genin, Feng Xu",
                "citations": 578
            },
            {
                "title": "The Future of Management: DAO to Smart Organizations and Intelligent Operations",
                "abstract": "In the future, management in smart societies will revolve around knowledge workers and the works they produce. This article is committed to explore new management framework, model, paradigm, and solution for organizing, managing, and measuring knowledge works. First, the parallel management framework is presented that would allow for the virtual-real interactions of humans in social space, robots in physical space, and digital humans in cyberspace to realize descriptive, predictive, and prescriptive intelligence for management. Then, the management foundation models are proposed by fusing scenarios engineering with artificial intelligence foundation models and cyber–physical-social systems. Moreover, the new management paradigm driven by decentralized autonomous organizations and operations is formulated for the advancement of smart organizations and intelligent operations. On these basis, the management operating systems that highlight features of simple intelligence, provable security, flexible scalability, and ecological harmony are finally put forward as new management solution.",
                "authors": "Juanjuan Li, Rui Qin, Fei-Yue Wang",
                "citations": 35
            },
            {
                "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
                "abstract": "We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for various computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform diverse tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with un-precedented zero-shot and fine-tuning capabilities.",
                "authors": "Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan",
                "citations": 61
            },
            {
                "title": "SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model",
                "abstract": null,
                "authors": "Dingyuan Zhang, Dingkang Liang, Hongcheng Yang, Zhikang Zou, Xiaoqing Ye, Zhe Liu, Xiang Bai",
                "citations": 33
            },
            {
                "title": "Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains",
                "abstract": "The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e., prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, Prompt Sapper, which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper.",
                "authors": "Yu Cheng, Jieshan Chen, Qing Huang, Zhenchang Xing, Xiwei Xu, Qinghua Lu",
                "citations": 34
            },
            {
                "title": "Influence Maximization on Social Graphs: A Survey",
                "abstract": "Influence Maximization (IM), which selects a set of <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math> <alternatives><inline-graphic xlink:href=\"li-ieq1-2807843.gif\"/></alternatives></inline-formula> users (called seed set) from a social network to maximize the expected number of influenced users (called influence spread), is a key algorithmic problem in social influence analysis. Due to its immense application potential and enormous technical challenges, IM has been extensively studied in the past decade. In this paper, we survey and synthesize a wide spectrum of existing studies on IM from an <italic>algorithmic perspective</italic>, with a special focus on the following key aspects: (1) a review of well-accepted diffusion models that capture the information diffusion process and build the foundation of the IM problem, (2) a fine-grained taxonomy to classify existing IM algorithms based on their design objectives, (3) a rigorous theoretical comparison of existing IM algorithms, and (4) a comprehensive study on the applications of IM techniques in combining with novel context features of social networks such as topic, location, and time. Based on this analysis, we then outline the key challenges and research directions to expand the boundary of IM research.",
                "authors": "Yuchen Li, Ju Fan, Yanhao Wang, K. Tan",
                "citations": 476
            },
            {
                "title": "Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks",
                "abstract": "The rapid development of AI systems has been greatly influenced by the emergence of foundation models. A common approach for targeted problems involves fine-tuning these pre-trained foundation models for specific target tasks, resulting in a rapid spread of models fine-tuned across a diverse array of tasks. This work focuses on the problem of merging multiple fine-tunings of the same foundation model derived from a spectrum of auxiliary tasks. We introduce a new simple method, Model Breadcrumbs, which consists of a sparsely defined weight set that guides model adaptation within the weight space of a pre-trained model. These breadcrumbs are constructed by subtracting the weights from a pre-trained model before and after fine-tuning, followed by a sparsification process that eliminates weight outliers and negligible perturbations. Our experiments demonstrate the effectiveness of Model Breadcrumbs to simultaneously improve performance across multiple tasks. This contribution aligns with the evolving paradigm of updatable machine learning, reminiscent of the collaborative principles underlying open-source software development, fostering a community-driven effort to reliably update machine learning models. Our method is shown to be more efficient and unlike previous proposals does not require hyperparameter tuning for each new task added. Through extensive experimentation involving various models, tasks, and modalities we establish that integrating Model Breadcrumbs offers a simple, efficient, and highly effective approach for constructing multi-task models and facilitating updates to foundation models.",
                "authors": "Mohammad-Javad Davari, Eugene Belilovsky",
                "citations": 29
            },
            {
                "title": "Segment Anything for Microscopy",
                "abstract": "We present Segment Anything for Microscopy, a tool for interactive and automatic segmentation and tracking of objects in multi-dimensional microscopy data. Our method is based on Segment Anything, a vision foundation model for image segmentation. We extend it by training specialized models for microscopy data that significantly improve segmentation quality for a wide range of imaging conditions. We also implement annotation tools for interactive (volumetric) segmentation and tracking, that speed up data annotation significantly compared to established tools. Our work constitutes the first application of vision foundation models to microscopy, laying the groundwork for solving image analysis problems in these domains with a small set of powerful deep learning architectures.",
                "authors": "Anwai Archit, Sushmita Nair, Nabeel Khalid, Paul Hilt, Vikas Rajashekar, Marei Freitag, Sagnik Gupta, A. Dengel, Sheraz Ahmed, Constantin Pape",
                "citations": 33
            },
            {
                "title": "MITRE ATT&CK ® : Design and Philosophy",
                "abstract": "MITRE ATT&CK is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations. The ATT&CK knowledge base is used as a foundation for the development of specific threat models and methodologies in the private sector, in government, and in the cybersecurity product and service community. ATT&CK provides a common taxonomy for both offense and defense, and has become a useful conceptual tool across many cyber security disciplines to convey threat intelligence, perform testing through red teaming or adversary emulation, and improve network and system defenses against intrusions. The process MITRE used to create ATT&CK, and the philosophy that has developed for curating new content, are critical aspects of the work and are useful for other efforts that strive to create similar adversary models and information repositories.",
                "authors": "Blake E. Strom, A. Applebaum, Doug Miller, Kathryn C. Nickels, Adam G. Pennington, Cody Thomas",
                "citations": 341
            },
            {
                "title": "PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting",
                "abstract": "This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes three real-world forecasting scenarios. We evaluate different SOTA numerical-based forecasting methods and language generation models. The benchmark results with various forecasting settings demonstrate the proposed PromptCast with language generation models is a promising research direction. Additionally, in comparison to conventional numerical-based forecasting, PromptCast shows a much better generalization ability under the zero-shot setting.",
                "authors": "Hao Xue, Flora D.Salim",
                "citations": 95
            },
            {
                "title": "FaceScape: A Large-Scale High Quality 3D Face Dataset and Detailed Riggable 3D Face Prediction",
                "abstract": "In this paper, we present a large-scale detailed 3D face dataset, FaceScape, and propose a novel algorithm that is able to predict elaborate riggable 3D face models from a single image input. FaceScape dataset provides 18,760 textured 3D faces, captured from 938 subjects and each with 20 specific expressions. The 3D models contain the pore-level facial geometry that is also processed to be topologically uniformed. These fine 3D facial models can be represented as a 3D morphable model for rough shapes and displacement maps for detailed geometry. Taking advantage of the large-scale and high-accuracy dataset, a novel algorithm is further proposed to learn the expression-specific dynamic details using a deep neural network. The learned relationship serves as the foundation of our 3D face prediction system from a single image input. Different than the previous methods, our predicted 3D models are riggable with highly detailed geometry under different expressions. The unprecedented dataset and code will be released to public for research purpose.",
                "authors": "Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, Xun Cao",
                "citations": 262
            },
            {
                "title": "Deep Bidirectional Language-Knowledge Graph Pretraining",
                "abstract": "Pretraining a language model (LM) on text has been shown to help various downstream NLP tasks. Recent works show that a knowledge graph (KG) can complement text data, offering structured background knowledge that provides a useful scaffold for reasoning. However, these works are not pretrained to learn a deep fusion of the two modalities at scale, limiting the potential to acquire fully joint representations of text and KG. Here we propose DRAGON (Deep Bidirectional Language-Knowledge Graph Pretraining), a self-supervised approach to pretraining a deeply joint language-knowledge foundation model from text and KG at scale. Specifically, our model takes pairs of text segments and relevant KG subgraphs as input and bidirectionally fuses information from both modalities. We pretrain this model by unifying two self-supervised reasoning tasks, masked language modeling and KG link prediction. DRAGON outperforms existing LM and LM+KG models on diverse downstream tasks including question answering across general and biomedical domains, with +5% absolute gain on average. In particular, DRAGON achieves notable performance on complex reasoning about language and knowledge (+10% on questions involving long contexts or multi-step reasoning) and low-resource QA (+8% on OBQA and RiddleSense), and new state-of-the-art results on various BioNLP tasks. Our code and trained models are available at https://github.com/michiyasunaga/dragon.",
                "authors": "Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy Liang, J. Leskovec",
                "citations": 171
            },
            {
                "title": "Language Processing",
                "abstract": null,
                "authors": "Manning Publications, Gloria Lukos, Jacek Majchrzak, Sven Balnojan, Marian Siwiak, Mariusz Sieraczkiewicz, Sinan Ozdemir, Ekaterina Kochmar, Luis Serrano, Edward Raff",
                "citations": 353
            },
            {
                "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
                "abstract": "Instruction tuning has become the de facto method to equip large language models (LLMs) with the ability of following user instructions. Usually, hundreds of thousands or millions of instruction-following pairs are employed to fine-tune the foundation LLMs. Recently, some studies show that a small number of high-quality instruction data is enough. However, how to select appropriate instruction data for a given LLM is still an open problem. To address this problem, in this paper we present a model-oriented data selection (MoDS) approach, which selects instruction data based on a new criteria considering three aspects: quality, coverage and necessity. First, our approach utilizes a quality evaluation model to filter out the high-quality subset from the original instruction dataset, and then designs an algorithm to further select from the high-quality subset a seed instruction dataset with good coverage. The seed dataset is applied to fine-tune the foundation LLM to obtain an initial instruction-following LLM. Finally, we develop a necessity evaluation model to find out the instruction data which are performed badly in the initial instruction-following LLM and consider them necessary instructions to further improve the LLMs. In this way, we can get a small high-quality, broad-coverage and high-necessity subset from the original instruction datasets. Experimental results show that, the model fine-tuned with 4,000 instruction pairs selected by our approach could perform better than the model fine-tuned with the full original dataset which includes 214k instruction data.",
                "authors": "Qianlong Du, Chengqing Zong, Jiajun Zhang",
                "citations": 55
            },
            {
                "title": "The DOE E3SM Coupled Model Version 1: Overview and Evaluation at Standard Resolution",
                "abstract": "This work documents the first version of the U.S. Department of Energy (DOE) new Energy Exascale Earth System Model (E3SMv1). We focus on the standard resolution of the fully coupled physical model designed to address DOE mission‐relevant water cycle questions. Its components include atmosphere and land (110‐km grid spacing), ocean and sea ice (60 km in the midlatitudes and 30 km at the equator and poles), and river transport (55 km) models. This base configuration will also serve as a foundation for additional configurations exploring higher horizontal resolution as well as augmented capabilities in the form of biogeochemistry and cryosphere configurations. The performance of E3SMv1 is evaluated by means of a standard set of Coupled Model Intercomparison Project Phase 6 (CMIP6) Diagnosis, Evaluation, and Characterization of Klima simulations consisting of a long preindustrial control, historical simulations (ensembles of fully coupled and prescribed SSTs) as well as idealized CO2 forcing simulations. The model performs well overall with biases typical of other CMIP‐class models, although the simulated Atlantic Meridional Overturning Circulation is weaker than many CMIP‐class models. While the E3SMv1 historical ensemble captures the bulk of the observed warming between preindustrial (1850) and present day, the trajectory of the warming diverges from observations in the second half of the twentieth century with a period of delayed warming followed by an excessive warming trend. Using a two‐layer energy balance model, we attribute this divergence to the model's strong aerosol‐related effective radiative forcing (ERFari+aci = −1.65 W/m2) and high equilibrium climate sensitivity (ECS = 5.3 K).",
                "authors": "J. Golaz, P. Caldwell, Luke P. van Roekel, M. Petersen, Q. Tang, J. Wolfe, G. Abeshu, V. Anantharaj, X. Asay-Davis, D. Bader, Sterling Baldwin, G. Bisht, P. Bogenschutz, M. Branstetter, M. Brunke, S. Brus, S. Burrows, P. Cameron-smith, A. S. Donahue, Michael Deakin, R. Easter, K. Evans, Yan Feng, M. Flanner, J. Foucar, J. Fyke, Brian M. Griffin, C. Hannay, B. Harrop, Mattthew J. Hoffman, E. Hunke, R. Jacob, D. Jacobsen, N. Jeffery, Philip W. Jones, N. Keen, S. Klein, V. Larson, L. Leung, Hong‐Yi Li, Wuyin Lin, W. Lipscomb, P. Ma, S. Mahajan, M. Maltrud, A. Mametjanov, J. McClean, R. McCoy, R. Neale, S. Price, Y. Qian, P. Rasch, J. R. Reeves Eyre, W. Riley, T. Ringler, A. F. Roberts, E. Roesler, A. Salinger, Z. Shaheen, Xiaoying Shi, Balwinder Singh, Jinyun Tang, M. Taylor, P. Thornton, A. Turner, M. Veneziani, H. Wan, Hailong Wang, Shanlin Wang, Dean N. Williams, P. Wolfram, P. Worley, S. Xie, Yang Yang, Jinho Yoon, M. Zelinka, C. Zender, X. Zeng, Chengzhu Zhang, Kai Zhang, Yuying Zhang, Xue Zheng, Tian Zhou, Qing Zhu",
                "citations": 482
            },
            {
                "title": "Human Language Understanding & Reasoning",
                "abstract": "Abstract The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.",
                "authors": "Christopher D. Manning",
                "citations": 91
            },
            {
                "title": "SAM-Path: A Segment Anything Model for Semantic Segmentation in Digital Pathology",
                "abstract": "Semantic segmentations of pathological entities have crucial clinical value in computational pathology workflows. Foundation models, such as the Segment Anything Model (SAM), have been recently proposed for universal use in segmentation tasks. SAM shows remarkable promise in instance segmentation on natural images. However, the applicability of SAM to computational pathology tasks is limited due to the following factors: (1) lack of comprehensive pathology datasets used in SAM training and (2) the design of SAM is not inherently optimized for semantic segmentation tasks. In this work, we adapt SAM for semantic segmentation by introducing trainable class prompts, followed by further enhancements through the incorporation of a pathology encoder, specifically a pathology foundation model. Our framework, SAM-Path enhances SAM's ability to conduct semantic segmentation in digital pathology without human input prompts. Through experiments on two public pathology datasets, the BCSS and the CRAG datasets, we demonstrate that the fine-tuning with trainable class prompts outperforms vanilla SAM with manual prompts and post-processing by 27.52% in Dice score and 71.63% in IOU. On these two datasets, the proposed additional pathology foundation model further achieves a relative improvement of 5.07% to 5.12% in Dice score and 4.50% to 8.48% in IOU.",
                "authors": "Jingwei Zhang, Ke Ma, S. Kapse, J. Saltz, M. Vakalopoulou, P. Prasanna, D. Samaras",
                "citations": 23
            },
            {
                "title": "Backpropagation through the Void: Optimizing control variates for black-box gradient estimation",
                "abstract": "Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables. Our method uses gradients of a neural network trained jointly with model parameters or policies, and is applicable in both discrete and continuous settings. We demonstrate this framework for training discrete latent-variable models. We also give an unbiased, action-conditional extension of the advantage actor-critic reinforcement learning algorithm.",
                "authors": "Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, D. Duvenaud",
                "citations": 294
            },
            {
                "title": "Integrating Physics-Based Modeling with Machine Learning: A Survey",
                "abstract": "In this manuscript, we provide a structured and comprehensive overview of techniques to integrate machine learning with physics-based modeling. First, we provide a summary of application areas for which these approaches have been applied. Then, we describe classes of methodologies used to construct physics-guided machine learning models and hybrid physics-machine learning frameworks from a machine learning standpoint. With this foundation, we then provide a systematic organization of these existing techniques and discuss ideas for future research.",
                "authors": "J. Willard, X. Jia, Shaoming Xu, M. Steinbach, Vipin Kumar",
                "citations": 318
            },
            {
                "title": "Automated Machine Learning: From Principles to Practices",
                "abstract": "Machine learning (ML) methods have been developing rapidly, but configuring and selecting proper methods to achieve a desired performance is increasingly difficult and tedious. To address this challenge, automated machine learning (AutoML) has emerged, which aims to generate satisfactory ML configurations for given tasks in a data-driven way. In this paper, we provide a comprehensive survey on this topic. We begin with the formal definition of AutoML and then introduce its principles, including the bi-level learning objective, the learning strategy, and the theoretical interpretation. Then, we summarize the AutoML practices by setting up the taxonomy of existing works based on three main factors: the search space, the search algorithm, and the evaluation strategy. Each category is also explained with the representative methods. Then, we illustrate the principles and practices with exemplary applications from configuring ML pipeline, one-shot neural architecture search, and integration with foundation models. Finally, we highlight the emerging directions of AutoML and conclude the survey.",
                "authors": "Quanming Yao, Mengshuo Wang, Hugo Jair Escalante, Isabelle M Guyon, Yi-Qi Hu, Yu-Feng Li, Wei-Wei Tu, Qiang Yang, Yang Yu",
                "citations": 257
            },
            {
                "title": "Self-Supervised Representations in Speech-Based Depression Detection",
                "abstract": "This paper proposes handling training data sparsity in speech-based automatic depression detection (SDD) using foundation models pre-trained with self-supervised learning (SSL). An analysis of SSL representations derived from different layers of pre-trained foundation models is first presented for SDD, which provides insight to suitable indicator for depression detection. Knowledge transfer is then performed from automatic speech recognition (ASR) and emotion recognition to SDD by fine-tuning the foundation models. Results show that the uses of oracle and ASR transcriptions yield similar SDD performance when the hidden representations of the ASR model is incorporated along with the ASR textual information. By integrating representations from multiple foundation models, state-of-the-art SDD results based on real ASR were achieved on the DAIC-WOZ dataset.",
                "authors": "Wen Wu, C. Zhang, P. Woodland",
                "citations": 20
            },
            {
                "title": "Blockchain technology innovations",
                "abstract": "Digital world has produced efficiencies, new innovative products, and close customer relationships globally by the effective use of mobile, IoT (Internet of Things), social media, analytics and cloud technology to generate models for better decisions. Blockchain is recently introduced and revolutionizing the digital world bringing a new perspective to security, resiliency and efficiency of systems. While initially popularized by Bitcoin, Blockchain is much more than a foundation for crypto currency. It offers a secure way to exchange any kind of good, service, or transaction. Industrial growth increasingly depends on trusted partnerships; but increasing regulation, cybercrime and fraud are inhibiting expansion. To address these challenges, Blockchain will enable more agile value chains, faster product innovations, closer customer relationships, and quicker integration with the IoT and cloud technology. Further Blockchain provides a lower cost of trade with a trusted contract monitored without intervention from third parties who may not add direct value. It facilitates smart contracts, engagements, and agreements with inherent, robust cyber security features. This paper is an effort to break the ground for presenting and demonstrating the use of Blockchain technology in multiple industrial applications. A healthcare industry application, Healthchain, is formalized and developed on the foundation of Blockchain using IBM Blockchain initiative. The concepts are transferable to a wide range of industries as finance, government and manufacturing where security, scalability and efficiency must meet.",
                "authors": "T. Ahram, A. Sargolzaei, S. Sargolzaei, Jeff Daniels, Ben A. Amaba",
                "citations": 423
            },
            {
                "title": "The Wayback Machine",
                "abstract": "abstract:We suffer from a radical autonomy which too often collapses the therapeutic alliance between patient and physician into a health-care transaction between consumer and provider, a fee-for-service exchange for something far short of true health. Some ethicists and physicians are seeking a better way, by employing a virtue ethics approach in which health is seen as a distinct good and the proper end of a medical encounter. Curlin and Tollefsen's The Way of Medicine (2021) synthesizes this material into a heuristic contrasting what they characterize as the Provider Services Model and the Way of Medicine. The authors believe physicians must choose between the two models and serve, respectively, either the well-being or the health of the people they meet as patients. Between the authors' dichotomous choices, many physicians will find a middle way in virtue ethics approaches, which instead characterize health as a communal foundation to human flourishing and autonomy as serving communal as well as individual goods.",
                "authors": "Abraham M. Nussbaum",
                "citations": 126
            },
            {
                "title": "Neuroscience of apathy and anhedonia: a transdiagnostic approach",
                "abstract": null,
                "authors": "M. Husain, J. Roiser",
                "citations": 410
            },
            {
                "title": "Elements of ∞-Category Theory",
                "abstract": "The language of\n ∞-categories provides an insightful new way of expressing many\n results in higher-dimensional mathematics but can be challenging\n for the uninitiated. To explain what exactly an ∞-category is\n requires various technical models, raising the question of how\n they might be compared. To overcome this, a model-independent\n approach is desired, so that theorems proven with any model\n would apply to them all. This text develops the theory of\n ∞-categories from first principles in a model-independent\n fashion using the axiomatic framework of an ∞-cosmos, the\n universe in which ∞-categories live as objects. An ∞-cosmos is a\n fertile setting for the formal category theory of ∞-categories,\n and in this way the foundational proofs in ∞-category theory\n closely resemble the classical foundations of ordinary category\n theory. Equipped with exercises and appendices with background\n material, this first introduction is meant for students and\n researchers who have a strong foundation in classical 1-category\n theory.",
                "authors": "E. Riehl, Dominic R. Verity",
                "citations": 94
            },
            {
                "title": "Contrastive Learning Inverts the Data Generating Process",
                "abstract": "Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.",
                "authors": "Roland S. Zimmermann, Yash Sharma, Steffen Schneider, M. Bethge, Wieland Brendel",
                "citations": 186
            },
            {
                "title": "Restricting the Flow: Information Bottlenecks for Attribution",
                "abstract": "Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work we adapt the information bottleneck concept for attribution. By adding noise to intermediate feature maps we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method’s information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not required for the network's decision.",
                "authors": "Karl Schulz, Leon Sixt, Federico Tombari, Tim Landgraf",
                "citations": 172
            },
            {
                "title": "Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks",
                "abstract": "Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss benefits and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.",
                "authors": "Paul Röttger, Bertie Vidgen, Dirk Hovy, J. Pierrehumbert",
                "citations": 141
            },
            {
                "title": "Genome graphs and the evolution of genome inference",
                "abstract": "The human reference genome is part of the foundation of modern human biology, and a monumental scientific achievement. However, because it excludes a great deal of common human variation, it introduces a pervasive reference bias into the field of human genomics. To reduce this bias, it makes sense to draw on representative collections of human genomes, brought together into reference cohorts. There are a number of techniques to represent and organize data gleaned from these cohorts, many using ideas implicitly or explicitly borrowed from graph based models. Here, we survey various projects underway to build and apply these graph based structures—which we collectively refer to as genome graphs—and discuss the improvements in read mapping, variant calling, and haplotype determination that genome graphs are expected to produce.",
                "authors": "B. Paten, Adam M. Novak, Jordan M. Eizenga, Garrison Erik",
                "citations": 266
            },
            {
                "title": "Organ-on-a-chip devices advance to market.",
                "abstract": "To curb the high cost of drug development, there is an urgent need to develop more predictive tissue models using human cells to determine drug efficacy and safety in advance of clinical testing. Recent insights gained through fundamental biological studies have validated the importance of dynamic cell environments and cellular communication to the expression of high fidelity organ function. Building on this knowledge, emerging organ-on-a-chip technology is poised to fill the gaps in drug screening by offering predictive human tissue models with methods of sophisticated tissue assembly. Organ-on-a-chip start-ups have begun to spawn from academic research to fill this commercial space and are attracting investment to transform the drug discovery industry. This review traces the history, examines the scientific foundation and envisages the prospect of these renowned organ-on-a-chip technologies. It serves as a guide for new members of this dynamic field to navigate the existing scientific and market space.",
                "authors": "Boyang Zhang, M. Radišić",
                "citations": 282
            },
            {
                "title": "On the Role of Neural Collapse in Transfer Learning",
                "abstract": "We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. In this paper we provide an explanation for this behavior based on the recently observed phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. We demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and -- more importantly -- to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting.",
                "authors": "Tomer Galanti, Andr'as Gyorgy, Marcus Hutter",
                "citations": 78
            },
            {
                "title": "Size-dependent transverse and longitudinal vibrations of embedded carbon and silica carbide nanotubes by nonlocal finite element method",
                "abstract": null,
                "authors": "Ö. Civalek, Büşra Uzun, M. Yaylı, B. Akgöz",
                "citations": 171
            },
            {
                "title": "The Hybrid High-Order Method for Polytopal Meshes",
                "abstract": null,
                "authors": "D. D. Pietro, J. Droniou",
                "citations": 163
            },
            {
                "title": "Elastic and viscoelastic foundations: a review on linear and nonlinear vibration modeling and applications",
                "abstract": null,
                "authors": "D. Younesian, A. Hosseinkhani, H. Askari, E. Esmailzadeh",
                "citations": 135
            },
            {
                "title": "License to evaluate: preparing learning analytics dashboards for educational practice",
                "abstract": "Learning analytics can bridge the gap between learning sciences and data analytics, leveraging the expertise of both fields in exploring the vast amount of data generated in online learning environments. A typical learning analytics intervention is the learning dashboard, a visualisation tool built with the purpose of empowering teachers and learners to make informed decisions about the learning process. Related work has investigated learning dashboards, yet none have explored the theoretical foundation that should inform the design and evaluation of such interventions. In this systematic literature review, we analyse the extent to which theories and models from learning sciences have been integrated into the development of learning dashboards aimed at learners. Our analysis revealed that very few dashboard evaluations take into account the educational concepts that were used as a theoretical foundation for their design. Furthermore, we report findings suggesting that comparison with peers, a common reference frame for contextualising information on learning analytics dashboards, was not perceived positively by all learners. We summarise the insights gathered through our literature review in a set of recommendations for the design and evaluation of learning analytics dashboards for learners.",
                "authors": "I. Jivet, Maren Scheffel, M. Specht, H. Drachsler",
                "citations": 234
            },
            {
                "title": "Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning",
                "abstract": "People say,\"A picture is worth a thousand words\". Then how can we get the rich information out of the image? We argue that by using visual clues to bridge large pretrained vision foundation models and language models, we can do so without any extra cross-modal training. Thanks to the strong zero-shot capability of foundation models, we start by constructing a rich semantic representation of the image (e.g., image tags, object attributes / locations, captions) as a structured textual prompt, called visual clues, using a vision foundation model. Based on visual clues, we use large language model to produce a series of comprehensive descriptions for the visual content, which is then verified by the vision model again to select the candidate that aligns best with the image. We evaluate the quality of generated descriptions by quantitative and qualitative measurement. The results demonstrate the effectiveness of such a structured semantic representation.",
                "authors": "Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, Nguyen Bach, Ce Liu, Michael Zeng",
                "citations": 26
            },
            {
                "title": "Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability",
                "abstract": "Explaining AI systems is fundamental both to the development of high performing models and to the trust placed in them by their users. The Shapley framework for explainability has strength in its general applicability combined with its precise, rigorous foundation: it provides a common, model-agnostic language for AI explainability and uniquely satisfies a set of intuitive mathematical axioms. However, Shapley values are too restrictive in one significant regard: they ignore all causal structure in the data. We introduce a less restrictive framework, Asymmetric Shapley values (ASVs), which are rigorously founded on a set of axioms, applicable to any AI system, and flexible enough to incorporate any causal structure known to be respected by the data. We demonstrate that ASVs can (i) improve model explanations by incorporating causal information, (ii) provide an unambiguous test for unfair discrimination in model predictions, (iii) enable sequentially incremental explanations in time-series models, and (iv) support feature-selection studies without the need for model retraining.",
                "authors": "Christopher Frye, Ilya Feige, C. Rowat",
                "citations": 168
            },
            {
                "title": "NeuralNetTools: Visualization and Analysis Tools for Neural Networks.",
                "abstract": "Supervised neural networks have been applied as a machine learning technique to identify and predict emergent patterns among multiple variables. A common criticism of these methods is the inability to characterize relationships among variables from a fitted model. Although several techniques have been proposed to \"illuminate the black box\", they have not been made available in an open-source programming environment. This article describes the NeuralNetTools package that can be used for the interpretation of supervised neural network models created in R. Functions in the package can be used to visualize a model using a neural network interpretation diagram, evaluate variable importance by disaggregating the model weights, and perform a sensitivity analysis of the response variables to changes in the input variables. Methods are provided for objects from many of the common neural network packages in R, including caret, neuralnet, nnet, and RSNNS. The article provides a brief overview of the theoretical foundation of neural networks, a description of the package structure and functions, and an applied example to provide a context for model development with NeuralNetTools. Overall, the package provides a toolset for neural networks that complements existing quantitative techniques for data-intensive exploration.",
                "authors": "M. Beck",
                "citations": 214
            },
            {
                "title": "Shoring Up the Foundations: Fusing Model Embeddings and Weak Supervision",
                "abstract": "Foundation models offer an exciting new paradigm for constructing models with out-of-the-box embeddings and a few labeled examples. However, it is not clear how to best apply foundation models without labeled data. A potential approach is to fuse foundation models with weak supervision frameworks, which use weak label sources -- pre-trained models, heuristics, crowd-workers -- to construct pseudolabels. The challenge is building a combination that best exploits the signal available in both foundation models and weak sources. We propose Liger, a combination that uses foundation model embeddings to improve two crucial elements of existing weak supervision techniques. First, we produce finer estimates of weak source quality by partitioning the embedding space and learning per-part source accuracies. Second, we improve source coverage by extending source votes in embedding space. Despite the black-box nature of foundation models, we prove results characterizing how our approach improves performance and show that lift scales with the smoothness of label distributions in embedding space. On six benchmark NLP and video tasks, Liger outperforms vanilla weak supervision by 14.1 points, weakly-supervised kNN and adapters by 11.8 points, and kNN and adapters supervised by traditional hand labels by 7.2 points.",
                "authors": "Mayee F. Chen, Daniel Y. Fu, Dyah Adila, Michael Zhang, Frederic Sala, Kayvon Fatahalian, Christopher R'e",
                "citations": 20
            },
            {
                "title": "Buckling and dynamic behavior of the simply supported CNT-RC beams using an integral-first shear deformation theory",
                "abstract": "In this work, the buckling and vibrational behavior of the composite beam armed with single-walled carbonnanotubes (SW-CNT) resting on Winkler-Pasternak elastic foundation are investigated. The CNT-RC beam is modeled by anovel integral first order shear deformation theory. The current theory contains three variables and uses the shear correctionfactors. The equivalent properties of the CNT-RC beam are computed using the mixture rule. The equations of motion arederived and resolved by Applying the Hamilton’s principle and Navier solution on the current model. The accuracy of thecurrent model is verified by comparison studies with others models found in the literature. Also, several parametric studies andtheir discussions are presented.",
                "authors": "A. A. Bousahla, F. Bourada, S. R. Mahmoud, A. Tounsi, A. Algarni, E. A. Bedia, A. Tounsi",
                "citations": 136
            },
            {
                "title": "High-throughput annotation of full-length long noncoding RNAs with Capture Long-Read Sequencing",
                "abstract": null,
                "authors": "Julien Lagarde, Barbara Uszczynska-Ratajczak, Silvia Carbonell, Sílvia Pérez-Lluch, A. Abad, Carrie A. Davis, T. Gingeras, A. Frankish, J. Harrow, R. Guigó, Rory Johnson",
                "citations": 212
            },
            {
                "title": "The Age‐Friendly Health System Imperative",
                "abstract": "The unprecedented changes happening in the American healthcare system have many on high alert as they try to anticipate legislative actions. Significant efforts to move from volume to value, along with changing incentives and alternative payment models, will affect practice and the health system budget. In tandem, growth in the population aged 65 and older is celebratory and daunting. The John A. Hartford Foundation is partnering with the Institute for Healthcare Improvement to envision an age‐friendly health system of the future. Our current prototyping for new ways of addressing the complex and interrelated needs of older adults provides great promise for a more‐effective, patient‐directed, safer healthcare system. Proactive models that address potential health needs, prevent avoidable harms, and improve care of people with complex needs are essential. The robust engagement of family caregivers, along with an appreciation for the value of excellent communication across care settings, is at the heart of our work. Five early‐adopter health systems are testing the prototypes with continuous improvement efforts that will streamline and enhance our approach to geriatric care.",
                "authors": "T. Fulmer, K. Mate, A. Berman",
                "citations": 206
            },
            {
                "title": "Emerging Adulthood, a Pre-adult Life-History Stage",
                "abstract": "The duration of human maturation has been underestimated; an additional 4–6-year pre-adult period of “emerging adulthood,” should be included in models of human maturation. It is a period of brain maturation, learning about intimacy and mutual support, intensification of pre-existing friendships, family-oriented socialization, and the attainment of those social skills that are needed for mating and reproduction. We propose that emerging adulthood is a life-history stage that is a foundation of the high reproductive success of human beings. The period of emerging adulthood has an evolutionary context and developmental markers, and we present evidence that supports the idea that emerging adults require protection because they are still learning and maturing.",
                "authors": "Z. Hochberg, M. Konner",
                "citations": 121
            },
            {
                "title": "Development of a global seismic risk model",
                "abstract": "Since 2015, the Global Earthquake Model (GEM) Foundation and its partners have been supporting regional programs and bilateral collaborations to develop an open global earthquake risk model. These efforts led to the development of a repository of probabilistic seismic hazard models, a global exposure dataset comprising structural and occupancy information regarding the residential, commercial and industrial buildings, and a comprehensive set of fragility and vulnerability functions for the most common building classes. These components were used to estimate probabilistic earthquake risk globally using the OpenQuake-engine, an open-source software for seismic hazard and risk analysis. This model allows estimating a number of risk metrics such as annualized average losses or aggregated losses for particular return periods, which are fundamental to the development and implementation of earthquake risk mitigation measures.",
                "authors": "V. Silva, Desmond Amo-Oduro, A. Calderon, C. Costa, J. Dabbeek, V. Despotaki, L. Martins, M. Pagani, A. Rao, M. Simionato, D. Viganò, C. Yepes-Estrada, A. Acevedo, H. Crowley, N. Horspool, Kishor Jaiswal, M. Journeay, M. Pittore",
                "citations": 117
            },
            {
                "title": "Propagation Modeling for Wireless Communications in the Terahertz Band",
                "abstract": "Terahertz band (0.1-10 THz) communication is envisioned as a key technology to support ultra-broadband wireless systems for beyond 5G. For realization of efficient wireless communication networks in the THz band, it is imperative to develop channel models that can accurately and efficiently characterize the THz spectrum peculiarities. This article provides an in-depth view of channel modeling in the THz band, based on the deterministic, statistical, and hybrid methods. The state-of-the-art THz channel models in single-antenna and ultra-massive MIMO systems are extensively reviewed, respectively. Furthermore, the open challenges and potential research directions are highlighted regarding THz propagation modeling. Associated with the channel models, key physical parameters of the THz channel and their implications for wireless communication design are analyzed. The provided analysis lays the foundation for reliable and efficient ultra-broadband wireless communications in the THz band.",
                "authors": "Chong Han, Yi Chen",
                "citations": 189
            },
            {
                "title": "Theoretical modeling of triboelectric nanogenerators (TENGs)",
                "abstract": "Triboelectric nanogenerators (TENGs), using Maxwell's displacement current as the driving force, can effectively convert mechanical energy into electricity. In this work, an extensive review of theoretical models of TENGs is presented. Based on Maxwell's equations, a formal physical model is established referred to as the quasi-electrostatic model of a TENG. Since a TENG is electrically neutral at any time owing to the low operation frequency, it is conveniently regarded as a lumped circuit element. Then, using the lumped parameter equivalent circuit theory, the conventional capacitive model and Norton's equivalent circuit model are derived. Optimal conditions for power, voltage, and total energy conversion efficiency can be calculated. The presented TENG models provide an effective theoretical foundation for understanding and predicting the performance of TENGs for practical applications.",
                "authors": "Jiajia Shao, M. Willatzen, Zhong Lin Wang",
                "citations": 120
            },
            {
                "title": "Best Practices for Generating and Using Alpha-Synuclein Pre-Formed Fibrils to Model Parkinson’s Disease in Rodents",
                "abstract": "Parkinson’s disease (PD) is the second most common neurodegenerative disease, affecting approximately one-percent of the population over the age of sixty. Although many animal models have been developed to study this disease, each model presents its own advantages and caveats. A unique model has arisen to study the role of alpha-synuclein (aSyn) in the pathogenesis of PD. This model involves the conversion of recombinant monomeric aSyn protein to a fibrillar form—the aSyn pre-formed fibril (aSyn PFF)—which is then injected into the brain or introduced to the media in culture. Although many groups have successfully adopted and replicated the aSyn PFF model, issues with generating consistent pathology have been reported by investigators. To improve the replicability of this model and diminish these issues, The Michael J. Fox Foundation for Parkinson’s Research (MJFF) has enlisted the help of field leaders who performed key experiments to establish the aSyn PFF model to provide the research community with guidelines and practical tips for improving the robustness and success of this model. Specifically, we identify key pitfalls and suggestions for avoiding these mistakes as they relate to generating the aSyn PFFs from monomeric protein, validating the formation of pathogenic aSyn PFFs, and using the aSyn PFFs in vivo or in vitro to model PD. With this additional information, adoption and use of the aSyn PFF model should present fewer challenges, resulting in a robust and widely available model of PD.",
                "authors": "N. Polinski, L. Volpicelli-Daley, C. Sortwell, Kelvin C. Luk, N. Cremades, Lindsey M. Gottler, Jessica M. Froula, Megan F. Duffy, V. Lee, Terina N. Martinez, K. Dave",
                "citations": 163
            },
            {
                "title": "Loss-Balanced Task Weighting to Reduce Negative Transfer in Multi-Task Learning",
                "abstract": "In settings with related prediction tasks, integrated multi-task learning models can often improve performance relative to independent single-task models. However, even when the average task performance improves, individual tasks may experience negative transfer in which the multi-task model’s predictions are worse than the single-task model’s. We show the prevalence of negative transfer in a computational chemistry case study with 128 tasks and introduce a framework that provides a foundation for reducing negative transfer in multitask models. Our Loss-Balanced Task Weighting approach dynamically updates task weights during model training to control the influence of individual tasks.",
                "authors": "Shengchao Liu, Yingyu Liang, A. Gitter",
                "citations": 134
            },
            {
                "title": "Multiscale causal networks identify VGF as a key regulator of Alzheimer’s disease",
                "abstract": null,
                "authors": "N. Beckmann, Wei-Jye Lin, Minghui Wang, A. Cohain, A. Charney, Pei Wang, Weiping Ma, Ying-Chih Wang, Cheng Jiang, M. Audrain, P. Comella, Amanda K. Fakira, Siddharth P. Hariharan, G. Belbin, K. Girdhar, A. Levey, N. Seyfried, E. Dammer, D. Duong, J. Lah, Jean-Vianney Haure-Mirande, B. Shackleton, T. Fanutza, R. Blitzer, E. Kenny, Jun Zhu, V. Haroutunian, P. Katsel, S. Gandy, Zhidong Tu, M. Ehrlich, Bin Zhang, S. Salton, E. Schadt",
                "citations": 102
            },
            {
                "title": "Simulation versus Optimisation: Theoretical Positions in Energy System Modelling",
                "abstract": "In recent years, several tools and models have been developed and used for the design and analysis of future national energy systems. Many of these models focus on the integration of various renewable energy resources and the transformation of existing fossil-based energy systems into future sustainable energy systems. The models are diverse and often end up with different results and recommendations. This paper analyses this diversity of models and their implicit or explicit theoretical backgrounds. In particular, two archetypes are defined and compared. On the one hand, the prescriptive investment optimisation or optimal solutions approach. On the other hand the analytical simulation or alternatives assessment approach. Awareness of the dissimilar theoretical assumption behind the models clarifies differences between the models, explains dissimilarities in results, and provides a theoretical and methodological foundation for understanding and interpreting results from the two archetypes.",
                "authors": "H. Lund, F. Arler, P. A. Østergaard, Frede Hvelplund, D. Connolly, B. Mathiesen, P. Karnøe",
                "citations": 201
            },
            {
                "title": "Mitochondrial function in development and disease",
                "abstract": "ABSTRACT Mitochondria are organelles with vital functions in almost all eukaryotic cells. Often described as the cellular ‘powerhouses’ due to their essential role in aerobic oxidative phosphorylation, mitochondria perform many other essential functions beyond energy production. As signaling organelles, mitochondria communicate with the nucleus and other organelles to help maintain cellular homeostasis, allow cellular adaptation to diverse stresses, and help steer cell fate decisions during development. Mitochondria have taken center stage in the research of normal and pathological processes, including normal tissue homeostasis and metabolism, neurodegeneration, immunity and infectious diseases. The central role that mitochondria assume within cells is evidenced by the broad impact of mitochondrial diseases, caused by defects in either mitochondrial or nuclear genes encoding for mitochondrial proteins, on different organ systems. In this Review, we will provide the reader with a foundation of the mitochondrial ‘hardware’, the mitochondrion itself, with its specific dynamics, quality control mechanisms and cross-organelle communication, including its roles as a driver of an innate immune response, all with a focus on development, disease and aging. We will further discuss how mitochondrial DNA is inherited, how its mutation affects cell and organismal fitness, and current therapeutic approaches for mitochondrial diseases in both model organisms and humans.",
                "authors": "M. Rossmann, Sonia M Dubois, Suneet Agarwal, L. Zon",
                "citations": 81
            },
            {
                "title": "Fashionable Modelling with Flux",
                "abstract": "Machine learning as a discipline has seen an incredible surge of interest in recent years due in large part to a perfect storm of new theory, superior tooling, renewed interest in its capabilities. We present in this paper a framework named Flux that shows how further refinement of the core ideas of machine learning, built upon the foundation of the Julia programming language, can yield an environment that is simple, easily modifiable, and performant. We detail the fundamental principles of Flux as a framework for differentiable programming, give examples of models that are implemented within Flux to display many of the language and framework-level features that contribute to its ease of use and high productivity, display internal compiler techniques used to enable the acceleration and performance that lies at the heart of Flux, and finally give an overview of the larger ecosystem that Flux fits inside of.",
                "authors": "Mike Innes, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, Viral B. Shah",
                "citations": 153
            },
            {
                "title": "Investigation on hygro-thermal vibration of P-FG and symmetric S-FG nanobeam using integral Timoshenko beam theory",
                "abstract": "In the current research, the free vibrational behavior of the FG nano-beams integrated in the hygro-thermal environment and reposed on the elastic foundation is investigated using a novel integral Timoshenko beam theory (ITBT). The current model has only three variables unknown and requires the introduction of the shear correction factor because her uniformed variation of the shear stress through the thickness. The effective properties of the nano-beam vary according to power-law and symmetric sigmoid distributions. Three models of the hygro-thermal loading are employed. The effect of the small scale effect is considered by using the nonlocal theory of Eringen. The equations of motion of the present model are determined and resolved via Hamilton principle and Navier method, respectively. Several numerical results are presented thereafter to illustrate the accuracy and efficiency of the actual integral Timoshenko beam theory. The effects of the various parameters influencing the vibrational responses of the P-FG and SS-FG nano-beam are also examined and discussed in detail.",
                "authors": "Hakima Matouk, A. A. Bousahla, H. Heireche, F. Bourada, E. A. Bedia, A. Tounsi, K. H. Benrahou",
                "citations": 102
            },
            {
                "title": "RETRACTED: Financial market trend analysis based on autoregressive conditional heteroscedasticity model and BP neural network prediction",
                "abstract": "Many experts and scholars at home and abroad have studied this topic in depth, laying a solid foundation for the research of financial market prediction. At present, the mainstream prediction method is to use neural network and autoregressive conditional heteroscedasticity to build models, which is a more scientific way, and also verified the feasibility of the way in many studies. In order to improve the accuracy of financial market trend prediction, this paper studies in detail the neural network system represented by BP and the autoregressive conditional heterogeneous variance model represented by GARCH. Analyze its structure and algorithm, combine the advantages of both, create a GARCH-BP model, and transform its combination structure and optimize the algorithm according to the uniqueness of the financial market, so as to meet the market as much as possible Characteristics. The novelty of this paper is the construction of the autoregressive conditional heteroscedasticity model, which lays the foundation for the prediction of financial market trends through the construction of the model. However, there are some shortcomings in this article. The overall overview of the financial market is not very clear, and the prediction of the BP network is not so comprehensive. Finally, through the actual data statistics of market transactions, the effectiveness of the GARCH-BP model was tested, analyzed and researched. The final results show that model has a good effect on the prediction and trend analysis of market, and its accuracy and availability greatly improved compared with the previous conventional approach, which is worth further study and extensive research It is believed that the financial market prediction model will become one of the mainstream tools in the industry after its later improvement.",
                "authors": "Xinzhe Yin, Jinghua Li",
                "citations": 82
            },
            {
                "title": "Hygro-thermo-mechanical bending response of FG platesresting on elastic foundations",
                "abstract": "The aim of this work is to study the hygro-thermo-mechanical bending responses of simply supported FG plate resting on a Winkler-Pasternak elastic foundation. The effect transverse shear strains is taken into account in which the zero transverse shear stress condition on the top and bottom surfaces of the plate is ensured without using any shear correction factors. The developed model contains only four unknowns variable which is reduced compared to other HSDTs models. The material properties of FG-plate are supposed to vary across the thickness of the plate according to power-law mixture. The differential governing equations are derived based on the virtual working principle. Numerical outcomes of bending analysis of FG plates under hygro-thermo-mechanical loads are performed and compared with those available in the literature. The effects of the temperature, moisture concentration, elastic foundation parameters, shear deformation, geometrical parameters, and power-law-index on the dimensionless deflections, axial and transverse shear stresses of the FG-plate are presented and discussed.",
                "authors": "Bouzid Merazka, A. Bouhadra, A. Menasria, M. Selim, A. A. Bousahla, F. Bourada, A. Tounsi, K. H. Benrahou, A. Tounsi, M. M. Al-Zahrani",
                "citations": 78
            },
            {
                "title": "Meta-analysis: integrating accumulated knowledge",
                "abstract": null,
                "authors": "Dhruv Grewal, Nancy M. Puccinelli, K. B. Monroe",
                "citations": 141
            },
            {
                "title": "The Science behind Scour at Bridge Foundations: A Review",
                "abstract": "Foundation scour is among the main causes of bridge collapse worldwide, resulting in significant direct and indirect losses. A vast amount of research has been carried out during the last decades on the physics and modelling of this phenomenon. The purpose of this paper is, therefore, to provide an up-to-date, comprehensive, and holistic literature review of the problem of scour at bridge foundations, with a focus on the following topics: (i) sediment particle motion; (ii) physical modelling and controlling dimensionless scour parameters; (iii) scour estimates encompassing empirical models, numerical frameworks, data-driven methods, and non-deterministic approaches; (iv) bridge scour monitoring including successful examples of case studies; (v) current approach for assessment and design of bridges against scour; and, (vi) research needs and future avenues.",
                "authors": "A. Pizarro, S. Manfreda, E. Tubaldi",
                "citations": 98
            },
            {
                "title": "Inferring high-resolution human mixing patterns for disease modeling",
                "abstract": null,
                "authors": "D. Mistry, M. Litvinova, A. P. Y. Piontti, Matteo Chinazzi, Laura Fumanelli, M. Gomes, S. Haque, Quan-Hui Liu, K. Mu, X. Xiong, M. Halloran, I. Longini, S. Merler, M. Ajelli, Alessandro Vespignani Institute for Disease Modeling, Bellevue, Wa, Usa, N. University, Boston, Ma., Institute for Scientific Interchange Foundation, Turin, Italy., Bruno Kessler Foundation, Trento, Fundaccao Oswaldo Cruz, R. Janeiro, Brazil, C. Science, Sichuan University, Chengdu, Sichuan, China., Fred Hutchinson Cancer Research Center, Seattle, D. Biostatistics, U. Washington, College of Public Health, Health Professions, U. Florida, Gainesville, Fl",
                "citations": 220
            },
            {
                "title": "Fair Class Balancing: Enhancing Model Fairness without Observing Sensitive Attributes",
                "abstract": "Machine learning models are at the foundation of modern society. Accounts of unfair models penalizing subgroups of a population have been reported in domains including law enforcement, job screening, etc. Unfairness can spur from biases in the training data, as well as from class imbalance, i.e., when a sensitive group's data is not sufficiently represented. Under such settings, balancing techniques are commonly used to achieve better prediction performance, but their effects on model fairness are largely unknown. In this paper, we first illustrate the extent to which common balancing techniques exacerbate unfairness in real-world data. Then, we propose a new method, called fair class balancing, that allows to enhance model fairness without using any information about sensitive attributes. We show that our method can achieve accurate prediction performance while concurrently improving fairness.",
                "authors": "Shen Yan, Hsien-Te Kao, Emilio Ferrara",
                "citations": 91
            },
            {
                "title": "Gaussian Process Regression Technique to Estimate the Pile Bearing Capacity",
                "abstract": null,
                "authors": "E. Momeni, M. B. Dowlatshahi, F. Omidinasab, H. Maizir, D. J. Armaghani",
                "citations": 92
            },
            {
                "title": "Domain Prompt Learning for Efficiently Adapting CLIP to Unseen Domains",
                "abstract": "Domain generalization (DG) is a difficult transfer learning problem aiming to learn a generalizable model for unseen domains. Recent foundation models (FMs) are robust to many distribution shifts and, therefore, should substantially improve the performance of DG. In this work, we study generic ways to adopt CLIP, a Visual-Language Foundation Model, for DG problems in image classification. While ERM greatly improves the accuracy with bigger backbones and training datasets using standard DG benchmarks, fine-tuning FMs is not practical in many real-world situations. We propose Domain Prompt Learning (DPL) as a novel approach for domain inference in the form of conditional prompt generation. DPL achieved a significant accuracy improvement with only training a lightweight prompt generator (a three-layer MLP), whose parameter is of equivalent scale to the classification projector in the previous DG literature. Combining \\dplshort~with CLIP provides surprising performance, raising the accuracy of zero-shot CLIP from 73.7% to 79.3% on several standard datasets, namely PACS, VLCS, OfficeHome, and TerraIncognita. We hope the simplicity and success of our approach lead to broader adoption and analysis of foundation models in the domain generalization field. Our code is available at https://github.com/shogi880/DPLCLIP.",
                "authors": "X. Zhang, Yusuke Iwasawa, Yutaka Matsuo, S. Gu",
                "citations": 30
            },
            {
                "title": "A theory of Automated Market Makers in DeFi",
                "abstract": "Automated market makers (AMMs) are one of the most prominent decentralized\nfinance (DeFi) applications. AMMs allow users to trade different types of\ncrypto-tokens, without the need to find a counter-party. There are several\nimplementations and models for AMMs, featuring a variety of sophisticated\neconomic mechanisms. We present a theory of AMMs. The core of our theory is an\nabstract operational model of the interactions between users and AMMs, which\ncan be concretised by instantiating the economic mechanisms. We exploit our\ntheory to formally prove a set of fundamental properties of AMMs,\ncharacterizing both structural and economic aspects. We do this by abstracting\nfrom the actual economic mechanisms used in implementations, and identifying\nsufficient conditions which ensure the relevant properties. Notably, we devise\na general solution to the arbitrage problem, the main game-theoretic foundation\nbehind the economic mechanisms of AMMs.",
                "authors": "Massimo Bartoletti, J. Chiang, Alberto Lluch-Lafuente",
                "citations": 53
            },
            {
                "title": "The Circular Economy Business Model: Examining Consumers’ Acceptance of Recycled Goods",
                "abstract": "The circular economy strategy supports the transformation of the linear consumption model into a closed-production model to achieve economic sustainability, with the consumers’ acceptance of circular products being one of the major challenges. Further, one important aspect of product circularity remains unexplored, such as the consumers’ purchase intention of recycled circular goods. In this context, the present study proposes and tests a conceptual model on consumers acceptance of recycled goods through PLS Structural Equation Modeling (PLS-SEM), based on the data obtained from 312 respondents. Results indicate that the positive image of circular products is the most important driver of consumers’ acceptance, followed by the product perceived safety. This study provides an empirical foundation for the important role of consumers in circular economy business models through the examination of consumers’ acceptance of recycled goods.",
                "authors": "Cristina Calvo-Porral, Jean-Pierre Lévy-Mangin",
                "citations": 82
            },
            {
                "title": "Dismantling the master’s house: new ways of knowing for equity and social justice in health professions education",
                "abstract": null,
                "authors": "Morag Paton, T. Naidu, T. Wyatt, Oluwasemipe Oni, G. Lorello, Umberin Najeeb, Zac Feilchenfeld, Stephanie J. Waterman, C. Whitehead, A. Kuper",
                "citations": 80
            },
            {
                "title": "AdTree: Accurate, Detailed, and Automatic Modelling of Laser-Scanned Trees",
                "abstract": "Laser scanning is an effective tool for acquiring geometric attributes of trees and vegetation, which lays a solid foundation for 3-dimensional tree modeling. Existing studies on tree modeling from laser scanning data are vast. However, some works cannot guarantee sufficient modeling accuracy, while some other works are mainly rule-based and therefore highly depend on user inputs. In this paper, we propose a novel method to accurately and automatically reconstruct detailed 3D tree models from laser scans. We first extract an initial tree skeleton from the input point cloud by establishing a minimum spanning tree using the Dijkstra shortest-path algorithm. Then, the initial tree skeleton is pruned by iteratively removing redundant components. After that, an optimization-based approach is performed to fit a sequence of cylinders to approximate the geometry of the tree branches. Experiments on various types of trees from different data sources demonstrate the effectiveness and robustness of our method. The overall fitting error (i.e., the distance between the input points and the output model) is less than 10 cm. The reconstructed tree models can be further applied in the precise estimation of tree attributes, urban landscape visualization, etc. The source code of this work is freely available at https://github.com/tudelft3d/adtree.",
                "authors": "Shenglan Du, R. Lindenbergh, H. Ledoux, J. Stoter, L. Nan",
                "citations": 101
            },
            {
                "title": "Seismic nonlinear vibration control algorithm for high-rise buildings",
                "abstract": "Abstract It is necessary to study the seismic problem of buildings and structures for various safety reasons. Such as prevent and to reduce the damage caused by earthquakes to the greatest extent possible, research feasible analysis methods, and simulate the performance of the structure under earthquake action by computer, to keep its design performance in future earthquake action, and to ensure the safety of people's lives and property. The seismic design based on nonlinear vibration control is proposed. SAP2000 structure analysis software is used to establish the seismic structure model, foundation isolation model, and floor isolation model of a 15-story large chassis frame structure, the nonlinear time-history analysis of the three models under the action of multiple and rare earthquakes is carried out, and the period, displacement and acceleration of the isolated structure and the aseismatic structure are compared. The results show that, under the action of multiple earthquakes, the base shear of foundation isolation is 1208.06 kN, and the floor isolation is 926.43 kN, which are 0.37 and 0.3 of the base shears of the aseismatic structure, respectively. Under the action of rare earthquakes, the base-isolation shear is 4039.59 kN, and the story isolation shear is 3119.99 kN, which are, respectively, 0.4 and 0.31 of the base-isolation shears of the aseismatic structure. This method provides a reliable evaluation for nonlinear seismic response analysis of large complex high-rise buildings and provides an effective reference.",
                "authors": "Qian Liu, Weikang Zhang, Mohammed Wasim Bhatt, Ajit Kumar",
                "citations": 51
            },
            {
                "title": "A Conceptual Model for Digital Shadows in Industry and Its Application",
                "abstract": null,
                "authors": "Fabian Becker, P. Bibow, M. Dalibor, Aymen Gannouni, Viviane Hahn, C. Hopmann, M. Jarke, I. Koren, M. Kröger, Johannes Lipp, J. Maibaum, Judith Michael, Bernhard Rumpe, Patrick Sapel, Niklas Schäfer, G. Schmitz, G. Schuh, A. Wortmann",
                "citations": 44
            },
            {
                "title": "A study of deep learning networks on mobile traffic forecasting",
                "abstract": "With evolution toward the fifth generation (5G) cellular technologies, forecasting and understanding of mobile Internet traffic based on big data is the foundation to enable intelligent management features. To take full advantage of machine learning, a more comprehensive investigation on a mobile traffic dataset with the latest deep learning models is desired. Therefore, a multitask learning architecture using deep learning networks for mobile traffic forecasting is presented in this work. State-of-the-art deep learning models are studied, including 1) recurrent neural network (RNN), 2) three-dimensional convolutional neural network (3D cNn), and 3) combination of CNN and RNN (CNN-RNN). The experiments reveal that CNN and RNN can extract geographical and temporal traffic features respectively. Comparing with either deep or non-deep learning approaches, CNN-RNN is a reliable model leading in all tasks with 70 to 80% forecasting accuracy.",
                "authors": "Chih-Wei Huang, Chiu-Ti Chiang, Qiuhui Li",
                "citations": 126
            },
            {
                "title": "Rethinking Complex Neural Network Architectures for Document Classification",
                "abstract": "Neural network models for many NLP tasks have grown increasingly complex in recent years, making training and deployment more difficult. A number of recent papers have questioned the necessity of such architectures and found that well-executed, simpler models are quite effective. We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. Surprisingly, our simple model is able to achieve these results without attention mechanisms. While these regularization techniques, borrowed from language modeling, are not novel, to our knowledge we are the first to apply them in this context. Our work provides an open-source platform and the foundation for future work in document classification.",
                "authors": "Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy J. Lin",
                "citations": 90
            },
            {
                "title": "Advancing lung organoids for COVID-19 research",
                "abstract": "ABSTRACT The COVID-19 pandemic has emphasised the need to develop effective treatments to combat emerging viruses. Model systems that poorly represent a virus' cellular environment, however, may impede research and waste resources. Collaborations between cell biologists and virologists have led to the rapid development of representative organoid model systems to study severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). We believe that lung organoids, in particular, have advanced our understanding of SARS-CoV-2 pathogenesis, and have laid a foundation to study future pandemic viruses and develop effective treatments.",
                "authors": "J. van der Vaart, M. Lamers, B. Haagmans, H. Clevers",
                "citations": 42
            },
            {
                "title": "Climate Change and Terrestrial Ecosystem Modeling",
                "abstract": "Climate models have evolved into Earth system models with representation of the physics, chemistry, and biology of terrestrial ecosystems. This companion book to Gordon Bonan's Ecological Climatology: Concepts and Applications, Third Edition, builds on the concepts introduced there, and provides the mathematical foundation upon which to develop and understand ecosystem models and their relevance for these Earth system models. The book bridges the disciplinary gap among land surface models developed by atmospheric scientists; biogeochemical models, dynamic global vegetation models, and ecosystem demography models developed by ecologists; and ecohydrology models developed by hydrologists. Review questions, supplemental code, and modeling projects are provided, to aid with understanding how the equations are used. The book is an invaluable guide to climate change and terrestrial ecosystem modeling for graduate students and researchers in climate change, climatology, ecology, hydrology, biogeochemistry, meteorology, environmental science, mathematical modeling, and environmental biophysics.",
                "authors": "G. Bonan",
                "citations": 88
            },
            {
                "title": "The 2018 version of the Global Earthquake Model: Hazard component",
                "abstract": "In December 2018, at the conclusion of its second implementation phase, the Global Earthquake Model (GEM) Foundation released its first version of a map outlining the spatial distribution of seismic hazard at a global scale. The map is the result of an extensive, joint effort combining the results obtained from a collection of probabilistic seismic hazard models, called the GEM Mosaic. Together, the map and the underlying database of models provide an up-to-date view of the earthquake threat globally. In addition, using the Mosaic, a synopsis of the current state-of-practice in modeling probabilistic seismic hazard at national and regional scales is possible. The process adopted for the compilation of the Mosaic adhered to the maximum extent possible to GEM’s principles of collaboration, inclusiveness, transparency, and reproducibility. For each region, priority was given to seismic hazard models either developed by well-recognized national agencies or by large collaborative projects involving local scientists. The version of the GEM Mosaic presented herein contains 30 probabilistic seismic hazard models, 14 of which represent national or sub-national models; the remainder are regional-scale models. We discuss the general qualities of these models, the underlying framework of the database, and the outlook for the Mosaic’s utility and its future versions.",
                "authors": "M. Pagani, J. García-Pelaez, R. Gee, Kendra L. Johnson, V. Poggi, V. Silva, M. Simionato, R. Styron, D. Viganò, L. Danciu, D. Monelli, G. Weatherill",
                "citations": 70
            },
            {
                "title": "The essential value of long‐term experimental data for hydrology and water management",
                "abstract": "Observations and data from long‐term experimental watersheds are the foundation of hydrology as a geoscience. They allow us to benchmark process understanding, observe trends and natural cycles, and are prerequisites for testing predictive models. Long‐term experimental watersheds also are places where new measurement technologies are developed. These studies offer a crucial evidence base for understanding and managing the provision of clean water supplies, predicting and mitigating the effects of floods, and protecting ecosystem services provided by rivers and wetlands. They also show how to manage land and water in an integrated, sustainable way that reduces environmental and economic costs.",
                "authors": "D. Tetzlaff, S. Carey, J. McNamara, H. Laudon, C. Soulsby",
                "citations": 113
            },
            {
                "title": "Dynamic behavior of FGM viscoelastic plates resting on elastic foundations",
                "abstract": null,
                "authors": "A. Sofiyev, Z. Zerin, N. Kuruoglu",
                "citations": 41
            },
            {
                "title": "A Biopsychosocial Model of Chronic Pain for Older Adults.",
                "abstract": "POPULATION\nComprehensive evaluation of chronic pain in older adults is multifaceted.\n\n\nOBJECTIVE AND METHODS\nResearch on chronic pain in older adults needs to be guided by sound conceptual models. The purpose of this paper is to describe an adaptation of the Biopsychosocial Model (BPS) of Chronic Pain for older adults. The extant literature was reviewed, and selected research findings that provide the empiric foundation for this adaptation of the BPS model of chronic pain are summarized. The paper concludes with a discussion of specific recommendations for how this adapted model can be used to guide future research.\n\n\nCONCLUSIONS\nThis adaptation of the BPS model of chronic pain for older adults provides a comprehensive framework to guide future research in this vulnerable population.",
                "authors": "C. Miaskowski, F. Blyth, F. Nicosia, M. Haan, F. Keefe, Alexander K. Smith, C. Ritchie",
                "citations": 81
            },
            {
                "title": "Integration of BIM and Procedural Modeling Tools for Road Design",
                "abstract": "Building Information Modeling (BIM) is a design and management methodology strongly used in the Industry of Architecture, Engineering, and Construction (AEC). It allows the creation of a 3D model through parametric modelling in a workflow that updates data, geometry and semantics using the Industry Foundation Classes (IFC) standard. The purpose of this paper is to develop and apply a BIM method for road infrastructures. The creation of the BIM 3D models was carried out using different visual programming software and BIM tools, designing the spatial and parametric representation of the roadway. This way, it has been possible to discover the advantages of using procedural modelling to design road infrastructure through software that are usually used in the mechanical and architectural field. Finally, the interoperability of the software to extract and exchange information between these BIM tools was assessed.",
                "authors": "S. A. Biancardo, Alessandra Capano, S. G. de Oliveira, A. Tibaut",
                "citations": 59
            },
            {
                "title": "Topological Approach for Mapping Technologies in Reference Architectural Model Industrie 4.0 (RAMI 4.0)",
                "abstract": "Companies are facing manifold challenges while trying to implement Industrie 4.0, which are in great parts rooted in the complexity of Industrie 4.0 and its associated fields of research. To mitigate these challenges and structure Industrie 4.0, initiatives have developed abstract reference architectural models. The research on hand uses the reference architectural model developed by the German Platform Industrie 4.0 (RAMI 4.0). This work aims to create a concrete, yet universal, application-oriented model that fosters the widespread of RAMI 4.0 in practice, supports further research and amendments, and hence, facilitates the implementation of Industrie 4.0 in small and medium-sized enterprises by means of an information tool. Finally, the foundation for a subsequent inclusion of IT security in RAMI 4.0 is laid.",
                "authors": "Yübo Wang, T. Towara, R. Anderl",
                "citations": 83
            },
            {
                "title": "Caenorhabditis elegans for rare disease modeling and drug discovery: strategies and strengths",
                "abstract": "ABSTRACT Although nearly 10% of Americans suffer from a rare disease, clinical progress in individual rare diseases is severely compromised by lack of attention and research resources compared to common diseases. It is thus imperative to investigate these diseases at their most basic level to build a foundation and provide the opportunity for understanding their mechanisms and phenotypes, as well as potential treatments. One strategy for effectively and efficiently studying rare diseases is using genetically tractable organisms to model the disease and learn about the essential cellular processes affected. Beyond investigating dysfunctional cellular processes, modeling rare diseases in simple organisms presents the opportunity to screen for pharmacological or genetic factors capable of ameliorating disease phenotypes. Among the small model organisms that excel in rare disease modeling is the nematode Caenorhabditis elegans. With a staggering breadth of research tools, C. elegans provides an ideal system in which to study human disease. Molecular and cellular processes can be easily elucidated, assayed and altered in ways that can be directly translated to humans. When paired with other model organisms and collaborative efforts with clinicians, the power of these C. elegans studies cannot be overstated. This Review highlights studies that have used C. elegans in diverse ways to understand rare diseases and aid in the development of treatments. With continuing and advancing technologies, the capabilities of this small round worm will continue to yield meaningful and clinically relevant information for human health.",
                "authors": "Peter A. Kropp, R. Bauer, Isabella Zafra, C. Graham, A. Golden",
                "citations": 23
            },
            {
                "title": "Aging in the Digital Age: Conceptualizing Technology Adoption and Digital Inequalities",
                "abstract": null,
                "authors": "J. Francis, Christopher Ball, T. Kadylak, S. Cotten",
                "citations": 71
            },
            {
                "title": "Data Envelopment Analysis in Energy and Environmental Economics: An Overview of the State-of-the-Art and Recent Development Trends",
                "abstract": "Measurement of environmental and energy economics presents an analytical foundation for environmental decision making and policy analysis. Applications of data envelopment analysis (DEA) models in the assessment of environmental and energy economics are increasing notably. The main objective of this review paper is to provide the comprehensive overview of the application of DEA models in the fields of environmental and energy economics. In this regard, a total 145 articles published in the high-quality international journals extracted from two important databases (Web of Science and Scopus) were selected for review. The 145 selected articles are reviewed and classified based on different criteria including author(s), application scheme, different DEA models, application fields, the name of journals and year of publication. This review article provided insights into the methodological and conceptualization study in the application of DEA models in the environmental and energy economics fields. This study should enable scholars and practitioners to understand the state of art of input and output indicators of DEA in the fields of environmental and energy economics.",
                "authors": "A. Mardani, D. Štreimikienė, T. Baležentis, M. Saman, K. Nor, Seyed Meysam Khoshnava",
                "citations": 81
            },
            {
                "title": "Working with Worms: Caenorhabditis elegans as a Model Organism",
                "abstract": "Since its introduction as a laboratory organism 50 years ago, the nematode worm Caenorhabditis elegans has become one of the most widely used and versatile models for nearly all aspects of biological and genomic research. Many experiments in C. elegans begin with the generation and analysis of mutants that affect a specific biological process, so genetic techniques are the foundation of worm research. Many different aspects of biology are being studied in C. elegans, and three different recent Nobel Prizes have recognized six researchers working with worms. In addition, C. elegans was the first multicellular organism to have its genome sequenced, so many of the standard genomic methods have also been pioneered in C. elegans. In fact, many novel techniques and ideas are initially tested in C. elegans because of its versatility as a research organism. It is also appropriate for introducing undergraduate students to research, and some of its strengths and challenges for this purpose are discussed. © 2019 The Authors.",
                "authors": "P. Meneely, C. Dahlberg, Jacqueline K. Rose",
                "citations": 67
            },
            {
                "title": "On the shear buckling of porous nanoplates using a new size-dependent quasi-3D shear deformation theory",
                "abstract": null,
                "authors": "D. Shahsavari, B. Karami, H. Fahham, Li Li",
                "citations": 77
            },
            {
                "title": "Working with Open BIM Standards to Source Legal Spaces for a 3D Cadastre",
                "abstract": "Much work has already been done on how a 3D Cadastre should best be developed. An inclusive information model, the Land Administration Model (LADM ISO 19152) has been developed to provide an international framework for how this can best be done. This conceptual model does not prescribe the technical data format. One existing source from which data could be obtained is 3D Building Information Models (BIMs), or, more specifically in this context, BIMs in the form of one of buildingSMART’s open standards: the Industry Foundation Classes (IFC). The research followed a standard BIM methodology of first defining the requirements through the use of the Information Delivery Manual (IDM ISO29481) and then translating the process described in the IDM into technical requirements using a Model View Definition (MVD), a practice to coordinate upfront the multidisciplinary stakeholders of a construction project. The proposed process model illustrated how the time it takes to register 3D spatial units in a Land Registry could substantially be reduced compared to the first 3D registration in the Netherlands. The modelling of an MVD or a subset of the IFC data model helped enable the creation and exchange of boundary representations of topological objects capable of being combined into a 3D legal space overview map.",
                "authors": "J. Oldfield, P. V. Oosterom, J. Beetz, T. Krijnen",
                "citations": 84
            },
            {
                "title": "3D Point Cloud to BIM: Semi-Automated Framework to Define IFC Alignment Entities from MLS-Acquired LiDAR Data of Highway Roads",
                "abstract": "Building information modeling (BIM) is a process that has shown great potential in the building industry, but it has not reached the same level of maturity for transportation infrastructure. There is a standardization need for information exchange and management processes in the infrastructure that integrates BIM and Geographic Information Systems (GIS). Currently, the Industry Foundation Classes standard has harmonized different infrastructures under the Industry Foundation Classes (IFC) 4.3 release. Furthermore, the usage of remote sensing technologies such as laser scanning for infrastructure monitoring is becoming more common. This paper presents a semi-automated framework that takes as input a raw point cloud from a mobile mapping system, and outputs an IFC-compliant file that models the alignment and the centreline of each road lane in a highway road. The point cloud processing methodology is validated for two of its key steps, namely road marking processing and alignment and road line extraction, and a UML diagram is designed for the definition of the alignment entity from the point cloud data.",
                "authors": "M. Soilán, A. Justo, A. Sánchez-Rodríguez, B. Riveiro",
                "citations": 44
            },
            {
                "title": "Market Structure in Bitcoin Mining",
                "abstract": "We analyze the Bitcoin protocol for electronic peer-to-peer payments and the operations that support the “blockchain” that underpins it. It is shown that that protocol maps formally into a dynamic game that is an extension of standard models of R&D racing. The model provides a technical foundation for any economic analysis of ‘proof of work’ protocols. Using the model, we demonstrate that free entry is solely responsible for determining resource usage by the system for a given reward to mining. The endogenous level of computational difficulty built into the Bitcoin protocol does not mitigate this usage and serves only to determine the time taken to process transactions. Regulating market structure will mitigate resource use highlighting the importance of identifying the benefits of competition for the operation of the blockchain.",
                "authors": "Jun Ma, J. Gans, R. Tourky",
                "citations": 73
            },
            {
                "title": "Annual Energy Production Estimation for Variable-speed Wind Turbine at High-altitude Site",
                "abstract": "This letter presents a systematic approach to estimate the annual energy production (AEP) of variable-speed wind turbines erected at high-altitude sites. Compared to the existing empirical-model based approaches, the proposed approach models the influence of the air density on the power production while employing the theoretical power curve. Consequently, the proposed approach provides a precise estimation of AEP, which can serve as a foundation of the optimum turbinesite matching design at different-altitude sites.",
                "authors": "Dongran Song, Songyue Zheng, Sheng Yang, Jian Yang, M. Dong, M. Su, Y. Joo",
                "citations": 43
            },
            {
                "title": "The dynamics of Kelp Forests in the Northeast Pacific Ocean and the relationship with environmental drivers",
                "abstract": "The dynamics of foundation species in ecosystems are key to the fate of many species. Kelp forests are foundation species in temperate ocean ecosystems and contribute to carbon storage, macronutrient dynamics, primary production and biodiversity of a myriad of associated species. Downward trends in their abundance globally have been of concern. We analysed 26 years of aerial censuses (1989–2015) of two canopy kelp species in Washington State (USA) waters. We compared these modern censuses with censuses in 1911 and 1912 to determine the persistence of kelp cover over the past century. Using Autoregressive Integrated Moving Average (ARIMA) models, we compared kelp dynamics with likely environmental drivers, including local environmental variables and ocean indices for this region. Kelp remains at historic levels in many areas, although some eastern populations in proximity to greater human populations are the exception to this pattern. Over the last 26 years, kelp abundance showed high spatial autocorrelation in western areas of Straits of Juan de Fuca, with more variable populations in the annual species and eastward towards Puget Sound. Both species covaried positively in their abundance throughout most of the study area, suggesting that environmental factors rather than competition, drove their dynamics. The population dynamics of these kelp species showed that the abundance 1 year previously was an important predictor, and cyclic dynamics were not indicated using ARIMA models. Kelp abundance correlated inversely with the Pacific Decadal Oscillation and the Oceanic Nino Index, and positively with the North Pacific Gyre Oscillation, indicating that large‐scale processes associated with colder seawater temperatures were associated with greater relative abundance of kelp. Synthesis. Kelp beds in the northern California Current Large Marine Ecosystem have mostly remained persistent over the past century and over many kilometres, but some areas may have decreased in abundance. The sensitivity of these populations to indices of ocean climate, our demonstration that a historic 93‐year sea surface temperature record (Race Rocks, Canada) showed a 0.72°C increase and the classification of some areas as high variability‐low abundance, suggest that the viability of these foundational species remain a concern into the future.",
                "authors": "C. Pfister, H. Berry, T. Mumford",
                "citations": 69
            },
            {
                "title": "Critical Success Factors of Digital Business Strategy",
                "abstract": "Digitalization does fundamentally impact firms’ strategy development. With the fusion of IT and business strategy, Digital Business Strategy (DBS) creates the foundation for digital business models [1]. In this paper, we develop a DBS framework, based on a structured review of 21 industry reports. From this analysis, we yield 8 generic dimensions with a total of 40 critical success factors (CSFs) for DBS. The CSFs represent a rich set of actions specific to DBS and to the design of business models in the digital business environment. The discussion shows that academic research is lagging behind in contributing to DBS. Future research is suggested to further formalize the concept of DBS and to create a better understanding about how firms can successfully establish DBS.",
                "authors": "Friedrich Holotiuk, D. Beimborn",
                "citations": 70
            },
            {
                "title": "Higher-order continuum theories for buckling response of silicon carbide nanowires (SiCNWs) on elastic matrix",
                "abstract": null,
                "authors": "K. Mercan, Hayri Metin Numanoğlu, B. Akgöz, C. Demir, Ö. Civalek",
                "citations": 41
            },
            {
                "title": "The complex variable reproducing kernel particle method for bending problems of thin plates on elastic foundations",
                "abstract": null,
                "authors": "L. Chen, Yumin Cheng",
                "citations": 40
            },
            {
                "title": "Assessment of bridge natural frequency as an indicator of scour using centrifuge modelling",
                "abstract": null,
                "authors": "Kasun D Kariyawasam, C. Middleton, G. Madabhushi, S. Haigh, J. Talbot",
                "citations": 28
            },
            {
                "title": "A cutting force predicting model in orthogonal machining of unidirectional CFRP for entire range of fiber orientation",
                "abstract": null,
                "authors": "Liang Chen, Kaifu Zhang, Hui Cheng, Z. Qi, Q. Meng",
                "citations": 69
            },
            {
                "title": "The MEMO Meta Modelling Language (MML) and Language Architecture: 2nd Edition",
                "abstract": "The family of languages that builds the foundation of the MEMO method is intended to feature a high degree of inter-language integration. For this purpose, the languages need to share common concepts. In order to define concepts that are semantically equivalent, it is recommendable to use the same meta modelling language for specifying the MEMO modelling languages. The previous version of the meta modelling language used for this purpose needed a revision. At the same time, there was need to account for alternative approaches to specifying modelling languages, especially those offered by the OMG or the Eclipse foundation. This report starts with an analysis of requirements that should be accounted for by a meta modelling language. Subsequently, the UML infrastructure library and meta object facility (MOF) are evaluated against these requirements. In addition to that, the report presents an evaluation of the Ecore model, which serves to represent meta models within the Eclipse Graphical Modeling Framework (GMF). The evaluation of both approaches shows that none of them is satisfactory as a meta modelling language for enterprise modelling. Then, the new version of the MEMO meta modelling language (MML) is presented. The language specification consists of a meta meta model that specifies that semantics and abstract syntax and a corresponding graphical notation (concrete syntax). The new version features a concept called intrinsic features that allows for differentiating between features that apply to types and those that apply to instances. It also includes a modified graphical notation that supports a clear distinction of meta models from models on other levels of abstraction. Finally, the report presents the outline of a tool that supports the creation and editing of MEMO meta models as well as their transformation into representations which can be used in the Eclipse modelling framework.",
                "authors": "U. Frank",
                "citations": 59
            },
            {
                "title": "On the dynamics of axially functionally graded CNT strengthened deformable beams",
                "abstract": null,
                "authors": "H. Khaniki, M. Ghayesh",
                "citations": 27
            },
            {
                "title": "Enterprise Risk Management: History and a Design-Science Proposal",
                "abstract": "This paper aims to investigate the evolution of enterprise risk management (ERM) out of fragmented disciplinary perspectives to provide a foundation for promoting interdisciplinary research and proposes a design science approach for more effective ERM implementation in organizations.,This conceptual paper synthesizes ERM research and practice from multiple disciplines.,Corporate risk management concepts were born in academic finance and developed further in the finance subset known as risk management and insurance. With the advent of ERM, efforts must broaden beyond applying statistical models to quantifiable risks. Other disciplines have expanded ERM research by embracing techniques to investigate risk management practices to produce knowledge that integrates practice and theory. ERM is promoted as integrated risk management, yet silos still remain in both practice and research.,This study provides a foundation and a proposal for moving ERM past academic and organizational silos, which is necessary to achieve the ERM philosophy and increase organizational resilience. Understanding the evolution and fragmented nature of ERM research and practice provides a foundation for interdisciplinary cooperation necessary to achieve the holistic ERM philosophy. A next frontier is effective ERM implementation. This paper argues for an organizational design science approach for mitigating the resistance to change that confounds effective implementation of ERM in organizations facing an increasingly uncertain environment and outlines future research for applying the approach to implementing the ISO 31000 risk management process.",
                "authors": "Michael K. McShane",
                "citations": 59
            },
            {
                "title": "Rational Inattention When Decisions Take Time",
                "abstract": "Decisions take time, and the time taken to reach a decision is likely to be informative about the cost of more precise judgments. We formalize this insight in the context of a dynamic rational inattention (RI) model. Under standard conditions on the flow cost of information in our discrete-time model, we obtain a tractable model in the continuous-time limit. We next provide conditions under which the resulting belief dynamics resemble either diffusion processes or processes with large jumps. We then demonstrate that the state-contingent choice probabilities predicted by our model are identical to those predicted by a static RI model, providing a micro-foundation for such models. In the diffusion case, our model provides a normative foundation for a variant of the DDM models studied in mathematical psychology.",
                "authors": "Benjamin Hébert, M. Woodford",
                "citations": 47
            },
            {
                "title": "Decentralized Finance: Blockchain Technology and the Quest for an Open Financial System",
                "abstract": "Blockchain technology can reduce transaction costs, facilitate decentralized platforms, and produce distributed trust, creating a foundation for new business models. In the financial industry, blockchain technology allows for the rise of decentralized financial services that may be more decentralized, innovative, interoperable, borderless, and transparent. Decentralized financial services have the potential to reduce transaction costs, broaden financial inclusion, facilitate open access, encourage permissionless innovation, and create new opportunities for entrepreneurs and innovators. In this article, we assess the benefits of decentralized finance, identify existing business models, and evaluate potential challenges and limits. As a new area of financial technology, decentralized finance may reshape the structure of modern finance and create a new landscape for entrepreneurship and innovation, showcasing the potential of decentralization as a foundation for new business models.",
                "authors": "Yuanchun Chen, C. Bellavitis",
                "citations": 47
            },
            {
                "title": "Learning soil parameters and updating geotechnical reliability estimates under spatial variability – theory and application to shallow foundations",
                "abstract": "ABSTRACT Field data is commonly used to determine soil parameters for geotechnical analysis. Bayesian analysis allows combining field data with other information on soil parameters in a consistent manner. We show that the spatial variability of the soil properties and the associated measurements can be captured through two different modelling approaches. In the first approach, a single random variable (RV) represents the soil property within the area of interest, while the second approach models the spatial variability explicitly with a random field (RF). We apply the Bayesian concept exemplarily to the reliability assessment of a shallow foundation in a silty soil with spatially variable data. We show that the simpler RV approach is applicable in cases where the measurements do not influence the correlation structure of the soil property at the vicinity of the foundation. In other cases, it is expected to underestimate the reliability, and a RF model is required to obtain accurate results.",
                "authors": "I. Papaioannou, D. Štraub",
                "citations": 65
            },
            {
                "title": "Foundational Challenges for Advancing the Field and Discipline of Risk Analysis",
                "abstract": "Risk analysis as a field and discipline is about concepts, principles, approaches, methods, and models for understanding, assessing, communicating, managing, and governing risk. The foundation of this field and discipline has been subject to continuous discussion since its origin some 40 years ago with the establishment of the Society for Risk Analysis and the Risk Analysis journal. This article provides a perspective on critical foundational challenges that this field and discipline faces today, for risk analysis to develop and have societal impact. Topics discussed include fundamental questions important for defining the risk field, discipline, and science; the multidisciplinary and interdisciplinary features of risk analysis; the interactions and dependencies with other sciences; terminology and fundamental principles; and current developments and trends, such as the use of artificial intelligence.",
                "authors": "T. Aven, Roger Flage",
                "citations": 25
            },
            {
                "title": "Handbook of Mixture Analysis",
                "abstract": "Mixtures of experts models provide a framework in which covariates may be included in mixture models. This is achieved by modelling the parameters of the mixture model as functions of the concomitant covariates. Given their mixture model foundation, mixtures of experts models possess a diverse range of analytic uses, from clustering observations to capturing parameter heterogeneity in cross-sectional data. This chapter focuses on delineating the mixture of experts modelling framework and demonstrates the utility and flexibility of mixtures of experts models as an analytic tool.",
                "authors": "I. C. Gormley, Sylvia Frühwirth-Schnatter",
                "citations": 42
            },
            {
                "title": "Mathematical modeling of within‐host Zika virus dynamics",
                "abstract": "Recent Zika virus outbreaks have been associated with severe outcomes, especially during pregnancy. A great deal of effort has been put toward understanding this virus, particularly the immune mechanisms responsible for rapid viral control in the majority of infections. Identifying and understanding the key mechanisms of immune control will provide the foundation for the development of effective vaccines and antiviral therapy. Here, we outline a mathematical modeling approach for analyzing the within‐host dynamics of Zika virus, and we describe how these models can be used to understand key aspects of the viral life cycle and to predict antiviral efficacy.",
                "authors": "K. Best, A. Perelson",
                "citations": 45
            },
            {
                "title": "Pathways for Theoretical Advances in Visualization",
                "abstract": "There is little doubt that having a theoretic foundation will benefit the field of visualization, including its main subfields. Because there has been a substantial amount of work on taxonomies and conceptual models in the visualization literature and some recent work on theoretic frameworks, such a theoretic foundation is not a foolish or impractical ambition. This article asks, “How can we build a theoretic foundation for visualization collectively as a community?” The authors envision the pathways for four different aspects of a theoretic foundation: taxonomies and ontologies, principles and guidelines, conceptual models and theoretic frameworks, and quantitative laws and theoretic systems.",
                "authors": "Min Chen, G. Grinstein, Chris R. Johnson, J. Kennedy, Melanie Tory",
                "citations": 40
            },
            {
                "title": "Study of the behaviour of skirted shallow foundations resting on sand",
                "abstract": "The performance of skirted shallow foundations resting on sand bed was investigated using physical modelling. Laboratory tests were performed on small-scale foundation models to study the behaviour of circular, skirted foundations subjected to vertical loads. The effects of foundation diameter, relative density of sand, skirt depth and roughness of the model surface on the bearing capacity and settlement of skirted foundations were studied. The results of the model tests have shown that using skirts improve the bearing capacity and settlement values of skirted foundations compared with shallow foundations without skirt. The improvement in bearing capacity and a reduction in settlement of shallow foundations increase with increasing the skirt depth, roughness of skirt sides and decreasing the relative density of sand. The ultimate bearing capacity of shallow foundations may be increased by up to 5·0 times and the settlement can be reduced to a value of 8% of those of foundations without skirts. On the basi...",
                "authors": "Gholipour Sajjad, Makarchian Masoud",
                "citations": 24
            },
            {
                "title": "Free vibration and flexural response of functionally graded plates resting on Winkler–Pasternak elastic foundations using nonpolynomial higher-order shear and normal deformation theory",
                "abstract": "ABSTRACT In the present work, the flexural and vibration response of a functionally graded plate resting on Pasternak elastic foundation is analyzed using a recently developed nonpolynomial higher-order shear and normal deformation theory by the authors. The novelty of this theory is that it contains only four unknowns and also accommodates the thickness stretching effect. Two kinds of micromechanics models, namely, the Voigt and Mori–Tanaka models, are considered. Material properties of the functionally graded plates are assumed to vary continuously in the thickness direction according to either a simple power law or an exponential law. Finite element formulation is done using C° continuous Lagrangian quadrilateral nine-noded elements with eight degrees of freedom per node. The equations of motion are derived using a variational approach. Convergence and comparison studies are carried out to establish the authenticity and reliability of the solutions. The effect of various boundary conditions, geometric conditions, micromechanics models, and foundation parameters on the flexural and vibration response of the functionally graded plate are investigated.",
                "authors": "Ankit K. Gupta, M. Talha, W. Seemann",
                "citations": 34
            },
            {
                "title": "Dewatering induced subsidence during excavation in a Shanghai soft deposit",
                "abstract": null,
                "authors": "Yeshuang Xu, Huai-na Wu, B. Wang, Tian-liang Yang",
                "citations": 34
            },
            {
                "title": "Reliability and variance-based sensitivity analysis of arch dams during construction and reservoir impoundment",
                "abstract": null,
                "authors": "M. H. Khaneghahi, M. Alembagheri, N. Soltani",
                "citations": 32
            },
            {
                "title": "From Field-Based Coordination to Aggregate Computing",
                "abstract": null,
                "authors": "Mirko Viroli, J. Beal, Ferruccio Damiani, Giorgio Audrito, Roberto Casadei, Danilo Pianini",
                "citations": 28
            },
            {
                "title": "Evaluation of Piled Raft Performance Using a Verified 3D Nonlinear Numerical Model",
                "abstract": null,
                "authors": "A. Alnuaim, H. El Naggar, M. E. El Naggar",
                "citations": 24
            },
            {
                "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
                "abstract": "Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/",
                "authors": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, P. Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, R. Kaczmarczyk, J. Jitsev",
                "citations": 2646
            },
            {
                "title": "A visual–language foundation model for pathology image analysis using medical Twitter",
                "abstract": null,
                "authors": "Zhi Huang, Federico Bianchi, Mert Yuksekgonul, T. Montine, J. Zou",
                "citations": 249
            },
            {
                "title": "UltraFeedback: Boosting Language Models with High-quality Feedback",
                "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality \\textit{AI feedback} automatically for a scalable alternative. Specifically, we identify \\textbf{scale and diversity} as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present \\textsc{UltraFeedback}, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon \\textsc{UltraFeedback}, we align a LLaMA-based model by best-of-$n$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research. Our data and models are available at https://github.com/thunlp/UltraFeedback.",
                "authors": "Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, Maosong Sun",
                "citations": 284
            },
            {
                "title": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs",
                "abstract": "Biomedical data is inherently multimodal, comprising physical measurements and natural language narratives. A generalist biomedical AI model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image-text pairs. Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image-text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision-language processing. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering (VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of standard datasets, substantially outperforming prior approaches. Intriguingly, by large-scale pretraining on diverse biomedical image types, BiomedCLIP even outperforms state-of-the-art radiology-specific models such as BioViL in radiology-specific tasks such as RSNA pneumonia detection. In summary, BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications. We release our models at https://aka.ms/biomedclip to facilitate future research in multimodal biomedical AI.",
                "authors": "Sheng Zhang, Yanbo Xu, Naoto Usuyama, J. Bagga, Robert Tinn, Sam Preston, Rajesh N. Rao, Mu-Hsin Wei, Naveen Valluri, Cliff Wong, M. Lungren, Tristan Naumann, Hoifung Poon",
                "citations": 134
            },
            {
                "title": "RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation Based on Visual Foundation Model",
                "abstract": "Leveraging the extensive training data from SA-1B, the segment anything model (SAM) demonstrates remarkable generalization and zero-shot capabilities. However, as a category-agnostic instance segmentation method, SAM heavily relies on prior manual guidance, including points, boxes, and coarse-grained masks. Furthermore, its performance in remote sensing image segmentation tasks remains largely unexplored and unproven. In this article, we aim to develop an automated instance segmentation approach for remote sensing images based on the foundational SAM model and incorporating semantic category information. Drawing inspiration from prompt learning, we propose a method to learn the generation of appropriate prompts for SAM. This enables SAM to produce semantically discernible segmentation results for remote sensing images, a concept that we have termed RSPrompter. We also propose several ongoing derivatives for instance segmentation tasks, drawing on recent advancements within the SAM community, and compare their performance with RSPrompter. Extensive experimental results, derived from the WHU building dataset, the NWPU VHR-10 dataset, and the SAR Ship Detection Dataset (SSDD) dataset, validate the effectiveness of our proposed method. The code for our method is publicly available at https://kychen.me/RSPrompter.",
                "authors": "Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou, Z. Shi",
                "citations": 131
            },
            {
                "title": "mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",
                "abstract": "Recent years have witnessed a big convergence of language, vision, and multi-modal pretraining. In this work, we present mPLUG-2, a new unified paradigm with modularized design for multi-modal pretraining, which can benefit from modality collaboration while addressing the problem of modality entanglement. In contrast to predominant paradigms of solely relying on sequence-to-sequence generation or encoder-based instance discrimination, mPLUG-2 introduces a multi-module composition network by sharing common universal modules for modality collaboration and disentangling different modality modules to deal with modality entanglement. It is flexible to select different modules for different understanding and generation tasks across all modalities including text, image, and video. Empirical study shows that mPLUG-2 achieves state-of-the-art or competitive results on a broad range of over 30 downstream tasks, spanning multi-modal tasks of image-text and video-text understanding and generation, and uni-modal tasks of text-only, image-only, and video-only understanding. Notably, mPLUG-2 shows new state-of-the-art results of 48.0 top-1 accuracy and 80.3 CIDEr on the challenging MSRVTT video QA and video caption tasks with a far smaller model size and data scale. It also demonstrates strong zero-shot transferability on vision-language and video-language tasks. Code and models will be released in https://github.com/alibaba/AliceMind.",
                "authors": "Haiyang Xu, Qinghao Ye, Mingshi Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qiuchen Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Feiran Huang, Jingren Zhou",
                "citations": 127
            },
            {
                "title": "Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions",
                "abstract": "The integration of large language models (LLMs), such as those in the Generative Pre-trained Transformers (GPT) series, into medical education has the potential to transform learning experiences for students and elevate their knowledge, skills, and competence. Drawing on a wealth of professional and academic experience, we propose that LLMs hold promise for revolutionizing medical curriculum development, teaching methodologies, personalized study plans and learning materials, student assessments, and more. However, we also critically examine the challenges that such integration might pose by addressing issues of algorithmic bias, overreliance, plagiarism, misinformation, inequity, privacy, and copyright concerns in medical education. As we navigate the shift from an information-driven educational paradigm to an artificial intelligence (AI)–driven educational paradigm, we argue that it is paramount to understand both the potential and the pitfalls of LLMs in medical education. This paper thus offers our perspective on the opportunities and challenges of using LLMs in this context. We believe that the insights gleaned from this analysis will serve as a foundation for future recommendations and best practices in the field, fostering the responsible and effective use of AI technologies in medical education.",
                "authors": "Alaa A. Abd-alrazaq, Rawan AlSaad, Dari Alhuwail, Arfan Ahmed, P. Healy, Syed Latifi, S. Aziz, R. Damseh, Sadam Alabed Alrazak, Javaid Sheikh",
                "citations": 196
            },
            {
                "title": "On the Hidden Mystery of OCR in Large Multimodal Models",
                "abstract": "Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. It remains less explored about their efﬁcacy in text-related visual tasks. We conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, and key information extraction. Our ﬁndings reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for word recognition and exhibit inferior perception of individual character shapes. They also display indifference towards text length and have limited capabilities in detecting ﬁne-grained features in images. Consequently, these results demonstrate that even the current most powerful large multimodal models cannot match domain-speciﬁc methods in traditional text tasks and face greater challenges in more complex tasks. Most importantly, the baseline results showcased in this study could provide a foundational framework for the conception and assessment of innovative strategies targeted at enhancing zero-shot multimodal techniques. Evaluation pipeline will be available at https://github.com/Yuliang-Liu/MultimodalOCR .",
                "authors": "Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, Xiang Bai",
                "citations": 163
            },
            {
                "title": "A foundation model for atomistic materials chemistry",
                "abstract": "Machine-learned force fields have transformed the atomistic modelling of materials by enabling simulations of ab initio quality on unprecedented time and length scales. However, they are currently limited by: (i) the significant computational and human effort that must go into development and validation of potentials for each particular system of interest; and (ii) a general lack of transferability from one chemical system to the next. Here, using the state-of-the-art MACE architecture we introduce a single general-purpose ML model, trained on a public database of 150k inorganic crystals, that is capable of running stable molecular dynamics on molecules and materials. We demonstrate the power of the MACE-MP-0 model - and its qualitative and at times quantitative accuracy - on a diverse set problems in the physical sciences, including the properties of solids, liquids, gases, chemical reactions, interfaces and even the dynamics of a small protein. The model can be applied out of the box and as a starting or\"foundation model\"for any atomistic system of interest and is thus a step towards democratising the revolution of ML force fields by lowering the barriers to entry.",
                "authors": "Ilyes Batatia, Philipp Benner, Chiang Yuan, A. Elena, D. Kov'acs, Janosh Riebesell, Xavier R Advincula, M. Asta, William J. Baldwin, Noam Bernstein, Arghya Bhowmik, Samuel M. Blau, Vlad Cuarare, James P Darby, Sandip De, Flaviano Della Pia, Volker L. Deringer, Rokas Elijovsius, Zakariya El-Machachi, Edvin Fako, A. C. Ferrari, A. Genreith‐Schriever, Janine George, Rhys E. A. Goodall, Clare P. Grey, Shuang Han, Will Handley, H. H. Heenen, K. Hermansson, Christian Holm, Jad Jaafar, Stephan Hofmann, Konstantin S. Jakob, Hyunwook Jung, V. Kapil, Aaron D. Kaplan, Nima Karimitari, Namu Kroupa, J. Kullgren, Matthew C Kuner, Domantas Kuryla, Guoda Liepuoniute, Johannes T. Margraf, Ioan B Magduau, A. Michaelides, J. H. Moore, A. Naik, Samuel P Niblett, Sam Walton Norwood, Niamh O'Neill, Christoph Ortner, Kristin A. Persson, Karsten Reuter, Andrew S. Rosen, Lars L. Schaaf, Christoph Schran, Eric Sivonxay, Tamás K Stenczel, Viktor Svahn, Christopher Sutton, Cas van der Oord, Eszter Varga-Umbrich, T. Vegge, Martin Vondr'ak, Yangshuai Wang, William C. Witt, Fabian Zills, G'abor Cs'anyi",
                "citations": 84
            },
            {
                "title": "When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions",
                "abstract": "The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations, challenges, and future directions. Through an exploration of the challenges faced by FL and FM individually and their interconnections, we aim to inspire future research directions that can further enhance both fields, driving advancements and propelling the development of privacy-preserving and scalable AI systems.",
                "authors": "Weiming Zhuang, Chen Chen, Lingjuan Lyu",
                "citations": 71
            },
            {
                "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
                "abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",
                "authors": "Hoagy Cunningham, Aidan Ewart, Logan Riggs, R. Huben, Lee Sharkey",
                "citations": 181
            },
            {
                "title": "SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery",
                "abstract": "Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense potential towards a generic model for Earth Observation. Nevertheless, these works primar-ily focus on a single modality without temporal and geo-context modeling, hampering their capabilities for diverse tasks. In this study, we present SkySense, a generic billion-scale model, pretrained on a curated multimodal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal sequences. SkySense incorporates a factorized multimodal spatiotemporal encoder taking temporal sequences of opti-cal and Synthetic Aperture Radar (SAR) data as input. This encoder is pretrained by our proposed Multi-Granularity Contrastive Learning to learn representations across different modal and spatial granularities. To further enhance the RSI representations by the geo-context clue, we introduce Geo-Context Prototype Learning to learn region-aware prototypes upon RSI's multimodal spatiotemporal features. To our best knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modules can be flexibly combined or used individually to accommodate various tasks. It demonstrates remarkable generalization capabilities on a thor-ough evaluation encompassing 16 datasets over 7 tasks, from single- to multimodal, static to temporal, and classification to localization. SkySense surpasses 18 recent RSFMs in all test scenarios. Specifically, it outperforms the latest models such as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and 3.61% on average respectively. We will release the pretrained weights to facilitate future research and Earth Observation applications.",
                "authors": "Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, Huimei He, Jian Wang, Jingdong Chen, Ming Yang, Yongjun Zhang, Yansheng Li",
                "citations": 68
            },
            {
                "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
                "abstract": "There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost. The ideas and findings presented here lay a foundation for using LLMs sustainably and efficiently.",
                "authors": "Lingjiao Chen, M. Zaharia, James Y. Zou",
                "citations": 152
            },
            {
                "title": "FinGPT: Open-Source Financial Large Language Models",
                "abstract": "Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data. In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT} and \\url{https://github.com/AI4Finance-Foundation/FinNLP}",
                "authors": "Hongyang Yang, Xiao-Yang Liu, Chris Wang",
                "citations": 153
            },
            {
                "title": "BioCLIP: A Vision Foundation Model for the Tree of Life",
                "abstract": "Images of the natural world, collected by a variety of cameras, from drones to individual phones, are increasingly abundant sources of biological information. There is an ex-plosion of computational methods and tools, particularly computer vision, for extracting biologically relevant information from images for science and conservation. Yet most of these are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. A vision model for general or-ganismal biology questions on images is of timely need. To approach this, we curate and release Tree Of Life-10m, the largest and most diverse ML-ready dataset of biology images. We then develop Bioclip, a foundation model for the tree of life, leveraging the unique properties of bi-ology captured by Treeoflife-10m, namely the abun-dance and variety of images of plants, animals, and fungi, together with the availability of rich structured biological knowledge. We rigorously benchmark our approach on di-verse fine-grained biology classification tasks and find that BloCLIP consistently and substantially outperforms existing baselines (by 16% to 17% absolute). Intrinsic evaluation reveals that BloCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability.11imageomics.github.io/bioclip has models, data and code.",
                "authors": "Samuel Stevens, Jiaman Wu, Matthew J. Thompson, Elizabeth G. Campolongo, Chan Hee Song, David Carlyn, Li Dong, W. Dahdul, Charles V. Stewart, Tanya Y. Berger-Wolf, Wei-Lun Chao, Yu Su",
                "citations": 33
            },
            {
                "title": "Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review",
                "abstract": "This comprehensive review delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). The development of Artificial Intelligence (AI), from its inception in the 1950s to the emergence of advanced neural networks and deep learning architectures, has made a breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in Vision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt engineering is the process of structuring inputs, which has emerged as a crucial technique to maximize the utility and accuracy of these models. This paper explores both foundational and advanced methodologies of prompt engineering, including techniques such as self-consistency, chain-of-thought, and generated knowledge, which significantly enhance model performance. Additionally, it examines the prompt method of VLMs through innovative approaches such as Context Optimization (CoOp), Conditional Context Optimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this discussion is the aspect of AI security, particularly adversarial attacks that exploit vulnerabilities in prompt engineering. Strategies to mitigate these risks and enhance model robustness are thoroughly reviewed. The evaluation of prompt methods is also addressed, through both subjective and objective metrics, ensuring a robust analysis of their efficacy. This review also reflects the essential role of prompt engineering in advancing AI capabilities, providing a structured framework for future research and application.",
                "authors": "Banghao Chen, Zhaofeng Zhang, Nicolas Langren'e, Shengxin Zhu",
                "citations": 136
            },
            {
                "title": "AIM: Adapting Image Models for Efficient Video Action Recognition",
                "abstract": "Recent vision transformer based video models mostly follow the ``image pre-training then finetuning\"paradigm and have achieved great success on multiple video benchmarks. However, full finetuning such a video model could be computationally expensive and unnecessary, given the pre-trained image transformer models have demonstrated exceptional transferability. In this work, we propose a novel method to Adapt pre-trained Image Models (AIM) for efficient video understanding. By freezing the pre-trained image model and adding a few lightweight Adapters, we introduce spatial adaptation, temporal adaptation and joint adaptation to gradually equip an image model with spatiotemporal reasoning capability. We show that our proposed AIM can achieve competitive or even better performance than prior arts with substantially fewer tunable parameters on four video action recognition benchmarks. Thanks to its simplicity, our method is also generally applicable to different image pre-trained models, which has the potential to leverage more powerful image foundation models in the future. The project webpage is \\url{https://adapt-image-models.github.io/}.",
                "authors": "Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, Mu Li",
                "citations": 122
            },
            {
                "title": "RingMo-SAM: A Foundation Model for Segment Anything in Multimodal Remote-Sensing Images",
                "abstract": "The proposal of the segment anything model (SAM) has created a new paradigm for the deep-learning-based semantic segmentation field and has shown amazing generalization performance. However, we find it may fail or perform poorly on multimodal remote-sensing scenarios, especially synthetic aperture radar (SAR) images. Besides, SAM does not provide category information for objects. In this article, we propose a foundation model for multimodal remote-sensing image segmentation called RingMo-SAM, which can not only segment anything in optical and SAR remote-sensing data, but also identify object categories. First, a large-scale dataset containing millions of segmentation instances is constructed by collecting multiple open-source datasets in this field to train the model. Then, by constructing an instance-type and terrain-type category-decoupling mask decoder (CDMDecoder), the categorywise segmentation of various objects is achieved. In addition, a prompt encoder embedded with the characteristics of multimodal remote-sensing data is designed. It not only supports multibox prompts to improve the segmentation accuracy of multiobjects in complicated remote-sensing scenes, but also supports SAR characteristics prompts to improve the segmentation performance on SAR images. Extensive experimental results on several datasets including iSAID, ISPRS Vaihingen, ISPRS Potsdam, AIR-PolSAR-Seg, and so on have demonstrated the effectiveness of our method.",
                "authors": "Zhiyuan Yan, Junxi Li, Xuexue Li, Ruixue Zhou, Wenkai Zhang, Yingchao Feng, W. Diao, Kun Fu, Xian Sun",
                "citations": 31
            },
            {
                "title": "Geometric Latent Diffusion Models for 3D Molecule Generation",
                "abstract": "Generative models, especially diffusion models (DMs), have achieved promising results for generating feature-rich geometries and advancing foundational science problems such as molecule design. Inspired by the recent huge success of Stable (latent) Diffusion models, we propose a novel and principled method for 3D molecule generation named Geometric Latent Diffusion Models (GeoLDM). GeoLDM is the first latent DM model for the molecular geometry domain, composed of autoencoders encoding structures into continuous latent codes and DMs operating in the latent space. Our key innovation is that for modeling the 3D molecular geometries, we capture its critical roto-translational equivariance constraints by building a point-structured latent space with both invariant scalars and equivariant tensors. Extensive experiments demonstrate that GeoLDM can consistently achieve better performance on multiple molecule generation benchmarks, with up to 7\\% improvement for the valid percentage of large biomolecules. Results also demonstrate GeoLDM's higher capacity for controllable generation thanks to the latent modeling. Code is provided at \\url{https://github.com/MinkaiXu/GeoLDM}.",
                "authors": "Minkai Xu, Alexander Powers, R. Dror, Stefano Ermon, J. Leskovec",
                "citations": 97
            },
            {
                "title": "An analytical solution for the horizontal vibration behavior of a cylindrical rigid foundation in poroelastic soil layer",
                "abstract": "The large size embedded foundation is widely used in the engineering, but the finite thickness of soil layer underlying this foundation is usually neglected in design, which leads to the non‐negligible error of calculation. By virtue of Biot's elastodynamic theory, this paper proposes a simple method to discuss the horizontal dynamic response of the cylindrical rigid foundation partially embedded in a poroelastic soil layer. First, based on the Novak plane strain model, the shaft resistance from the surrounding soil is simulated and solved. Second, the foundation end soil is assumed as a continuous medium of finite thickness, whose initial mechanism is derived from the dynamic interaction between the rigid disk and soil. Finally, the horizontal dynamic response factor is calculated by adopting newton's second law. Several cases are set to verify the rationality of the presented solution and to develop the analysis of key parameters. The numerical results suggest that the soil layer thickness has a significant influence on the dynamic vibration of the embedded foundation, and its effect is consistent with that of poroelastic half‐space when the thickness exceeds certain value.",
                "authors": "Zijian Yang, X. Zou",
                "citations": 30
            },
            {
                "title": "General Object Foundation Model for Images and Videos at Scale",
                "abstract": "We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos. Through a unified framework, GLEE accomplishes detection, segmentation, tracking, grounding, and identification of arbitrary objects in the open world scenario for various object perception tasks. Adopting a cohesive learning strategy, GLEE acquires knowledge from diverse data sources with varying supervision levels to formu-late general object representations, excelling in zero-shot transfer to new data and tasks. Specifically, we employ an image encoder, text encoder, and visual prompter to handle multimodal inputs, enabling to simultaneously solve various object-centric downstream tasks while maintaining state-of-the-art performance. Demonstrated through extensive training on over five million images from diverse benchmarks, GLEE exhibits remarkable versatility and improved generalization performance, efficiently tack-ling downstream tasks without the need for task-specific adaptation. By integrating large volumes of automatically labeled data, we further enhance its zero-shot generalization capabilities. Additionally, GLEE is capable of being integrated into Large Language Models, serving as a foun-dational model to provide universal object-level information for multimodal tasks. We hope that the versatility and universality of our method will mark a significant step in the development of efficient visual foundation models for AGI systems. The models and code are released at https://github.com/FoundationVision/GLEE.",
                "authors": "Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai",
                "citations": 25
            },
            {
                "title": "Foundational Models Defining a New Era in Vision: A Survey and Outlook",
                "abstract": "Vision systems to see and reason about the compositional nature of visual scenes are fundamental to understanding our world. The complex relations between objects and their locations, ambiguities, and variations in the real-world environment can be better described in human language, naturally governed by grammatical rules and other modalities such as audio and depth. The models learned to bridge the gap between such modalities coupled with large-scale training data facilitate contextual reasoning, generalization, and prompt capabilities at test time. These models are referred to as foundational models. The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box, having interactive dialogues by asking questions about an image or video scene or manipulating the robot's behavior through language instructions. In this survey, we provide a comprehensive review of such emerging foundational models, including typical architecture designs to combine different modalities (vision, text, audio, etc), training objectives (contrastive, generative), pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous. We discuss the open challenges and research directions for foundational models in computer vision, including difficulties in their evaluations and benchmarking, gaps in their real-world understanding, limitations of their contextual understanding, biases, vulnerability to adversarial attacks, and interpretability issues. We review recent developments in this field, covering a wide range of applications of foundation models systematically and comprehensively. A comprehensive list of foundational models studied in this work is available at \\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.",
                "authors": "Muhammad Awais, Muzammal Naseer, Salman Siddique Khan, R. Anwer, Hisham Cholakkal, M. Shah, Ming Yang, F. Khan",
                "citations": 92
            },
            {
                "title": "A foundation model for clinician-centered drug repurposing",
                "abstract": "Of the several thousand diseases that affect humans, only about 500 have treatments approved by the U.S. Food and Drug Administration. Even for those with approved treatments, discovering new drugs can offer alternative options that cause fewer side effects and replace drugs that are ineffective for certain patient groups. However, identifying new therapeutic opportunities for diseases with limited treatment options remains a challenge, as existing algorithms often perform poorly. Here, we leverage recent advances in geometric deep learning and human-centered AI to introduce TxGNN, a model for identifying therapeutic opportunities for diseases with limited treatment options and minimal molecular understanding. TxGNN is a graph neural network pre-trained on a comprehensive knowledge graph of 17,080 clinically-recognized diseases and 7,957 therapeutic candidates. The model can process various therapeutic tasks, such as indication and contraindication prediction, in a unified formulation. Once trained, we show that TxGNN can perform zero-shot inference on new diseases without additional parameters or fine-tuning on ground truth labels. Evaluation of TxGNN shows significant improvements over existing methods, with up to 49.2% higher accuracy in indication tasks and 35.1% higher accuracy in contraindication tasks. TxGNN can also predict therapeutic use for new drugs developed since June 2021. To facilitate interpretation and analysis of the model's predictions by clinicians, we develop a human-AI explorer for TxGNN and evaluate its usability with medical experts. Finally, we demonstrate that TxGNN's novel predictions are consistent with off-label prescription decisions made by clinicians in a large healthcare system. These label-efficient and clinician-centered learning systems pave the way for improvements for many therapeutic tasks.",
                "authors": "K. Huang, P. Chandak, Q. Wang, Shreya Havaldar, A. Vaid, J. Leskovec, G. Nadkarni, B. Glicksberg, Nils Gehlenborg, M. Zitnik",
                "citations": 25
            },
            {
                "title": "Leveraging medical Twitter to build a visual–language foundation model for pathology AI",
                "abstract": "The lack of annotated publicly available medical images is a major barrier for innovations. At the same time, many de-identified images and much knowledge are shared by clinicians on public forums such as medical Twitter. Here we harness these crowd platforms to curate OpenPath, a large dataset of 208,414 pathology images paired with natural language descriptions. This is the largest public dataset for pathology images annotated with natural text. We demonstrate the value of this resource by developing PLIP, a multimodal AI with both image and text understanding, which is trained on OpenPath. PLIP achieves state-of-the-art zero-shot and transfer learning performances for classifying new pathology images across diverse tasks. Moreover, PLIP enables users to retrieve similar cases by either image or natural language search, greatly facilitating knowledge sharing. Our approach demonstrates that publicly shared medical information is a tremendous resource that can be harnessed to advance biomedical AI.",
                "authors": "Zhi Huang, Federico Bianchi, Mert Yuksekgonul, T. Montine, J. Zou",
                "citations": 26
            },
            {
                "title": "OmniVL: One Foundation Model for Image-Language and Video-Language Tasks",
                "abstract": "This paper presents OmniVL, a new foundation model to support both image-language and video-language tasks using one universal architecture. It adopts a unified transformer-based visual encoder for both image and video inputs, and thus can perform joint image-language and video-language pretraining. We demonstrate, for the first time, such a paradigm benefits both image and video tasks, as opposed to the conventional one-directional transfer (e.g., use image-language to help video-language). To this end, we propose a decoupled joint pretraining of image-language and video-language to effectively decompose the vision-language modeling into spatial and temporal dimensions and obtain performance boost on both image and video tasks. Moreover, we introduce a novel unified vision-language contrastive (UniVLC) loss to leverage image-text, video-text, image-label (e.g., image classification), video-label (e.g., video action recognition) data together, so that both supervised and noisily supervised pretraining data are utilized as much as possible. Without incurring extra task-specific adaptors, OmniVL can simultaneously support visual only tasks (e.g., image classification, video action recognition), cross-modal alignment tasks (e.g., image/video-text retrieval), and multi-modal understanding and generation tasks (e.g., image/video question answering, captioning). We evaluate OmniVL on a wide range of downstream tasks and achieve state-of-the-art or competitive results with similar model size and data scale.",
                "authors": "Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, Lu Yuan",
                "citations": 135
            },
            {
                "title": "LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation",
                "abstract": "Layout-to-image generation refers to the task of synthesizing photo-realistic images based on semantic layouts. In this paper, we propose LayoutDiffuse that adapts a foundational diffusion model pretrained on large-scale image or text-image datasets for layout-to-image generation. By adopting a novel neural adaptor based on layout attention and task-aware prompts, our method trains efficiently, generates images with both high perceptual quality and layout alignment, and needs less data. Experiments on three datasets show that our method significantly outperforms other 10 generative models based on GANs, VQ-VAE, and diffusion models.",
                "authors": "Jiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun Xiao, Mu Li",
                "citations": 54
            },
            {
                "title": "Brant: Foundation Model for Intracranial Neural Signal",
                "abstract": "We propose a foundation model named Brant for modeling intracranial recordings, which learns powerful representations of intracranial neural signals by pre-training, providing a large-scale, off-the-shelf model for medicine. Brant is the largest model in the field of brain signals and is pre-trained on a large corpus of intracranial data collected by us. The design of Brant is to capture long-term temporal dependency and spatial correlation from neural signals, combining the information in both time and frequency domains. As a foundation model, Brant achieves SOTA performance on various downstream tasks (i.e. neural signal forecasting, frequency-phase forecasting, imputation and seizure detection), showing the generalization ability to a broad range of tasks. The low-resource label analysis and representation visualization further illustrate the effectiveness of our pre-training strategy. In addition, we explore the effect of model size to show that a larger model with a higher capacity can lead to performance improvements on our dataset. The source code and pre-trained weights are available at: https://zju-brainnet.github. io/Brant.github.io/ .",
                "authors": "Daoze Zhang, Zhizhang Yuan, Yang Yang, Junru Chen, Jingjing Wang, Yafeng Li",
                "citations": 12
            },
            {
                "title": "Test of Time: Instilling Video-Language Models with a Sense of Time",
                "abstract": "Modelling and understanding time remains a challenge in contemporary video understanding models. With language emerging as a key driver towards powerful generalization, it is imperative for foundational video-language models to have a sense of time. In this paper, we consider a specific aspect of temporal understanding: consistency of time order as elicited by before/after relations. We establish that seven existing video-language models struggle to understand even such simple temporal relations. We then question whether it is feasible to equip these foundational models with temporal awareness without re-training them from scratch. Towards this, we propose a temporal adaptation recipe on top of one such model, VideoCLIp, based on post-pretraining on a small amount of video-text data. We conduct a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness. We observe encouraging performance gains especially when the task needs higher time awareness. Our work serves as a first step towards probing and instilling a sense of time in existing video-language models without the need for data and compute-intense training from scratch.",
                "authors": "Piyush Bagad, Makarand Tapaswi, Cees G. M. Snoek",
                "citations": 30
            },
            {
                "title": "TransWorldNG: Traffic Simulation via Foundation Model",
                "abstract": "Traffic simulation is a crucial tool for transportation decision-making and policy development. However, achieving realistic simulations in the face of the high dimensionality and heterogeneity of traffic environments is a longstanding challenge. In this paper, we present TransWordNG, a traffic simulator that uses Data-driven algorithms and Graph Computing techniques to learn traffic dynamics from real data. The functionality and structure of TransWorldNG are introduced, which utilize a foundation model for transportation management and control. The results demonstrate that TransWorldNG can generate more realistic traffic patterns compared to traditional simulators. Additionally, TransWorldNG exhibits better scalability, as it shows linear growth in computation time as the scenario scale increases. To the best of our knowledge, this is the first traffic simulator that can automatically learn traffic patterns from real-world data and efficiently generate accurate and realistic traffic environments.",
                "authors": "Dingsu Wang, Xuhong Wang, Liang Chen, Shengyue Yao, Mi Jing, Honghai Li, Li Li, Shiqiang Bao, Feiyue Wang, Yilun Lin",
                "citations": 11
            },
            {
                "title": "Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions",
                "abstract": "Non-coding RNA structure and function are essential to understanding various biological processes, such as cell signaling, gene expression, and post-transcriptional regulations. These are all among the core problems in the RNA field. With the rapid growth of sequencing technology, we have accumulated a massive amount of unannotated RNA sequences. On the other hand, expensive experimental observatory results in only limited numbers of annotated data and 3D structures. Hence, it is still challenging to design computational methods for predicting their structures and functions. The lack of annotated data and systematic study causes inferior performance. To resolve the issue, we propose a novel RNA foundation model (RNA-FM) to take advantage of all the 23 million non-coding RNA sequences through self-supervised learning. Within this approach, we discover that the pre-trained RNA-FM could infer sequential and evolutionary information of non-coding RNAs without using any labels. Furthermore, we demonstrate RNA-FM’s effectiveness by applying it to the downstream secondary/3D structure prediction, SARS-CoV-2 genome structure and evolution prediction, protein-RNA binding preference modeling, and gene expression regulation modeling. The comprehensive experiments show that the proposed method improves the RNA structural and functional modelling results significantly and consistently. Despite only being trained with unlabelled data, RNA-FM can serve as the foundational model for the field.",
                "authors": "Jiayang Chen, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze Yu, Licheng Zong, Liang Hong, Jin Xiao, Irwin King, Yu Li",
                "citations": 90
            },
            {
                "title": "Towards a Foundation Model of the Mouse Visual Cortex",
                "abstract": ",",
                "authors": "Eric Y. Wang, Paul G. Fahey, Kayla Ponder, Zhuokun Ding, Andersen Chang, Taliah Muhammad, Saumil S. Patel, Zhiwei Ding, Dat Tran, Jiakun Fu, S. Papadopoulos, K. Franke, Alexander S. Ecker, Jacob Reimer, X. Pitkow, Fabian H Sinz, A. Tolias",
                "citations": 18
            },
            {
                "title": "Visually sensitive seizures: An updated review by the Epilepsy Foundation",
                "abstract": "Light flashes, patterns, or color changes can provoke seizures in up to 1 in 4000 persons. Prevalence may be higher because of selection bias. The Epilepsy Foundation reviewed light‐induced seizures in 2005. Since then, images on social media, virtual reality, three‐dimensional (3D) movies, and the Internet have proliferated. Hundreds of studies have explored the mechanisms and presentations of photosensitive seizures, justifying an updated review. This literature summary derives from a nonsystematic literature review via PubMed using the terms “photosensitive” and “epilepsy.” The photoparoxysmal response (PPR) is an electroencephalography (EEG) phenomenon, and photosensitive seizures (PS) are seizures provoked by visual stimulation. Photosensitivity is more common in the young and in specific forms of generalized epilepsy. PS can coexist with spontaneous seizures. PS are hereditable and linked to recently identified genes. Brain imaging usually is normal, but special studies imaging white matter tracts demonstrate abnormal connectivity. Occipital cortex and connected regions are hyperexcitable in subjects with light‐provoked seizures. Mechanisms remain unclear. Video games, social media clips, occasional movies, and natural stimuli can provoke PS. Virtual reality and 3D images so far appear benign unless they contain specific provocative content, for example, flashes. Images with flashes brighter than 20 candelas/m2 at 3‐60 (particularly 15‐20) Hz occupying at least 10 to 25% of the visual field are a risk, as are red color flashes or oscillating stripes. Equipment to assay for these characteristics is probably underutilized. Prevention of seizures includes avoiding provocative stimuli, covering one eye, wearing dark glasses, sitting at least two meters from screens, reducing contrast, and taking certain antiseizure drugs. Measurement of PPR suppression in a photosensitivity model can screen putative antiseizure drugs. Some countries regulate media to reduce risk. Visually‐induced seizures remain significant public health hazards so they warrant ongoing scientific and regulatory efforts and public education.",
                "authors": "R. Fisher, J. Acharya, Fiona M. Baumer, J. French, P. Parisi, Jessica Solodar, J. Szaflarski, L. Thio, B. Tolchin, A. Wilkins, D. Kasteleijn-Nolst Trenité",
                "citations": 46
            },
            {
                "title": "Sustainability in the Circular Economy: Insights and Dynamics of Designing Circular Business Models",
                "abstract": "The integration of sustainability in the circular economy is an emerging paradigm that can offer a long term vision to achieve environmental and social sustainability targets in line with the United Nation’s Sustainable Development Goals. Developing scalable and sustainable impacts in circular economy business models (CEBMs) has many challenges. While many advanced technology manufacturing firms start as small enterprises, remarkably little is known about how material reuse firms in sociotechnical systems transition towards circular business models. Research into CEBMs integrating sustainability and environmental conservation is still in its early stages. There has been increased interest in sustainability and circular economy research, but current research is fragmented. The innovation surrounding CEBMs eludes some firms with relatively limited evidence of the transitional perspective necessary to integrate aspects of sustainability. This lack of evidence is especially applicable to the context of circular economy practices in small and medium enterprises in the United States regarding capabilities, operations obstacles, and elements of success in designing circular business models. Based on a qualitative, interview-based inductive study of a material reuse firm, our research develops a conceptual model of the critical success factors and obstacles that are part of implementing circular economy practices. Firms must first manage strategic enablers and monitor tactical enablers to achieve sustainability goals. In this study, we identify the underlying enablers of how these capabilities affect the transition to a CEBM that integrates sustainability. The framework emerging from our findings highlights the interplay of CEBM, innovation success factors, and obstacles at a micro-level. The investigation of a material reuse firm serves as the foundation for developing a framework for how managers can alter a company and revise the business model to transition towards a more innovative circular economy.",
                "authors": "Usama Awan, Robert Sroufe",
                "citations": 182
            },
            {
                "title": "Permeability Prediction Model Modified on Kozeny-Carman for Building Foundation of Clay Soil",
                "abstract": "Clay soil is a common building foundation material, and its permeability is very important for the safety of foundation pits and the later settlement of buildings. However, the traditional Kozeny-Carman (K-C) equation shows serious discrepancies when predicting the permeability of clay in building foundation treatment. Therefore, solving the application of K-C equation in clay is a problem faced by the engineers and scholars. In this paper, the influence of clay mineralogy on pore structure and permeability is analyzed, and then the effective e (eeff) and effective SSA (Seff) are proposed. Based on the eeff and Seff, the permeability prediction model modified on Kozeny-Carman is built. Then, seepage experiments are conducted on two types of clay samples to test this prediction model; at the same time, the MIP combining freeze-drying methods are used to obtain the Seff and eeff. Through the discussion of the test results, three main conclusions are obtained: (1) there are invalid pores in clay due to the influence of clay mineral, this is the reason for which K-C equation is unsuitable for clay; (2) the eeff and Seff can reflect the structural state of clay during seepage; (3) the results of the permeability prediction model in this paper agree well with the test results, which indicates that this prediction model is applicable to clay. The research results of this paper are significant to solve the academic problem that K-C equation is not applicable to clay and significant to ensure the safety of building foundation pits in clay areas.",
                "authors": "Jian Chen, Huawei Tong, Jie Yuan, Y. Fang, R. Gu",
                "citations": 32
            },
            {
                "title": "Using Identity-Based Cryptography as a Foundation for an Effective and Secure Cloud Model for E-Health",
                "abstract": "Nowadays, one of the most popular applications is cloud computing for storing data and information through World Wide Web. Since cloud computing has become available, users are rapidly increasing. Cloud computing enables users to obtain a better and more effective application at a lower cost in a more satisfactory way. Health services data must therefore be kept as safe and secure as possible because the release of this data could have serious consequences for patients. A framework for security and privacy must be employed to store and manage extremely sensitive data. Patients' confidential health records have been encrypted and saved in the cloud using cypher text so far. To ensure privacy and security in a cloud computing environment is a big issue. The medical system has been designed as a standard, access of records, and effective use by medical practitioners as required. In this paper, we propose a novel algorithm along with implementation details as an effective and secure E-health cloud model using identity-based cryptography. The comparison of the proposed and existing techniques has been carried out in terms of time taken for encryption and decryption, energy, and power. Decryption time has been decreased up to 50% with the proposed method of cryptography. As it will take less time for decryption, less power is consumed for doing the cryptography operations.",
                "authors": "Shikha Mittal, Ankit Bansal, D. Gupta, Sapna Juneja, H. Turabieh, Mahmoud M Elarabawy, Ashish Sharma, Zelalem Kiros Bitsue",
                "citations": 63
            },
            {
                "title": "Foundation Transformers",
                "abstract": "A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name\"Transformers\", the above areas use different implementations for better performance, e.g., Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).",
                "authors": "Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, Furu Wei",
                "citations": 24
            },
            {
                "title": "Model and Predictive Uncertainty: A Foundation for Smooth Ambiguity Preferences",
                "abstract": "Smooth ambiguity preferences (Klibanoff, Marinacci, and Mukerji (2005)) describe a decision maker who evaluates each act \n f according to the twofold expectation\n \n \n \n V\n (\n f\n )\n =\n \n \n ∫\n \n \n P\n \n \n ϕ\n (\n \n \n ∫\n \n \n Ω\n \n \n u\n (\n f\n )\n \n d\n p\n )\n \n d\n μ\n (\n p\n )\n \n \n defined by a utility function \n u, an ambiguity index \n ϕ, and a belief \n μ over a set \n \n P\n of probabilities. We provide an axiomatic foundation for the representation, taking as a primitive a preference over Anscombe–Aumann acts. We study a special case where \n \n P\n is a subjective statistical model that is point identified, that is, the decision maker believes that the true law \n \n p\n ∈\n P\n can be recovered empirically. Our main axiom is a joint weakening of Savage's sure‐thing principle and Anscombe–Aumann's mixture independence. In addition, we show that the parameters of the representation can be uniquely recovered from preferences, thereby making operational the separation between ambiguity attitude and perception, a hallmark feature of the smooth ambiguity representation.\n",
                "authors": "Tommaso Denti, L. Pomatto",
                "citations": 23
            },
            {
                "title": "Research on the Settlement Prediction Model of Foundation Pit Based on the Improved PSO-SVM Model",
                "abstract": "This paper presents a settlement prediction method based on PSO optimized SVM for improving the accuracy of foundation pit settlement prediction. Firstly, the method uses the SA algorithm to improve the traditional PSO algorithm, and thus, the overall optimization-seeking ability of the PSO algorithm is improved. Secondly, the improved PSO algorithm is used to train the SVM algorithm. Finally, the optimal SVM model is obtained, and the trained model is used in foundation pit settlement prediction. The results suggest that the settling results obtained from the optimized model are closer to the actual values and also more advantageous in indicators such as RMSE. The fitting value R2 = 0.9641, which is greater, indicates a better fitting effect. Thus, it is indicated that the improvement method is feasible.",
                "authors": "Zhibin Song, Shurong Liu, Mingyue Jiang, Suling Yao",
                "citations": 17
            },
            {
                "title": "Towards artificial general intelligence via a multimodal foundation model",
                "abstract": null,
                "authors": "Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jing Wen, Haoyu Lu, Ruihua Song, Xin Gao, Tao Xiang, Haoran Sun, Jiling Wen",
                "citations": 162
            },
            {
                "title": "Large Language Models and the Reverse Turing Test",
                "abstract": "Abstract Large language models (LLMs) have been transformative. They are pretrained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and, more recently, LaMDA, both of them LLMs, can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a reverse Turing test. If so, then by studying interviews, we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable, they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function.",
                "authors": "T. Sejnowski",
                "citations": 84
            },
            {
                "title": "An efficient computational model for vibration behavior of a functionally graded sandwich plate in a hygrothermal environment with viscoelastic foundation effects",
                "abstract": null,
                "authors": "Mohamad W. Zaitoun, ABDELBAKI CHIKH, A. Chikh, A. Tounsi, A. Sharif, M. Al-Osta, S. Al-Dulaijan, M. M. Al-Zahrani",
                "citations": 116
            },
            {
                "title": "Free vibration analysis of an electro-elastic GPLRC cylindrical shell surrounded by viscoelastic foundation using modified length-couple stress parameter",
                "abstract": "Abstract Due to the rapid development of process manufacturing, composite materials with graphene-reinforcement have obtained commercially notices in promoted engineering applications. For this regard, vibrational characteristics of a cylindrical nanoshell reinforced by graphene nanoplatelets (GPL) and coupled with piezoelectric actuator (PIAC) is investigated. Also, the nanostructure is embedded in a viscoelastic medium. The material properties of piece-wise graphene-reinforced composite (GPLRC) are assumed to be graded in the thickness direction of a cylindrical nanoshell and estimated through a nanomechanical model. For the first time in the current study is considering the effects of piezoelectric layer, viscoelastic foundation, GPLRC, and size-effects on the frequency responses of the GPLRC cylindrical nanoshell coupled with PIAC and by assuming perfect bonding between the core (GPLRC cylindrical shell) and the piezoelectric layer. The governing equations and boundary conditions have been developed using minimum potential energy and solved with the aid of the generalized differential quadrature method. In addition, because of piezoelectric layer, Maxwell's equation is derived. The results show that viscoelastic foundation, piezoelectric layer, GPL distribution pattern, length scale parameter and GPL weight function have important role in the frequency characteristics of the GPLRC cylindrical nanoshell coupled with PIAC and surrounded by viscoelastic foundation. The results of the current study are useful suggestions for design of materials science, micro-electro-mechanical systems, and nanoelectromechanical systems such as nanoactuators and nanosensors. Communicated by Francesco Tornabene.",
                "authors": "Aria Ghabussi, N. Ashrafi, Aghil Shavalipour, A. Hosseinpour, M. Habibi, H. Moayedi, B. Babaei, H. Safarpour",
                "citations": 119
            },
            {
                "title": "Seismic performance analysis of a wind turbine with a monopile foundation affected by sea ice based on a simple numerical method",
                "abstract": "To investigate the seismic performance of a wind turbine that is influenced by both the ice load and the seismic load, the research proposes a numerical approach for simulating the seismic behavior of a wind turbine on a monopile foundation. First, the fluid–solid coupled equation for the water–ice–wind turbine is simplified by assigning reasonable boundary conditions and solving the motion equation, and the seismic motion equation of the wind turbine is developed. Then, on this basis, we propose a simplified 3D numerical model that can simulate the interactions among the wind turbine, water and sea ice. By conducting shaking table tests, the results demonstrate that the established numerical model is effective. Finally, we investigate the effect of the boundary range and ice thickness on the seismic performance of a turbine under near-field and far-field seismic actions. Research results illustrate that ice changes the distribution form of the hydrodynamic pressure. Moreover, the thickness of the ice greatly influences the seismic behavior, while the influence of the ice boundary range is only within a certain range. Additionally, the ice load decreases the energy-dissipating capacity of the wind turbine, so the earthquake resilience of the wind turbine is significantly decreased.",
                "authors": "Shuai Huang, Mingming Huang, Y. Lyu",
                "citations": 98
            },
            {
                "title": "Digital Twin in the IoT Context: A Survey on Technical Features, Scenarios, and Architectural Models",
                "abstract": "Digital twin (DT) is an emerging concept that is gaining attention in various industries. It refers to the ability to clone a physical object (PO) into a software counterpart. The softwarized object, termed logical object, reflects all the important properties and characteristics of the original object within a specific application context. To fully determine the expected properties of the DT, this article surveys the state-of-the-art starting from the original definition within the manufacturing industry. It takes into account related proposals emerging in other fields, namely augmented and virtual reality (e.g., avatars), multiagent systems, and virtualization. This survey thereby allows for the identification of an extensive set of DT features that point to the “softwarization” of POs. To properly consolidate a shared DT definition, a set of foundational properties is identified and proposed as a common ground outlining the essential characteristics (must-haves) of a DT. Once the DT definition has been consolidated, its technical and business value is discussed in terms of applicability and opportunities. Four application scenarios illustrate how the DT concept can be used and how some industries are applying it. The scenarios also lead to a generic DT architectural model. This analysis is then complemented by the identification of software architecture models and guidelines in order to present a general functional framework for the DT. This article, eventually, analyses a set of possible evolution paths for the DT considering its possible usage as a major enabler for the softwarization process.",
                "authors": "R. Minerva, G. Lee, N. Crespi",
                "citations": 371
            },
            {
                "title": "TARDIS: A Foundation of Time-Lock Puzzles in UC",
                "abstract": null,
                "authors": "Carsten Baum, B. David, Rafael Dowsley, J. Nielsen, Sabine Oechsner",
                "citations": 41
            },
            {
                "title": "Application of GA-BP Neural Network Optimized by Grey Verhulst Model around Settlement Prediction of Foundation Pit",
                "abstract": "Due to the limitation in the prediction of the foundation pit settlement, this paper proposed a new methodology which takes advantage of the grey Verhulst model and a genetic algorithm. In the previous study, excavation times are often the only factor to predict the settlement, which is mainly because the correspondence between real-time excavation depth and the excavation time is hard to determine. To solve this issue, the supporting times are precisely recorded and the excavation depth rate can be obtained through the excavation time length and excavation depth between two adjacent supports. After the correspondence between real-time excavation depth and the excavation time is obtained, the internal friction angle, cohesion, bulk density, Poisson’s ratio, void ratio, water level changes, permeability coefficient, number of supports, and excavation depth, which can influence the settlement, are taken to be considered in this study. For the application of the methodology, the settlement monitoring point of D4, which is near the bridge pier of the highway, is studied in this paper. The predicted values of the BP neural network, GA-BP neural network, BP neural network optimized by the grey Verhulst model, and GA-BP neural network optimized by the grey Verhulst model are detailed compared with the measured values. And the evaluation indexes of RMSE, MAE, MSE, MAPE, and \n \n \n \n R\n \n \n 2\n \n \n \n are calculated for these models. The results show that the grey Verhulst model can greatly improve the consistency between predicted values and measured values, while the accuracy and resolution is still low. The genetic algorithm (GA) can greatly improve the accuracy of the predicted values, while the GA-BP neural network shows low reflection to the fluctuation of measured values. The GA-BP neural network optimized by the grey Verhulst model, which has taken the advantages of GA and the grey Verhulst model, has extremely high accuracy and well consistency with the measured values.",
                "authors": "C. Liu, Y. Wang, X. M. Hu, Y. Han, X. Zhang, L. Du",
                "citations": 49
            },
            {
                "title": "Stability of the Foundation of Buried Energy Pipeline in Permafrost Region",
                "abstract": "During operation, a buried pipeline is threatened by a variety of geological hazards, particularly in permafrost regions, where freezing-thawing disasters have a significant influence on the integrity and safety of the buried pipelines. The topographical environmental conditions along the pipeline, as well as the influence of frost heave and thaw settlement on the pipeline’s foundation soil, must be considered in the design and construction stage. Theoretical analysis, numerical modeling, field testing, and mitigation measures on vital energy pipelines in permafrost have been widely documented, but no attempt has been made to review the freezing-thawing disasters, current research methodologies, and mitigation strategies. This article reviews the formation mechanisms and mitigation measures for frost hazards (e.g., differential frost heave, thaw settlement, slope instability, frost mounds, icing, river ice scouring, and pipeline floating) along buried pipelines in permafrost regions and summarizes and prospects the major progress in the research on mechanisms, analysis methods, model test, and field monitoring based on publications of studies of key energy pipelines in permafrost regions. This review will provide scholars with a basic understanding of the challenging freezing-thawing hazards encountered by energy pipelines in permafrost regions, as well as research on the stability and mitigation of pipeline foundation soils plagued by freezing-thawing hazards in permafrost regions under a warming climate and degrading permafrost environment.",
                "authors": "Yan Li, H. Jin, Z. Wen, Xinze Li, Qi Zhang",
                "citations": 38
            },
            {
                "title": "A probabilistic graphical model foundation for enabling predictive digital twins at scale",
                "abstract": null,
                "authors": "Michael G. Kapteyn, Jacob V. R. Pretorius, K. Willcox",
                "citations": 188
            },
            {
                "title": "Parallel Factories for Smart Industrial Operations: From Big AI Models to Field Foundational Models and Scenarios Engineering",
                "abstract": null,
                "authors": "Jingwei Lu, Xingxia Wang, Xiang Cheng, J. Yang, Oliver Kwan, Xiao Wang",
                "citations": 26
            },
            {
                "title": "Influence of track foundation on the performance of ballast and concrete slab tracks under cyclic loading: Physical modelling and numerical model calibration",
                "abstract": null,
                "authors": "A. Ramos, A. Correia, R. Calçada, P. Costa, A. Esen, P. Woodward, D. Connolly, O. Laghrouche",
                "citations": 38
            },
            {
                "title": "SBML Level 3: an extensible format for the exchange and reuse of biological models",
                "abstract": "Systems biology has experienced dramatic growth in the number, size, and complexity of computational models. To reproduce simulation results and reuse models, researchers must exchange unambiguous model descriptions. We review the latest edition of the Systems Biology Markup Language (SBML), a format designed for this purpose. A community of modelers and software authors developed SBML Level 3 over the past decade. Its modular form consists of a core suited to representing reaction‐based models and packages that extend the core with features suited to other model types including constraint‐based models, reaction‐diffusion models, logical network models, and rule‐based models. The format leverages two decades of SBML and a rich software ecosystem that transformed how systems biologists build and interact with models. More recently, the rise of multiscale models of whole cells and organs, and new data sources such as single‐cell measurements and live imaging, has precipitated new ways of integrating data with models. We provide our perspectives on the challenges presented by these developments and how SBML Level 3 provides the foundation needed to support this evolution.",
                "authors": "Sarah M. Keating, Dagmar Waltemath, M. König, Fengkai Zhang, Andreas Dräger, C. Chaouiya, Frank T. Bergmann, A. Finney, C. Gillespie, T. Helikar, S. Hoops, Rahuman S. Malik-Sheriff, Stuart L. Moodie, I. Moraru, C. Myers, A. Naldi, B. Olivier, S. Sahle, J. Schaff, Lucian P. Smith, Maciej J. Swat, D. Thieffry, Leandro H. Watanabe, D. Wilkinson, M. Blinov, K. Begley, J. Faeder, Harold F. Gómez, T. Hamm, Y. Inagaki, Wolfram Liebermeister, Allyson L. Lister, D. Lucio, E. Mjolsness, C. Proctor, Karthik Raman, Nicolas Rodriguez, C. Shaffer, B. Shapiro, J. Stelling, Neil Swainston, Naoki Tanimura, J. Wagner, Martin Meier-Schellersheim, H. Sauro, B. Palsson, H. Bolouri, H. Kitano, Akira Funahashi, H. Hermjakob, J. Doyle, M. Hucka",
                "citations": 201
            },
            {
                "title": "Physical modeling of the dynamics of a revetment breakwater built on reclaimed coral calcareous sand foundation in the South China Sea—tsunami wave",
                "abstract": null,
                "authors": "Kunpeng He, Jianhong Ye",
                "citations": 23
            },
            {
                "title": "Wall Displacement and Ground-Surface Settlement Caused by Pit-in-Pit Foundation Pit in Soft Clays",
                "abstract": null,
                "authors": "Yuyong Sun, Hongju Xiao",
                "citations": 26
            },
            {
                "title": "A study on the structural behaviour of functionally graded porous plates on elastic foundation using a new quasi-3D model: Bending and free vibration analysis",
                "abstract": "This work investigates a new type of quasi-3D hyperbolic shear deformation theory is proposed in this study to \ndiscuss the statics and free vibration of functionally graded porous plates resting on elastic foundations. Material properties of porous FG plate are defined by rule of the mixture with an additional term of porosity in the through-thickness direction. By including indeterminate integral variables, the number of unknowns and governing equations of the present theory is reduced, and therefore, it is easy to use. The present approach to plate theory takes into account both transverse shear and normal \ndeformations and satisfies the boundary conditions of zero tensile stress on the plate surfaces. The equations of motion are derived from the Hamilton principle. Analytical solutions are obtained for a simply supported plate. Contrary to any other theory, the number of unknown functions involved in the displacement field is only five, as compared to six or more in the case of other shear and normal deformation theories. A comparison with the corresponding results is made to verify the accuracy and \nefficiency of the present theory. The influences of the porosity parameter, power-law index, aspect ratio, thickness ratio and the foundation parameters on bending and vibration of porous FG plate.",
                "authors": "Miloud Kaddari, A. Kaci, A. A. Bousahla, A. Tounsi, F. Bourada, AbdeldjebbarTounsi, E. A. Bedia, Mohammed Al-Osta",
                "citations": 159
            },
            {
                "title": "The Cultural Foundation of Human Memory.",
                "abstract": "Human memory, as a product of the mind and brain, is inherently private and personal. Yet, arising from the interaction between the organism and its ecology in the course of phylogeny and ontogeny, human memory is also profoundly collective and cultural. In this review, I discuss the cultural foundation of human memory. I start by briefly reflecting on the conception of memory against a historical and cultural background. I then detail a model of a culturally saturated mnemonic system in which cultural elements constitute and condition various processes of remembering, focusing on memory representation, perceptual encoding, memory function, memory reconstruction, memory expression, and memory socialization. Then I discuss research on working memory, episodic memory, and autobiographical memory as examples that further demonstrate how cultural elements shape the processes and consequences of remembering and lay the foundation for human memory. I conclude by outlining some important future directions in memory research. Expected final online publication date for the Annual Review of Psychology, Volume 72 is January 4, 2021. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",
                "authors": "Qi Wang",
                "citations": 86
            },
            {
                "title": "Torsional vibration of functionally graded nano-rod under magnetic field supported by a generalized torsional foundation based on nonlocal elasticity theory",
                "abstract": "Abstract This article contains the nonlocal elasticity theory to capture size effects in functionally graded (FG) nano-rod under magnetic field supported by a torsional foundation. Torque effect of an axial magnetic field on an FG nano-rod has been defined using Maxwell’s relation. The material properties were assumed to vary according to the power law in radial direction. The Navier equation and boundary conditions of the size-dependent FG nano-rod were derived by the Hamilton’s principle. These equations were solved by employing the generalized differential quadrature method (GDQM). Presented model has the ability to turn into the classical model if the material length scale parameter is taken to be zero. The effects of some parameters, such as inhomogeneity constant, magnetic field and small-scale parameter, were studied. As an important result of this study can be stated that an FG nano-rod model based on the nonlocal elasticity theory behaves softer and has smaller natural frequency.",
                "authors": "E. Zarezadeh, V. Hosseini, A. Hadi",
                "citations": 85
            },
            {
                "title": "Deformation Monitoring Analysis and Numerical Simulation in a Deep Foundation Pit",
                "abstract": null,
                "authors": "P. Lin, P. Liu, G. Ankit, Y. Singh",
                "citations": 20
            },
            {
                "title": "Under the Skin of Foundation NFT Auctions",
                "abstract": "Non Fungible Tokens (NFTs) have gained a solid foothold within the crypto community, and substantial amounts of money have been allocated to their trades. In this paper, we studied one of the most prominent marketplaces dedicated to NFT auctions and trades, Foundation. We analyzed the activities on Foundation and identified several intriguing underlying dynamics that occur on this platform. Moreover, We performed social network analysis on a graph that we had created based on transferred NFTs on Foundation, and then described the characteristics of this graph. Lastly, We built a neural network-based similarity model for retrieving and clustering similar NFTs. We also showed that for most NFTs, their performances in auctions were comparable with the auction performance of other NFTs in their cluster.",
                "authors": "Mohammadamin Fazli, Ali Owfi, Mohammad Reza Taesiri",
                "citations": 18
            },
            {
                "title": "Transverse Force Analysis of Adjacent Shield Tunnel Caused by Foundation Pit Excavation Considering Deformation of Retaining Structures",
                "abstract": "In order to research the theory for the variety of transverse forces of the adjacent shield tunnels caused by foundation pits excavation, the effect mechanism of foundation pit excavation on the adjacent shield tunnel was analyzed. The sidewall unloading model of the foundation pit, considering the deformation of the retaining structures, was introduced to calculate the additional stress of soil caused by foundation pit excavation. On this basis, the additional confining pressure variation model of the adjacent shield tunnel was established, considering the influence of the longitudinal deformation. Take the deep foundation pit project by the side of the shield tunnel of Hangzhou Metro Line 2 as a case study, the variation in confining pressure distribution of the adjacent shield tunnel caused by foundation pit excavation was analyzed, and a simplified finite element model was established to calculate the internal force of the segment ring structure. Moreover, the influence factors were analyzed, such as the deformation of the foundation pit retaining structure, the clearance between the foundation pit and the adjacent tunnel, and the buried depth of the tunnel. The present study suggests that the foundation pit excavation reduces the confining pressure of the adjacent shield tunnel, increases the absolute value of bending moment and shear force, and decreases the axial force at the top and bottom of the tunnel’s segment ring. With the increase in the deformation of the foundation pit’s retaining structure, the absolute value of the additional confining pressure on the adjacent tunnel increases, and the response of the bending moment to the foundation pit excavation unloading is more obvious than the variation in the confining pressure. When the buried depth of the adjacent shield tunnel is deeper than the excavation depth of the foundation pit, the influence of the excavation on the tunnel will be obviously weakened. With the decrease in the distance between the pit and tunnel, the influence of the excavation on the tunnel will be enhanced.",
                "authors": "Xinhai Zhang, G. Wei, Xinbei Lin, Chang Xia, X. Wei",
                "citations": 17
            },
            {
                "title": "Mitigation of Liquefaction Triggering and Foundation Settlement by MICP Treatment",
                "abstract": "AbstractCentrifuge modeling was used to study the performance of loose sand treated with microbial-induced calcium carbonate precipitation (MICP) to improve liquefaction resistance to triggering an...",
                "authors": "A. Zamani, Peng Xiao, T. Baumer, T. Carey, B. Sawyer, J. DeJong, R. Boulanger",
                "citations": 31
            },
            {
                "title": "Deformation Prediction of a Deep Foundation Pit Based on the Combination Model of Wavelet Transform and Gray BP Neural Network",
                "abstract": "The purpose of this study was to predict the deformation of a deep foundation pit based on a combination model of wavelet transform and gray BP neural network. Using a case of a deep foundation pit, a combination model of wavelet transform and gray BP neural network was used to predict the deformation of the deep foundation pit. The results show that compared with the traditional gray BP neural network model, the relative error of the combination model of wavelet transform and gray BP neural network was reduced by 2.38%. This verified that the combined model has high accuracy and reliability in the prediction of foundation pit deformation and also conforms to the actual situation of the project. The research results can provide a valuable reference for foundation pit deformation monitoring.",
                "authors": "Qiang Liu, Chun-yan Yang, Li Lin",
                "citations": 14
            },
            {
                "title": "Fractional Modeling in Action: a Survey of Nonlocal Models for Subsurface Transport, Turbulent Flows, and Anomalous Materials",
                "abstract": null,
                "authors": "J. Suzuki, Mamikon A. Gulian, Mohsen Zayernouri, M. D'Elia",
                "citations": 44
            },
            {
                "title": "Fractional Modeling in Action: a Survey of Nonlocal Models for Subsurface Transport, Turbulent Flows, and Anomalous Materials",
                "abstract": null,
                "authors": "J. Suzuki, Mamikon A. Gulian, Mohsen Zayernouri, M. D'Elia",
                "citations": 44
            },
            {
                "title": "Recent Advances in Models, Mechanisms, Biomarkers, and Interventions in Cisplatin-Induced Acute Kidney Injury",
                "abstract": "Cisplatin is a widely used chemotherapeutic agent used to treat solid tumours, such as ovarian, head and neck, and testicular germ cell. A known complication of cisplatin administration is acute kidney injury (AKI). The development of effective tumour interventions with reduced nephrotoxicity relies heavily on understanding the molecular pathophysiology of cisplatin-induced AKI. Rodent models have provided mechanistic insight into the pathophysiology of cisplatin-induced AKI. In the subsequent review, we provide a detailed discussion of recent advances in the cisplatin-induced AKI phenotype, principal mechanistic findings of injury and therapy, and pre-clinical use of AKI rodent models. Cisplatin-induced AKI murine models faithfully develop gross manifestations of clinical AKI such as decreased kidney function, increased expression of tubular injury biomarkers, and tubular injury evident by histology. Pathways involved in AKI include apoptosis, necrosis, inflammation, and increased oxidative stress, ultimately providing a translational platform for testing the therapeutic efficacy of potential interventions. This review provides a discussion of the foundation laid by cisplatin-induced AKI rodent models for our current understanding of AKI molecular pathophysiology.",
                "authors": "S. Holditch, C. N. Brown, A. M. Lombardi, Khoa N. Nguyen, C. Edelstein",
                "citations": 265
            },
            {
                "title": "Dynamic Analysis of Sandwich Auxetic Honeycomb Plates Subjected to Moving Oscillator Load on Elastic Foundation",
                "abstract": "Based on Mindlin plate theory and finite element method (FEM), dynamic response analysis of sandwich composite plates with auxetic honeycomb core resting on the elastic foundation (EF) under moving oscillator load is investigated in this work. Moving oscillator load includes spring-elastic k and damper c. The EF with two coefficients was modelled by Winkler and Pasternak. The system of equations of motion of the sandwich composite plate can be solved by Newmark’s direct integration method. The reliability of the present method is verified through comparison with the results other methods available in the literature. In addition, the effects of structural parameters, material properties, and moving oscillator loads to the dynamic response of the auxetic honeycomb plate are studied.",
                "authors": "T. Tran, Q. Pham, T. Nguyen-Thoi, The-Van Tran",
                "citations": 66
            },
            {
                "title": "Application of shifted Chebyshev polynomial-based Rayleigh–Ritz method and Navier’s technique for vibration analysis of a functionally graded porous beam embedded in Kerr foundation",
                "abstract": null,
                "authors": "S. K. Jena, S. Chakraverty, M. Malikan",
                "citations": 64
            },
            {
                "title": "A transversely isotropic magneto-electro-elastic Timoshenko beam model incorporating microstructure and foundation effects",
                "abstract": null,
                "authors": "G. Y. Zhang, Y. Qu, Xin-Lin Gao, F. Jin",
                "citations": 75
            },
            {
                "title": "Three‐dimensional hydromechanical modeling of internal erosion in dike‐on‐foundation",
                "abstract": "Currently, numerical studies at the real scale of an entire engineering structure considering internal erosion are still rare. This paper presents a three‐dimensional (3D) numerical simulation of the effects of internal erosion within a linear dike located on a foundation. A two‐dimensional (2D) finite element code has been extended to 3D in order to analyze the impact of internal erosion under more realistic hydromechanical conditions. The saturated soil has been considered as a mixture of four interacting constituents: soil skeleton, erodible fines, fluidized fine particles, and fluid. The detachment and transport of the fine particles have been modeled with a mass exchange model between the solid and the fluid phases. An elastoplastic constitutive model for sand‐silt mixtures has been developed to monitor the effect of the evolution of both the porosity and the fines content induced by internal erosion upon the behavior of the soil skeleton. An unsaturated flow condition has been implemented into this coupled hydromechanical model to describe more accurately the seepage within the dike and the foundation. A stabilized finite element method was used to eliminate spurious numerical oscillations in solving the convection‐dominated transport of fluidized particles. This numerical tool was then applied to a specific dike‐on‐foundation case subjected to internal erosion induced by a leakage located at the bottom of the foundation. Different failure modes were observed and analyzed for different boundary conditions, including the significant influence of the leakage cavity size and the elevation of the water level at the upstream and downstream sides of the dike.",
                "authors": "Jie Yang, Z. Yin, F. Laouafa, P. Hicher",
                "citations": 45
            },
            {
                "title": "Cybersecurity Dynamics: A Foundation for the Science of Cybersecurity",
                "abstract": null,
                "authors": "Shouhuai Xu",
                "citations": 44
            },
            {
                "title": "Uterine differentiation as a foundation for subsequent fertility.",
                "abstract": "Uterine differentiation in cattle and sheep begins prenatally, but is completed postnatally. Mechanisms regulating this process are not well defined. However, studies of urogenital tract development in murine systems, particularly those involving tissue recombination and targeted gene mutation, indicate that the ideal uterine organizational programme evolves epigenetically through dynamic cell-cell and cell-matrix interactions that define the microenvironmental context within which gene expression occurs and may ensure adult tissue stability. In the cow and ewe, transient postnatal exposure of the developing uterus to steroids can produce immutable changes in adult uterine tissues that may alter the embryotrophic potential of the uterine environment. Thus, success of steroid-sensitive postnatal events supporting uterine growth and development can dictate the functional potential of the adult uterus. Studies to determine effects of specific steroidal agents on patterns of uterine development during defined neonatal periods, as well as the functional consequences of targeted neonatal steroid exposure in the adult uterus, should enable identification of critical developmental mechanisms and determinants of uterine integrity and function. Extreme adult uterine phenotypes (lesion models) created in cattle and sheep by strategic postnatal steroid exposure hold promise as powerful tools for the study of factors affecting uterine function and the rapid identification of novel uterine genes.",
                "authors": "FF Bartol, A. Wiley, JG Floyd, TL Ott, F. Bazer, C. Gray, T. Spencer",
                "citations": 118
            },
            {
                "title": "Linking Industrial Ecology and Ecological Economics: A Theoretical and Empirical Foundation for the Circular Economy",
                "abstract": "The circular economy (CE) is a new model for the production and consumption of goods, which has attracted wide political attention as a strategy toward sustainability. However, the theoretical foundation of CE remains poorly structured and insufficiently explored. Recent studies have shown that the CE model draws on different schools of thought and that its origins are mainly rooted in fields such as industrial ecology (IE) and ecological economics (EE). In this article, we investigate the links between CE, IE, and EE and provide an overview of the similarities and differences between these fields. At the same time, we analyze to what extent the linkages between IE and EE can create a coherent body of knowledge for CE, and be used to identify further research opportunities. This paper shows that, until now, research on CE seems to be mainly rooted in the field of IE and based on concepts and tools that already exist in other fields, rather than inventing new ones. The reconciliation of IE and EE could provide a mechanism to extend beyond such a narrow focus, and increase knowledge of the theoretical and practical framework of CE to benefit sustainability.",
                "authors": "A. Bruel, Jakub Kronenberg, N. Troussier, B. Guillaume",
                "citations": 97
            },
            {
                "title": "Analytical study of bending and free vibration responses of functionallygraded beams resting on elastic foundation",
                "abstract": "In this investigation, study of the static and dynamic behaviors of functionally graded beams (FGB) is presented using a hyperbolic shear deformation theory (HySDT). The simply supported FG-beam is resting on the elastic foundation (Winkler-Pasternak types). The properties of the FG-beam vary according to exponential (E-FGB) and power-law (P-FGB) distributions. The governing equations are determined via Hamilton\\'s principle and solved by using Navier\\'s method. To show the accuracy of this model (HySDT), the current results are compared with those available in the literature. Also, various numerical results are discussed to show the influence of the variation of the volume fraction of the materials, the power index, the slenderness ratio and the effect of Winkler spring constant on the fundamental frequency, center deflection, normal and shear stress of FG-beam.",
                "authors": "Lynda Amel Chaabane, F. Bourada, M. Sekkal, Sara Zerouati, F. Zaoui, A. Tounsi, Abdelhak Derras, A. A. Bousahla, A. Tounsi",
                "citations": 98
            },
            {
                "title": "Optimizing an ANN model with genetic algorithm (GA) predicting load-settlement behaviours of eco-friendly raft-pile foundation (ERP) system",
                "abstract": null,
                "authors": "Long Liu, H. Moayedi, A. S. Rashid, Siti Fatimah Abdul Rahman, Hoang Nguyen",
                "citations": 85
            },
            {
                "title": "Model Uncertainties in Foundation Design",
                "abstract": null,
                "authors": "Chong Tang, K. Phoon",
                "citations": 21
            },
            {
                "title": "EFQM Excellence Model – European Foundation for Quality Management",
                "abstract": "The EFQM Excellence model allows an integrative approach to quality management, which includes all key aspects, such as: results obtained, customer orientation, information management, employee satisfaction. The major benefit, as presented by the most successful European managers, is the same model of Excellence EFQM allows them to gain advantages by quickly introducing innovations in practice, which is actually a competitive advantage.",
                "authors": "Curpănaru Gabriela-Livia",
                "citations": 21
            },
            {
                "title": "Vibration of FG nano-sized beams embedded in Winkler elastic foundation and with various boundary conditions",
                "abstract": "Abstract In the current study, vibration analysis of functionally graded (FG) nano-sized beams resting on a elastic foundation is presented via a finite element method. The elastic foundation is simulated by using one-parameter Winkler type elastic foundation model. Euler-Bernoulli beam theory and Eringen’s nonlocal elasticity theory are utilized to model the functionally graded nano-sized beams with various boundary conditions such as simply supported at both ends (S-S), clamped-clamped (C-C) and clamped-simply supported (C-S). Material properties of functionally graded nanobeam vary across the thickness direction according to the power-law distribution. The vibration behaviors of functionally graded nanobeam composed of alumina (Al2O3) and steel are shown using nonlocal finite element formulation. The importance of this paper is the utilize of shape functions and the Eringen's nonlocal elasticity theory to set up the stiffness matrices and mass matrices of the functionally graded nano-sized beam resting on Winkler elastic foundation for free vibration analysis. Bending stiffness, foundation stiffness and mass matrices are obtained to realize the solution of vibration problem of the FG nanobeam. The influences of power-law exponent (k), dimensionless nonlocal parameters (e0a/L), dimensionless Winkler foundation parameters (KW), mode numbers and boundary conditions on frequencies are investigated via several numerical examples and shown by a number of tables and figures.",
                "authors": "Büşra Uzun, Ö. Civalek, M. Yaylı",
                "citations": 41
            },
            {
                "title": "The General Adaptation Syndrome: A Foundation for the Concept of Periodization",
                "abstract": null,
                "authors": "Aaron J Cunanan, Brad Deweese, John P. Wagle, Kevin M Carroll, Robert W. Sausaman, W. Hornsby, G. Haff, N. Triplett, K. Pierce, M. Stone",
                "citations": 149
            },
            {
                "title": "Dynamic Analysis of Functionally Graded Porous Plates Resting on Elastic Foundation Taking into Mass subjected to Moving Loads Using an Edge-Based Smoothed Finite Element Method",
                "abstract": "The paper presents the extension of an edge-based smoothed finite element method using three-node triangular elements for dynamic analysis of the functionally graded porous (FGP) plates subjected to moving loads resting on the elastic foundation taking into mass (EFTIM). In this study, the edge-based smoothed technique is integrated with the mixed interpolation of the tensorial component technique for the three-node triangular element (MITC3) to give so-called ES-MITC3, which helps improve significantly the accuracy for the standard MITC3 element. The EFTIM model is formed by adding a mass parameter of foundation into the Winkler–Pasternak foundation model. Two parameters of the FGP materials, the power-law index (k) and the maximum porosity distributions (Ω), take forms of cosine functions. Some numerical results of the proposed method are compared with those of published works to verify the accuracy and reliability. Furthermore, the effects of geometric parameters and materials on forced vibration of the FGP plates resting on the EFTIM are also studied in detail.",
                "authors": "T. Tran, Q. Pham, T. Nguyen-Thoi",
                "citations": 39
            },
            {
                "title": "Nonlinear vibration of fluid conveying cantilever nanotube resting on visco‐pasternak foundation using non‐local strain gradient theory",
                "abstract": "Frequency analysis and forced vibration response of fluid conveying viscoelastic nanotubes that resting on nonlinear visco-pasternak foundation under magnetic field using size-dependent non-local strain gradient theory are considered in this study. It is supposed that the nanotube is modelled as cantilever type beam and subjected to a harmonic load. The material property of the nanotube is modelled by Kelvin–Voigt viscoelastic constitutive relation and slip boundary conditions of nanotube conveying fluid are taken into account. Extended Galerkin method is used to obtain the nonlinear differential equation of the motion and the multiple time-scales method is utilised to investigate the primary vibration resonance of the nanotube. Firstly, the frequency analysis is performed on the linear system and the effects of foundation coefficients on the natural frequency are investigated at several flow velocities. Moreover, the resonance properties of the system are solved in closed form and analysed from the frequency-response curves, and then the effects of the non-local parameter, length scale parameter and magnetic field are fully investigated. In this case, non-local parameter, length scale parameter and foundation coefficients are highly influential on the frequency response of the considered system.",
                "authors": "P. R. Saffari, M. Fakhraie, M. A. Roudbari",
                "citations": 39
            },
            {
                "title": "Seismic Response from Centrifuge Model Tests of a Scoured Bridge with a Pile-Group Foundation",
                "abstract": "Abstract A series of seismic centrifuge tests were performed on an RC girder-type bridge with a 3 × 3 pile-group foundation embedded in dry sand to investigate the seismic behavior under various sc...",
                "authors": "F. Liang, Xu Liang, Hao Zhang, Chen Wang",
                "citations": 37
            },
            {
                "title": "The hydrodynamic foundation for salmon lice dispersion modeling along the Norwegian coast",
                "abstract": null,
                "authors": "Lars Asplin, J. Albretsen, I. Johnsen, A. Sandvik",
                "citations": 51
            },
            {
                "title": "Size-dependent buckling analysis of nanobeams resting on two-parameter elastic foundation through stress-driven nonlocal elasticity model",
                "abstract": "Abstract The instability of nanobeams rested on two-parameter elastic foundations is studied through the Bernoulli–Euler beam theory and the stress-driven nonlocal elasticity model. The size-dependency is incorporated into the formulation by defining the strain at each point as an integral convolution in terms of the stresses in all the points and a kernel. The nonlocal elasticity problem in a bounded domain is well-posed and inconsistencies within the Eringen nonlocal theory are overcome. Excellent agreement is found with the results in the literature, and new insightful results are presented for the buckling loads of nanobeams rested on the Winkler and Pasternak foundations.",
                "authors": "H. Darban, F. Fabbrocino, L. Feo, R. Luciano",
                "citations": 65
            },
            {
                "title": "Sbeach, Numerical Model for Simulating Storm-Induced Beach Change: Report 1: Empirical Foundation and Model Development",
                "abstract": "Abstract : A two-dimensional numerical model is presented for calculating dune and beach erosion produced by storm waves and water levels. The empirically based model was first developed from a large data set of net cross-shore sand transport rates and geomorphic change observed in large wave tanks, then verified using high-quality field data. The aim is to reproduce macroscale features of the beach profile, with focus on the formation and movement of longshore bars. The ultimate goal is prediction of storm-induced beach erosion and post-storm recovery. Bars are simulated satisfactorily, but berm processes are less well reproduced, due in part to a lack of data for defining accretionary wave and profile processes. A new criterion is developed for predicting erosion and accretion, and the model uses this criterion to calculate net sand transport rates in four regions of the nearshore extending from deep water to the limit of wave runup. Wave height and setup across the profile are calculated to obtain the net cross-shore sand transport rate. The model is driven by engineering data, with main inputs of time series of wave height and period in deep water, time series of water level, median beach grain size, and initial profile shape. Comprehensive sensitivity testing is performed, and example applications are made to evaluate the response of the profile to the presence of a vertical seawall and behavior of different beach fill cross sections in adjustment to normal and storm wave action. Keywords: Accretion; Cross shore sand transport; Dune erosion; Longshore bars.",
                "authors": "M. Larson, N. Kraus",
                "citations": 105
            },
            {
                "title": "A simple quasi-3D HSDT for the dynamics analysis of FG thick plate on elastic foundation",
                "abstract": "This work presents a dynamic investigation of functionally graded (FG) plates resting on elastic foundation using a simple quasi-3D higher shear deformation theory (quasi-3D HSDT) in which the stretching effect is considered. The culmination of this theory is that in addition to taking into account the effect of thickness extension (ez ≠ 0), the kinematic is defined with only 4 unknowns, which is even lower than the first order shear deformation theory (FSDT). The elastic foundation is included in the formulation using the Pasternak mathematical model. The governing equations are deduced through the Hamilton's principle. These equations are then solved via closed-type solutions of the Navier type. The fundamental frequencies are predicted by solving the eigenvalue problem. The degree of accuracy of present solutions can be shown by comparing it to the 3D solution and other closed-form solutions available in the literature.",
                "authors": "F. Bourada, Zoulikha Boukhlif, Mohammed Bouremana, A. A. Bousahla, M. Bourada, A. Tounsi, Mohammed Al-Osta",
                "citations": 79
            },
            {
                "title": "The effect of parameters of visco-Pasternak foundation on the bending and vibration properties of a thick FG plate",
                "abstract": "In this research, a simple quasi 3D hyperbolic shear deformation model is employed for bending and dynamic behavior of functionally graded (FG) plates resting on visco-Pasternak foundations. The important feature of this theory is that, it includes the thickness stretching effect with considering only 4 unknowns, which less than what is used in the First Order Shear Deformation (FSDT) theory. The visco-Pasternak\\'s foundation is taken into account by adding the influence of damping to the usual foundation model which characterized by the linear Winkler\\'s modulus and Pasternak\\'s foundation modulus. The equations of motion for thick FG plates are obtained in the Hamilton principle. Analytical solutions for the bending and dynamic analysis are determined for simply supported plates resting on visco-Pasternak foundations. Some numerical results are presented to indicate the effects of material index, elastic foundation type, and damping coefficient of the foundation, on the bending and dynamic behavior of rectangular FG plates.",
                "authors": "L. Boulefrakh, H. Hebali, ABDELBAKI CHIKH, A. Chikh, A. A. Bousahla, A. Tounsi, S. R. Mahmoud",
                "citations": 78
            },
            {
                "title": "On the future of macroeconomic models",
                "abstract": "Macroeconomics has been under scrutiny as a field since the financial crisis, which brought an abrupt end to the optimism of the Great Moderation. There is widespread acknowledgement that the prevailing dynamic stochastic general equilibrium (DSGE) models performed poorly, but little agreement on what alternative future paradigm should be pursued. This article is the elaboration of four blog posts that together present a clear message: current DSGE models are flawed, but they contain the right foundations and must be improved rather than discarded. Further, we need different types of macroeconomic models for different purposes. Specifically, there should be five kinds of general equilibrium models: a common core, plus foundational theory, policy, toy, and forecasting models. The different classes of models have a lot to learn from each other, but the goal of full integration has proven counterproductive. No model can be all things to all people.",
                "authors": "O. Blanchard",
                "citations": 206
            },
            {
                "title": "Abiotic Stress Adaptation in Plants : Physiological, Molecular and Genomic Foundation",
                "abstract": "Stress Perception and Signal Transduction.- Abiotic Tolerance and Crop Improvement.- Sensors and Signal Transducers of Environmental Stress in Cyanobacteria.- Stress Signaling I: The Role of Abscisic Acid (ABA).- Stress Signaling II: Calcium Sensing and Signaling.- Stress Signaling III: Reactive Oxygen Species (ROS).- A Biotic or Abiotic Stress?.- Protein Kinases and Phosphatases for Stress Signal Transduction in Plants.- Nitrogen Source Influences Root to Shoot Signaling Under Drought.- Stress Regulation of Gene Expression.- Abiotic Stress Responses: Complexities in Gene Expression.- Promoters and Transcription Factors in Abiotic Stress-Responsive Gene Expression.- Epigenetic Regulation: Chromatin Modeling and Small RNAs.- Physiology and Metabolism.- Ion Homeostasis.- Glutathione Homeostasis: Crucial for Abiotic Stress Tolerance in Plants.- Water Balance and the Regulation of Stomatal Movements.- Responses to Macronutrient Deprivation.- Osmolyte Regulation in Abiotic Stress.- Programmed Cell Death in Plants.- Overcoming Stress.- Varietal Improvement for Abiotic Stress Tolerance in Crop Plants: Special Reference to Salinity in Rice.- Transgenic Approaches.- Marker Assisted Breeding.- Stress, Mutators, Mutations and Stress Resistance.- Systems Biology of Abiotic Stress: The Elephant and the Blind Men.- Global Climate Change, Stress and Plant Productivity.",
                "authors": "A. Pareek",
                "citations": 92
            },
            {
                "title": "Foundation for a classification of collaboration levels for human-robot cooperation in manufacturing",
                "abstract": "ABSTRACT Industry 4.0 aims to support the factory of the future, involving increased use of information systems and new ways of using automation, such as collaboration where a robot and a human share work on a single task. We propose a classification of collaboration levels for Human-Robot collaboration (HRC) in manufacturing that we call levels of collaboration (LoC), formed to provide a conceptual model conducive to the design of assembly lines incorporating HRC. This paper aims to provide a more theoretical foundation for such a tool based on relevant theories from cognitive science and other perspectives of human-technology interaction, strengthening the validity and scientific rigour of the envisioned LoC tool. The main contributions consist of a theoretical grounding to motivate the transition from automation to collaboration, which are intended to facilitate expanding the LoC classification to support HRC, as well as an initial visualization of the LoC approach. Future work includes fully defining the LoC classification as well as operationalizing functionally different cooperation types. We conclude that collaboration is a means to an end, so collaboration is not entered for its own sake, and that collaboration differs fundamentally from more commonly used views where automation is the focus.",
                "authors": "Ari Kolbeinsson, E. Lagerstedt, J. Lindblom",
                "citations": 59
            },
            {
                "title": "Thermal buckling analysis of SWBNNT on Winkler foundation by non local FSDT",
                "abstract": "In this work, the thermal buckling characteristics of zigzag single-walled boron nitride (SWBNNT) embedded in a one-parameter elastic medium modeled as Winkler-type foundation are investigated using a nonlocal first-order shear deformation theory (NFSDT). This model can take into account the small scale effect as well as the transverse shear deformation effects of nanotubes. A closed-form solution for nondimensional critical buckling temperature is obtained in this investigation. Further the effect of nonlocal parameter, Winkler elastic foundation modulus, the ratio of the length to the diameter, the transverse shear deformation and rotary inertia on the critical buckling temperature are being investigated and discussed. The results presented in this paper can provide useful guidance for the study and design of the next generation of nanodevices that make use of the thermal buckling properties of boron nitride nanotubes.",
                "authors": "A. Semmah, H. Heireche, A. A. Bousahla, A. Tounsi",
                "citations": 64
            },
            {
                "title": "Quantitative proteomics and single-nucleus transcriptomics of the sinus node elucidates the foundation of cardiac pacemaking",
                "abstract": null,
                "authors": "Nora Linscheid, S. Logantha, P. C. Poulsen, Shanzhuo Zhang, M. Schrölkamp, Kristoffer L. Egerod, Jonatan J Thompson, A. Kitmitto, G. Galli, M. Humphries, Henggui Zhang, T. Pers, J. Olsen, M. Boyett, A. Lundby",
                "citations": 86
            },
            {
                "title": "Wave propagation of functionally graded anisotropic nanoplates resting on Winkler-Pasternak foundation",
                "abstract": "This work deals with the size-dependent wave propagation analysis of functionally graded (FG) anisotropic nanoplates based on a nonlocal strain gradient refined plate model. The present model incorporates two scale coefficients to examine wave dispersion relations more accurately. Material properties of FG anisotropic nanoplates are exponentially varying in the z-direction. In order to solve the governing equations for bulk waves, an analytical method is performed and wave frequencies and phase velocities are obtained as a function of wave number. The influences of several important parameters such as material graduation exponent, geometry, Winkler-Pasternak foundation parameters and wave number on the wave propagation of FG anisotropic nanoplates resting on the elastic foundation are investigated and discussed in detail. It is concluded that these parameters play significant roles on the wave propagation behavior of the nanoplates. From the best knowledge of authors, it is the first time that FG nanoplate made of anisotropic materials is investigated, so, presented numerical results can serve as benchmarks for future analysis of such structures.",
                "authors": "B. Karami, M. Janghorban, A. Tounsi",
                "citations": 59
            },
            {
                "title": "Database of rocking shallow foundation performance: Dynamic shaking",
                "abstract": "Several experimental studies have shown that rocking shallow foundations have beneficial seismic performance features: recentering and energy dissipation with little damage. A new publicly available database, “FoRDy” (Foundation Rocking—Dynamic), summarizes the results of dynamic physical model tests of single-degree-of-freedom-like structures supported on rocking foundations. It contains data from five centrifuge and three 1-g shaking table test series that were conducted at experimental facilities in the United States, Greece, and Japan. The database includes 200 model “case histories” that span a wide range of model sizes, soil and structure properties, and seismic excitations. It is compiled as the first step toward building a comprehensive dynamic rocking foundation database, and it has the potential to grow in the future. To illustrate its usefulness, the data are used to show example correlations between the peak drift ratio demand and selected ground motion intensity measures. The results suggest that peak ground velocity (PGV), peak ground displacement (PGD), and the geometric mean of the linear spectral displacement over the period range of 0.2–3 times the initial natural period predict the peak drift ratio response reliably.",
                "authors": "Andreas G. Gavras, B. Kutter, M. Hakhamaneshi, Sivapalan Gajan, A. Tsatsis, K. Sharma, Tetsuya Kohno, L. Deng, I. Anastasopoulos, G. Gazetas",
                "citations": 32
            },
            {
                "title": "An Edge-Based Smoothed Finite Element for Free Vibration Analysis of Functionally Graded Porous (FGP) Plates on Elastic Foundation Taking into Mass (EFTIM)",
                "abstract": "In this paper, free vibration analysis of the functionally graded porous (FGP) plates on the elastic foundation taking into mass (EFTIM) is presented. The fundamental equations of the FGP plate are derived using Hamilton’s principle. The mixed interpolation of the tensorial components (MITC) approach and the edge-based smoothed finite element method (ES-FEM) is employed to avoid the shear locking as well as to improve the accuracy for the triangular element. The EFTIM is a foundation model based on the two-parameter Winkler–Pasternak model but added a mass parameter of foundation. Materials of the plate are FGP with a power-law distribution and maximum porosity distributions in the forms of cosine functions. Some numerical examples are examined to demonstrate the accuracy and reliability of the proposed method in comparison with those available in the literature.",
                "authors": "T. Tran, Q. Pham, T. Nguyen-Thoi",
                "citations": 33
            },
            {
                "title": "Physical model test of transparent soil on coupling effect of cut-off wall and pumping wells during foundation pit dewatering",
                "abstract": null,
                "authors": "Jianxiu Wang, Xiaotian Liu, Shaoli Liu, Yanfei Zhu, W. Pan, Jie Zhou",
                "citations": 54
            },
            {
                "title": "A review of Winkler's foundation and its profound influence on adhesion and soft matter applications.",
                "abstract": "Few advanced mechanics of materials solutions have found broader and more enduring applications than Emil Winkler's beam on elastic foundation analysis, first published in 1867. Now, 150 years after its introduction, this concept continues to enjoy widespread use in its original application field of civil engineering, and has also had a profound effect on the field of adhesion mechanics, including for soft matter adhesion phenomena. A review of the model is presented with a focus on applications to adhesion science, highlighting classical works that utilize the model as well as recent usages that extend its scope. The special case of the behavior of plates on incompressible (e.g., elastomeric and viscous liquid) foundations is reviewed because of the significant relevance to the behavior of soft matter interlayers between one or more flexible adherends.",
                "authors": "D. Dillard, B. Mukherjee, Preetika Karnal, R. Batra, J. Frechette",
                "citations": 87
            },
            {
                "title": "The Study for Longitudinal Deformation of Adjacent Shield Tunnel Due to Foundation Pit Excavation with Consideration of the Retaining Structure Deformation",
                "abstract": "By selecting the ratio of the cumulative maximum deformation of the retaining structure to the excavation depth as the control parameter of the retaining structure deformation, this paper established a sidewall unloading model which can consider the deformation of the retaining structure and the spatial effect of foundation pit excavation. Meanwhile, the impact region of the sidewall was divided to calculate the distribution of additional stress caused by foundation pit excavation. On this basis, through introducing the collaborative deformation model for rotation and dislocation of a shield tunnel, this paper studied the longitudinal deformation of the adjacent shield tunnel due to foundation pit excavation. Moreover, several engineering cases were given to verify the reliability of the proposed method, and the influencing factors were analyzed. The following conclusions were obtained: the axial horizontal displacement of the shield tunnel by the side of the foundation pit was normally distributed, and the calculated value was in good agreement with the measured value; the longitudinal deformation of the shield tunnel was mainly induced by the unloading effect of the sidewall of the foundation pit, which was parallel and closed to the tunnel; the soil excavation in the vicinity of the buried depth of the tunnel would result in a significant increase in longitudinal deformation; with the increase in the retaining structure deformation of the foundation pit, the longitudinal deformation of the adjacent shield tunnel and its influence scope also increased; the longitudinal deformation of the shield tunnel decreased with the increase of clearances between the foundation pit and tunnel; and finally, the excavation of the foundation pit had a great influence on the shallowly buried shield tunnel nearby, and the effect of foundation pit excavation on the tunnel decreased with the increase of the burial depth of the shield tunnel.",
                "authors": "Xinhai Zhang, G. Wei, Chengwu Jiang",
                "citations": 30
            },
            {
                "title": "Centrifuge study on behavior of rigid pile composite foundation under embankment in soft soil",
                "abstract": null,
                "authors": "Jian-lin Yu, Jia-jin Zhou, X. Gong, R. Xu, Jun-yuan Li, Shanshan Xu",
                "citations": 29
            },
            {
                "title": "Free vibration of functionally graded beams resting on Winkler-Pasternak foundation",
                "abstract": null,
                "authors": "M. Avcar, Waleed Khalid Mohammed Mohammed",
                "citations": 85
            },
            {
                "title": "Introducing Hypergraph Signal Processing: Theoretical Foundation and Practical Applications",
                "abstract": "Signal processing over graphs has recently attracted significant attention for dealing with the structured data. Normal graphs, however, only model pairwise relationships between nodes and are not effective in representing and capturing some high-order relationships of data samples, which are common in many applications, such as Internet of Things (IoT). In this article, we propose a new framework of hypergraph signal processing (HGSP) based on the tensor representation to generalize the traditional graph signal processing (GSP) to tackle high-order interactions. We introduce the core concepts of HGSP and define the hypergraph Fourier space. We then study the spectrum properties of hypergraph Fourier transform (HGFT) and explain its connection to mainstream digital signal processing. We derive the novel hypergraph sampling theory and present the fundamentals of hypergraph filter design based on the tensor framework. We present HGSP-based methods for several signal processing and data analysis applications. Our experimental results demonstrate significant performance improvement using our HGSP framework over some traditional signal processing solutions.",
                "authors": "Songyang Zhang, Zhi Ding, Shuguang Cui",
                "citations": 73
            },
            {
                "title": "Dynamic response of plates resting on a fractional viscoelastic foundation and subjected to a moving load",
                "abstract": "Abstract The dynamic behavior of the thin plates resting on a fractionally damped viscoelastic foundation subjected to a moving point load is investigated. This work explores the application of fractional calculus in the modeling of a viscoelastic foundation. A parametric study is executed to determine the impacts of the fractional-order derivative, foundation parameters, velocity and acceleration of the moving load on the dynamic behavior of the plate. The results show that the damping of the foundation system increases with increasing the order of the fractional derivative, which leads to a decrease in the dynamic response.",
                "authors": "R. K. Praharaj, N. Datta",
                "citations": 27
            },
            {
                "title": "Simplified solution for tunnel-soil-pile interaction in Pasternak’s foundation model",
                "abstract": null,
                "authors": "Zhi-guo Zhang, Maosong Huang, Chen Xu, Yunjuan Jiang, Weidong Wang",
                "citations": 103
            },
            {
                "title": "Application of Topic Modeling to Tweets as the Foundation for Health Disparity Research for COVID-19.",
                "abstract": "We randomly extracted publicly available Tweets mentioning COVID-19 related terms (n=2,558,474 Tweets) from Tweet corpora collected daily using an API from Jan 21st to May 3rd, 2020. We applied a clustering algorithm to publicly available Tweets authored by African Americans (n=1,763) to detect topics and sentiment applying natural language processing (NLP). We visualized fifteen topics (four themes) using network diagrams (Newman modularity 0.74). Compared to the COVID-19 related Tweets authored by others, positive sentiments, cohesively encouraging online discussions (e.g., Black strong 27.1%, growing up Blacks 22.8%, support Black business 17.0%, how to build resilience 7.8%), and COVID-19 prevention behaviors (e.g., masks 4.7%, encouraging social distancing 9.4%) were uniquely observed in African American Twitter communities. Application of topic modeling techniques to streaming social media Twitter provides the foundation for research team insights regarding information and future virtual based intervention and social media based health disparity research for COVID-19.",
                "authors": "Michelle Odlum, Hwayoung Cho, Peter M. Broadwell, Nicole Davis, Mariana Patrão, Deborah Schauer, Michael E. Bales, C. Alcántara, Sunmoo Yoon",
                "citations": 25
            },
            {
                "title": "A size-dependent exact theory for thermal buckling, free and forced vibration analysis of temperature dependent FG multilayer GPLRC composite nanostructures restring on elastic foundation",
                "abstract": null,
                "authors": "H. Safarpour, Zanyar Esmailpoor Hajilak, M. Habibi",
                "citations": 105
            },
            {
                "title": "Peridynamic Model for a Mindlin Plate Resting on a Winkler Elastic Foundation",
                "abstract": null,
                "authors": "B. Važić, E. Oterkus, S. Oterkus",
                "citations": 26
            },
            {
                "title": "Autonomics: In search of a foundation for next-generation autonomous systems",
                "abstract": "Significance Autonomous systems are replacing humans in a variety of tasks, and in the years to come, such systems will become central and crucial to human life. They will include vehicles of all kinds, medical and industrial robots, agricultural and manufacturing facilities, traffic management systems, and much more. While many organizations strive to develop the next generation of trustworthy, cost-effective autonomous systems, a major gap exists between the challenges in developing these and the state of the art. There is a crucial need for a common scientific and engineering foundation for developing these systems, which we term “autonomics.” We believe that such a foundation will dramatically accelerate the deployment and acceptance of high-quality autonomous systems, for the benefit of human society. The potential benefits of autonomous systems are obvious. However, there are still major issues to be dealt with before developing such systems becomes a commonplace engineering practice, with accepted and trustworthy deliverables. We argue that a solid, evolving, publicly available, community-controlled foundation for developing next-generation autonomous systems is a must, and term the desired foundation “autonomics.” We focus on three main challenges: 1) how to specify autonomous system behavior in the face of unpredictability; 2) how to carry out faithful analysis of system behavior with respect to rich environments that include humans, physical artifacts, and other systems; and 3) how to build such systems by combining executable modeling techniques from software engineering with artificial intelligence and machine learning.",
                "authors": "D. Harel, Assaf Marron, J. Sifakis",
                "citations": 47
            },
            {
                "title": "Generative linguistics and neural networks at 60: Foundation, friction, and fusion",
                "abstract": "Abstract:The birthdate of both generative linguistics and neural networks can be taken as 1957, the year of the publication of foundational work by both Noam Chomsky and Frank Rosenblatt. This article traces the development of these two approaches to cognitive science, from their largely autonomous early development in the first thirty years, through their collision in the 1980s around the past-tense debate (Rumelhart & McClelland 1986, Pinker & Prince 1988) and their integration in much subsequent work up to the present. Although this integration has produced a considerable body of results, the continued general gulf between these two lines of research is likely impeding progress in both: on learning in generative linguistics, and on the representation of language in neural modeling. The article concludes with a brief argument that generative linguistics is unlikely to fulfill its promise of accounting for language learning if it continues to maintain its distance from neural and statistical approaches to learning.",
                "authors": "Joe Pater",
                "citations": 72
            },
            {
                "title": "Feasibility study of offshore wind turbines with hybrid monopile foundation based on centrifuge modeling",
                "abstract": null,
                "authors": "Xuefei Wang, X. Zeng, Xu Yang, Jiale Li",
                "citations": 94
            },
            {
                "title": "On the Philosophical Foundations of Conceptual Models",
                "abstract": "This paper contributes to the philosophical foundations of conceptual modeling by addressing a number of foundational questions such as: What is a conceptual model? Among models used in computer science, which are conceptual, and which are not? How are conceptual models different from other models used in the Sciences and Engineering? The paper takes a stance in answering these questions and, in order to do that, it draws from a broad literature in philosophy, cognitive science, Logics, as well as several areas of Computer Science (including Databases, Software Engineering, Artificial Intelligence, Information Systems Engineering, among others). After a brief history of conceptual modeling, the paper addresses the aforementioned questions by proposing a characterization of conceptual models with respect to conceptual semantics and ontological commitments. Finally, we position our work w.r.t. to a “Reference Framework for Conceptual” modeling recently proposed in the literature.",
                "authors": "",
                "citations": 55
            },
            {
                "title": "Relationship Development: A Micro-Foundation for the Internationalization Process of the Multinational Business Enterprise",
                "abstract": null,
                "authors": "J. Vahlne, Waheed Akbar Bhatti",
                "citations": 42
            },
            {
                "title": "The consumer decision journey: A literature review of the foundational models and theories and a future perspective",
                "abstract": null,
                "authors": "S. Santos, H. Gonçalves",
                "citations": 27
            },
            {
                "title": "Magneto-electro-elastic analysis of piezoelectric–flexoelectric nanobeams rested on silica aerogel foundation",
                "abstract": null,
                "authors": "F. Ebrahimi, M. Karimiasl, A. Singhal",
                "citations": 40
            },
            {
                "title": "A Survey of Top-Level Ontologies - to inform the ontological choices for a Foundation Data Model",
                "abstract": null,
                "authors": "Chris Partridge, Andrew Mitchell, Alastair Cook, Jan Sullivan, M. West",
                "citations": 31
            },
            {
                "title": "Seismic response of offshore wind turbine with hybrid monopile foundation based on centrifuge modelling",
                "abstract": null,
                "authors": "Xuefei Wang, X. Zeng, Xu Yang, Jiale Li",
                "citations": 57
            },
            {
                "title": "Investigation of electric field effect on size-dependent bending analysis of functionally graded porous shear and normal deformable sandwich nanoplate on silica Aerogel foundation",
                "abstract": "In the present research electro-mechanical bending behavior of sandwich nanoplate with functionally graded porous core and piezoelectric face sheets is carried out. Vlasov’s model foundation is utilized to model the silica Aerogel foundation. Two functions are considered for nonuniform variation of material properties of the core layer along the thickness direction such as Young’s modulus, shear modulus, and density. The governing equations are deduced from Hamilton’s principle based on sinusoidal shear and normal deformation theory. In order to solve seven governing equations, an iterative technique is accomplished. After all, deflection and stresses are verified with corresponding literatures. Eventually, the numerical results reveal that applied voltage, plate aspect ratio, thickness ratio, nonlocal parameter, porosity index, Young’s modulus, and height of silica Aerogel foundation have substantial effects on the electro-mechanical bending response of functionally graded porous sandwich nanoplate.",
                "authors": "A. Ghorbanpour Arani, M. Zamani",
                "citations": 39
            },
            {
                "title": "University capability as a micro-foundation for the Triple Helix model: The case of China",
                "abstract": null,
                "authors": "Yipeng Liu, Qihai Huang",
                "citations": 88
            },
            {
                "title": "Simplified analytical method for evaluating the effects of overcrossing tunnelling on existing shield tunnels using the nonlinear Pasternak foundation model",
                "abstract": null,
                "authors": "Rong-zhu Liang",
                "citations": 53
            },
            {
                "title": "Modeling as the Foundation of Digital Twins",
                "abstract": "The role of digital twins in the modern world is growing every day. Therefore, modeling issues as the foundation of digital twins are always relevant. The application area of digital twins are discussed in the article, affecting both laboratory equipment for educational institutions and research centers, and the physical product or technical systems in general at all stages of design, manufacture and subsequent operation. An example of a PID controller model is given in the article. The model is implemented in the mathematical package Matlab. The structure and timing diagrams explaining its operation are given. The possibility of implementing controllers of various types based on a multi-zone regulator is shown. The advantages of such a decision are explained. An example of a multi-zone speed stabilization system of a DC motor is also given. The structure of the multi-zone regulator implemented in the Matlab environment is shown, and timing diagrams explaining the work of the model are given. The article may be interesting for students studying electrical engineering or specialists working in the field of automated electric drive.",
                "authors": "O. G. Brylina, N. Kuzmina, K. Osintsev",
                "citations": 14
            },
            {
                "title": "Zebrafish models of sarcopenia",
                "abstract": "ABSTRACT Sarcopenia – the accelerated age-related loss of muscle mass and function – is an under-diagnosed condition, and is central to deteriorating mobility, disability and frailty in older age. There is a lack of treatment options for older adults at risk of sarcopenia. Although sarcopenia's pathogenesis is multifactorial, its major phenotypes – muscle mass and muscle strength – are highly heritable. Several genome-wide association studies of muscle-related traits were published recently, providing dozens of candidate genes, many with unknown function. Therefore, animal models are required not only to identify causal mechanisms, but also to clarify the underlying biology and translate this knowledge into new interventions. Over the past several decades, small teleost fishes had emerged as powerful systems for modeling the genetics of human diseases. Owing to their amenability to rapid genetic intervention and the large number of conserved genetic and physiological features, small teleosts – such as zebrafish, medaka and killifish – have become indispensable for skeletal muscle genomic studies. The goal of this Review is to summarize evidence supporting the utility of small fish models for accelerating our understanding of human skeletal muscle in health and disease. We do this by providing a basic foundation of the (zebra)fish skeletal muscle morphology and physiology, and evidence of muscle-related gene homology. We also outline challenges in interpreting zebrafish mutant phenotypes and in translating them to human disease. Finally, we conclude with recommendations on future directions to leverage the large body of tools developed in small fish for the needs of genomic exploration in sarcopenia. Summary: Zebrafish and other small fish have become powerful disease models. Here, we summarize the evidence for the utility of small teleost models for genetic research in sarcopenia – the age-related loss of muscle mass and function.",
                "authors": "A. Daya, Rajashekar Donaka, D. Karasik",
                "citations": 37
            },
            {
                "title": "System competence modelling: Theoretical foundation and empirical validation of a model involving natural, social and human‐environment systems",
                "abstract": null,
                "authors": "Rainer Mehren, Armin Rempfler, Janine Buchholz, J. Hartig, Eva M. Ulrich‐Riedhammer",
                "citations": 64
            },
            {
                "title": "3D Numerical Model for Piled Raft Foundation",
                "abstract": "AbstractLoad sharing of piled raft foundations is known as an economical design for deep foundations. Nevertheless, research in this area has been lagging because of the complexity of the problem and lack of field data. Numerical modeling can be used to provide valuable data with a high level of success. A three-dimensional finite-element model of a piled raft foundation was developed to simulate the case of a piled raft foundation. The model accounts for pile-to-pile, raft-to-pile, pile-to-soil, and raft-to-soil interactions. The model was used to examine the effect of the key parameters governing the performance of this foundation during loading and, accordingly, the load shared by the piles and the raft. After validating the numerical model with available data in the literature, the model was used to develop data for a wide range of parameters and to examine the role of the foundation geometry, including pile spacing in the group, pile length, pile shape, pile diameter, and raft thickness. Furthermore,...",
                "authors": "A. Sinha, A. Hanna",
                "citations": 82
            },
            {
                "title": "Understanding Dyslexia Through Personalized Large-Scale Computational Models",
                "abstract": "Learning to read is foundational for literacy development, yet many children in primary school fail to become efficient readers despite normal intelligence and schooling. This condition, referred to as developmental dyslexia, has been hypothesized to occur because of deficits in vision, attention, auditory and temporal processes, and phonology and language. Here, we used a developmentally plausible computational model of reading acquisition to investigate how the core deficits of dyslexia determined individual learning outcomes for 622 children (388 with dyslexia). We found that individual learning trajectories could be simulated on the basis of three component skills related to orthography, phonology, and vocabulary. In contrast, single-deficit models captured the means but not the distribution of reading scores, and a model with noise added to all representations could not even capture the means. These results show that heterogeneity and individual differences in dyslexia profiles can be simulated only with a personalized computational model that allows for multiple deficits.",
                "authors": "C. Perry, M. Zorzi, J. Ziegler",
                "citations": 79
            },
            {
                "title": "Product Profiles: Modelling customer benefits as a foundation to bring inventions to innovations",
                "abstract": null,
                "authors": "A. Albers, J. Heimicke, Benjamin Walter, Gustav N. Basedow, Nicolas Reiß, Nicolas Heitger, S. Ott, N. Bursac",
                "citations": 64
            },
            {
                "title": "Influence of Winkler-Pasternak Foundation on the Vibrational Behavior of Plates and Shells Reinforced by Agglomerated Carbon Nanotubes",
                "abstract": "This paper aims to investigate the effect of the Winkler-Pasternak elastic foundation on the natural frequencies of Carbon Nanotube (CNT)-reinforced laminated composite plates and shells. The micromechanics of reinforcing CNT particles are described by a two-parameter agglomeration model. CNTs are gradually distributed along the thickness direction according to various functionally graded laws. Elastic foundations are modeled according to the Winkler-Pasternak theory. The theoretical model considers several Higher-order Shear Deformation Theories (HSDTs) based on the so-called Carrera Unified Formulation (CUF). The theory behind CNTs is explained in detail. The theoretical model presented is solved numerically by means of the Generalized Differential Quadrature (GDQ) method. Several parametric studies are conducted, and their results are discussed.",
                "authors": "D. Banić, M. Bacciocchi, Francesco Tornabene, A. Ferreira",
                "citations": 93
            },
            {
                "title": "Analytical Prediction for Tunnel-Soil-Pile Interaction Mechanics based on Kerr Foundation Model",
                "abstract": null,
                "authors": "Zhi-guo Zhang, Chengping Zhang, Kangming Jiang, Zhiwei Wang, Yunjuan Jiang, Qi-hua Zhao, Minghao Lu",
                "citations": 34
            },
            {
                "title": "Seismic analysis of dam-foundation-reservoir system including the effects of foundation mass and radiation damping",
                "abstract": null,
                "authors": "H. Mohammadnezhad, M. Ghaemian, A. Noorzad",
                "citations": 33
            },
            {
                "title": "Combined Application of Optical Fibers and CRLD Bolts to Monitor Deformation of a Pit-in-Pit Foundation",
                "abstract": "“Pit-in-pit” foundations, where the overall pit is divided into inner and outer pits, present a wide range of engineering problems and yet have received little detailed study. Among the many factors that affect the stability of a deep foundation pit, loading and rainfall are the two most important. Therefore, in this study, physical model experiments are carried out in the laboratory based on a pit-in-pit foundation that is typical of engineering applications in China, simulating the deformation of the system under different loading and rainfall flow conditions. Optical fibers along with constant resistance and large deformation (CRLD) bolts are adopted to collectively monitor the stress and strain inside the pit-in-pit foundation, assisted by fiber Bragg grating (FBG) displacement meters. The results of the monitoring show that the position of the inner pit relative to the outer pit has a strong influence on the stability of the outer pit. The side on which the inner pit is closest to the outer pit wall is the most prone to instability and should thus be reinforced. Comparison and analysis of monitoring results obtained with optical fibers and CRLD bolts allow a potentially dangerous slip surface to be identified, indicating the value of using this type of collective monitoring in deep foundation pits.",
                "authors": "Chun Zhu, Kai Zhang, H. Cai, Z. Tao, Bo An, M. He, Xing Xia, Jian-kang Liu",
                "citations": 34
            },
            {
                "title": "Education as a Foundation of Humanity: Learning from the Pedagogy of Pesantren in Indonesia",
                "abstract": "This study uses the historical-critical method. The purpose of this study is to uncover the pedagogy of pesantren, including the education model in pesantren and the dynamics of pesantren in responding to contemporary issues. The result of the study shows that pesantren has pedagogical concepts that integrate science with the art of teaching. There are various methods and strategies of the pesantren pedagogy which are maintained until today, and they can be a model of the educational system in the Indonesian archipelago as they have contributed to humanity and built peace. In addition, in responding to contemporary issues, pesantren has participated in building a democratic society, encouraging awareness of gender equality, improving public service quality, and promoting paradigm shift within pesantren, i.e. integrative pesantren.",
                "authors": "Syamsul Ma’arif",
                "citations": 67
            },
            {
                "title": "Statistical Shape Models: Understanding and Mastering Variation in Anatomy.",
                "abstract": null,
                "authors": "F. Ambellan, H. Lamecker, Christoph von Tycowicz, S. Zachow",
                "citations": 74
            },
            {
                "title": "Vibration analysis of viscoelastic inhomogeneous nanobeams resting on a viscoelastic foundation based on nonlocal strain gradient theory incorporating surface and thermal effects",
                "abstract": null,
                "authors": "F. Ebrahimi, M. Barati",
                "citations": 71
            },
            {
                "title": "Direct finite element method for nonlinear earthquake analysis of 3‐dimensional semi‐unbounded dam–water–foundation rock systems",
                "abstract": "A direct finite element method for nonlinear earthquake analysis of 2‐dimensional dam–water–foundation rock systems has recently been presented. The analysis procedure uses standard viscous‐damper absorbing boundaries to model the semi‐unbounded foundation‐rock and fluid domains and specifies the seismic input as effective earthquake forces at these boundaries. Presented in this paper is a generalization of the direct finite element method with viscous‐damper boundaries to 3‐dimensional dam–water–foundation rock systems. Step‐by‐step procedures for determining the effective earthquake forces starting from a ground motion specified at a control point on the foundation‐rock surface is developed, and several numerical examples are computed and compared with independent benchmark solutions to demonstrate the effectiveness of the analysis procedure for modeling 3‐dimensional systems.",
                "authors": "A. Løkke, A. Chopra",
                "citations": 38
            },
            {
                "title": "Internal erosion in dike‐on‐foundation modeled by a coupled hydromechanical approach",
                "abstract": "One of the major causes of instability in geotechnical structures such as dikes or earth dams is internal erosion, an insidious process that occurs over a long period of time. Research on this topic is still fairly new and much more needs to be understood in order to solve the problems posed by this phenomenon. This paper proposes a hydromechanical model based on porous continuous medium theory to assess how internal erosion impacts the safety of earthen structures. The saturated soil is considered as a mixture of four interacting constituents: soil skeleton, erodible fines, fluidized fine particles, and fluid. The detachment and transport of the fine particles are described by a mass exchange model between the solid and the fluid phases. An elastoplastic constitutive model for sand‐silt mixtures has been developed to monitor the effect of the evolution of both porosity and fines content induced by internal erosion upon the behavior of the soil skeleton. The model has been numerically solved with the finite element method. It has then been applied to the specific case study of a dike foundation subjected to internal erosion induced by the presence of a karstic cavity beneath the alluvium layer. The numerical results show the onset of erosion, the time‐space evolution of the eroded zone, and the hydromechanical response of the soil constituting the dike, all of which highlights the effects of the cavity location, the erosion rate, and the fines content.",
                "authors": "Junliang Yang, Z. Yin, F. Laouafa, P. Hicher",
                "citations": 65
            },
            {
                "title": "An account of the foundation in assessment of earth structure dynamics",
                "abstract": "An assessment of the dynamic behavior of a plane earth structure with account of its foundation is considered in the paper. A structure with a foundation is considered as an inhomogeneous system, the material of its certain parts is considered elastic or viscoelastic. To assess the effect of the foundation on dynamic behavior of the structure, a finite domain is cut from the foundation and conditions are set at the boundary of this domain that provide energy entrainment from the structure to infinity in the form of the Rayleigh wave. To describe the internal dissipation in material, a linearly hereditary theory of viscoelasticity with the Rzhanitsin kernel is used. A mathematical model, method and algorithm have been developed to assess the dynamic behavior of the structure-finite foundation system. To ensure the adequacy of the mathematical model and to assess the accuracy of the calculation, model problems have been solved when describing the process under consideration. Dynamic behavior of inhomogeneous viscoelastic system of earth dam-foundation with non-reflecting boundary conditions on the boundary of the final domain of the foundation is investigated. In the process of studying the dynamic behavior of inhomogeneous viscoelastic “structure-foundation” systems, a number of mechanical effects.",
                "authors": "M. Mirsaidov",
                "citations": 31
            },
            {
                "title": "Lateral bearing capacity of hybrid monopile-friction wheel foundation for offshore wind turbines by centrifuge modelling",
                "abstract": null,
                "authors": "Xuefei Wang, X. Zeng, Jiale Li, Xu Yang",
                "citations": 58
            },
            {
                "title": "Reconceptualizing Communication Overload and Building a Theoretical Foundation",
                "abstract": "This study reconceptualizes communication overload and builds a theoretical foundation to understand how this phenomenon applies in contemporary life. We build theory by relying on past research and using a Q-method to capture the subjective perspectives of people who experience communication overload. In our refinement of this abstract concept, we identified seven dimensions composing communication overload. The dimensions included: compromising message quality, having many distractions, using many information and communication technologies, pressuring for decisions, feeling responsible to respond, overwhelming with information, and piling up of messages. Our reconceptualization integrates disparate research, links the availability–expectation–pressure pattern to overload, and elaborates on communication quality, quantity, and generalized perceptions of feeling overwhelmed. The resulting formative theoretical model sets the stage for additional theorizing and empirical studies.",
                "authors": "K. Stephens, Dron M. Mandhana, Jihye Kim, Xiaoqian Li, Elizabeth M. Glowacki, Ignacio Cruz",
                "citations": 64
            },
            {
                "title": "A deformation separation method for gravity dam body and foundation based on the observed displacements",
                "abstract": "The displacement at arbitrary point in the dam is composed of two parts: One is the elastic deformation of dam body and the other that is due to the constrained deformation of foundation. The two parts should be separated to obtain reliable information reflecting the different characteristics of dam body and foundation. A simplified simulation method for gravity dam foundations is proposed that reflects the constrained deformation of foundation in a rational manner while taking into account the complex and diverse mechanical properties. Only the effects of the foundation on dam is investigated in the proposed model, and they are considered either centralized constraints or distributed constraints. The solution of the global foundation deformation is based on the monitoring displacements at measuring points in the gravity dam section using the hybrid partition finite element–interface boundary element approach. The reaction of the foundation on dam can be reflected by the global foundation deformation and the constrained force at dam bottom. On the basis, the whole dam response under a given load combination can be estimated using finite element theory. Three analyses have been performed on a typical gravity dam section to verify the feasibility of simplified simulations for different foundations as well as to allow for discussions regarding the differences among the simulations. An example analysis based on the proposed method is performed on a prototype gravity dam, and the results, which compared with actual measurements for discussions, show that the proposed method is reasonable and practical.",
                "authors": "Chaoning Lin, Tongchun Li, Xiaoqing Liu, Lanhao Zhao, Siyu Chen, Huijun Qi",
                "citations": 36
            },
            {
                "title": "Expanding the Developmental Models of Writing: A Direct and Indirect Effects Model of Developmental Writing (DIEW)",
                "abstract": "We investigated direct and indirect effects of component skills on writing (DIEW) using data from 193 children in Grade 1. In this model, working memory was hypothesized to be a foundational cognitive ability for language and cognitive skills as well as transcription skills, which, in turn, contribute to writing. Foundational oral language skills (vocabulary and grammatical knowledge) and higher-order cognitive skills (inference and theory of mind) were hypothesized to be component skills of text generation (i.e., discourse-level oral language). Results from structural equation modeling largely supported a complete mediation model among 4 variations of the DIEW model. Discourse-level oral language, spelling, and handwriting fluency completely mediated the relations of higher-order cognitive skills, foundational oral language, and working memory to writing. Moreover, language and cognitive skills had both direct and indirect relations to discourse-level oral language. Total effects, including direct and indirect effects, were substantial for discourse-level oral language (.46), working memory (.43), and spelling (.37); followed by vocabulary (.19), handwriting (.17), theory of mind (.12), inference (.10), and grammatical knowledge (.10). The model explained approximately 67% of variance in writing quality. These results indicate that multiple language and cognitive skills make direct and indirect contributions, and it is important to consider both direct and indirect pathways of influences when considering skills that are important to writing.",
                "authors": "Y. Kim, C. Schatschneider",
                "citations": 197
            },
            {
                "title": "Dynamics of a beam on a bilinear elastic foundation under harmonic moving load",
                "abstract": null,
                "authors": "D. Froio, E. Rizzi, F. Simões, A. Pinto da Costa",
                "citations": 35
            },
            {
                "title": "Simulation of Grouting Process in Rock Masses Under a Dam Foundation Characterized by a 3D Fracture Network",
                "abstract": null,
                "authors": "Shaohui Deng, Xiaoling Wang, Jia Yu, Yichi Zhang, Zhen Liu, Yushan Zhu",
                "citations": 35
            },
            {
                "title": "Local Intrinsic Dimensionality I: An Extreme-Value-Theoretic Foundation for Similarity Applications",
                "abstract": null,
                "authors": "Michael E. Houle",
                "citations": 81
            },
            {
                "title": "A macro-element pile foundation model for integrated analyses of monopile-based offshore wind turbines",
                "abstract": null,
                "authors": "A. Page, G. Grimstad, G. Eiksund, H. Jostad",
                "citations": 55
            },
            {
                "title": "Systems Thinking in Practice: Participatory Modeling as a Foundation for Integrated Approaches to Health",
                "abstract": "One Health (OH), EcoHealth (EH), and Planetary Health (PH) share an interest in transdisciplinary efforts that bring together scientists, citizens, government and private sectors to implement contextualized actions that promote adaptive health management across human, animal and ecosystem interfaces. A key operational element underlying these Integrated Approaches to Health (IAH) is use of Systems Thinking as a set of tools for integration. In this paper we discuss the origins and epistemology of systems thinking and argue that participatory modeling, informed by both systems theory and expertise in facilitating engagement and social learning, can help ground IAH theoretically and support its development. Participatory modeling is iterative and adaptive, which is necessary to deal with complexity in practice. Participatory modeling (PM) methods actively involve affected interests and stakeholders to ground the field of inquiry in a specific social-ecological context. Furthermore, PM processes act to reconcile the diverse understandings of the empirical world that stem from divergent discipline and community viewpoints. In this perspective article, we argue that PM can support systems thinking in practice and is essential for IAH implementation. Accordingly we invite PH, OH, and EH practitioners to systematically incorporate specialists in systems science and social engagement and facilitation. This will enable the appropriate contextualization of research practice and interventions, and ensure a balanced representation of the roles and relationships of medical, biological, mathematical, and social disciplines. For completeness, funding schemes supporting IAH need to follow the same iterative, adaptive, and participative processes to accompany IAH projects throughout their implementation.",
                "authors": "R. Duboz, Pierre Echaubard, P. Promburom, M. Kilvington, H. Ross, W. Allen, J. Ward, G. Deffuant, Michel de Garine-Wichatitsky, A. Binot",
                "citations": 54
            },
            {
                "title": "Transient dynamics of terrestrial carbon storage: mathematical foundation and its applications",
                "abstract": "Abstract. Terrestrial ecosystems have absorbed roughly 30 % of anthropogenic CO2 emissions over the past decades, but it is unclear whether this carbon (C) sink will endure into the future. Despite extensive modeling and experimental and observational studies, what fundamentally determines transient dynamics of terrestrial C storage under global change is still not very clear. Here we develop a new framework for understanding transient dynamics of terrestrial C storage through mathematical analysis and numerical experiments. Our analysis indicates that the ultimate force driving ecosystem C storage change is the C storage capacity, which is jointly determined by ecosystem C input (e.g., net primary production, NPP) and residence time. Since both C input and residence time vary with time, the C storage capacity is time-dependent and acts as a moving attractor that actual C storage chases. The rate of change in C storage is proportional to the C storage potential, which is the difference between the current storage and the storage capacity. The C storage capacity represents instantaneous responses of the land C cycle to external forcing, whereas the C storage potential represents the internal capability of the land C cycle to influence the C change trajectory in the next time step. The influence happens through redistribution of net C pool changes in a network of pools with different residence times. Moreover, this and our other studies have demonstrated that one matrix equation can replicate simulations of most land C cycle models (i.e., physical emulators). As a result, simulation outputs of those models can be placed into a three-dimensional (3-D) parameter space to measure their differences. The latter can be decomposed into traceable components to track the origins of model uncertainty. In addition, the physical emulators make data assimilation computationally feasible so that both C flux- and pool-related datasets can be used to better constrain model predictions of land C sequestration. Overall, this new mathematical framework offers new approaches to understanding, evaluating, diagnosing, and improving land C cycle models.",
                "authors": "Yiqi Luo, Zheng Shi, Xingjie Lu, J. Xia, Junyi Liang, Jiang Jiang, Ying Wang, Matthew J. Smith, Lifen Jiang, A. Ahlström, Benito Chen, O. Hararuk, A. Hastings, F. Hoffman, B. Medlyn, S. Niu, M. Rasmussen, K. Todd-Brown, Ying‐ping Wang",
                "citations": 91
            },
            {
                "title": "Sorting through Search and Matching Models in Economics",
                "abstract": "Toward understanding assortative matching, this is a self-contained introduction to research on search and matching. We first explore the nontransferable and perfectly transferable utility matching paradigms, and then a unifying imperfectly transferable utility matching model. Motivated by some unrealistic predictions of frictionless matching, we flesh out the foundational economics of search theory. We then revisit the original matching paradigms with search frictions. We finally allow informational frictions that often arise, such as in college-student sorting.",
                "authors": "Hector Chade, J. Eeckhout, Lones Smith",
                "citations": 167
            },
            {
                "title": "Seismic centrifuge modelling of suction bucket foundation for offshore wind turbine",
                "abstract": null,
                "authors": "Xuefei Wang, Xu Yang, X. Zeng",
                "citations": 98
            },
            {
                "title": "A size-dependent beam model for stability of axially loaded carbon nanotubes surrounded by Pasternak elastic foundation",
                "abstract": null,
                "authors": "B. Akgöz, Ö. Civalek",
                "citations": 93
            },
            {
                "title": "Silk Fibroin Biomaterial Shows Safe and Effective Wound Healing in Animal Models and a Randomized Controlled Clinical Trial",
                "abstract": "Due to its excellent biological and mechanical properties, silk fibroin has been intensively explored for tissue engineering and regenerative medicine applications. However, lack of translational evidence has hampered its clinical application for tissue repair. Here a silk fibroin film is developed and its translational potential is investigated for skin repair by performing comprehensive preclinical and clinical studies to fully evaluate its safety and effectiveness. The silk fibroin film fabricated using all green chemistry approaches demonstrates remarkable characteristics, including transmittance, fluid handling capacity, moisture vapor permeability, waterproofness, bacterial barrier properties, and biocompatibility. In vivo rabbit full‐thickness skin defect study shows that the silk fibroin film effectively reduces the average wound healing time with better skin regeneration compared with the commercial wound dressings. Subsequent assessment in porcine model confirms its long‐term safety and effectiveness for full‐thickness skin defects. Finally, a randomized single‐blind parallel controlled clinical trial with 71 patients shows that the silk fibroin film significantly reduces the time to wound healing and incidence of adverse events compared to commercial dressing. Therefore, the study provides systematic preclinical and clinical evidence that the silk fibroin film promotes wound healing thereby establishing a foundation towards its application for skin repair and regeneration in the clinic.",
                "authors": "Wei Zhang, Longkun Chen, Jialin Chen, Lingshuang Wang, Xuexian Gui, Jisheng Ran, Guowei Xu, Hongshi Zhao, Mengfeng Zeng, Junfeng Ji, L. Qian, Jianda Zhou, H. Ouyang, Xiaohui Zou",
                "citations": 204
            },
            {
                "title": "Analytical solutions for Euler‐Bernoulli Beam on Pasternak foundation subjected to arbitrary dynamic loads",
                "abstract": "In this paper, the dynamic response of an infinite beam resting on a Pasternak foundation and subjected to arbitrary dynamic loads is developed in the form of analytical solution. The beam responses investigated are deflection, velocity, acceleration, bending moment, and shear force. The mechanical resistance of the Pasternak foundation is modeled using two parameters, that is, one accounts for soil resistance due to compressive strains in the soil and the other accounts for the resistance due to shear strains. Because the Winkler model only represents the compressive resistance of soil, comparatively, the Pasternak model is more realistic to consider shear interactions between the soil springs. The governing equation of the beam is simplified into an algebraic equation by employing integration transforms, so that the analytical solution for the dynamic response of the beam can be obtained conveniently in the frequency domain. Both inverse Laplace and inverse Fourier transforms combined with convolution theorem are applied to convert the solution into the time domain. The solutions for several special cases, such as harmonic line loads, moving line loads, and travelling loads are also discussed and numerical examples are conducted to investigate the influence of the shear modulus of foundation on the beam responses. The proposed solutions can be an effective tool for practitioners. Copyright © 2017 John Wiley & Sons, Ltd.",
                "authors": "H. Yu, C. Cai, Y. Yuan, M. Jia",
                "citations": 61
            },
            {
                "title": "Natural Frequency and Dynamic Analyses of Functionally Graded Saturated Porous Beam Resting on Viscoelastic Foundation Based on Higher Order Beam Theory",
                "abstract": "In this paper, natural frequencies and dynamic response of thick beams made of saturated porous materials resting on viscoelastic foundation are investigated for the first time. The beam is modeled using higher-order beam theory. Kelvin-voight model is used to model the viscoelastic foundation. Distribution of porosity along the thickness is considered in two different patterns, which are symmetric nonlinear and nonlinear asymmetric distributions. The relationship between stress and strain is based on the Biot constitutive law. Lagrange equations are used to express the motion equations. Finite element and Newark methods are used to solve the governing equations. The effect of different boundary conditions and various parameters such as porosity and Skempton coefficients, slenderness ratio as well as stiffness and damping coefficients of viscoelastic foundation on natural frequency and transient response of beam have been studied. Results show that in a drained condition, beam has smallest fundamental frequency and by increasing the Skempton coefficient, the fundamental frequency of the beam increases.",
                "authors": "M. Babaei, K. Asemi, P. Safarpour",
                "citations": 21
            },
            {
                "title": "Nonlocal Free Vibration Analysis of FG-Porous Shear and Normal Deformable Sandwich Nanoplate with Piezoelectric Face Sheets Resting on Silica Aerogel Foundation",
                "abstract": null,
                "authors": "A. Ghorbanpour Arani, M. Zamani",
                "citations": 34
            },
            {
                "title": "Misspecification in Latent Change Score Models: Consequences for Parameter Estimation, Model Evaluation, and Predicting Change",
                "abstract": "ABSTRACT Latent change score models (LCS) are conceptually powerful tools for analyzing longitudinal data (McArdle & Hamagami, 2001). However, applications of these models typically include constraints on key parameters over time. Although practically useful, strict invariance over time in these parameters is unlikely in real data. This study investigates the robustness of LCS when invariance over time is incorrectly imposed on key change-related parameters. Monte Carlo simulation methods were used to explore the impact of misspecification on parameter estimation, predicted trajectories of change, and model fit in the dual change score model, the foundational LCS. When constraints were incorrectly applied, several parameters, most notably the slope (i.e., constant change) factor mean and autoproportion coefficient, were severely and consistently biased, as were regression paths to the slope factor when external predictors of change were included. Standard fit indices indicated that the misspecified models fit well, partly because mean level trajectories over time were accurately captured. Loosening constraint improved the accuracy of parameter estimates, but estimates were more unstable, and models frequently failed to converge. Results suggest that potentially common sources of misspecification in LCS can produce distorted impressions of developmental processes, and that identifying and rectifying the situation is a challenge.",
                "authors": "D. A. Clark, Amy K. Nuttall, R. Bowles",
                "citations": 64
            },
            {
                "title": "A novel quasi-3D hyperbolic shear deformation theory for functionally graded thick rectangular plates on elastic foundation",
                "abstract": "In this work, an efficient and simple quasi-3D hyperbolic shear deformation theory is developed for bending and vibration analyses of functionally graded (FG) plates resting on two-parameter elastic foundation. The significant feature of this theory is that, in addition to including the thickness stretching effect, it deals with only 5 unknowns as the first order shear deformation theory (FSDT). The foundation is described by the Pasternak (twoparameter) model. The material properties of the plate are assumed to vary continuously in the thickness direction by a simple power law distribution in terms of the volume fractions of the constituents. Equations of motion for thick FG plates are obtained within the Hamilton's principle. Analytical solutions for the bending and free vibration analysis are obtained for simply supported plates. The numerical results are given in detail and compared with the existing works such as 3-dimensional solutions and those predicted by other plate theories. It can be concluded that the present theory is not only accurate but also simple in predicting the bending and free vibration responses of functionally graded plates resting on elastic foundation.",
                "authors": "A. Benahmed, Mohammed Sid Ahmed Houari, S. Benyoucef, K. Belakhdar, A. Tounsi",
                "citations": 48
            },
            {
                "title": "Landscape genomic prediction for restoration of a Eucalyptus foundation species under climate change",
                "abstract": "As species face rapid environmental change, we can build resilient populations through restoration projects that incorporate predicted future climates into seed sourcing decisions. Eucalyptus melliodora is a foundation species of a critically endangered community in Australia that is a target for restoration. We examined patterns of genomic and phenotypic variation to make empirical based recommendations for seed sourcing. We examined isolation by distance and isolation by environment, determining gene flow up to 500 km and associations with environmental variables. Climate chamber studies revealed extensive phenotypic variation both within and among sampling sites, but no site-specific differentiation in phenotypic plasticity. Overall our results suggest that seed can be sourced broadly across the landscape, providing ample diversity for adaptation to environmental change. Application of our landscape genomic model to E. melliodora restoration projects can identify genomic variation suitable for predicted future climates, thereby increasing the long term probability of successful restoration.",
                "authors": "Megan A. Supple, Jason G. Bragg, L. Broadhurst, A. Nicotra, M. Byrne, R. Andrew, Abigail Widdup, N. Aitken, J. Borevitz",
                "citations": 52
            },
            {
                "title": "Vertical bearing capacity of the pile foundation with restriction plate via centrifuge modelling",
                "abstract": null,
                "authors": "Jiale Li, Xuefei Wang, Yuan Guo, X. Yu",
                "citations": 35
            },
            {
                "title": "Three-dimensional modeling of wave-induced residual seabed response around a mono-pile foundation",
                "abstract": null,
                "authors": "Hongyi Zhao, D. Jeng, C. Liao, J. Zhu",
                "citations": 73
            },
            {
                "title": "An online monitoring technology of tower foundation deformation of transmission lines",
                "abstract": "In this article, an online monitoring technology applied to transmission line towers is proposed to overcome the problems that the foundation deformation is difficult to find in the mining area, river, hillside, and other special areas. The measurement of the stress or the strain caused by the tower foundation deformation are rarely issued, though the tower status can be possibly assessed by the stress or the strain. A new online monitoring technology of tower foundation deformation of transmission lines is developed which consists of three parts. In the first part, the stress or the strain of the key elements is effectively analyzed under the different deformations such as foundation settlement, inclination, and side-slip with the tower finite element model built and the wind load and ice load applied on the tower. In the second part, the experimental platform of tower stress tests is set up, and the internal relations between the stress variation and the tower foundation deformation are determined. In the third part, the online monitoring technology of tower foundation deformation of transmission lines is developed based on fiber Bragg grating stress sensor and successfully applied in Jibei power grid in China. The results show that all the monitored stresses fluctuating with the wind speed are small in case that the condition of tower foundation is normal; on the contrary, the corresponding stresses will change greatly.",
                "authors": "Xinbo Huang, Long Zhao, Ziliang Chen, Cheng Liu",
                "citations": 34
            },
            {
                "title": "Benchmark solution for free vibration of thick open cylindrical shells on Pasternak foundation with general boundary conditions",
                "abstract": null,
                "authors": "Qingshan Wang, D. Shi, Fuzhen Pang, Fazl e Ahad",
                "citations": 49
            },
            {
                "title": "Comparison of modeling a conical nanotube resting on the Winkler elastic foundation based on the modified couple stress theory and molecular dynamics simulation",
                "abstract": null,
                "authors": "Kianoosh Mohammadi, M. Mahinzare, A. Rajabpour, M. Ghadiri",
                "citations": 48
            },
            {
                "title": "Model test on partial expansion in stratified subsidence during foundation pit dewatering",
                "abstract": null,
                "authors": "Jianxiu Wang, Yansheng Deng, Ma Ruiqiang, Xiaotian Liu, G. Qingfeng, Liu Shaoli, Shao Yule, Linbo Wu, Jie Zhou, Tian-liang Yang, Hanmei Wang, Xinlei Huang",
                "citations": 48
            },
            {
                "title": "Centrifuge modeling of lateral bearing behavior of offshore wind turbine with suction bucket foundation in sand",
                "abstract": null,
                "authors": "Xuefei Wang, Xu Yang, X. Zeng",
                "citations": 75
            },
            {
                "title": "Drug screening for human genetic diseases using iPSC models.",
                "abstract": "Induced pluripotent stem cells (iPSCs) enable the generation of previously unattainable, scalable quantities of disease-relevant tissues from patients suffering from essentially any genetic disorder. This cellular material has proven instrumental for drug screening efforts on these disorders, and has facilitated the identification of novel therapeutics for patients. Here we will review the foundational technologies that have enabled iPSCs, the power and limitations of iPSC-based compound screens along with screening guidelines, and recent examples of screening efforts. Additionally we will provide a brief commentary on the future scientific roadmap using pluripotent- and 3D organoid-based, combinatorial approaches.",
                "authors": "Matthew S. Elitt, L. Barbar, P. Tesar",
                "citations": 97
            },
            {
                "title": "Flexible Foundation Effect on Seismic Analysis of Roller Compacted Concrete (RCC) Dams Using Finite Element Method",
                "abstract": null,
                "authors": "Khaled Ghaedi, F. Hejazi, Z. Ibrahim, P. Khanzaei",
                "citations": 26
            },
            {
                "title": "Nonlinear primary resonance of imperfect spiral stiffened functionally graded cylindrical shells surrounded by damping and nonlinear elastic foundation",
                "abstract": null,
                "authors": "H. Ahmadi",
                "citations": 27
            },
            {
                "title": "Soil Dynamics and Foundation Modeling",
                "abstract": null,
                "authors": "J. Jia",
                "citations": 37
            },
            {
                "title": "User Entrepreneur Business Models in 3D Printing",
                "abstract": "Purpose \n \n \n \n \n3D printing possesses certain characteristics that are beneficial for user entrepreneurship. The purpose of this paper is to investigate the business models of user entrepreneurs in the 3D printing industry. In addition, various business opportunities in 3D printing open to user entrepreneurs are classified according to their attractiveness. \n \n \n \n \nDesign/methodology/approach \n \n \n \n \nThe authors review the literatures on user entrepreneurship and on business models. Data from eight user entrepreneurs in Europe and North America are analyzed, applying qualitative content analysis. Multiple correspondence analysis is used to analyze their respective business models. \n \n \n \n \nFindings \n \n \n \n \nUser entrepreneurs in the 3D printing utilize a number of different business models, which show similarities in particular business model components. User entrepreneurs focus primarily on the combination of low opportunity exploitation cost and a large number of potential customers. \n \n \n \n \nResearch limitations/implications \n \n \n \n \nOnline business seems to be beneficial for user entrepreneurship in 3D printing. Policy makers can foster user entrepreneurship by expanding entrepreneurship education and lowering administrative barriers of business foundation. The results of this study are based on a small European and North American sample. Thus, they might not be applicable to other markets. \n \n \n \n \nOriginality/value \n \n \n \n \nThis is the first study of user entrepreneur business models in 3D printing and, thus, contributes to the literature on business models and on user entrepreneurship. In view of the novelty of the field, the business models identified in the study could serve as blueprints for prospective user entrepreneurs in 3D printing.",
                "authors": "Patrick Holzmann, R. J. Breitenecker, Aqeel Ahmed Soomro, E. Schwarz",
                "citations": 90
            },
            {
                "title": "A Theoretical Foundation of Ambiguity Measurement",
                "abstract": "Ordering alternatives by their degree of ambiguity is crucial in economic and financial decision-making processes. To quantify the degree of ambiguity, this paper introduces an empirically-applicable, outcome-independent (up to a state space partition), risk-independent, and attitude-independent measure of ambiguity. In the presence of ambiguity, the Bayesian approach can be extended to uncertain probabilities such that aversion to ambiguity is defined as aversion to mean-preserving spreads in these probabilities. Thereby, the degree of ambiguity can be measured by the volatility of probabilities, just as the degree of risk can be measured by the volatility of outcomes. The applicability of this measure is demonstrated by incorporating ambiguity into an asset pricing model.",
                "authors": "Yehuda Izhakian",
                "citations": 58
            },
            {
                "title": "Bayesian identification of soil‐foundation stiffness of building structures",
                "abstract": "A probabilistic method is presented for identifying the dynamic soil‐foundation stiffnesses of building structures. It is based on model updating of a Timoshenko beam resting on sway and rocking springs, which respectively represent the superstructure and the soil‐foundation system. Unlike those previously employed for this particular problem, the proposed method is a Bayesian one, which accounts for the prevailing uncertainties due to modeling and measurement errors. As such, it yields the probability distribution of the system parameters as opposed to average/deterministic values. In this approach, the joint probability density function of the parameters that control the flexible‐base Timoshenko beam model, together with the fundamental natural frequency and mode shape of the system, forms the prior distribution. Using Bayes' theorem, a posterior distribution is obtained by updating the prior distribution with a sparsely measured mode shape and frequency. The most probable realizations of the system parameters are then determined by maximizing the posterior distribution. For this purpose, first‐ and second‐order derivatives of the objective function are analytically computed via direct differentiation. The proposed method is verified using a synthetic example. Additionally, sensitivity analyses are carried out on both the system parameters and standard deviations of the sources of error. Subsequently, the proposed method is applied to real‐life data recorded at the Millikan Library building, which is located at the California Institute of Technology campus in Pasadena, California, and the results are compared with a previous deterministic study.",
                "authors": "Nima Shirzad-Ghaleroudkhani, M. Mahsuli, S. Farid Ghahari, E. Taciroğlu",
                "citations": 25
            },
            {
                "title": "Toward a Dynamical Foundation for Organized Convection Parameterization in GCMs",
                "abstract": "A dynamically based parameterization of organized moist convection in the form of multiscale coherent structures and slantwise layer overturning principles provides upscale heat and counter‐gradient momentum transports distinct from diffusive down‐gradient mixing by unorganized cumulus. Implementation of a minimalist version of this new parameterization in a state‐of‐the‐art global climate model (GCM) improves the Madden‐Julian Oscillation and convectively coupled waves and generates large‐scale patterns of tropical precipitation consistent with Tropical Rainfall Measuring Mission satellite measurements. A question on the need to parameterize mesoscale convective organization to mitigate mean state biases is addressed.",
                "authors": "M. Moncrieff",
                "citations": 30
            },
            {
                "title": "An experimental study to analyse the behaviour of piled-raft foundation model under the application of vertical load",
                "abstract": null,
                "authors": "Vikas Kumar, Arvind Kumar",
                "citations": 23
            },
            {
                "title": "Centrifuge modeling of mitigation-soil-foundation-structure interaction on liquefiable ground",
                "abstract": null,
                "authors": "J. Olarte, B. Paramasivam, S. Dashti, A. Liel, Jacopo Zannin",
                "citations": 63
            },
            {
                "title": "Assessment of Foundation Mass and Earthquake Input Mechanism Effect on Dam–Reservoir–Foundation System Response",
                "abstract": null,
                "authors": "M. Ghaemian, A. Noorzad, H. Mohammadnezhad",
                "citations": 17
            },
            {
                "title": "The edge waves on a Kirchhoff plate bilaterally supported by a two-parameter elastic foundation",
                "abstract": "In this paper, the bending waves propagating along the edge of a semi-infinite Kirchhoff plate resting on a two-parameter Pasternak elastic foundation are studied. Two geometries of the foundation are considered: either it is infinite or it is semi-infinite with the edges of the plate and of the foundation coinciding. Dispersion relations along with phase and group velocity expressions are obtained. It is shown that the semi-infinite foundation setup exhibits a cut-off frequency which is the same as for a Winkler foundation. The phase velocity possesses a minimum which corresponds to the critical velocity of a moving load. The infinite foundation exhibits a cut-off frequency which depends on its relative stiffness and occurs at a nonzero wavenumber, which is in fact hardly observed in elastodynamics. As a result, the associated phase velocity minimum is admissible only up to a limiting value of the stiffness. In the case of a foundation with small stiffness, asymptotic expansions are derived and beam-like one-dimensional equivalent models are deduced accordingly. It is demonstrated that for the infinite foundation the related nonclassical beam-like model comprises a pseudo-differential operator.",
                "authors": "J. Kaplunov, A. Nobili",
                "citations": 33
            },
            {
                "title": "Contract Negotiation and the Coase Conjecture: A Strategic Foundation for Renegotiation‐Proof Contracts",
                "abstract": "What does contract negotiation look like when some parties hold private information and negotiation frictions are negligible? This paper analyzes this question and provides a foundation for renegotiation‐proof contracts in this environment. The model extends the framework of the Coase conjecture to situations in which the quantity or quality of the good is endogenously determined and to more general environments in which preferences are nonseparable in the traded goods. As frictions become negligible, all equilibria converge to a unique outcome which is separating, efficient, and straightforward to characterize.",
                "authors": "Bruno H. Strulovici",
                "citations": 33
            },
            {
                "title": "Category Theory as a Formal Mathematical Foundation for Model-Based Systems Engineering",
                "abstract": ": In this paper, we introduce Category Theory as a formal foundation for model-based systems engineering. A generalised view of the system based on category theory is presented, where any system can be considered as a category. The objects of the category represent all the elements and components of the system and the arrows represent the relations between these components (objects). The relationship between these objects are the arrows or the morphisms in the category. The Olog is introduced as a formal language to describe a given real-world situation description and requirement writing. A simple example is provided.",
                "authors": "M. A. Mabrok, M. Ryan",
                "citations": 29
            },
            {
                "title": "Effect of Tunnel Progress on the Settlement of Existing Piled Foundation",
                "abstract": "Abstract Tunnel construction below or adjacent to piles will affect the performance and eventually the stability of piles due to ground deformation resulting in the movement of piles and changes in the axial force distribution along the piles. A three dimensional finite element analysis using PLAXIS 3D (2013) was performed to study the behaviour of a single pile and 3 x 3 piles group during the advancement of shield tunnelling in ground. The 10-node tetrahedral elements were used to model both the soil and the tunnel lining. The Hardening Soil (HS) model was used to simulate the soil structure interaction at the tunnel-soil interface. An isotropic elastic model was used for the pile, piles cap, tunnel lining and tunnel boring machine shield (TBM). Several parametric studies were attempted including the longitudinal, lateral, and vertical tunnel location relative to pile embedded in different types of soil (clay or sand). The results showed that the pile head settlement increases during the tunnelling advancement in larger values than that for ground surface settlement. A zone of influence was determined in the range of twice the tunnel diameter in the longitudinal direction (forward and backward of the pile), and transverse direction (left and right of the tunnel centreline). If the tunnel boring is kept off this zone then there is no fear of pile collapse.",
                "authors": "R. al-Omari, M. S. Al-Soud, Osamah Ibrahim Al-Zuhairi",
                "citations": 25
            },
            {
                "title": "Fractional modeling of Pasternak-type viscoelastic foundation",
                "abstract": null,
                "authors": "Wei Cai, Wen Chen, Wenxiang Xu",
                "citations": 31
            },
            {
                "title": "Uncertainty Driven Action (UDA) model: A foundation for unifying perspectives on design activity",
                "abstract": "This paper proposes the Uncertainty Driven Action (UDA) model, which unifies the fragmented literature on design activity. The UDA model conceptualises design activity as a process consisting of three core actions: information action, knowledge-sharing action, and representation action, which are linked via uncertainty perception. The foundations of the UDA model in the design literature are elaborated in terms of the three core actions and their links to designer cognition and behaviour, utilising definitions and concepts from Activity Theory. The practical relevance and theoretical contributions of the UDA model are discussed. This paper contributes to the design literature by offering a comprehensive formalisation of design activity of individual designers, which connects cognition and action, to provide a foundation for understanding previously disparate descriptions of design activity.",
                "authors": "P. Cash, Melanie E. Kreye",
                "citations": 31
            },
            {
                "title": "The Attention Schema Theory: A Foundation for Engineering Artificial Consciousness",
                "abstract": "The purpose of the attention schema theory is to explain how an information-processing device, the brain, arrives at the claim that it possesses a non-physical, subjective awareness, and assigns a high degree of certainty to that extraordinary claim. The theory does not address how the brain might actually possess a non-physical essence. It is not a theory that deals in the non-physical. It is about the computations that cause a machine to make a claim and to assign a high degree of certainty to the claim. The theory is offered as a possible starting point for building artificial consciousness. Given current technology, it should be possible to build a machine that contains a rich internal model of what consciousness is, attributes that property of consciousness to itself and to the people it interacts with, and uses that attribution to make predictions about human behavior. Such a machine would “believe” it is conscious and act like it is conscious, in the same sense that the human machine believes and acts.",
                "authors": "M. Graziano",
                "citations": 44
            },
            {
                "title": "Damped vibration of a nonlocal nanobeam resting on viscoelastic foundation: fractional derivative model with two retardation times and fractional parameters",
                "abstract": null,
                "authors": "M. Cajić, D. Karličić, M. Lazarevic",
                "citations": 30
            },
            {
                "title": "Efficient model updating of a multi-story frame and its foundation stiffness from earthquake records using a timoshenko beam model",
                "abstract": null,
                "authors": "E. Taciroğlu, S. Farid Ghahari, F. Abazarsa",
                "citations": 44
            },
            {
                "title": "Calculation of foundation pit deformation caused by deep excavation considering influence of loading and unloading",
                "abstract": null,
                "authors": "Ming Huang, Xing-rong Liu, Nai-yang Zhang, Qilong Shen",
                "citations": 27
            },
            {
                "title": "Vibration velocity of X-section cast-in-place concrete (XCC) pile–raft foundation model for a ballastless track",
                "abstract": "This paper presents two case studies of the dynamic response of a ballastless track, X-section cast-in-place concrete (XCC) pile–raft (referred to as BTXPR) foundation embedded in sand subsoil. Model tests were conducted at a scale of 1/5 using a 7 m deep box with cross-sectional dimensions of 5 m × 4 m. In one case the box was filled with subsoil consisting of air-dried sand, whereas in the other case the box was filled with saturated sand. The tests involved measurement and analysis of the response in velocity under different applied cyclic load frequencies. It has been shown that the magnitude and variation of vibration velocity in the BTXPR foundation are closely related to the degree of saturation of the subsoil. Due to the existence of pore water in the saturated sand subsoil, the first natural frequency of the BTXPR foundation embedded in saturated sand is 5 Hz lower than that in air-dried sand. In addition, the amplitude of vibration velocity of the BTXPR foundation embedded in the saturated sand ...",
                "authors": "G. Sun, G. Kong, Han-long Liu, A. Amenuvor",
                "citations": 28
            },
            {
                "title": "Implementation of a non-linear foundation model for soil-structure interaction analysis of offshore wind turbines in FAST",
                "abstract": "Bottom-fixed offshore wind turbines (OWTs) involve a wide range of engineering fields. Of these, modelling of foundation flexibility has been given little priority. This paper investigates the modelling of bottom-fixed OWTs in the non-linear aero-hydro-servo-elastic simulation tool FAST v7. The OWTs considered is supported on a monopile. The objective of this paper was to implement a non-linear foundation model in this software. The National Renewable Energy Laboratory's idealized 5MW reference turbine was used as a base for the analyses.\r\n\r\nDefault modelling of foundation in FAST v7 is by means of a rigid foundation. This implies that soil stiffness and damping is disregarded. Damping may lead to lower design loads. A softer foundation, on the other hand, will increase the natural periods of the system, shifting them closer to the frequencies of the environmental loads. This may in turn lead to amplified moments at the mudline. Therefore, it is important to include soil stiffness and damping in analyses.\r\n\r\nIn this paper, a non-linear foundation model is introduced in FAST v7 by means of uncoupled parallel springs. To verify that the implementation is successful, non-linear load-displacement curves of the foundation spring are presented. These show the typical hysteresis loops of an inelastic material, which confirms the implementation. Copyright © 2016 John Wiley & Sons, Ltd.",
                "authors": "V. L. Krathe, A. Kaynia",
                "citations": 27
            },
            {
                "title": "A Mathematical Foundation of Big Data",
                "abstract": "The recent research evolution on big data has brought exciting aspiration to mathematicians, computer scientists and business professionals alike. However, the lack of a sound mathematical foundation presents itself as a real challenge amidst the swarm of big data marketing activities. This paper intends to propose a possible mathematical theory as a foundation for big data research. Specifically, we propose the concept of the adjective “big” as a mathematical operator, furthermore, the concept of so-called “big” logically and naturally fits the concept of being “linguistics variable” as per fuzzy logic research community for decades. The consequence of adopting such a mathematical modeling can be profoundly considered as an abstraction of the technologies, systems, tools for data management and processing that transforms data into big data. In addition, the concept of infinity of the big data is based on the theory of calculus and the set theory. Furthermore, the concept of relativity of the big data, as we find out, is based on the operations of the fuzzy subsets theory. The proposed approach in this paper, we hope, can facilitate and open up more opportunities for big data research and developments on big data analytics, business analytics, big data intelligence, big data computing as well as big data science.",
                "authors": "Zhaohao Sun, Paul P. Wang",
                "citations": 21
            },
            {
                "title": "The Life Span Model of Suicide and Its Neurobiological Foundation",
                "abstract": "The very incomprehensibility of the suicidal act has been occupying the minds of researchers and health professionals for a long time. Several theories of suicide have been proposed since the beginning of the past century, and a myriad of neurobiological studies have been conducted over the past two decades in order to elucidate its pathophysiology. Both neurobiology and psychological theories tend to work in parallel lines that need behavioral and empirical data respectively, to confirm their hypotheses. In this review, we are proposing a “Life Span Model of Suicide” with an attempt to integrate the “Stress-Diathesis Model” and the “Interpersonal Model of Suicide” into a neurobiological narrative and support it by providing a thorough compilation of related genetic, epigenetic, and gene expression findings. This proposed model comprises three layers, forming the capability of suicide: genetic factors as the predisposing Diathesis on one side and Stress, characterized by epigenetic marks on the other side, and in between gene expression and gene function which are thought to be influenced by Diathesis and Stress components. The empirical evidence of this model is yet to be confirmed and further research, specifically epigenetic studies in particular, are needed to support the presence of a life-long, evolving capability of suicide and identify its neurobiological correlates.",
                "authors": "B. Ludwig, B. Roy, Qingzhong Wang, Badari Birur, Yogesh K. Dwivedi",
                "citations": 39
            },
            {
                "title": "Superstructure–foundation interaction in multi-objective pile group optimization considering settlement response",
                "abstract": "The full potential of pile optimization has not been realized as the interactions between superstructures and foundations, and the relationships between material usage and foundation performance are rarely investigated. This paper introduces an analysis and optimization approach for pile group and piled raft foundations, which allows coupling of superstructure stiffness with the foundation model, through a condensed matrix representing the flexural characteristics of the superstructure. This coupled approach is implemented within a multi-objective optimization algorithm, capable of providing a series of optimized pile configurations at various amounts of material. The approach is illustrated through two case studies. The first case involves evaluation of the coupled superstructure–foundation analyses against field measurements of a piled raft–supported building in London, UK. The potential benefits of pile optimization are also demonstrated through re-analyses of the foundation by the proposed optimizatio...",
                "authors": "Y. Leung, A. Klar, K. Soga, N. Hoult",
                "citations": 21
            },
            {
                "title": "The Lexical Foundation of the Big Five Factor Model",
                "abstract": "A dictionary is the tangible repository of the common stock of words, although dictionaries comprise at best 10% of the full lexicon. Part of the lexicon is made up of the words used to describe what people do and what people are like. The psycholexical approach to personality focuses on this subset of words and on its exploitation, or what can be said to be the glossary of personality. This chapter is concerned with the history of the psycholexical approach to personality description, from ancient history to the more recent efforts, albeit focusing in particular on its modern history. Psycholexical taxonomies from around the world will be considered, as well as taxonomies based on nouns, verbs, adverbs, and their combinations. Ongoing controversies, difficulties, and disputes regarding alternative psycholexical personality structures will be considered, as well as recommendations for future research.",
                "authors": "B. D. Raad, Boris Mlacic",
                "citations": 35
            },
            {
                "title": "Vibration analysis of viscoelastic single-walled carbon nanotubes resting on a viscoelastic foundation",
                "abstract": null,
                "authors": "Da-peng Zhang, Y. Lei, Chengyuan Wang, Zhibin Shen",
                "citations": 18
            },
            {
                "title": "Foundation punch-through in clay with sand: centrifuge modelling",
                "abstract": "This paper is concerned with the vertical penetration resistance of conical spudcan and flat footings in layered soils. Centrifuge tests are reported for a clay bed with strength increasing with depth interbedded with dense and medium dense sand. Both non-visualising (full-model) and visualising (half-model) tests were conducted with high-quality digital images captured and analysed using the particle image velocimetry technique for the latter. The load–displacement curves often show a reduction in resistance on passing through the sand layers, which creates a risk of punch-through failure for the foundations when supporting a jack-up drilling unit. For a given foundation, the peak punch-through capacity (qpeak) is dependent on the thickness of both the overlying clay and the sand layer. The failure mechanism associated with the peak resistance in the sand layer involves entrapment of a thin band of top clay above the sand layer that subsequently shears along an inclined failure surface before being pushe...",
                "authors": "S. Ullah, S. Stanier, Yuxia Hu, D. White",
                "citations": 16
            },
            {
                "title": "A New Foundation Model for Integrated Analyses of Monopile-based Offshore Wind Turbines",
                "abstract": null,
                "authors": "A. Page, K. Skau, H. Jostad, G. Eiksund",
                "citations": 33
            },
            {
                "title": "Variability-based model transformation: formal foundation and application",
                "abstract": null,
                "authors": "D. Strüber, J. Rubin, T. Arendt, M. Chechik, G. Taentzer, J. Plöger",
                "citations": 31
            },
            {
                "title": "Conceptual data model: A foundation for successful concurrent engineering",
                "abstract": "Today, phase A studies of future space systems are often conducted in special design facilities such as the Concurrent Engineering Facility at the German Aerospace Center (DLR). Within these facilities, the studies are performed following a defined process making use of a data model for information exchange. Quite often it remains unclear what exactly such a data model is and how it is implemented and applied. Nowadays, such a data model is usually a software using a formal specification describing its capabilities within a so-called meta-model. This meta-model, often referred as conceptual data model, is finally used and instantiated as system model during these concurrent engineering studies. Such software also provides a user interface for instantiating and sharing the system model within the design team and it provides capabilities to analyze the system model on the fly. This is possible due to the semantics of the underlying conceptual data model creating a common language used to exchange and process design information. This article explains the implementation of the data model at DLR and shows information how it is applied in the concurrent engineering process of the Concurrent Engineering Facility. It highlights important aspects concerning the modeling capabilities during a study and discusses how they can be implemented into a corresponding conceptual data model. Accordingly, the article presents important aspects such as rights management and data consistency and the implications of them to the software’s underlying technology. A special use case of the data model is depicted and shows the flexibility of the implementation proven by a study of a multi-module space station.",
                "authors": "P. Fischer, Meenakshi Deshmukh, V. Maiwald, D. Quantius, Antonio Martelo Gomez, A. Gerndt",
                "citations": 26
            },
            {
                "title": "Centrifuge modeling of seismic foundation-soil-foundation interaction on liquefiable sand",
                "abstract": null,
                "authors": "Y. Jafarian, B. Mehrzad, Chung-Jung Lee, A. Haddad",
                "citations": 25
            },
            {
                "title": "Principles of Power Electronics",
                "abstract": "Substantially\n expanded and updated, the new edition of this classic textbook\n provides unrivalled coverage of the fundamentals of power\n electronics. Comprehensive coverage of foundational concepts in\n circuits, magnetics, devices, dynamic models, and control\n establishes a strong conceptual framework for further study.\n Extensive discussion of contemporary practical considerations,\n enhanced by real-world examples, prepares readers for design\n scenarios ranging from low-power dc/dc converters to\n multi-megawatt ac machine drives. New topics include SiC and GaN\n wide-bandgap materials, superjunction MOSFET and IGBT devices,\n advanced magnetics design, multi-level and switched-capacitor\n converters, RF converter circuits, and EMI. Over 300 new and\n revised end-of-chapter problems enhance and expand understanding\n of the material, with solutions for instructors. Unique in its\n breadth and depth, and providing a range of flexible teaching\n pathways at multiple levels, this is the definitive guide to\n power electronics for graduate and senior undergraduate students\n in electrical engineering, and practicing electrical\n engineers.",
                "authors": "J. Kassakian, M. Schlecht, G. Verghese",
                "citations": 1118
            },
            {
                "title": "Scientific discovery in the age of artificial intelligence",
                "abstract": null,
                "authors": "Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, P. Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, K. Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, J. Leskovec, Tie-Yan Liu, A. Manrai, Debora S. Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Velickovic, Max Welling, Linfeng Zhang, Connor W. Coley, Y. Bengio, M. Zitnik",
                "citations": 584
            },
            {
                "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)",
                "abstract": "Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: https://cdn.openai.com/contributions/gpt-4v.pdf",
                "authors": "Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang",
                "citations": 479
            },
            {
                "title": "Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks",
                "abstract": "A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEIT-3, which achieves excellent transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We use Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked “language” modeling on images (Imglish), texts (English), and image-text pairs (“parallel sentences”) in a unified manner. Experimental results show that BEIT-3 obtains remarkable performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).",
                "authors": "Wen Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiangbo Liu, Kriti Aggarwal, O. Mohammed, Saksham Singhal, Subhojit Som, Furu Wei",
                "citations": 223
            },
            {
                "title": "MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
                "abstract": "We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and text-books, covering six core disciplines: Art & Design, Busi-ness, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly het-erogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 28 open-source LMMs as well as the propri-etary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.",
                "authors": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen",
                "citations": 453
            },
            {
                "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
                "abstract": "Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers.In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks.Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.",
                "authors": "Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, Li Yuan",
                "citations": 345
            },
            {
                "title": "Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging",
                "abstract": "The segment anything model (SAM) was released as a foundation model for image segmentation. The promptable segmentation model was trained by over 1 billion masks on 11M licensed and privacy-respecting images. The model supports zero-shot image segmentation with various segmentation prompts (e.g., points, boxes, masks). It makes the SAM attractive for medical image analysis, especially for digital pathology where the training data are rare. In this study, we evaluate the zero-shot segmentation performance of SAM model on representative segmentation tasks on whole slide imaging (WSI), including (1) tumor segmentation, (2) non-tumor tissue segmentation, (3) cell nuclei segmentation. Core Results: The results suggest that the zero-shot SAM model achieves remarkable segmentation performance for large connected objects. However, it does not consistently achieve satisfying performance for dense instance object segmentation, even with 20 prompts (clicks/boxes) on each image. We also summarized the identified limitations for digital pathology: (1) image resolution, (2) multiple scales, (3) prompt selection, and (4) model fine-tuning. In the future, the few-shot fine-tuning with images from downstream pathological segmentation tasks might help the model to achieve better performance in dense object segmentation.",
                "authors": "Ruining Deng, C. Cui, Quan Liu, Tianyuan Yao, Lucas W. Remedios, Shunxing Bao, Bennett A. Landman, L. Wheless, Lori A. Coburn, K. Wilson, Yaohong Wang, Shilin Zhao, A. Fogo, Haichun Yang, Yucheng Tang, Yuankai Huo",
                "citations": 170
            },
            {
                "title": "mPLUG-OwI2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
                "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily fo-cus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collab-oration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modal-ity collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experi-ments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.",
                "authors": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Mingshi Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou",
                "citations": 290
            },
            {
                "title": "ACCESS: Advancing Innovation: NSF’s Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support",
                "abstract": "As the National Science Foundation evolves its investments in cyberinfrastructure, it has made a significant investment in the ACCESS (Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support) program instantiating a novel set of services along with a novel governance and management model. Research cyberinfrastructure (CI) is a key catalyst for discovery and innovation and plays a critical role in ensuring U.S. leadership in science and engineering, economic competitiveness, and national security, consistent with NSF’s mission. Funding of a set of awards through the ACCESS program has established a suite of CI coordination services targeted at supporting a broad and diverse set of requirements, researchers, and usage modalities spanning all areas of science and engineering research and education complemented by support for the collective and coordinated operation of the overall ACCESS program.",
                "authors": "Timothy J. Boerner, Stephen Deems, T. Furlani, Shelley Knuth, John Towns",
                "citations": 168
            },
            {
                "title": "High-Fidelity Audio Compression with Improved RVQGAN",
                "abstract": "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.",
                "authors": "Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, I. Kumar, Kundan Kumar",
                "citations": 208
            },
            {
                "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners",
                "abstract": "Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io",
                "authors": "Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, L. Takayama, F. Xia, Jacob Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, Anirudha Majumdar",
                "citations": 175
            },
            {
                "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
                "abstract": "A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked\"language\"modeling on images (Imglish), texts (English), and image-text pairs (\"parallel sentences\") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).",
                "authors": "Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, O. Mohammed, Saksham Singhal, S. Som, Furu Wei",
                "citations": 583
            },
            {
                "title": "3D foundation model library",
                "abstract": null,
                "authors": "A. Løkke, A. Page, K. Skau",
                "citations": 1
            },
            {
                "title": "Bayesian Optimization",
                "abstract": "Bayesian optimization is a methodology for optimizing expensive objective functions that has proven success in the sciences, engineering, and beyond. This timely text provides a self-contained and comprehensive introduction to the subject, starting from scratch and carefully developing all the key ideas along the way. This bottom-up approach illuminates unifying themes in the design of Bayesian optimization algorithms and builds a solid theoretical foundation for approaching novel situations. The core of the book is divided into three main parts, covering theoretical and practical aspects of Gaussian process modeling, the Bayesian approach to sequential decision making, and the realization and computation of practical and effective optimization policies. Following this foundational material, the book provides an overview of theoretical convergence results, a survey of notable extensions, a comprehensive history of Bayesian optimization, and an extensive annotated bibliography of applications.",
                "authors": "R. Garnett",
                "citations": 123
            },
            {
                "title": "Generative Pretraining in Multimodality",
                "abstract": "We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.",
                "authors": "Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang",
                "citations": 110
            },
            {
                "title": "Accuracy of Segment-Anything Model (SAM) in medical image segmentation tasks",
                "abstract": "— The segment-anything model (SAM), was introduced as a fundamental model for segmenting images. It was trained using over 1 billion masks from 11 million natural images. The model can perform zero-shot segmentation of images by using various prompts such as masks, boxes, and points. In this report, we explored (1) the accuracy of SAM on 12 public medical image segmentation datasets which cover various organs (brain, breast, chest, lung, skin, liver, bowel, pancreas, and prostate), image modalities (2D X-ray, histology, endoscropy, and 3D MRI and CT), and health conditions (normal, lesioned). (2) if the computer vision foundational segmentation model SAM can provide promising research directions for medical image segmentation. We found that SAM without re-training on medical images do not perform as accurately as U-Net or other deep learning models trained on medical images.",
                "authors": "Sheng He, Rina Bao, Jingpeng Li, P. Grant, Yangming Ou",
                "citations": 134
            },
            {
                "title": "Self-regulating Prompts: Foundational Model Adaptation without Forgetting",
                "abstract": "Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks. Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP. This leads to the loss of the model’s original generalization capability. To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating prompted representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch. To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity. PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization. We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods. Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC.",
                "authors": "Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Siddique Khan, Ming Yang, F. Khan",
                "citations": 104
            },
            {
                "title": "AI and the transformation of social science research",
                "abstract": "Careful bias management and data fidelity are key Advances in artificial intelligence (AI), particularly large language models (LLMs), are substantially affecting social science research. These transformer-based machine-learning models pretrained on vast amounts of text data are increasingly capable of simulating human-like responses and behaviors (1, 2), offering opportunities to test theories and hypotheses about human behavior at great scale and speed. This presents urgent challenges: How can social science research practices be adapted, even reinvented, to harness the power of foundational AI? And how can this be done while ensuring transparent and replicable research?",
                "authors": "I. Grossmann, M. Feinberg, D. C. Parker, N. Christakis, P. Tetlock, William A. Cunningham",
                "citations": 107
            },
            {
                "title": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects",
                "abstract": "We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without finetuning, as long as its CAD model is given, or a small number of reference images are captured. Thanks to the unified framework, the downstream pose estimation modules are the same in both setups, with a neural implicit representation used for efficient novel view synthesis when no CAD model is available. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In addition, it even achieves comparable results to instance-level methods despite the reduced assumptions. Project page: https://nvlabs.github.io/FoundationPose/",
                "authors": "Bowen Wen, Wei Yang, Jan Kautz, Stanley T. Birchfield",
                "citations": 85
            },
            {
                "title": "SAM on Medical Images: A Comprehensive Study on Three Prompt Modes",
                "abstract": "The Segment Anything Model (SAM) made an eye-catching debut recently and inspired many researchers to explore its potential and limitation in terms of zero-shot generalization capability. As the first promptable foundation model for segmentation tasks, it was trained on a large dataset with an unprecedented number of images and annotations. This large-scale dataset and its promptable nature endow the model with strong zero-shot generalization. Although the SAM has shown competitive performance on several datasets, we still want to investigate its zero-shot generalization on medical images. As we know, the acquisition of medical image annotation usually requires a lot of effort from professional practitioners. Therefore, if there exists a foundation model that can give high-quality mask prediction simply based on a few point prompts, this model will undoubtedly become the game changer for medical image analysis. To evaluate whether SAM has the potential to become the foundation model for medical image segmentation tasks, we collected more than 12 public medical image datasets that cover various organs and modalities. We also explore what kind of prompt can lead to the best zero-shot performance with different modalities. Furthermore, we find that a pattern shows that the perturbation of the box size will significantly change the prediction accuracy. Finally, Extensive experiments show that the predicted mask quality varied a lot among different datasets. And providing proper prompts, such as bounding boxes, to the SAM will significantly increase its performance.",
                "authors": "D. Cheng, Ziyuan Qin, Zekun Jiang, Shaoting Zhang, Qicheng Lao, Kang Li",
                "citations": 91
            },
            {
                "title": "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation",
                "abstract": "Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on a confined set of training datasets. In this work, we investigate scaling up EHPS towards the first generalist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments. 1) For the data scaling, we perform a systematic investigation on 32 EHPS datasets, including a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. 2) For the model scaling, we take advantage of vision transformers to study the scaling law of model sizes in EHPS. Moreover, our finetuning strategy turn SMPLer-X into specialist models, allowing them to achieve further performance boosts. Notably, our foundation model SMPLer-X consistently delivers state-of-the-art results on seven benchmarks such as AGORA (107.2 mm NMVE), UBody (57.4 mm PVE), EgoBody (63.6 mm PVE), and EHF (62.3 mm PVE without finetuning). Homepage: https://caizhongang.github.io/projects/SMPLer-X/",
                "authors": "Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Yanjun Wang, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Lei Yang, Ziwei Liu",
                "citations": 52
            },
            {
                "title": "arXiv",
                "abstract": "The innovative preprint repository, arXiv, was created in the early 1990s to improve access to scientific research. arXiv contains millions of Open Access articles in physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering\n and systems science, and economics. All articles are available for free download on the open web. Often research findings are available on arXiv before they are published in a peer-reviewed journal. arXiv relies on a collaborative support business model where institutions that most heavily\n utilize arXiv contribute financially. Support also comes from Cornell University and the Simons Foundation.",
                "authors": "Lucy Rosenbloom",
                "citations": 9889
            },
            {
                "title": "Squeeze-and-Excitation Networks",
                "abstract": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.",
                "authors": "Jie Hu, Li Shen, Samuel Albanie, Gang Sun, E. Wu",
                "citations": 23982
            },
            {
                "title": "Ladder of Citizen Participation",
                "abstract": "Proposed by Sherry Arnstein in 1969, the Ladder of Citizen Participation is one of the most widely referenced and influential models in the field of democratic public participation. For local leaders, organizers, and facilitators who want to understand foundational theories of public engagement and participation, and the ways in which empowered public institutions and officials deny power to citizens, Arnstein’s seminal article is essential reading. The model also influenced many later models, including Elizabeth Rocha’s Ladder of Empowerment and Roger Hart’s Ladder of Children’s Participation.",
                "authors": "Sherry Arnstein's",
                "citations": 8891
            },
            {
                "title": "Single-cell transcriptomics of 20 mouse organs creates a Tabula Muris",
                "abstract": null,
                "authors": "N. Schaum, Jim Karkanias, N. Neff, A. May, S. Quake, T. Wyss-Coray, S. Darmanis, Joshua Batson, O. Botvinnik, Michelle B. Chen, Steven Chen, Foad Green, Robert C. Jones, A. Maynard, L. Penland, A. Pisco, R. Sit, Geoffrey M. Stanley, James T. Webber, F. Zanini, Ankit S. Baghel, Isaac Bakerman, I. Bansal, D. Berdnik, Biter Bilen, D. Brownfield, Corey Cain, Min Cho, Giana Cirolia, Stephanie D. Conley, Aaron Demers, K. Demir, A. Morrée, T. Divita, H. D. Bois, Laughing Bear Torrez Dulgeroff, H. Ebadi, F. H. Espinoza, M. Fish, Qiang Gan, B. George, A. Gillich, Geraldine Genetiano, Xueying Gu, G. Gulati, Y. Hang, Shayan Hosseinzadeh, Albin Huang, Tal Iram, Taichi Isobe, Feather Ives, Kevin S. Kao, G. Karnam, Aaron M. Kershner, B. Kiss, W. Kong, Maya E. Kumar, Jonathan Y. Lam, Davis P. Lee, S. Lee, Guang Li, Qingyun Li, Ling Liu, Annie Lo, Wan-Jin Lu, A. Manjunath, Kaia L. May, O. L. May, Marina Mckay, Ross J. Metzger, Marco Mignardi, D. Min, Ahmad N. Nabhan, K. Ng, Joseph J. Noh, R. Patkar, Weng Chuan Peng, R. Puccinelli, Eric J. Rulifson, Shaheen S. Sikandar, Rahul Sinha, K. Szade, Weilun Tan, C. Tato, Krissie Tellez, K. Travaglini, C. Tropini, Lucas M. Waldburger, L. V. Weele, Michael N. Wosczyna, Jinyi Xiang, Soso Xue, Justin Youngyunpipatkul, Macy E. Zardeneta, Fan Zhang, Lu Zhou, Paola Castro, Derek Croote, J. Derisi, Christin S Kuo, B. Lehallier, P. Nguyen, Serena Y Tan, Bruce Wang, H. Yousef, P. Beachy, Charles K. F. Chan, K. C. Huang, K. Weinberg, Sean M. Wu, B. Barres, M. Clarke, Seung K. Kim, M. Krasnow, R. Nusse, T. Rando, J. Sonnenburg, I. Weissman",
                "citations": 1978
            },
            {
                "title": "Why Are Some STEM Fields More Gender Balanced Than Others?",
                "abstract": "Women obtain more than half of U.S. undergraduate degrees in biology, chemistry, and mathematics, yet they earn less than 20% of computer science, engineering, and physics undergraduate degrees (National Science Foundation, 2014a). Gender differences in interest in computer science, engineering, and physics appear even before college. Why are women represented in some science, technology, engineering, and mathematics (STEM) fields more than others? We conduct a critical review of the most commonly cited factors explaining gender disparities in STEM participation and investigate whether these factors explain differential gender participation across STEM fields. Math performance and discrimination influence who enters STEM, but there is little evidence to date that these factors explain why women’s underrepresentation is relatively worse in some STEM fields. We introduce a model with three overarching factors to explain the larger gender gaps in participation in computer science, engineering, and physics than in biology, chemistry, and mathematics: (a) masculine cultures that signal a lower sense of belonging to women than men, (b) a lack of sufficient early experience with computer science, engineering, and physics, and (c) gender gaps in self-efficacy. Efforts to increase women’s participation in computer science, engineering, and physics may benefit from changing masculine cultures and providing students with early experiences that signal equally to both girls and boys that they belong and can succeed in these fields.",
                "authors": "S. Cheryan, Sianna A. Ziegler, A. Montoya, Lily Jiang",
                "citations": 856
            },
            {
                "title": "TRY plant trait database - enhanced coverage and open access.",
                "abstract": "Plant traits-the morphological, anatomical, physiological, biochemical and phenological characteristics of plants-determine how plants respond to environmental factors, affect other trophic levels, and influence ecosystem properties and their benefits and detriments to people. Plant trait data thus represent the basis for a vast area of research spanning from evolutionary biology, community and functional ecology, to biodiversity conservation, ecosystem and landscape management, restoration, biogeography and earth system modelling. Since its foundation in 2007, the TRY database of plant traits has grown continuously. It now provides unprecedented data coverage under an open access data policy and is the main plant trait database used by the research community worldwide. Increasingly, the TRY database also supports new frontiers of trait-based plant research, including the identification of data gaps and the subsequent mobilization or measurement of new data. To support this development, in this article we evaluate the extent of the trait data compiled in TRY and analyse emerging patterns of data coverage and representativeness. Best species coverage is achieved for categorical traits-almost complete coverage for 'plant growth form'. However, most traits relevant for ecology and vegetation modelling are characterized by continuous intraspecific variation and trait-environmental relationships. These traits have to be measured on individual plants in their respective environment. Despite unprecedented data coverage, we observe a humbling lack of completeness and representativeness of these continuous traits in many aspects. We, therefore, conclude that reducing data gaps and biases in the TRY database remains a key challenge and requires a coordinated approach to data mobilization and trait measurements. This can only be achieved in collaboration with other initiatives.",
                "authors": "J. Kattge, G. Bönisch, S. Díaz, S. Lavorel, I. Prentice, P. Leadley, S. Tautenhahn, G. D. Werner, T. Aakala, M. Abedi, A. Acosta, G. Adamidis, Kairi Adamson, Masahiro Aiba, C. Albert, J. Alcántara, Carolina Alcázar C, I. Aleixo, Hamada E. Ali, Bernard Amiaud, C. Ammer, M. Amoroso, M. Anand, C. Anderson, N. Anten, J. Antos, D. Apgaua, T. Ashman, D. H. Asmara, G. Asner, Michael J. Aspinwall, O. Atkin, I. Aubin, Lars Baastrup‐Spohr, Khadijeh Bahalkeh, M. Bahn, T. Baker, W. Baker, J. Bakker, D. Baldocchi, J. Baltzer, A. Banerjee, A. Baranger, J. Barlow, Diego R. Barneche, Z. Baruch, D. Bastianelli, J. Battles, W. Bauerle, M. Bauters, E. Bazzato, Michael Beckmann, H. Beeckman, C. Beierkuhnlein, R. Bekker, Gavin Belfry, M. Belluau, Mirela Beloiu, Raquel Benavides, Lahcen Benomar, Mary Lee Berdugo-Lattke, E. Berenguer, R. Bergamin, Joana Bergmann, Marcos Bergmann Carlucci, L. Berner, M. Bernhardt‐Römermann, C. Bigler, Anne D. Bjorkman, Chris J. Blackman, C. Blanco, B. Blonder, D. Blumenthal, Kelly T. Bocanegra-González, P. Boeckx, Stephanie A. Bohlman, K. Böhning‐Gaese, Laura Boisvert‐Marsh, W. Bond, B. Bond‐Lamberty, A. Boom, Coline C. F. Boonman, K. Bordin, E. Boughton, V. Boukili, D. Bowman, S. Bravo, Marco R. Brendel, M. Broadley, K. Brown, H. Bruelheide, F. Brumnich, H. H. Bruun, David Bruy, S. Buchanan, S. F. Bucher, N. Buchmann, R. Buitenwerf, D. Bunker, Jana Bürger, S. Burrascano, D. Burslem, Bradley J. Butterfield, Chaeho Byun, M. Marques, M. Scalon, M. Caccianiga, M. Cadotte, M. Cailleret, J. Camac, J. Camarero, C. Campany, G. Campetella, J. A. Campos, Laura V. Cano-Arboleda, R. Canullo, M. Carbognani, Fabio Carvalho, F. Casanoves, B. Castagneyrol, J. Catford, J. Cavender-Bares, B. Cerabolini, M. Cervellini, Eduardo Chacón‐Madrigal, K. Chapin, F. Chapin, S. Chelli, Si‐Chong Chen, Anping Chen, P. Cherubini, F. Chianucci, B. Choat, Kyong-Sook Chung, M. Chytrý, D. Ciccarelli, L. Coll, Courtney G. Collins, L. Conti, D. Coomes, J. Cornelissen, W. Cornwell, P. Corona, M. Coyea, J. Craine, D. Craven, J. Cromsigt, A. Csecserits, K. Čufar, M. Cuntz, A. C. da Silva, K. Dahlin, M. Dainese, I. Dalke, M. Dalle Fratte, Anh Tuan Dang-Le, J. Danihelka, M. Dannoura, S. Dawson, Arend Jacobus de Beer, A. D. de Frutos, Jonathan R. De Long, Benjamin Dechant, S. Delagrange, N. Delpierre, G. Derroire, A. S. Dias, Milton H. Díaz‐Toribio, P. Dimitrakopoulos, M. Dobrowolski, D. Doktor, P. Dřevojan, N. Dong, J. Dransfield, S. Dressler, L. Duarte, E. Ducouret, S. Dullinger, W. Durka, R. Duursma, O. Dymova, A. E‐Vojtkó, R. L. Eckstein, H. Ejtehadi, J. Elser, T. Emílio, K. Engemann, Mohammad Bagher Erfanian, Alexandra Erfmeier, Adriane Esquível-Muelbert, G. Esser, M. Estiarte, T. Domingues, W. Fagan, J. Fagúndez, D. Falster, Ying Fan, Jingyun Fang, E. Farris, Fatih Fazlioglu, Yanhao Feng, Fernando Fernández-Méndez, C. Ferrara, J. Ferreira, A. Fidelis, B. Finegan, J. Firn, T. Flowers, D. Flynn, V. Fontana, E. Forey, Cristiane Forgiarini, L. François, M. Frangipani, D. Frank, Cédric Frenette‐Dussault, G. T. Freschet, Ellen L. Fry, N. Fyllas, G. G. Mazzochini, S. Gachet, R. Gallagher, G. Ganade, Francesca Ganga, P. García‐Palacios, V. Gargaglione, E. Garnier, J. Garrido, A. L. de Gasper, G. Gea‐Izquierdo, D. Gibson, A. Gillison, A. Giroldo, Mary‐Claire Glasenhardt, S. Gleason, Mariana Gliesch, E. Goldberg, B. Göldel, E. Gonzalez-Akre, J. González-Andújar, Andrés González-Melo, Ana González‐Robles, B. Graae, E. Granda, Sarah J. Graves, W. Green, T. Gregor, N. Gross, G. Guerin, Angela Günther, Á. Gutiérrez, L. Haddock, Anna L. Haines, Jefferson S. Hall, A. Hambuckers, W. Han, S. Harrison, W. Hattingh, J. Hawes, Tianhua He, Pengcheng He, J. M. Heberling, A. Helm, Stefan Hempel, J. Hentschel, B. Hérault, A. Hereș, Katharina Herz, M. Heuertz, T. Hickler, P. Hietz, P. Higuchi, A. Hipp, A. Hirons, Maria Hock, J. Hogan, K. Holl, O. Honnay, Daniel Hornstein, E. Hou, Nate Hough-Snee, K. Hovstad, T. Ichie, B. Igić, Estela Illa, M. Isaac, M. Ishihara, L. Ivanov, L. Ivanova, C. Iversen, J. Izquierdo, R. B. Jackson, B. Jackson, H. Jactel, A. Jagodziński, Ute Jandt, S. Jansen, T. Jenkins, A. Jentsch, J. R. P. Jespersen, Guo-Feng Jiang, J. L. Johansen, David Johnson, E. Jokela, C. Joly, G. Jordan, G. Joseph, D. Junaedi, R. Junker, E. Justes, R. Kabzems, J. Kane, Z. Kaplan, T. Kattenborn, L. Kavelenova, E. Kearsley, Anne Kempel, T. Kenzo, A. Kerkhoff, Mohammed Ibrahim Khalil, N. Kinlock, W. Kissling, K. Kitajima, T. Kitzberger, R. Kjøller, T. Klein, M. Kleyer, J. Klimešová, Joice Klipel, Brian Kloeppel, S. Klotz, J. M H Knops, T. Kohyama, F. Koike, J. Kollmann, B. Komac, K. Komatsu, C. König, Nathan J B Kraft, K. Kramer, H. Kreft, I. Kühn, D. Kumarathunge, J. Kuppler, H. Kurokawa, Y. Kurosawa, S. Kuyah, J. Laclau, Benoit Lafleur, E. Lallai, E. Lamb, A. Lamprecht, D. Larkin, D. Laughlin, Yoann Le Bagousse-Pinguet, Guerric le Maire, P. L. le Roux, Elizabeth le Roux, Tali D. Lee, F. Lens, S. Lewis, B. Lhotsky, Yuanzhi Li, Xin'e Li, J. W. Lichstein, Mario Liebergesell, J. Y. Lim, Yan-Shih Lin, J. Linares, Chunjiang Liu, Daijun Liu, Udayangani Liu, Stuart W. Livingstone, J. Llusià, Madelon Lohbeck, Á. López‐García, G. Lopez-Gonzalez, Zdeňka Lososová, F. Louault, B. Lukács, P. Lukeš, Yunjian Luo, Michele Lussu, Siyan Ma, Camilla Maciel Rabelo Pereira, M. Mack, V. Maire, A. Mäkelä, H. Mäkinen, A. C. M. Malhado, A. Mallik, P. Manning, S. Manzoni, Z. Marchetti, L. Marchino, Vinícius Marcilio-Silva, Eric Marcon, M. Marignani, Lars Markesteijn, Adam R. Martin, C. Martínez-Garza, J. Martínez‐Vilalta, T. Mašková, Kelly E. Mason, N. Mason, T. Massad, J. Masse, I. Mayrose, James K. McCarthy, M. L. McCormack, K. McCulloh, I. McFadden, B. McGill, M. McPartland, J. Medeiros, B. Medlyn, P. Meerts, Z. Mehrabi, P. Meir, Felipe P. L. Melo, Maurizio Mencuccini, Céline Meredieu, J. Messier, I. Mészáros, J. Metsaranta, S. Michaletz, Chrysanthi Michelaki, S. Migalina, R. Milla, Jesse E. D. Miller, V. Minden, R. Ming, K. Mokany, A. Moles, A. Molnár, J. Molofsky, Martin Molz, R. Montgomery, A. Monty, L. Moravcová, Á. Moreno-Martínez, M. Moretti, A. Mori, S. Mori, D. Morris, J. Morrison, L. Mucina, Sandra Mueller, C. Muir, S. Müller, F. Munoz, I. Myers-Smith, R. Myster, M. Nagano, Shawna L. Naidu, Ayyappan Narayanan, Balachandran Natesan, L. Negoita, A. Nelson, E. Neuschulz, J. Ni, G. Niedrist, Jhon Nieto, Ü. Niinemets, R. Nolan, H. Nottebrock, Y. Nouvellon, A. Novakovskiy, K. O. Nystuen, A. O'Grady, K. O’Hara, Andrew O’Reilly-Nugent, S. Oakley, W. Oberhuber, T. Ohtsuka, R. Oliveira, K. Öllerer, M. Olson, V. Onipchenko, Y. Onoda, R. Onstein, J. Ordoñez, N. Osada, I. Ostonen, G. Ottaviani, S. Otto, G. Overbeck, W. Ozinga, A. Pahl, C. E. T. Paine, R. Pakeman, A. Papageorgiou, Evgeniya Parfionova, M. Pärtel, Marco Patacca, S. Paula, Juraj Paule, H. Pauli, J. Pausas, B. Peco",
                "citations": 1208
            },
            {
                "title": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation",
                "abstract": "Precise image segmentation provides clinical study with instructive information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical segmentation tasks verify that SegVol outperforms the competitors in 19 tasks, with improvements up to 37.24% compared to the runner-up methods. We demonstrate the effectiveness and importance of specific designs by ablation study. We expect this foundation model can promote the development of volumetric medical image analysis. The model and code are publicly available at: https://github.com/BAAI-DCAI/SegVol.",
                "authors": "Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao",
                "citations": 27
            },
            {
                "title": "Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",
                "abstract": "Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks» Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning.",
                "authors": "J. Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang",
                "citations": 891
            },
            {
                "title": "Ambient Particulate Air Pollution and Daily Mortality in 652 Cities.",
                "abstract": "BACKGROUND\nThe systematic evaluation of the results of time-series studies of air pollution is challenged by differences in model specification and publication bias.\n\n\nMETHODS\nWe evaluated the associations of inhalable particulate matter (PM) with an aerodynamic diameter of 10 μm or less (PM10) and fine PM with an aerodynamic diameter of 2.5 μm or less (PM2.5) with daily all-cause, cardiovascular, and respiratory mortality across multiple countries or regions. Daily data on mortality and air pollution were collected from 652 cities in 24 countries or regions. We used overdispersed generalized additive models with random-effects meta-analysis to investigate the associations. Two-pollutant models were fitted to test the robustness of the associations. Concentration-response curves from each city were pooled to allow global estimates to be derived.\n\n\nRESULTS\nOn average, an increase of 10 μg per cubic meter in the 2-day moving average of PM10 concentration, which represents the average over the current and previous day, was associated with increases of 0.44% (95% confidence interval [CI], 0.39 to 0.50) in daily all-cause mortality, 0.36% (95% CI, 0.30 to 0.43) in daily cardiovascular mortality, and 0.47% (95% CI, 0.35 to 0.58) in daily respiratory mortality. The corresponding increases in daily mortality for the same change in PM2.5 concentration were 0.68% (95% CI, 0.59 to 0.77), 0.55% (95% CI, 0.45 to 0.66), and 0.74% (95% CI, 0.53 to 0.95). These associations remained significant after adjustment for gaseous pollutants. Associations were stronger in locations with lower annual mean PM concentrations and higher annual mean temperatures. The pooled concentration-response curves showed a consistent increase in daily mortality with increasing PM concentration, with steeper slopes at lower PM concentrations.\n\n\nCONCLUSIONS\nOur data show independent associations between short-term exposure to PM10 and PM2.5 and daily all-cause, cardiovascular, and respiratory mortality in more than 600 cities across the globe. These data reinforce the evidence of a link between mortality and PM concentration established in regional and local studies. (Funded by the National Natural Science Foundation of China and others.).",
                "authors": "Cong Liu, Renjie Chen, F. Sera, A. Vicedo-Cabrera, Yuming Guo, S. Tong, M. Coelho, P. Saldiva, É. Lavigne, P. Matus, Nicolás Valdés Ortega, Samuel David Osorio Garcia, M. Pascal, M. Stafoggia, M. Scortichini, M. Hashizume, Y. Honda, M. Hurtado-Díaz, J. Cruz, B. Nunes, J. Teixeira, Ho Kim, A. Tobías, C. Íñiguez, B. Forsberg, Christofer Åström, Martina S. Ragettli, Y. Guo, Bing-yu Chen, M. Bell, C. Wright, N. Scovronick, R. Garland, Ai Milojevic, J. Kyselý, A. Urban, H. Orru, Ene Indermitte, J. Jaakkola, N. Ryti, K. Katsouyanni, A. Analitis, A. Zanobetti, J. Schwartz, Jianmin Chen, Tangchun Wu, A. Cohen, A. Gasparrini, H. Kan",
                "citations": 977
            },
            {
                "title": "Federated Learning",
                "abstract": "How is it possible to allow multiple data owners to collaboratively train and use a shared prediction model while keeping all the local training data private? Traditional machine learning approaches need to combine all data at one location, typically a data center, which may very well violate the laws on user privacy and data confidentiality. Today, many parts of the world demand that technology companies treat user data carefully according to user-privacy laws. The European Union’s General Data Protection Regulation (GDPR) is a prime example. In this book, we describe how federated machine learning addresses this problem with novel solutions combining distributed machine learning, cryptography and security, and incentive mechanism design based on economic principles and game theory. We explain different types of privacypreserving machine learning solutions and their technological backgrounds, and highlight some representative practical use cases.We show how federated learning can become the foundation of next-generation machine learning that caters to technological and societal needs for responsible AI development and application.",
                "authors": "Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, Han Yu",
                "citations": 790
            },
            {
                "title": "A foundational vision transformer improves diagnostic performance for electrocardiograms",
                "abstract": null,
                "authors": "A. Vaid, Joy Jiang, Ashwin S. Sawant, S. Lerakis, E. Argulian, Yuri Ahuja, J. Lampert, A. Charney, H. Greenspan, J. Narula, Benjamin S. Glicksberg, G. Nadkarni",
                "citations": 32
            },
            {
                "title": "Turbulence Modeling in the Age of Data",
                "abstract": "Data from experiments and direct simulations of turbulence have historically been used to calibrate simple engineering models such as those based on the Reynolds-averaged Navier–Stokes (RANS) equations. In the past few years, with the availability of large and diverse data sets, researchers have begun to explore methods to systematically inform turbulence models with data, with the goal of quantifying and reducing model uncertainties. This review surveys recent developments in bounding uncertainties in RANS models via physical constraints, in adopting statistical inference to characterize model coefficients and estimate discrepancy, and in using machine learning to improve turbulence models. Key principles, achievements, and challenges are discussed. A central perspective advocated in this review is that by exploiting foundational knowledge in turbulence modeling and physical constraints, researchers can use data-driven approaches to yield useful predictive models.",
                "authors": "K. Duraisamy, G. Iaccarino, Heng Xiao",
                "citations": 1082
            },
            {
                "title": "Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding",
                "abstract": "Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human and computer vision through the Brain-Computer Interface. However, reconstructing high-quality images with correct semantics from brain recordings is a challenging problem due to the complex underlying representations of brain signals and the scarcity of data annotations. In this work, we present MinD-Vis: Sparse Masked Brain Modeling with Double-Conditioned Latent Diffusion Model for Human Vision Decoding. Firstly, we learn an effective self-supervised representation of fMRI data using mask modeling in a large latent space inspired by the sparse coding of information in the primary visual cortex. Then by augmenting a latent diffusion model with double-conditioning, we show that MinD-Vis can reconstruct highly plausible images with semantically matching details from brain recordings using very few paired annotations. We benchmarked our model qualitatively and quantitatively; the experimental results indicate that our method outperformed state-of-the-art in both semantic mapping (100-way semantic classification) and generation quality (FID) by 66% and 41% respectively. An exhaustive ablation study was also conducted to analyze our framework.",
                "authors": "Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, J. Zhou",
                "citations": 123
            },
            {
                "title": "Personal Sensing: Understanding Mental Health Using Ubiquitous Sensors and Machine Learning.",
                "abstract": "Sensors in everyday devices, such as our phones, wearables, and computers, leave a stream of digital traces. Personal sensing refers to collecting and analyzing data from sensors embedded in the context of daily life with the aim of identifying human behaviors, thoughts, feelings, and traits. This article provides a critical review of personal sensing research related to mental health, focused principally on smartphones, but also including studies of wearables, social media, and computers. We provide a layered, hierarchical model for translating raw sensor data into markers of behaviors and states related to mental health. Also discussed are research methods as well as challenges, including privacy and problems of dimensionality. Although personal sensing is still in its infancy, it holds great promise as a method for conducting mental health research and as a clinical tool for monitoring at-risk populations and providing the foundation for the next generation of mobile health (or mHealth) interventions.",
                "authors": "D. Mohr, Mi Zhang, S. Schueller",
                "citations": 563
            },
            {
                "title": "Prediction of Pile Bearing Capacity Using XGBoost Algorithm: Modeling and Performance Evaluation",
                "abstract": "The major criteria that control pile foundation design is pile bearing capacity (Pu). The load bearing capacity of piles is affected by the various characteristics of soils and the involvement of multiple parameters related to both soil and foundation. In this study, a new model for predicting bearing capacity is developed using an extreme gradient boosting (XGBoost) algorithm. A total of 200 driven piles static load test-based case histories were used to construct and verify the model. The developed XGBoost model results were compared to a number of commonly used algorithms—Adaptive Boosting (AdaBoost), Random Forest (RF), Decision Tree (DT) and Support Vector Machine (SVM) using various performance measure metrics such as coefficient of determination, mean absolute error, root mean square error, mean absolute relative error, Nash–Sutcliffe model efficiency coefficient and relative strength ratio. Furthermore, sensitivity analysis was performed to determine the effect of input parameters on Pu. The results show that all of the developed models were capable of making accurate predictions however the XGBoost algorithm surpasses others, followed by AdaBoost, RF, DT, and SVM. The sensitivity analysis result shows that the SPT blow count along the pile shaft has the greatest effect on the Pu.",
                "authors": "Maaz Amjad, I. Ahmad, Mahmood Ahmad, P. Wróblewski, P. Kamiński, Uzair Amjad",
                "citations": 112
            },
            {
                "title": "Dakota A Multilevel Parallel Object-Oriented Framework for Design Optimization Parameter Estimation Uncertainty Quantification and Sensitivity Analysis: Version 6.14 Theory Manual.",
                "abstract": "The Dakota (Design Analysis Kit for Optimization and Terascale Applications) toolkit provides a flexible and extensible interface between simulation codes and iterative analysis methods. Dakota contains algorithms for optimization with gradient and nongradient-based methods; uncertainty quantification with sampling, reliability, and stochastic expansion methods; parameter estimation with nonlinear least squares methods; and sensitivity/variance analysis with design of experiments and parameter study methods. These capabilities may be used on their own or as components within advanced strategies such as surrogate-based optimization, mixed integer nonlinear programming, or optimization under uncertainty. By employing object-oriented design to implement abstractions of the key components required for iterative systems analyses, the Dakota toolkit provides a flexible and extensible problem-solving environment for design and performance analysis of computational models on high performance computers. This report serves as a theoretical manual for selected algorithms implemented within the Dakota software. It is not intended as a comprehensive theoretical treatment, since a number of existing texts cover general optimization theory, statistical analysis, and other introductory topics. Rather, this manual is intended to summarize a set of Dakota-related research publications in the areas of surrogate-based optimization, uncertainty quantification, and optimization under uncertainty that provide the foundation for many of Dakota’s iterative analysis capabilities. Dakota Version 6.1 Theory Manual generated on November 7, 2014",
                "authors": "Brian M. Adams, Mohamed S. Ebeida, M. Eldred, J. Jakeman, L. Swiler, J. Stephens, Dena M. Vigil, Timothy M. Wildey, W. Bohnhoff, John P. Eddy, Kenneth T. Hu, K. Dalbey, Lara Bauman, P. Hough",
                "citations": 491
            },
            {
                "title": "PLACES: Local Data for Better Health",
                "abstract": "Local-level data on the health of populations are important to inform and drive effective and efficient actions to improve health, but such data are often expensive to collect and thus rare. Population Level Analysis and Community EStimates (PLACES) (www.cdc.gov/places/), a collaboration between the Centers for Disease Control and Prevention (CDC), the Robert Wood Johnson Foundation, and the CDC Foundation, provides model-based estimates for 29 measures among all counties and most incorporated and census-designated places, census tracts, and ZIP Code tabulation areas across the US. PLACES allows local health departments and others to better understand the burden and geographic distribution of chronic disease–related outcomes in their areas regardless of population size and urban–rural status and assists them in planning public health interventions. Online resources allow users to visually explore health estimates geographically, compare estimates, and download data for further use and exploration. By understanding the PLACES overall approach and using the easy-to-use PLACES applications, practitioners, policy makers, and others can enhance their efforts to improve public health, including informing prevention activities, programs, and policies; identifying priority health risk behaviors for action; prioritizing investments to areas with the biggest gaps or inequities; and establishing key health objectives to achieve community health and health equity.",
                "authors": "K. Greenlund, Hua Lu, Yan Wang, Kevin A. Matthews, Jennifer LeClercq, Benjamin Lee, S. Carlson",
                "citations": 104
            },
            {
                "title": "Digital business model innovation: toward construct clarity and future research directions",
                "abstract": null,
                "authors": "M. Trischler, J. Li‐Ying",
                "citations": 100
            },
            {
                "title": "Prefrontal cortex as a meta-reinforcement learning system",
                "abstract": null,
                "authors": "Jane X. Wang, Z. Kurth-Nelson, D. Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, D. Hassabis, M. Botvinick",
                "citations": 515
            },
            {
                "title": "A Detailed Model for Investigating Vertical Interaction between Railway Vehicle and Track",
                "abstract": "SUMMARY A new detailed model is developed to investigate the vertical interactions between railway vehicles and tracks. The model consists of two subsystems of vehicle and track in which the vehicle subsystem is modelled as a multi-body system with 10 DOFs running on the track with a constant velocity, and the track substructure as an infinite Euler beam supported on a discrete-continuous elastic foundation consisting of the three layers of rail, sleeper, and ballast. The interface between these two subsystems is the wheel / rail interaction described by Hertzian nonlinear elastic contact theory. In order to get numerical results of such a complex system, a new simple fast integration method is adopted. And typical numerical results are compared with field measured data to verify the detailed model. Finally, the model is applied to analysing the wheel / rail dynamic interactions in high-speed rail and in heavy haul railway.",
                "authors": "W. Zhai, Xiang Sun",
                "citations": 264
            },
            {
                "title": "A Structural View of SARS-CoV-2 RNA Replication Machinery: RNA Synthesis, Proofreading and Final Capping",
                "abstract": "The current coronavirus disease-2019 (COVID-19) pandemic is due to the novel coronavirus SARS-CoV-2. The scientific community has mounted a strong response by accelerating research and innovation, and has quickly set the foundation for understanding the molecular determinants of the disease for the development of targeted therapeutic interventions. The replication of the viral genome within the infected cells is a key stage of the SARS-CoV-2 life cycle. It is a complex process involving the action of several viral and host proteins in order to perform RNA polymerization, proofreading and final capping. This review provides an update of the structural and functional data on the key actors of the replicatory machinery of SARS-CoV-2, to fill the gaps in the currently available structural data, which is mainly obtained through homology modeling. Moreover, learning from similar viruses, we collect data from the literature to reconstruct the pattern of interactions among the protein actors of the SARS-CoV-2 RNA polymerase machinery. Here, an important role is played by co-factors such as Nsp8 and Nsp10, not only as allosteric activators but also as molecular connectors that hold the entire machinery together to enhance the efficiency of RNA replication.",
                "authors": "M. Romano, A. Ruggiero, F. Squeglia, G. Maga, R. Berisio",
                "citations": 409
            },
            {
                "title": "Quantitative single-cell proteomics as a tool to characterize cellular hierarchies",
                "abstract": null,
                "authors": "E. Schoof, B. Furtwängler, Nil Üresin, N. Rapin, Simonas Savickas, Coline Gentil, E. Lechman, U. A. D. Keller, J. Dick, B. Porse",
                "citations": 250
            },
            {
                "title": "Understanding Human Hands in Contact at Internet Scale",
                "abstract": "Hands are the central means by which humans manipulate their world and being able to reliably extract hand state information from Internet videos of humans engaged in their hands has the potential to pave the way to systems that can learn from petabytes of video data. This paper proposes steps towards this by inferring a rich representation of hands engaged in interaction method that includes: hand location, side, contact state, and a box around the object in contact. To support this effort, we gather a large-scale dataset of hands in contact with objects consisting of 131 days of footage as well as a 100K annotated hand-contact video frame dataset. The learned model on this dataset can serve as a foundation for hand-contact understanding in videos. We quantitatively evaluate it both on its own and in service of predicting and learning from 3D meshes of human hands.",
                "authors": "Dandan Shan, Jiaqi Geng, Michelle Shu, D. Fouhey",
                "citations": 278
            },
            {
                "title": "An Overview of Signal Processing Techniques for Terahertz Communications",
                "abstract": "Terahertz (THz)-band communications are a key enabler for future-generation wireless communication systems that promise to integrate a wide range of data-demanding applications. Recent advances in photonic, electronic, and plasmonic technologies are closing the gap in THz transceiver design. Consequently, prospect THz signal generation, modulation, and radiation methods are converging, and corresponding channel model, noise, and hardware-impairment notions are emerging. Such progress establishes a foundation for well-grounded research into THz-specific signal processing techniques for wireless communications. This tutorial overviews these techniques, emphasizing ultramassive multiple-input–multiple-output (UM-MIMO) systems and reconfigurable intelligent surfaces, vital for overcoming the distance problem at very high frequencies. We focus on the classical problems of waveform design and modulation, beamforming and precoding, index modulation, channel estimation, channel coding, and data detection. We also motivate signal processing techniques for THz sensing and localization.",
                "authors": "Hadi Sarieddeen, Mohamed-Slim Alouini, Tareq Y.Al-Naffouri",
                "citations": 272
            },
            {
                "title": "A Modern Approach towards an Industry 4.0 Model: From Driving Technologies to Management",
                "abstract": "Every so often, a confluence of novel technologies emerges that radically transforms every aspect of the industry, the global economy, and finally, the way we live. These sharp leaps of human ingenuity are known as industrial revolutions, and we are currently in the midst of the fourth such revolution, coined Industry 4.0 by the World Economic Forum. Building on their guideline set of technologies that encompass Industry 4.0, we present a full set of pillar technologies on which Industry 4.0 project portfolio management rests as well as the foundation technologies that support these pillars. A complete model of an Industry 4.0 factory which relies on these pillar technologies is presented. The full set of pillars encompasses cyberphysical systems and Internet of Things (IoT), artificial intelligence (AI), machine learning (ML) and big data, robots and drones, cloud computing, 5G and 6G networks, 3D printing, virtual and augmented reality, and blockchain technology. These technologies are based on a set of foundation technologies which include advances in computing, nanotechnology, biotechnology, materials, energy, and finally cube satellites. We illustrate the confluence of all these technologies in a single model factory. This new factory model succinctly demonstrates the advancements in manufacturing introduced by these modern technologies, which qualifies this as a seminal industrial revolutionary event in human history.",
                "authors": "Georgios Tsaramirsis, Antreas Kantaros, Izzat Al-Darraji, D. Piromalis, Charalampos Apostolopoulos, Athanasia Pavlopoulou, M. Alrammal, Z. Ismail, S. Buhari, M. Stojmenovic, Hatem Tamimi, Princy Randhawa, Akshet Patel, F. Khan",
                "citations": 68
            },
            {
                "title": "Status and prospects of genome‐wide association studies in plants",
                "abstract": "Genome‐wide association studies (GWAS) have developed into a powerful and ubiquitous tool for the investigation of complex traits. In large part, this was fueled by advances in genomic technology, enabling us to examine genome‐wide genetic variants across diverse genetic materials. The development of the mixed model framework for GWAS dramatically reduced the number of false positives compared with naïve methods. Building on this foundation, many methods have since been developed to increase computational speed or improve statistical power in GWAS. These methods have allowed the detection of genomic variants associated with either traditional agronomic phenotypes or biochemical and molecular phenotypes. In turn, these associations enable applications in gene cloning and in accelerated crop breeding through marker assisted selection or genetic engineering. Current topics of investigation include rare‐variant analysis, synthetic associations, optimizing the choice of GWAS model, and utilizing GWAS results to advance knowledge of biological processes. Ongoing research in these areas will facilitate further advances in GWAS methods and their applications.",
                "authors": "Laura Tibbs Cortes, Zhiwu Zhang, Jianming Yu",
                "citations": 229
            },
            {
                "title": "Training Deep Neural Networks with 8-bit Floating Point Numbers",
                "abstract": "The state-of-the-art hardware platforms for training deep neural networks are moving from traditional single precision (32-bit) computations towards 16 bits of precision - in large part due to the high energy efficiency and smaller bit storage associated with using reduced-precision representations. However, unlike inference, training with numbers represented with less than 16 bits has been challenging due to the need to maintain fidelity of the gradient computations during back-propagation. Here we demonstrate, for the first time, the successful training of deep neural networks using 8-bit floating point numbers while fully maintaining the accuracy on a spectrum of deep learning models and datasets. In addition to reducing the data and computation precision to 8 bits, we also successfully reduce the arithmetic precision for additions (used in partial product accumulation and weight updates) from 32 bits to 16 bits through the introduction of a number of key ideas including chunk-based accumulation and floating point stochastic rounding. The use of these novel techniques lays the foundation for a new generation of hardware training platforms with the potential for 2-4 times improved throughput over today's systems.",
                "authors": "Naigang Wang, Jungwook Choi, D. Brand, Chia-Yu Chen, K. Gopalakrishnan",
                "citations": 468
            },
            {
                "title": "Decentralized Privacy Using Blockchain-Enabled Federated Learning in Fog Computing",
                "abstract": "As the extension of cloud computing and a foundation of IoT, fog computing is experiencing fast prosperity because of its potential to mitigate some troublesome issues, such as network congestion, latency, and local autonomy. However, privacy issues and the subsequent inefficiency are dragging down the performances of fog computing. The majority of existing works hardly consider a reasonable balance between them while suffering from poisoning attacks. To address the aforementioned issues, we propose a novel blockchain-enabled federated learning (FL-Block) scheme to close the gap. FL-Block allows local learning updates of end devices exchanges with a blockchain-based global learning model, which is verified by miners. Built upon this, FL-Block enables the autonomous machine learning without any centralized authority to maintain the global model and coordinates by using a Proof-of-Work consensus mechanism of the blockchain. Furthermore, we analyze the latency performance of FL-Block and further derive the optimal block generation rate by taking communication, consensus delays, and computation cost into consideration. Extensive evaluation results show the superior performances of FL-Block from the aspects of privacy protection, efficiency, and resistance to the poisoning attack.",
                "authors": "Youyang Qu, Longxiang Gao, T. Luan, Yong Xiang, Shui Yu, Bai Li, Gavin Zheng",
                "citations": 305
            },
            {
                "title": "A Comparative Survey of VANET Clustering Techniques",
                "abstract": "A vehicular ad hoc network (VANET) is a mobile ad hoc network in which network nodes are vehicles—most commonly road vehicles. VANETs present a unique range of challenges and opportunities for routing protocols due to the semi-organized nature of vehicular movements subject to the constraints of road geometry and rules, and the obstacles which limit physical connectivity in urban environments. In particular, the problems of routing protocol reliability and scalability across large urban VANETs are currently the subject of intense research. Clustering can be used to improve routing scalability and reliability in VANETs, as it results in the distributed formation of hierarchical network structures by grouping vehicles together based on correlated spatial distribution and relative velocity. In addition to the benefits to routing, these groups can serve as the foundation for accident or congestion detection, information dissemination and entertainment applications. This paper explores the design choices made in the development of clustering algorithms targeted at VANETs. It presents a taxonomy of the techniques applied to solve the problems of cluster head election, cluster affiliation, and cluster management, and identifies new directions and recent trends in the design of these algorithms. Additionally, methodologies for validating clustering performance are reviewed, and a key shortcoming—the lack of realistic vehicular channel modeling—is identified. The importance of a rigorous and standardized performance evaluation regime utilizing realistic vehicular channel models is demonstrated.",
                "authors": "C. Cooper, D. Franklin, M. Ros, F. Safaei, M. Abolhasan",
                "citations": 410
            },
            {
                "title": "A survey of deep meta-learning",
                "abstract": null,
                "authors": "M. Huisman, Jan N. van Rijn, A. Plaat",
                "citations": 288
            },
            {
                "title": "Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?",
                "abstract": "The success of deep learning heavily relies on large-scale data with comprehensive labels, which is more expensive and time-consuming to fetch in 3D compared to 2D images or natural languages. This promotes the potential of utilizing models pretrained with data more than 3D as teachers for cross-modal knowledge transferring. In this paper, we revisit masked modeling in a unified fashion of knowledge distillation, and we show that foundational Transformers pretrained with 2D images or natural languages can help self-supervised 3D representation learning through training Autoencoders as Cross-Modal Teachers (ACT). The pretrained Transformers are transferred as cross-modal 3D teachers using discrete variational autoencoding self-supervision, during which the Transformers are frozen with prompt tuning for better knowledge inheritance. The latent features encoded by the 3D teachers are used as the target of masked point modeling, wherein the dark knowledge is distilled to the 3D Transformer students as foundational geometry understanding. Our ACT pretrained 3D learner achieves state-of-the-art generalization capacity across various downstream benchmarks, e.g., 88.21% overall accuracy on ScanObjectNN. Codes have been released at https://github.com/RunpeiDong/ACT.",
                "authors": "Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jian‐Yuan Sun, Zheng Ge, Li Yi, Kaisheng Ma",
                "citations": 73
            },
            {
                "title": "ProMod3—A versatile homology modelling toolbox",
                "abstract": "Computational methods for protein structure modelling are routinely used to complement experimental structure determination, thus they help to address a broad spectrum of scientific questions in biomedical research. The most accurate methods today are based on homology modelling, i.e. detecting a homologue to the desired target sequence that can be used as a template for modelling. Here we present a versatile open source homology modelling toolbox as foundation for flexible and computationally efficient modelling workflows. ProMod3 is a fully scriptable software platform that can perform all steps required to generate a protein model by homology. Its modular design aims at fast prototyping of novel algorithms and implementing flexible modelling pipelines. Common modelling tasks, such as loop modelling, sidechain modelling or generating a full protein model by homology, are provided as production ready pipelines, forming the starting point for own developments and enhancements. ProMod3 is the central software component of the widely used SWISS-MODEL web-server.",
                "authors": "G. Studer, G. Tauriello, S. Bienert, M. Biasini, N. Johner, T. Schwede",
                "citations": 183
            },
            {
                "title": "Supporting Artificial Social Intelligence With Theory of Mind",
                "abstract": "In this paper, we discuss the development of artificial theory of mind as foundational to an agent's ability to collaborate with human team members. Agents imbued with artificial social intelligence will require various capabilities to gather the social data needed to inform an artificial theory of mind of their human counterparts. We draw from social signals theorizing and discuss a framework to guide consideration of core features of artificial social intelligence. We discuss how human social intelligence, and the development of theory of mind, can contribute to the development of artificial social intelligence by forming a foundation on which to help agents model, interpret and predict the behaviors and mental states of humans to support human-agent interaction. Artificial social intelligence will need the processing capabilities to perceive, interpret, and generate combinations of social cues to operate within a human-agent team. Artificial Theory of Mind affords a structure by which a socially intelligent agent could be imbued with the ability to model their human counterparts and engage in effective human-agent interaction. Further, modeling Artificial Theory of Mind can be used by an ASI to support transparent communication with humans, improving trust in agents, so that they may better predict future system behavior based on their understanding of and support trust in artificial socially intelligent agents.",
                "authors": "Jessica Williams, S. Fiore, F. Jentsch",
                "citations": 54
            },
            {
                "title": "Soil erosion modelling: A global review and statistical analysis",
                "abstract": "To gain a better understanding of the global application of soil erosion prediction models, we comprehensively reviewed relevant peer-reviewed research literature on soil-erosion modelling published between 1994 and 2017. We aimed to identify (i) the processes and models most frequently addressed in the literature, (ii) the regions within which models are primarily applied, (iii) the regions which remain unaddressed and why, and (iv) how frequently studies are conducted to validate/evaluate model outcomes relative to measured data. To perform this task, we combined the collective knowledge of 67 soil-erosion scientists from 25 countries. The resulting database, named ‘Global Applications of Soil Erosion Modelling Tracker (GASEMT)’, includes 3030 individual modelling records from 126 countries, encompassing all continents (except Antarctica). Out of the 8471 articles identified as potentially relevant, we reviewed 1697 appropriate articles and systematically evaluated and transferred 42 relevant attributes into the database. This GASEMT database provides comprehensive insights into the state-of-the-art of soil- erosion models and model applications worldwide. This database intends to support the upcoming country-based United Nations global soil-erosion assessment in addition to helping to inform soil erosion research priorities by building a foundation for future targeted, in-depth analyses. GASEMT is an open-source database available to the entire user-community to develop research, rectify errors, and make future expansions.",
                "authors": "P. Borrelli, C. Alewell, P. Alvarez, J. A. Anache, J. Baartman, C. Ballabio, N. Bezak, M. Biddoccu, A. Cerdà, D. Chalise, Songchao Chen, Walter W. Chen, A. De Girolamo, G. D. Gessesse, D. Deumlich, N. Diodato, N. Efthimiou, G. Erpul, P. Fiener, M. Freppaz, F. Gentile, A. Gericke, Nigussie Haregeweyn, Bifeng Hu, A. Jeanneau, K. Kaffas, Mahboobeh Kiani-Harchegani, Iván Lizaga Villuendas, Changjia Li, L. Lombardo, M. López‐Vicente, M. Lucas‐Borja, M. Märker, Francis Matthews, C. Miao, M. Mikoš, S. Modugno, M. Möller, V. Naipal, M. Nearing, S. Owusu, D. Panday, E. Patault, C. Patriche, L. Poggio, Raquel Portes, L. Quijano, M. Rahdari, Mohammed Renima, G. Ricci, J. Rodrigo‐Comino, S. Saia, A. N. Samani, C. Schillaci, V. Syrris, H. Kim, D. Spinola, P. S. Oliveira, Hongfen Teng, R. Thapa, K. Vantas, Diana C. S. Vieira, Jae E. Yang, S. Yin, D. Zema, Guangju Zhao, P. Panagos",
                "citations": 339
            },
            {
                "title": "InternVideo-Ego4D: A Pack of Champion Solutions to Ego4D Challenges",
                "abstract": "In this report, we present our champion solutions to five tracks at Ego4D challenge. We leverage our developed InternVideo, a video foundation model, for five Ego4D tasks, including Moment Queries, Natural Language Queries, Future Hand Prediction, State Change Object Detection, and Short-term Object Interaction Anticipation. InternVideo-Ego4D is an effective paradigm to adapt the strong foundation model to the downstream ego-centric video understanding tasks with simple head designs. In these five tasks, the performance of InternVideo-Ego4D comprehensively surpasses the baseline methods and the champions of CVPR2022, demonstrating the powerful representation ability of InternVideo as a video foundation model. Our code will be released at https://github.com/OpenGVLab/ego4d-eccv2022-solutions",
                "authors": "Guo Chen, Sen Xing, Zhe Chen, Yi Wang, Kunchang Li, Yizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng, Bingkun Huang, Zhiyu Zhao, Junting Pan, Yifei Huang, Zun Wang, Jiashuo Yu, Yinan He, Hongjie Zhang, Tong Lu, Yali Wang, Liming Wang, Yu Qiao",
                "citations": 43
            },
            {
                "title": "Machine Learning on Graphs: A Model and Comprehensive Taxonomy",
                "abstract": "There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.",
                "authors": "Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Ré, K. Murphy",
                "citations": 275
            },
            {
                "title": "Deep Representation Learning for Human Motion Prediction and Classification",
                "abstract": "Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction.",
                "authors": "Judith Bütepage, Michael J. Black, D. Kragic, H. Kjellström",
                "citations": 407
            },
            {
                "title": "DocBERT: BERT for Document Classification",
                "abstract": "We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.",
                "authors": "Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy J. Lin",
                "citations": 289
            },
            {
                "title": "The Rosa genome provides new insights into the domestication of modern roses",
                "abstract": null,
                "authors": "O. Raymond, J. Gouzy, J. Just, Hélène Badouin, Marion Verdenaud, A. Lemainque, P. Vergne, S. Moja, N. Choisne, C. Pont, S. Carrère, J. Caissard, A. Couloux, Ludovic Cottret, J. Aury, J. Szécsi, D. Latrasse, Mohammed-Amin Madoui, Léa François, Xiaopeng Fu, Shuhua Yang, A. Dubois, F. Piola, Antoine Larrieu, Magalie Perez, K. Labadie, Lauriane Perrier, Benjamin Govetto, Y. Labrousse, Priscilla Villand, C. Bardoux, Véronique Boltz, C. Lopez-Roques, P. Heitzler, T. Vernoux, M. Vandenbussche, H. Quesneville, A. Boualem, A. Bendahmane, Chang Liu, Manuel Le Bris, J. Salse, S. Baudino, M. Benhamed, P. Wincker, M. Bendahmane",
                "citations": 352
            },
            {
                "title": "Langmuir's Theory of Adsorption: A Centennial Review.",
                "abstract": "The 100th anniversary of Langmuir's theory of adsorption is a significant landmark for the physical chemistry and chemical engineering communities. Despite its simplicity, the Langmuir adsorption model captures the key physics of molecular interactions at interfaces and laid the foundation for further progress in understanding interfacial phenomena, developing new adsorbent materials, and designing engineering processes. The Langmuir model has had an exceptional impact on diverse fields within the chemical sciences (ranging from chemical biology to materials science), an impact that became clearer with the development of modified adsorption theories and continues to be relevant today.",
                "authors": "Hans Swenson, N. Stadie",
                "citations": 310
            },
            {
                "title": "Porosity-dependent vibration analysis of FG microplates embedded by polymeric nanocomposite patches considering hygrothermal effect via an innovative plate theory",
                "abstract": null,
                "authors": "Ehsan Arshid, M. Khorasani, Zeinab Soleimani-Javid, S. Amir, A. Tounsi",
                "citations": 162
            },
            {
                "title": "Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?",
                "abstract": "As the scope of machine learning broadens, we observe a recurring theme of algorithmic monoculture: the same systems, or systems that share components (e.g. training data), are deployed by multiple decision-makers. While sharing offers clear advantages (e.g. amortizing costs), does it bear risks? We introduce and formalize one such risk, outcome homogenization: the extent to which particular individuals or groups experience negative outcomes from all decision-makers. If the same individuals or groups exclusively experience undesirable outcomes, this may institutionalize systemic exclusion and reinscribe social hierarchy. To relate algorithmic monoculture and outcome homogenization, we propose the component-sharing hypothesis: if decision-makers share components like training data or specific models, then they will produce more homogeneous outcomes. We test this hypothesis on algorithmic fairness benchmarks, demonstrating that sharing training data reliably exacerbates homogenization, with individual-level effects generally exceeding group-level effects. Further, given the dominant paradigm in AI of foundation models, i.e. models that can be adapted for myriad downstream tasks, we test whether model sharing homogenizes outcomes across tasks. We observe mixed results: we find that for both vision and language settings, the specific methods for adapting a foundation model significantly influence the degree of outcome homogenization. We conclude with philosophical analyses of and societal challenges for outcome homogenization, with an eye towards implications for deployed machine learning systems.",
                "authors": "Rishi Bommasani, Kathleen A. Creel, Ananya Kumar, Dan Jurafsky, Percy Liang",
                "citations": 62
            },
            {
                "title": "Zipf’s Law in Passwords",
                "abstract": "Despite three decades of intensive research efforts, it remains an open question as to what is the underlying distribution of user-generated passwords. In this paper, we make a substantial step forward toward understanding this foundational question. By introducing a number of computational statistical techniques and based on 14 large-scale data sets, which consist of 113.3 million real-world passwords, we, for the first time, propose two Zipf-like models (i.e., PDF-Zipf and CDF-Zipf) to characterize the distribution of passwords. More specifically, our PDF-Zipf model can well fit the popular passwords and obtain a coefficient of determination larger than 0.97; our CDF-Zipf model can well fit the entire password data set, with the maximum cumulative distribution function (CDF) deviation between the empirical distribution and the fitted theoretical model being 0.49%~4.59% (on an average 1.85%). With the concrete knowledge of password distributions, we suggest a new metric for measuring the strength of password data sets. Extensive experimental results show the effectiveness and general applicability of the proposed Zipf-like models and security metric.",
                "authors": "Ding Wang, Haibo Cheng, Ping Wang, Xinyi Huang, Gaopeng Jian",
                "citations": 373
            },
            {
                "title": "The customer value proposition: evolution, development, and application in marketing",
                "abstract": null,
                "authors": "A. Payne, Pennie Frow, Andreas Eggert",
                "citations": 303
            },
            {
                "title": "Using deep learning to model the hierarchical structure and function of a cell",
                "abstract": null,
                "authors": "Jianzhu Ma, M. Yu, Samson H. Fong, K. Ono, Eric Sage, Barry Demchak, R. Sharan, T. Ideker",
                "citations": 322
            },
            {
                "title": "Relationships between burnout, turnover intention, job satisfaction, job demands and job resources for mental health personnel in an Australian mental health service",
                "abstract": null,
                "authors": "J. Scanlan, M. Still",
                "citations": 253
            },
            {
                "title": "Multicenter screening for pre‐eclampsia by maternal factors and biomarkers at 11–13 weeks' gestation: comparison with NICE guidelines and ACOG recommendations",
                "abstract": "To compare the performance of screening for pre‐eclampsia (PE) based on risk factors from medical history, as recommended by NICE and ACOG, with the method proposed by The Fetal Medicine Foundation (FMF), which uses Bayes' theorem to combine the a‐priori risk from maternal factors, derived by a multivariable logistic model, with the results of various combinations of biophysical and biochemical measurements.",
                "authors": "N. O’Gorman, D. Wright, Liona C. Poon, Liona C. Poon, D. Rolnik, A. Syngelaki, M. D. Alvarado, I. Carbone, V. Dütemeyer, M. Fiolna, A. Frick, N. Karagiotis, S. Mastrodima, S. Mastrodima, C. Matallana, G. Papaioannou, A. Pazos, W. Plasencia, K. Nicolaides",
                "citations": 294
            },
            {
                "title": "GPCR activation mechanisms across classes and macro/microscales",
                "abstract": null,
                "authors": "A. Hauser, A. Kooistra, Christian Munk, F. Heydenreich, D. Veprintsev, M. Bouvier, M. Babu, David E. Gloriam",
                "citations": 142
            },
            {
                "title": "Understanding trends in C-H bond activation in heterogeneous catalysis.",
                "abstract": null,
                "authors": "Allegra A. Latimer, A. Kulkarni, Hassan Aljama, Joseph H. Montoya, Jong Suk Yoo, Charlie Tsai, F. Abild-Pedersen, F. Studt, J. Nørskov",
                "citations": 369
            },
            {
                "title": "Quality 4.0: The EFQM 2020 Model and Industry 4.0 Relationships and Implications",
                "abstract": "The European Foundation for Quality Management (EFQM) 2020 model is a comprehensive and updated business model that encompasses sustainability and shares features with Industry 4.0, emphasizing transformation and improved organizational performance, yet with different theoretical and practical foundations. This research highlights the EFQM 2020 model’s novelties and its relationships/implications with the Industry 4.0 paradigm, contributing to the Quality 4.0 body of knowledge. Several linkages between the EFQM 2020 model and Industry 4.0 have been identified, namely, at the criteria level and guidance points, which can support successful digital transformation by combining quality and excellence with Industry 4.0. However, given the model’s generic and non-prescriptive nature, there is no specific reference to the nine Industry 4.0 pillars. Additionally, the links between direction and organizational culture and leadership criteria and driving performance and transformation are not evident, which might be a concern for business and technology transformation strategies. Managing knowledge, skills, and capabilities is critical for the successful adoption of Industry 4.0. The EFQM model adds a strategic and technologically unbiased perspective to Industry 4.0, providing an integrated business excellence framework for Quality 4.0. With empirical support of the model application, future research is recommended to develop this subject further.",
                "authors": "L. Fonseca, A. J. Amaral, J. M. Oliveira",
                "citations": 138
            },
            {
                "title": "The Role of Parent Educational Attainment in Parenting and Children’s Development",
                "abstract": "Socioeconomic status (SES)—indexed via parent educational attainment, parent occupation, and family income—is a powerful predictor of children’s developmental outcomes. Variations in these resources predict large academic disparities among children from different socioeconomic backgrounds that persist over the years of schooling, perpetuating educational inequalities across generations. In this article, we provide an overview of a model that has guided our approach to studying these influences, focusing particularly on parent educational attainment. Parents’ educational attainment typically drives their occupations and income and is often used interchangeably with SES in research. We posit that parent educational attainment provides a foundation that supports children’s academic success indirectly through parents’ beliefs about and expectations for their children, as well as through the cognitive stimulation that parents provide in and outside of the home environment. We then expand this model to consider the intergenerational contributions and dynamic transactions within families that are important considerations for informing potential avenues for intervention.",
                "authors": "P. Davis‐Kean, L. Tighe, Nicholas E. Waters",
                "citations": 131
            },
            {
                "title": "Understanding behavioral intention to use mobile wallets in vietnam: Extending the tam model with trust and enjoyment",
                "abstract": "Abstract Mobile wallet (M-wallet), as an innovative alternative payment, is trendy in developed economies because it provides users with the ease, safety, and fast completion of daily financial activities. However, M-wallets have still been seen in the infancy stage in Vietnam. Therefore, the study aims to discover the main factors shaping behavioral intention to use mobile wallets in Vietnam. The extended version of the Technology Acceptance Model (TAM) with perceived enjoyment and trust was considered as a theoretical foundation for this study. The primary empirical data from 332 respondents were analyzed using structural equation modeling (SEM). Perceived ease of use, perceived usefulness, and enjoyment have positive and significant impacts on behavioral intention to use M-wallets, whereas trust showing no direct effect.",
                "authors": "Anh Tho To, T. M. Trinh",
                "citations": 123
            },
            {
                "title": "An evolution-based model for designing chorismate mutase enzymes",
                "abstract": "Learning from evolution Protein sequences contain information specifying their three-dimensional structure and function, and statistical analysis of families of sequences has been used to predict these properties. Building from sequence data, Russ et al. used statistical models that take into account conservation at amino acid positions and correlations in the evolution of pairs of amino acids to predict new artificial sequences that will have the properties of the protein family. For the chorismate mutase family of metabolic enzymes, the authors demonstrate experimentally that the artificial sequences display natural-like catalytic function. Because the models access an enormous space of diverse sequences, such evolution-based statistical approaches may guide the search for functional proteins with altered chemical activities. Science, this issue p. 440 An evolution-based, data-driven engineering process can build artificial functional enzymes. The rational design of enzymes is an important goal for both fundamental and practical reasons. Here, we describe a process to learn the constraints for specifying proteins purely from evolutionary sequence data, design and build libraries of synthetic genes, and test them for activity in vivo using a quantitative complementation assay. For chorismate mutase, a key enzyme in the biosynthesis of aromatic amino acids, we demonstrate the design of natural-like catalytic function with substantial sequence diversity. Further optimization focuses the generative model toward function in a specific genomic context. The data show that sequence-based statistical models suffice to specify proteins and provide access to an enormous space of functional sequences. This result provides a foundation for a general process for evolution-based design of artificial proteins.",
                "authors": "W. P. Russ, M. Figliuzzi, Christian Stocker, Pierre Barrat-Charlaix, M. Socolich, P. Kast, D. Hilvert, R. Monasson, S. Cocco, M. Weigt, R. Ranganathan",
                "citations": 218
            },
            {
                "title": "Bayesian data analysis for newcomers",
                "abstract": null,
                "authors": "J. Kruschke, Torrin M. Liddell",
                "citations": 309
            },
            {
                "title": "Modelling COVID-19",
                "abstract": null,
                "authors": "A. Vespignani, H. Tian, C. Dye, J. Lloyd-Smith, R. Eggo, Munik Shrestha, S. Scarpino, B. Gutiérrez, M. Kraemer, Joseph T. Wu, K. Leung, G. Leung",
                "citations": 206
            },
            {
                "title": "Foundations and Modeling of Dynamic Networks Using Dynamic Graph Neural Networks: A Survey",
                "abstract": "Dynamic networks are used in a wide range of fields, including social network analysis, recommender systems and epidemiology. Representing complex networks as structures changing over time allow network models to leverage not only structural but also temporal patterns. However, as dynamic network literature stems from diverse fields and makes use of inconsistent terminology, it is challenging to navigate. Meanwhile, graph neural networks (GNNs) have gained a lot of attention in recent years for their ability to perform well on a range of network science tasks, such as link prediction and node classification. Despite the popularity of graph neural networks and the proven benefits of dynamic network models, there has been little focus on graph neural networks for dynamic networks. To address the challenges resulting from the fact that this research crosses diverse fields as well as to survey dynamic graph neural networks, this work is split into two main parts. First, to address the ambiguity of the dynamic network terminology we establish a foundation of dynamic networks with consistent, detailed terminology and notation. Second, we present a comprehensive survey of dynamic graph neural network models using the proposed terminology.",
                "authors": "Joakim Skarding, B. Gabrys, Katarzyna Musial",
                "citations": 194
            },
            {
                "title": "A Digital Twin Based Industrial Automation and Control System Security Architecture",
                "abstract": "The digital twin is a rather new industrial control and automation systems concept. While the approach so far has gained interest mainly due to capabilities to make advanced simulations and optimizations, recently the possibilities for enhanced security have got attention within the research community. In this article, we discuss how a digital twin replication model and corresponding security architecture can be used to allow data sharing and control of security-critical processes. We identify design-driving security requirements for digital twin based data sharing and control. We show that the proposed state synchronization design meets the expected digital twin synchronization requirements and give a high-level design and evaluation of other security components of the architecture. We also make performance evaluations of a proof of concept for protected software upgrade using the proposed digital twin design. Our new security framework provides a foundation for future research work in this promising new area.",
                "authors": "C. Gehrmann, M. Gunnarsson",
                "citations": 189
            },
            {
                "title": "Employer Branding: A Brand Equity‐Based Literature Review and Research Agenda",
                "abstract": "Over the past two decades, scholarly interest in employer branding has strongly increased. Simultaneously, however, employer branding research has developed into a fragmented field with heterogeneous interpretations of the employer branding concept and its scope, which has impeded further theoretical and empirical advancement. To strengthen the foundation for future work, this paper takes a brand equity perspective to review the extant literature and create an integrative model of employer branding. Using an analytical approach, the authors identify 187 articles, which they integrate along different employer brand dimensions and branding strategies: (i) conceptual; (ii) employer knowledge dimensions; (iii) employer branding activities and strategies. On the basis of this review, the authors develop an employer branding value chain model and derive future research avenues as well as practical implications.",
                "authors": "Christian P Theurer, A. Tumasjan, I. Welpe, F. Lievens",
                "citations": 248
            },
            {
                "title": "The Coping Circumplex Model: An Integrative Model of the Structure of Coping With Stress",
                "abstract": "It seems obvious that the identification of coping structure is necessary to understand how stress affects human health and functioning. Despite numerous coping conceptualization proposals, there is no agreement as to the core coping categories. This article presents the Coping Circumplex Model (CCM), which is designed to integrate various coping distinctions, drawing inspiration from the tradition of circumplex models in psychology. The model is based on the assumption that individuals in stressful situations face two tasks: they need to solve the problem and regulate their emotions, which are reflected in two corresponding dimensions, that is, the problem coping dimension and emotion coping dimension. Problem coping and emotion coping are interpreted as bipolar dimensions. Importantly, these dimensions define a space for other coping categories. The model contains a total of eight coping styles forming a circumplex: positive emotional coping, efficiency, problem solving, preoccupation with the problem, negative emotional coping, helplessness, problem avoidance, and hedonic disengagement. The paper discusses the potential of the CCM to overcome some of the problems of stress psychology by: (a) supplementing the set of coping categories (i.e., process, strategy, style) with coping mode; (b) providing a foundation for the integration of numerous coping constructs; (c) enabling the interpretation of results obtained by means of different coping measures, thus facilitating knowledge consolidation; (d) explaining relationships between coping and adjustment after trauma, as well as explaining the mechanisms of psychological interventions (e.g., cognitive therapy, exposure therapy); (e) clarifying linkages between the effectiveness of coping strategies and situation controllability. Moreover, the CCM may elucidate the relationship between coping and emotion regulation (e.g., cognitive reappraisal and expressive suppression).",
                "authors": "Krzysztof Stanisławski",
                "citations": 234
            },
            {
                "title": "Blended Learning Adoption and Implementation in Higher Education: A Theoretical and Systematic Review",
                "abstract": null,
                "authors": "Bokolo Anthony Jnr, Adzhar Kamaludin, Awanis M. Romli, Anis Farihan Mat Raffei, D. N. E. Phon, Aziman Abdullah, G. Ming",
                "citations": 162
            },
            {
                "title": "A comprehensive computational approach for nonlinear thermal instability of the electrically FG-GPLRC disk based on GDQ method",
                "abstract": null,
                "authors": "M. Al-Furjan, H. Safarpour, M. Habibi, M. Safarpour, A. Tounsi",
                "citations": 140
            },
            {
                "title": "Book Review: Flipped Learning: A Guide for Higher Education Faculty",
                "abstract": "The book Flipped Learning: A Guide for Higher Education Faculty by Robert Talbert (2017) provides a solid foundation for designing a course by using a flipped learning approach. Talbert, a mathematics professor and assistant department chair at Grand Valley State University, wrote this book to share effective practices in flipped learning design that he uncovered through trial, error, and research, both his and others’. Talbert’s aim is to provide an understanding of what is meant by flipped learning and a roadmap to effectively implement this pedagogical approach. This review includes a description of the text, a comparison with similarly focused instructional resources, and an explanation of why this book is an excellent addition to the recommended readings of language instructors and interpretation/translation instructors. Talbert provides a conceptual framework of learning by describing the foundation for the approach, which is based on a collection of well-researched effective teaching/learning practices, not on a hunch about what will work well in a college course. The foundational principles are self-determination theory, cognitive load theory and the framework of self-regulated learning . Talbert also discusses backward design and integrated course design as frameworks that can be used alongside the flipped learning approach. With the explanations and examples in the text, reader can easily grasp how this approach differs from the traditional “assignments and homework” model. Talbert’s audience is tertiary teaching staff, not elementary and secondary teachers or workshop providers.",
                "authors": "K. Hale",
                "citations": 91
            },
            {
                "title": "Predictive representations can link model-based reinforcement learning to model-free mechanisms",
                "abstract": "Humans and animals are capable of evaluating actions by considering their long-run future rewards through a process described using model-based reinforcement learning (RL) algorithms. The mechanisms by which neural circuits perform the computations prescribed by model-based RL remain largely unknown; however, multiple lines of evidence suggest that neural circuits supporting model-based behavior are structurally homologous to and overlapping with those thought to carry out model-free temporal difference (TD) learning. Here, we lay out a family of approaches by which model-based computation may be built upon a core of TD learning. The foundation of this framework is the successor representation, a predictive state representation that, when combined with TD learning of value predictions, can produce a subset of the behaviors associated with model-based learning, while requiring less decision-time computation than dynamic programming. Using simulations, we delineate the precise behavioral capabilities enabled by evaluating actions using this approach, and compare them to those demonstrated by biological organisms. We then introduce two new algorithms that build upon the successor representation while progressively mitigating its limitations. Because this framework can account for the full range of observed putatively model-based behaviors while still utilizing a core TD framework, we suggest that it represents a neurally plausible family of mechanisms for model-based evaluation. Author Summary According to standard models, when confronted with a choice, animals and humans rely on two separate, distinct processes to come to a decision. One process deliberatively evaluates the consequences of each candidate action and is thought to underlie the ability to flexibly come up with novel plans. The other process gradually increases the propensity to perform behaviors that were previously successful and is thought to underlie automatically executed, habitual reflexes. Although computational principles and animal behavior support this dichotomy, at the neural level, there is little evidence supporting a clean segregation. For instance, although dopamine — famously implicated in drug addiction and Parkinson’s disease — currently only has a well-defined role in the automatic process, evidence suggests that it also plays a role in the deliberative process. In this work, we present a computational framework for resolving this mismatch. We show that the types of behaviors associated with either process could result from a common learning mechanism applied to different strategies for how populations of neurons could represent candidate actions. In addition to demonstrating that this account can produce the full range of flexible behavior observed in the empirical literature, we suggest experiments that could detect the various approaches within this framework.",
                "authors": "E. Russek, I. Momennejad, M. Botvinick, S. Gershman, N. Daw",
                "citations": 247
            },
            {
                "title": "Development of a fragility and vulnerability model for global seismic risk analyses",
                "abstract": null,
                "authors": "L. Martins, V. Silva",
                "citations": 145
            },
            {
                "title": "A SYSTEM’S VIEW OF E-LEARNING SUCCESS MODEL AND ITS A SYSTEM’S VIEW OF E-LEARNING SUCCESS MODEL AND ITS IMPLICATIONS TO E-LEARNING EMPIRICAL RESEARCH IMPLICATIONS TO E-LEARNING EMPIRICAL RESEARCH",
                "abstract": ": A stream of empirical research over the past decade that identiﬁed predictors of e-learning success suggests that there are several critical success factors (CSFs) that must be managed effectively to fully realize promise for e-learning. A problem with empirical distance learning research comes with modeling methods. Especially, there are two approaches of modeling, with or without mediating variables. This paper argues that the simple cause-effect relationship modelling approach in some cases may lead to misleading and false conclusions. As a basis of theoretical foundation to justify our approach, we introduce a system’s view of e-learning success model. Then, we present two examples from previous published papers that may mislead the effect of a CSF on the outcomes of e-learning systems. We conclude that the simple cause-effect relationship model approach in some cases may lead to misleading and false conclusions. Future e-learning empirical research should avoid the simple complex cause-effect relationship model.",
                "authors": "Sean B. Eom",
                "citations": 84
            },
            {
                "title": "Google Classroom for mobile learning in higher education: Modelling the initial perceptions of students",
                "abstract": null,
                "authors": "Jeya Amantha Kumar, Brandford Bervell",
                "citations": 167
            },
            {
                "title": "Co-creation in higher education: towards a conceptual model",
                "abstract": "ABSTRACT Students have begun to show interest in adopting active and participatory roles that allow them to interact and work collaboratively with educators. One important aspect of students as partners is a process known as value co-creation. Value co-creation is the process of students’ feedback, opinions, and other resources such as their intellectual capabilities and personalities, integrated alongside institutional resources, which can offer mutual value to both students and institutions. This paper presents the first conceptual model of value co-creation in higher education using a lens of co-creation cultivated through business and marketing literature. The model includes key components of value co-creation, co-production, and value-in-use as well as links to the anticipated benefits of value co-creation. The model can be used to inform and guide practice for the faculty and administration within higher education as well as to broaden the foundation of value co-creation literature.",
                "authors": "Mollie Dollinger, J. Lodge, H. Coates",
                "citations": 209
            },
            {
                "title": "Applying protection motivation theory to understand international tourists’ behavioural intentions under the threat of air pollution: A case of Beijing, China",
                "abstract": "ABSTRACT Air pollution represents a major concern for the tourism industry worldwide; however, few studies have investigated the influence of smog pollution on international tourists’ behavioural intentions. Protection motivation theory was taken as the theoretical foundation of this study, and ‘perceived government support’ was integrated as a new construct into the research model. Using data collected from international tourists visiting Beijing, China, structural equation modelling was employed to identify significant variables that could predict and explain international tourists’ protective behavioural intentions. Results reveal that severity, vulnerability, response efficacy, and self-efficacy significantly and positively influenced protective behavioural intention, whereas perceived government support exerted a significant and negative effect. Among significant and positive variables, the influence of the severity of threat appraisal was largest. Based on these findings, theoretical and practical implications related to protection motivation theory are discussed in a tourism context.",
                "authors": "W. Ruan, Sanghoon Kang, HakJun Song",
                "citations": 119
            },
            {
                "title": "A review on battery management system from the modeling efforts to its multiapplication and integration",
                "abstract": "Progress in battery technology accelerates the transition of battery management system (BMS) from a mere monitoring unit to a multifunction integrated one. It is necessary to establish a battery model for the implementation of BMS's effective control. With more comprehensive and faster battery model, it would be accurate and effective to reflect the behavior of the battery level to the vehicle. On this basis, to ensure battery safety, power, and durability, some key technologies based on the model are advanced, such as battery state estimation, energy equalization, thermal management, and fault diagnosis. Besides, the communication of interactions between BMS and vehicle controllers, motor controllers, etc is an essential consideration for optimizing driving and improving vehicle performance. As concluded, a synergistic and collaborative BMS is the foundation for green‐energy vehicles to be intelligent, electric, networked, and shared. Thus, this paper reviews the research and development (R&D) of multiphysics model simulation and multifunction integrated BMS technology. In addition, summary of the relevant research and state‐of‐the‐art technology is dedicated to improving the synergy and coordination of BMS and to promote the innovation and optimization of new energy vehicle technology.",
                "authors": "Ming-Chou Shen, Qing Gao",
                "citations": 164
            },
            {
                "title": "Interoperability between Building Information Modelling (BIM) and Building Energy Model (BEM)",
                "abstract": "Building information modelling (BIM) is the first step towards the implementation of the industrial revolution 4.0, in which virtual reality and digital twins are key elements. At present, buildings are responsible for 40% of the energy consumption in Europe and, so, there is a growing interest in reducing their energy use. In this context, proper interoperability between BIM and building energy model (BEM) is paramount for integrating the digital world into the construction sector and, therefore, increasing competitiveness by saving costs. This paper evaluates whether there is an automated or semi-automated BIM to BEM workflow that could improve the building design process. For this purpose, a residential building and a warehouse are constructed using the same BIM authoring tool (Revit), where two open schemas were used: green building extensible markup language (gbXML) and industry foundation classes (IFC). These transfer files were imported into software compatible with the EnergyPlus engine—Design Builder, Open Studio, and CYPETHERM HE—in which simulations were performed. Our results showed that the energy models were built up to 7.50% smaller than in the BIM and with missing elements in their thermal envelope. Nevertheless, the materials were properly transferred to gbXML and IFC formats. Moreover, the simulation results revealed a huge difference in values between the models generated by the open schemas, in the range of 6 to 900 times. Overall, we conclude that there exists a semi-automated workflow from BIM to BEM which does not work well for big and complex buildings, as they present major problems when creating the energy model. Furthermore, most of the issues encountered in BEM were errors in the transfer of BIM data to gbXML and IFC files. Therefore, we emphasise the need to improve compatibility between BIM and model exchange formats by their developers, in order to promote BIM–BEM interoperability.",
                "authors": "Gabriela Bastos Porsani, Kattalin Del Valle de Lersundi, Ana Sánchez-Ostiz Gutiérrez, Carlos Fernández Bandera",
                "citations": 76
            },
            {
                "title": "On the mechanism of soot nucleation.",
                "abstract": "The mechanism of carbon particulate (soot) inception has been a subject of numerous studies and debates. The article begins with a critical review of prior proposals, proceeds to the analysis of factors enabling the development of a meaningful nucleation flux, and then introduces new ideas that lead to the fulfillment of these requirements. In the new proposal, a rotationally-activated dimer is formed in the collision of an aromatic molecule and an aromatic radical; the two react during the lifetime of the dimer to form a stable, doubly-bonded bridge between them, with the reaction rooted in a five-member ring present on the molecule edge. Several such reactions were examined theoretically and the most promising one generated a measurable nucleation flux. The consistency of the proposed model with known aspects of soot particle nanostructure is discussed. The foundation of the new model is fundamentally the H-Abstraction-Carbon-Addition (HACA) mechanism with the reaction affinity enhanced by rotational excitation.",
                "authors": "M. Frenklach, A. Mebel",
                "citations": 120
            },
            {
                "title": "Measuring employee innovation:a review of existing scales and the development of the innovative behavior and innovation support inventories across cultures",
                "abstract": "Purpose - The paper develops a model of employee innovative behavior conceptualizing it as distinct from innovation outputs and as a multi-faceted behavior rather than a simple count of ‘innovative acts’ by employees. It understands individual employee innovative behaviors as a micro-foundation of firm intrapreneurship that is embedded in and influenced by contextual factors such as managerial, organizational and cultural support for innovation. Building from a review of existing employee innovative behavior scales and theoretical considerations we develop and validate the Innovative Behavior Inventory (IBI) and the Innovation Support Inventory (ISI). Design/methodology/approach – Two pilot studies, a third validation study in the Czech Republic and a fourth cross-cultural validation study using population representative samples from Switzerland, Germany, Italy and the Czech Republic (n=2,812 employees and 450 entrepreneurs) were conducted. Findings - Both inventories were reliable and showed factorial, criterion, convergent and discriminant validity as well as cross-cultural equivalence. Employee innovative behavior was supported as comprising of idea generation, idea search, idea communication, implementation starting activities, involving others and overcoming obstacles. Managerial support was the most proximal contextual influence on innovative behavior and mediated the effect of organizational support and national culture. Originality/value - The paper advances our understanding of employee innovative behavior as a multi-faceted phenomenon and the contextual factors influencing it. Where past research typically focuses on convenience samples within a particular country, we offer first robust evidence that our model of employee innovative behavior generalizes across cultures and types of samples. Our model and the IBI and ISI inventories enable researchers to build a deeper understanding of the important micro-foundation underpinning intrapreneurial behavior in organizations and allow practitioners to identify their organizations’ strengths and weaknesses related to intrapreneurship.",
                "authors": "M. Lukeš, U. Stephan",
                "citations": 188
            },
            {
                "title": "Detection of Apple Lesions in Orchards Based on Deep Learning Methods of CycleGAN and YOLOV3-Dense",
                "abstract": "Plant disease is one of the primary causes of crop yield reduction. With the development of computer vision and deep learning technology, autonomous detection of plant surface lesion images collected by optical sensors has become an important research direction for timely crop disease diagnosis. In this paper, an anthracnose lesion detection method based on deep learning is proposed. Firstly, for the problem of insufficient image data caused by the random occurrence of apple diseases, in addition to traditional image augmentation techniques, Cycle-Consistent Adversarial Network (CycleGAN) deep learning model is used in this paper to accomplish data augmentation. These methods effectively enrich the diversity of training data and provide a solid foundation for training the detection model. In this paper, on the basis of image data augmentation, densely connected neural network (DenseNet) is utilized to optimize feature layers of the YOLO-V3 model which have lower resolution. DenseNet greatly improves the utilization of features in the neural network and enhances the detection result of the YOLO-V3 model. It is verified in experiments that the improved model exceeds Faster R-CNN with VGG16 NET, the original YOLO-V3 model, and other three state-of-the-art networks in detection performance, and it can realize real-time detection. The proposed method can be well applied to the detection of anthracnose lesions on apple surfaces in orchards.",
                "authors": "Yunong Tian, Guodong Yang, Zhe Wang, E. Li, Zi-ze Liang",
                "citations": 143
            },
            {
                "title": "Vibration analysis of functionally graded graphene platelet reinforced cylindrical shells with different porosity distributions",
                "abstract": "ABSTRACT This paper studies free vibrational behavior of porous nanocomposite shells reinforced with graphene platelets (GPLs). GPLs are uniformly and nonuniformly distributed thorough the thickness direction. Different porosity distributions called uniform, symmetric, and asymmetric are considered. The elastic properties of the nanocomposite are obtained by employing Halpin–Tsai micromechanics model. The GPL-reinforced shell is modeled via first order shear deformation theory and Galerkin's method is implemented to obtain vibration frequencies. New results show the importance of porosity coefficient, porosity distribution, GPL distribution, GPL weight fraction, and geometrical and foundation parameters on vibration behavior of porous nanocomposite shells.",
                "authors": "M. Barati, A. Zenkour",
                "citations": 154
            },
            {
                "title": "A four-unknown refined plate theory for dynamic analysis of FG-sandwich plates under various boundary conditions",
                "abstract": "The current work, present dynamic analysis of the FG-sandwich plate seated on elastic foundation with various kinds of support using refined shear deformation theory. The present analytical model is simplified which the unknowns number are reduced. The zero-shear stresses at the free surfaces of the FG-sandwich plate are ensured without introducing any correction factors. The four equations of motion are determined via Hamilton\\' principle and solved by Galerkin\\'s approach for FG-sandwich plate with three kinds of the support. The proposed analytical model is verified by comparing the results with those obtained by other theories existing in the literature. The parametric studies are presented to detect the various parameters influencing the fundamental frequencies of the symmetric and non-symmetric FG-sandwich plate with various boundary conditions.",
                "authors": "A. Menasria, A. Kaci, A. A. Bousahla, F. Bourada, A. Tounsi, K. H. Benrahou, A. Tounsi, E. A. Bedia, S. R. Mahmoud",
                "citations": 114
            },
            {
                "title": "The recent past and promising future for data integration methods to estimate species’ distributions",
                "abstract": "With the advance of methods for estimating species distribution models has come an interest in how to best combine datasets to improve estimates of species distributions. This has spurred the development of data integration methods that simultaneously harness information from multiple datasets while dealing with the specific strengths and weaknesses of each dataset. We outline the general principles that have guided data integration methods and review recent developments in the field. We then outline key areas that allow for a more general framework for integrating data and provide suggestions for improving sampling design and validation for integrated models. Key to recent advances has been using point‐process thinking to combine estimators developed for different data types. Extending this framework to new data types will further improve our inferences, as well as relaxing assumptions about how parameters are jointly estimated. These along with the better use of information regarding sampling effort and spatial autocorrelation will further improve our inferences. Recent developments form a strong foundation for implementation of data integration models. Wider adoption can improve our inferences about species distributions and the dynamic processes that lead to distributional shifts.",
                "authors": "David A. W. Miller, K. Pacifici, J. Sanderlin, B. Reich",
                "citations": 170
            },
            {
                "title": "The Evolution of Data-Driven Modeling in Organic Chemistry",
                "abstract": "Organic chemistry is replete with complex relationships: for example, how a reactant’s structure relates to the resulting product formed; how reaction conditions relate to yield; how a catalyst’s structure relates to enantioselectivity. Questions like these are at the foundation of understanding reactivity and developing novel and improved reactions. An approach to probing these questions that is both longstanding and contemporary is data-driven modeling. Here, we provide a synopsis of the history of data-driven modeling in organic chemistry and the terms used to describe these endeavors. We include a timeline of the steps that led to its current state. The case studies included highlight how, as a community, we have advanced physical organic chemistry tools with the aid of computers and data to augment the intuition of expert chemists and to facilitate the prediction of structure–activity and structure–property relationships.",
                "authors": "W. Williams, Lingyu Zeng, T. Gensch, M. Sigman, A. Doyle, E. Anslyn",
                "citations": 70
            },
            {
                "title": "Homophily influences ranking of minorities in social networks",
                "abstract": null,
                "authors": "F. Karimi, Mathieu Génois, Claudia Wagner, Philipp Singer, M. Strohmaier",
                "citations": 170
            },
            {
                "title": "Affectionate Touch to Promote Relational, Psychological, and Physical Well-Being in Adulthood: A Theoretical Model and Review of the Research",
                "abstract": "Throughout the life span, individuals engage in affectionate touch with close others. Touch receipt promotes well-being in infancy, but the impacts of touch in adult close relationships have been largely unexplored. In this article, we propose that affectionate touch receipt promotes relational, psychological, and physical well-being in adulthood, and we present a theoretical mechanistic model to explain why affectionate touch may promote these outcomes. The model includes pathways through which touch could affect well-being by reducing stress and by promoting well-being independent of stress. Specifically, two immediate outcomes of affectionate touch receipt—relational-cognitive changes and neurobiological changes—are described as important mechanisms underlying the effects of affectionate touch on well-being. We also review and evaluate the existing research linking affectionate touch to well-being in adulthood and propose an agenda to advance research in this area. This theoretical perspective provides a foundation for future work on touch in adult close relationships.",
                "authors": "Brittany K. Jakubiak, B. Feeney",
                "citations": 203
            },
            {
                "title": "Entrepreneurial ecosystems: a dynamic lifecycle model",
                "abstract": null,
                "authors": "U. Cantner, James A. Cunningham, Erik E. Lehmann, M. Menter",
                "citations": 148
            },
            {
                "title": "Modeling of the spectroscopy of core electrons with density functional theory",
                "abstract": "The availability of X‐ray light sources with increased resolution and intensity has provided a foundation for increasingly sophisticated experimental studies exploiting the spectroscopy of core electrons to probe fundamental chemical, physical, and biological processes. Quantum chemical calculations can play a critical role in the analysis of these experimental measurements. The relatively low computational cost of density functional theory (DFT) and time‐dependent density functional theory (TDDFT) make them attractive choices for simulating the spectroscopy of core electrons. An overview of current developments to apply DFT and TDDFT to study the key techniques of X‐ray photoelectron spectroscopy, X‐ray absorption spectroscopy, X‐ray emission spectroscopy, and resonant inelastic X‐ray scattering is presented. Insight into the accuracy that can be achieved, in conjunction with an examination of the limitations and challenges to modeling the spectroscopy of core electrons with DFT is provided.",
                "authors": "N. Besley",
                "citations": 67
            },
            {
                "title": "Modeling Overuse Injuries in Sport as a Mechanical Fatigue Phenomenon",
                "abstract": "This paper postulates that overuse injury in sport is a biomechanical event resulting from the mechanical fatigue of biological tissue. A theoretical foundation and operational framework necessary to model overuse injury as a mechanical fatigue phenomenon is introduced. Adopting this framework may provide a more mechanistic understanding of overuse injury and inform training and preventive strategies to reduce their occurrence.",
                "authors": "W. B. Edwards",
                "citations": 160
            },
            {
                "title": "Addiction Research Consortium: Losing and regaining control over drug intake (ReCoDe)—From trajectories to mechanisms and interventions",
                "abstract": "One of the major risk factors for global death and disability is alcohol, tobacco, and illicit drug use. While there is increasing knowledge with respect to individual factors promoting the initiation and maintenance of substance use disorders (SUDs), disease trajectories involved in losing and regaining control over drug intake (ReCoDe) are still not well described. Our newly formed German Collaborative Research Centre (CRC) on ReCoDe has an interdisciplinary approach funded by the German Research Foundation (DFG) with a 12‐year perspective. The main goals of our research consortium are (i) to identify triggers and modifying factors that longitudinally modulate the trajectories of losing and regaining control over drug consumption in real life, (ii) to study underlying behavioral, cognitive, and neurobiological mechanisms, and (iii) to implicate mechanism‐based interventions. These goals will be achieved by: (i) using mobile health (m‐health) tools to longitudinally monitor the effects of triggers (drug cues, stressors, and priming doses) and modify factors (eg, age, gender, physical activity, and cognitive control) on drug consumption patterns in real‐life conditions and in animal models of addiction; (ii) the identification and computational modeling of key mechanisms mediating the effects of such triggers and modifying factors on goal‐directed, habitual, and compulsive aspects of behavior from human studies and animal models; and (iii) developing and testing interventions that specifically target the underlying mechanisms for regaining control over drug intake.",
                "authors": "A. Heinz, F. Kiefer, M. Smolka, T. Endrass, C. Beste, A. Beck, Shuyan Liu, Alexander Genauck, L. Romund, T. Banaschewski, F. Bermpohl, L. Deserno, R. Dolan, D. Durstewitz, U. Ebner-Priemer, H. Flor, A. Hansson, Christine Marcelle Heim, D. Hermann, S. Kiebel, P. Kirsch, C. Kirschbaum, G. Koppe, M. Marxen, A. Meyer-Lindenberg, Wolfgang E Nagel, H. R. Noori, M. Pilhatsch, J. Priller, M. Rietschel, N. Romanczuk-Seiferth, F. Schlagenhauf, W. Sommer, Jan Stallkamp, A. Ströhle, A. Stock, G. Winterer, C. Winter, H. Walter, S. Witt, S. Vollstädt-Klein, M. Rapp, H. Tost, R. Spanagel",
                "citations": 136
            },
            {
                "title": "County Health Rankings & Roadmaps",
                "abstract": "County Health Rankings & Roadmaps, a collaboration between the Robert Wood Johnson Foundation and the University of Wisconsin Population Health Institute, provides data, evidence, guidance, and examples that youth development professionals can use in their work to improve community health and well-being. The Rankings use a population health model to describe the multiple factors that contribute to health and equity. The information and resources available at the County Health Rankings & Roadmaps website is useful to youth development organization leaders, as it provides guidance for taking action to improve the well-being of youth and communities. Key resources available at www.countyealthrankings.org are discussed.",
                "authors": "Kate L. Kingery",
                "citations": 162
            },
            {
                "title": "Optimal Regulation of Virtual Power Plants",
                "abstract": "This paper develops a real-time algorithmic framework for aggregations of distributed energy resources (DERs) in distribution networks to provide regulation services in response to transmission-level requests. Leveraging online primal-dual-type methods for time-varying optimization problems and suitable linearizations of the nonlinear AC power-flow equations, this work establishes a system-theoretic foundation to realize the vision of distribution-level virtual power plants. The optimization framework controls the output powers of dispatchable DERs such that, in aggregate, they respond to automatic generation control and/or regulation-services commands. This is achieved while concurrently regulating voltages within the feeder and maximizing customers’ and utility's performance objectives. Convergence and tracking capabilities are analytically established under suitable modeling assumptions. Simulations are provided to validate the proposed approach.",
                "authors": "E. Dall’Anese, Swaroop S. Guggilam, Andrea Simonetto, Y. Chen, S. Dhople",
                "citations": 150
            },
            {
                "title": "An Introduction to Probabilistic Programming",
                "abstract": "This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. \nWe start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. \nIn the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. \nThis document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.",
                "authors": "Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, Frank Wood",
                "citations": 189
            },
            {
                "title": "A novel four-unknown integral model for buckling response of FG sandwich plates resting on elastic foundations under various boundary conditions using Galerkin\\'s approach",
                "abstract": "In this work, the buckling analysis of material sandwich plates based on a two-parameter elastic foundation under various boundary conditions is investigated on the basis of a new theory of refined trigonometric shear deformation. This theory includes indeterminate integral variables and contains only four unknowns in which any shear correction factor not used, with even less than the conventional theory of first shear strain (FSDT). Applying the principle of virtual displacements, the governing equations and boundary conditions are obtained. To solve the buckling problem for different boundary conditions, Galerkin\\'s approach is utilized for symmetric EGM sandwich plates with six different boundary conditions. A detailed numerical study is carried out to examine the influence of plate aspect ratio, elastic foundation coefficients, ratio, side-to-thickness ratio and boundary conditions on the buckling response of FGM sandwich plates. A good agreement between the results obtained and the available solutions of existing shear deformation theories that have a greater number of unknowns proves to demonstrate the precision of the proposed theory.",
                "authors": "Sara Chelahi Chikr, A. Kaci, A. A. Bousahla, F. Bourada, A. Tounsi, E. A. Bedia, S. R. Mahmoud, K. H. Benrahou, A. Tounsi",
                "citations": 110
            },
            {
                "title": "Buckling analysis of nonlocal third-order shear deformable functionally graded piezoelectric nanobeams embedded in elastic medium",
                "abstract": null,
                "authors": "F. Ebrahimi, M. Barati",
                "citations": 196
            },
            {
                "title": "The struggle of translating science into action: Foundational concepts of implementation science",
                "abstract": "Abstract Rationale, aims, and objectives “Implementation science,” the scientific study of methods translating research findings into practical, useful outcomes, is contested and complex, with unpredictable use of results from routine clinical practice and different levels of continuing assessment of implementable interventions. The authors aim to reveal how implementation science is presented and understood in health services research contexts and clarify the foundational concepts: diffusion, dissemination, implementation, adoption, and sustainability, to progress knowledge in the field. Method Implementation science models, theories, and frameworks are critiqued, and their value for laying the groundwork from which to implement a study's findings is emphasised. The paper highlights the challenges of turning research findings into practical outcomes that can be successfully implemented and the need for support from change agents, to ensure improvements to health care provision, health systems, and policy. The paper examines how researchers create implementation plans and what needs to be considered for study outputs to lead to sustainable interventions. This aspect needs clear planning, underpinned by appropriate theoretical paradigms that rigorously respond to a study's aims and objectives. Conclusion Researchers might benefit from a return to first principles in implementation science, whereby applications that result from research endeavours are both effective and readily disseminated and where interventions can be supported by appropriate health care personnel. These should be people specifically identified to promote change in service organisation, delivery, and policy that can be systematically evaluated over time, to ensure high‐quality, long‐term improvements to patients' health.",
                "authors": "F. Rapport, R. Clay-Williams, K. Churruca, Patti Shih, A. Hogden, J. Braithwaite",
                "citations": 209
            },
            {
                "title": "Foundational ontologies in action",
                "abstract": "The idea of proposing a special issue on the use of foundational ontologies for modelling simple everyday, or commonsense, situations came after several discussions with people in different domains which pointed to difficulties in understanding the philosophical and general presentations in the documentation accompanying foundational systems. This problem, we noticed, is worsened by the lack of step-by-step guidance to model a real situation from the point of view of a specific ontology. The result was that different knowledge engineers could generate mutually inconsistent models starting from the very same foundational ontology even when working on the same topic. Given that one of the main roles of ontologies is to serve as interoperability drivers, this conclusion was puzzling if not discouraging. We observed that it would not be possible to provide solutions to these issues without involving the very teams that developed foundational ontologies. On the other hand, if we could collect a series of presentations centred around the presentation of modelling cases, we would also have fostered a more consistent use of foundational ontologies, and set the bases for practical and comparative evaluations of the consequences in adopting one ontology rather than another. Another contribution that a special issue on “Foundational Ontologies in Action” could provide, and whose consideration eventually led to the particular selection of use cases discussed in this special issue, is to indicate how these ontologies differ in modelling core aspects in knowledge engineering practices. Such core aspects include the description of artefacts and their components, the modelling of changes",
                "authors": "S. Borgo, Antony Galton, O. Kutz",
                "citations": 15
            },
            {
                "title": "Recommendation on Design, Execution, and Reporting of Animal Atherosclerosis Studies: A Scientific Statement From the American Heart Association.",
                "abstract": "Animal studies are a foundation for defining mechanisms of atherosclerosis and potential targets of drugs to prevent lesion development or reverse the disease. In the current literature, it is common to see contradictions of outcomes in animal studies from different research groups, leading to the paucity of extrapolations of experimental findings into understanding the human disease. The purpose of this statement is to provide guidelines for development and execution of experimental design and interpretation in animal studies. Recommendations include the following: (1) animal model selection, with commentary on the fidelity of mimicking facets of the human disease; (2) experimental design and its impact on the interpretation of data; and (3) standard methods to enhance accuracy of measurements and characterization of atherosclerotic lesions.",
                "authors": "A. Daugherty, A. Tall, V. Chair, M. Daemen, E. Falk, E. Fisher, G. Garcı́a-Cardeña, A. Lusis, A. P. Owens, M. Rosenfeld, R. Virmani",
                "citations": 184
            },
            {
                "title": "Universality and individuality in neural dynamics across large populations of recurrent networks",
                "abstract": "Task-based modeling with recurrent neural networks (RNNs) has emerged as a popular way to infer the computational function of different brain regions. These models are quantitatively assessed by comparing the low-dimensional neural representations of the model with the brain, for example using canonical correlation analysis (CCA). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve common tasks uniquely determine the network dynamics, independent of modeling architectural choices? Or alternatively, are the learned dynamics highly sensitive to different model choices? Knowing the answer to these questions has strong implications for whether and how we should use task-based RNN modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks, with commonly used RNN architectures, trained to solve neuroscientifically motivated tasks and characterize their nonlinear dynamics. We find the geometry of the RNN representations can be highly sensitive to different network architectures, yielding a cautionary tale for measures of similarity that rely on representational geometry, such as CCA. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold-the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics-often appears universal across all architectures.",
                "authors": "Niru Maheswaranathan, Alex H. Williams, Matthew D. Golub, S. Ganguli, David Sussillo",
                "citations": 132
            },
            {
                "title": "Edge-Computing-Based Trustworthy Data Collection Model in the Internet of Things",
                "abstract": "It is generally accepted that the edge computing paradigm is regarded as capable of satisfying the resource requirements for the emerging mobile applications such as the Internet of Things (IoT) ones. Undoubtedly, the data collected by underlying sensor networks are the foundation of both the IoT systems and IoT applications. However, due to the weakness and vulnerability to attacks of underlying sensor networks, the data collected are usually untrustworthy, which may cause disastrous consequences. In this article, a new model is proposed to collect trustworthy data on the basis of edge computing in the IoT. In this model, the sensor nodes are evaluated from multiple dimensions to obtain accurately quantified trust values. Besides, by mapping the trust value of a node onto a force for the mobile data collector, the best mobility path is generated with high trust. Moreover, a mobile edge data collector is used to visit both the sensors with quantified trust values and collect trustworthy data. The extensive experiment validates that the IoT systems based on trustworthy data collection model gain a significant improvement in their performance, in terms of both system security and energy conservation.",
                "authors": "Tian Wang, Lei Qiu, A. K. Sangaiah, Anfeng Liu, Md Zakirul Alam Bhuiyan, Ying Ma",
                "citations": 99
            },
            {
                "title": "A refined quasi-3D shear deformation theory for thermo-mechanical behavior of functionally graded sandwich plates on elastic foundations",
                "abstract": "In this paper, a refined quasi-three-dimensional shear deformation theory for thermo-mechanical analysis of functionally graded sandwich plates resting on a two-parameter (Pasternak model) elastic foundation is developed. Unlike the other higher-order theories the number of unknowns and governing equations of the present theory is only four against six or more unknown displacement functions used in the corresponding ones. Furthermore, this theory takes into account the stretching effect due to its quasi-three-dimensional nature. The boundary conditions in the top and bottoms surfaces of the sandwich functionally graded plate are satisfied and no correction factor is required. Various types of functionally graded material sandwich plates are considered. The governing equations and boundary conditions are derived using the principle of virtual displacements. Numerical examples, selected from the literature, are illustrated. A good agreement is obtained between numerical results of the refined theory and the reference solutions. A parametric study is presented to examine the effect of the material gradation and elastic foundation on the deflections and stresses of functionally graded sandwich plate resting on elastic foundation subjected to thermo-mechanical loading.",
                "authors": "Abdelkader Mahmoudi, S. Benyoucef, A. Tounsi, A. Benachour, E. A. Adda Bedia, S. Mahmoud",
                "citations": 125
            },
            {
                "title": "Current Research and Open Problems in Attribute-Based Access Control",
                "abstract": "Attribute-based access control (ABAC) is a promising alternative to traditional models of access control (i.e., discretionary access control (DAC), mandatory access control (MAC), and role-based access control (RBAC)) that is drawing attention in both recent academic literature and industry application. However, formalization of a foundational model of ABAC and large scale adoption is still in its infancy. The relatively recent emergence of ABAC still leaves a number of problems unexplored. Issues like delegation, administration, auditability, scalability, hierarchical representations, and the like, have been largely ignored or left to future work. This article provides a basic introduction to ABAC and a comprehensive review of recent research efforts toward developing formal models of ABAC. A taxonomy of ABAC research is presented and used to categorize and evaluate surveyed articles. Open problems are identified based on the shortcomings of the reviewed works and potential solutions discussed.",
                "authors": "Daniel Servos, Sylvia L. Osborn",
                "citations": 189
            },
            {
                "title": "Psychophysical scaling reveals a unified theory of visual memory strength",
                "abstract": null,
                "authors": "M. Schurgin, J. Wixted, Timothy F. Brady",
                "citations": 160
            },
            {
                "title": "Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources",
                "abstract": "Apache Calcite is a foundational software framework that provides query processing, optimization, and query language support to many popular open-source data processing systems such as Apache Hive, Apache Storm, Apache Flink, Druid, and MapD. The goal of this paper is to formally introduce Calcite to the broader research community, brie y present its history, and describe its architecture, features, functionality, and patterns for adoption. Calcite's architecture consists of a modular and extensible query optimizer with hundreds of built-in optimization rules, a query processor capable of processing a variety of query languages, an adapter architecture designed for extensibility, and support for heterogeneous data models and stores (relational, semi-structured, streaming, and geospatial). This exible, embeddable, and extensible architecture is what makes Calcite an attractive choice for adoption in big-data frameworks. It is an active project that continues to introduce support for the new types of data sources, query languages, and approaches to query processing and optimization.",
                "authors": "Edmon Begoli, Jesús Camacho-Rodríguez, Julian Hyde, Michael J. Mior, D. Lemire",
                "citations": 152
            },
            {
                "title": "Foundations of Teamwork and Collaboration",
                "abstract": "The term teamwork has graced countless motivational posters and office walls. However, although teamwork is often easy to observe, it is somewhat more difficult to describe and yet more difficult to produce. At a broad level, teamwork is the process through which team members collaborate to achieve task goals. Teamwork refers to the activities through which team inputs translate into team outputs such as team effectiveness and satisfaction. In this article, we describe foundational research underlying current research on teamwork. We examine the evolution of team process models and outline primary teamwork dimensions. We discuss selection, training, and design approaches to enhancing teamwork, and note current applications of teamwork research in real-world settings.",
                "authors": "J. Driskell, E. Salas, T. Driskell",
                "citations": 172
            },
            {
                "title": "Face image recognition based on convolutional neural network",
                "abstract": "With the continuous progress of The Times and the development of technology, the rise of network social media has also brought the \"explosive\" growth of image data. As one of the main ways of People's Daily communication, image is widely used as a carrier of communication because of its rich content, intuitive and other advantages. Image recognition based on convolution neural network is the first application in the field of image recognition. A series of algorithm operations such as image eigenvalue extraction, recognition and convolution are used to identify and analyze different images. The rapid development of artificial intelligence makes machine learning more and more important in its research field. Use algorithms to learn each piece of data and predict the outcome. This has become an important key to open the door of artificial intelligence. In machine vision, image recognition is the foundation, but how to associate the low-level information in the image with the high-level image semantics becomes the key problem of image recognition. Predecessors have provided many model algorithms, which have laid a solid foundation for the development of artificial intelligence and image recognition. The multi-level information fusion model based on the VGG16 model is an improvement on the fully connected neural network. Different from full connection network, convolutional neural network does not use full connection method in each layer of neurons of neural network, but USES some nodes for connection. Although this method reduces the computation time, due to the fact that the convolutional neural network model will lose some useful feature information in the process of propagation and calculation, this paper improves the model to be a multi-level information fusion of the convolution calculation method, and further recovers the discarded feature information, so as to improve the recognition rate of the image. VGG divides the network into five groups (mimicking the five layers of AlexNet), yet it USES 3∗3 filters and combines them as a convolution sequence. Network deeper DCNN, channel number is bigger. The recognition rate of the model was verified by 0RL Face Database, BioID Face Database and CASIA Face Image Database.",
                "authors": "Guangxin Lou, Hongzhen Shi",
                "citations": 93
            },
            {
                "title": "Actionable health app evaluation: translating expert frameworks into objective metrics",
                "abstract": null,
                "authors": "Sarah Lagan, Patrick Aquino, Margaret R. Emerson, Karen L. Fortuna, Robert Walker, J. Torous",
                "citations": 86
            },
            {
                "title": "Optimizing an ANN model with ICA for estimating bearing capacity of driven pile in cohesionless soil",
                "abstract": null,
                "authors": "H. Moayedi, D. J. Armaghani",
                "citations": 154
            },
            {
                "title": "The ergodicity problem in economics",
                "abstract": null,
                "authors": "O. Peters",
                "citations": 119
            },
            {
                "title": "Buckling behavior of a single-layered graphene sheet resting on viscoelastic medium via nonlocal four-unknown integral model",
                "abstract": "In the present work, the buckling behavior of a single-layered graphene sheet (SLGS) embedded in visco-Pasternak\\' s medium is studied using nonlocal four-unknown integral model. This model has a displacement field with integral terms which includes the effect of transverse shear deformation without using shear correction factors. The visco-Pasternak\\'s medium is introduced by considering the damping effect to the classical foundation model which modeled by the linear Winkler\\'s coefficient and Pasternak\\' s coefficients, damping parameter, and mode numbers on the buckling response of the SLGSs are studied and discussed.",
                "authors": "M. Bellal, H. Hebali, H. Heireche, A. A. Bousahla, A. Tounsi, F. Bourada, S. R. Mahmoud, E. A. Bedia, A. Tounsi",
                "citations": 81
            },
            {
                "title": "International Conference on Information Systems ( ICIS ) December 1999 IT Capabilities : Theoretical Perspectives and Empirical Operationalization",
                "abstract": "With increased emphasis on the strategic role of IT in contemporary organizations, it is imperative to gain a deeper understanding of the factors that govern a firm’s IT capability. Yet, there exists very little understanding as to what constitutes a firm’s IT capability and how it could be measured. Drawing from theoretical perspectives and a systematic multi-stage research framework based on Delphi panels and focus groups, we conceptualize an enterprise-wide IT capability as a second order factor model. Using structural equation modeling techniques, the IT capability construct is empirically verified. Our study results provide a useful tool for benchmarking IT capability and serves as a foundation for operationalizing a key dependent variable in ITbusiness value research.",
                "authors": "Anandhi Bharadwaj",
                "citations": 132
            },
            {
                "title": "An enactive approach to pain: beyond the biopsychosocial model",
                "abstract": null,
                "authors": "Peter Stilwell, K. Harman",
                "citations": 114
            },
            {
                "title": "Learning Compositional Koopman Operators for Model-Based Control",
                "abstract": "Finding an embedding space for a linear approximation of a nonlinear dynamical system enables efficient system identification and control synthesis. The Koopman operator theory lays the foundation for identifying the nonlinear-to-linear coordinate transformations with data-driven methods. Recently, researchers have proposed to use deep neural networks as a more expressive class of basis functions for calculating the Koopman operators. These approaches, however, assume a fixed dimensional state space; they are therefore not applicable to scenarios with a variable number of objects. In this paper, we propose to learn compositional Koopman operators, using graph neural networks to encode the state into object-centric embeddings and using a block-wise linear transition matrix to regularize the shared structure across objects. The learned dynamics can quickly adapt to new environments of unknown physical parameters and produce control signals to achieve a specified goal. Our experiments on manipulating ropes and controlling soft robots show that the proposed method has better efficiency and generalization ability than existing baselines.",
                "authors": "Yunzhu Li, Hao He, Jiajun Wu, D. Katabi, A. Torralba",
                "citations": 109
            },
            {
                "title": "The Mathematical Theories of Diffusion: Nonlinear and Fractional Diffusion",
                "abstract": null,
                "authors": "J. V'azquez",
                "citations": 131
            },
            {
                "title": "A Class of Variational-Hemivariational Inequalities in Reflexive Banach Spaces",
                "abstract": null,
                "authors": "S. Migórski, A. Ochal, M. Sofonea",
                "citations": 134
            },
            {
                "title": "Modeling and optimization of the injection-molding process: a review",
                "abstract": "Contract grant sponsor: This work is funded by FEDER funds through the COMPETE 2020 Programme and National Funds through FCT - Portuguese Foundation for Science and Technology under the project UID/CTM/50025/2013.",
                "authors": "C. Fernandes, A. Pontes, J. Viana, A. Gaspar-Cunha",
                "citations": 133
            },
            {
                "title": "Modeling of Deformable Objects for Robotic Manipulation: A Tutorial and Review",
                "abstract": "Manipulation of deformable objects has given rise to an important set of open problems in the field of robotics. Application areas include robotic surgery, household robotics, manufacturing, logistics, and agriculture, to name a few. Related research problems span modeling and estimation of an object's shape, estimation of an object's material properties, such as elasticity and plasticity, object tracking and state estimation during manipulation, and manipulation planning and control. In this survey article, we start by providing a tutorial on foundational aspects of models of shape and shape dynamics. We then use this as the basis for a review of existing work on learning and estimation of these models and on motion planning and control to achieve desired deformations. We also discuss potential future lines of work.",
                "authors": "V. Arriola-Rios, Püren Güler, F. Ficuciello, D. Kragic, B. Siciliano, J. Wyatt",
                "citations": 91
            },
            {
                "title": "Global and Local Path Planning Study in a ROS-Based Research Platform for Autonomous Vehicles",
                "abstract": "The aim of this work is to integrate and analyze the performance of a path planning method based on Time Elastic Bands (TEB) in real research platform based on Ackermann model. Moreover, it will be proved that all modules related to the navigation can coexist and work together to achieve the goal point without any collision. The study is done by analyzing the trajectory generated from global and local planners. The software prototyping tool is Robot Operating System (ROS) from Open Source Robotics Foundation and the research platform is the iCab (Intelligent Campus Automobile) from University Carlos III. This work has been validated from a test inside the campus where the iCab has performed the navigation between the starting point and the goal point without any collision. During the experiment, we proved the low sensitivity of the TEB method to variations of the vehicle model configuration and constraints.",
                "authors": "Pablo Marín-Plaza, A. Hussein, David Martín, A. D. L. Escalera",
                "citations": 125
            },
            {
                "title": "Nonlinear vibration of functionally graded graphene-reinforced composite laminated beams resting on elastic foundations in thermal environments",
                "abstract": null,
                "authors": "Hui‐Shen Shen, F. Lin, Yang Xiang",
                "citations": 128
            },
            {
                "title": "Complex Systems Research in Educational Psychology: Aligning Theory and Method",
                "abstract": "The purpose of this work is to provide an overview of complex systems research for educational psychologists. We outline a philosophically and theoretically sourced definition of complex systems research organized around complex, dynamic, and emergent ontological characteristics that is useful and appropriate for educational psychology. A complex systems approach is positioned as a means to align underexplored elements of existing theory with appropriate interaction dominant theoretical models, research methods, and equation-based analytic techniques. We conclude with a brief discussion of several foundational topics for complex systems research in educational psychology.",
                "authors": "J. Hilpert, G. Marchand",
                "citations": 132
            },
            {
                "title": "Characterisation of geotechnical model uncertainty",
                "abstract": "ABSTRACT The calculated response from a numerical model will deviate from the measured one given the presence of modelling idealizations and real world construction effects. This deviation can be directly captured by a ratio between the measured and the calculated quantity. The ratio is also called a model factor in many design guides. The probabilistic distribution of the model factor is arguably the most common and simplest complete representation of model uncertainty. The characterisation of model uncertainty is identified as one of the critical elements in a geotechnical reliability-based design process in Annex D of ISO 2394:2015 “General Principles on Reliability of Structures”. This Spotlight paper reviews the databases for various geo-structures and determines their associated model statistics. Foundation load test databases are the most prevalent. A recent effort to compile a large generic database (PILE/2739) that contains 2739 field load tests conducted on various piles and installed in different soils and countries, is highlighted. This systematic compilation of load test data is part of a broader research agenda to digitalise foundation design for “precision construction”, which is targeted at characterising “site-specific” model factors and soil parameters based on both site-specific and generic data for further customisation of design to a particular site. The mean and COV of the model factor for a range of geo-structures, geomaterials, and limit states (both ultimate and serviceability) are summarized in a form suitable for adoption in design and codes of practice. Based on this summary, it is proposed that a model factor for a design model can be classified as: (1) moderately conservative (1 ≤ mean < 2), (2) highly conservative (2 ≤ mean < 3), or (3) very highly conservative (mean ≥ 3). The model uncertainty can be as: (1) low dispersion (COV < 0.3), (2) medium dispersion (0.3 ≤ COV < 0.6), (3) high dispersion (0.6 ≤ COV < 0.9), and (4) very high dispersion (COV ≥ 0.9). This summary represents the most extensive and significant update of Table 3.7.5.1 in the 2006 JCSS Probabilistic Model Code.",
                "authors": "K. Phoon, Chong Tang",
                "citations": 99
            },
            {
                "title": "Physical stability response of a SLGS resting on viscoelastic medium using nonlocal integral first-order theory",
                "abstract": "The buckling properties of a single-layered graphene sheet (SLGS) are examined using nonlocal integral first shear deformation theory (FSDT) by incorporating the influence of visco-Pasternaks medium. This model contains only four variables, which is even less than the conventional FSDT. The visco-Pasternak's medium is introduced by considering the damping influence to the conventional foundation model which modeled by the linear Winkler's coefficient and Pasternak's (shear) foundation coefficient. The nanoplate under consideration is subjected to compressive in- plane edge loads per unit length. The impacts of many parameters such as scale parameter, aspect ratio, the visco-Pasternak' s coefficients, damping parameter, and mode numbers on the stability investigation of the SLGSs are examined in detail. The obtained results are compared with the corresponding available in the literature.",
                "authors": "A. Rouabhia, ABDELBAKI CHIKH, A. Chikh, A. A. Bousahla, F. Bourada, H. Heireche, A. Tounsi, K. H. Benrahou, A. Tounsi, M. M. Al-Zahrani",
                "citations": 69
            },
            {
                "title": "Robust, linear correlations between growth rates and β-lactam–mediated lysis rates",
                "abstract": "Significance How fast bacteria grow influences the efficacy of β-lactams, one of the most commonly used classes of antibiotics. However, the quantitative nature of this correlation is not well established. With precise measurements and analyses enabled by experimental automation, we found a robust relationship between growth and lysis rates that is generally applicable to diverse pairs of β-lactams and bacteria. That is, the growth rate of population serves as a reliable predictor for the lysis rate in response to a β-lactam. This quantitative correlation lays the foundation for predicting bacterial population dynamics during β-lactam treatments. This predictive capability is critical for designing effective antibiotic dosing protocols, in addressing the rising antibiotic resistance crisis. It is widely acknowledged that faster-growing bacteria are killed faster by β-lactam antibiotics. This notion serves as the foundation for the concept of bacterial persistence: dormant bacterial cells that do not grow are phenotypically tolerant against β-lactam treatment. Such correlation has often been invoked in the mathematical modeling of bacterial responses to antibiotics. Due to the lack of thorough quantification, however, it is unclear whether and to what extent the bacterial growth rate can predict the lysis rate upon β-lactam treatment under diverse conditions. Enabled by experimental automation, here we measured >1,000 growth/killing curves for eight combinations of antibiotics and bacterial species and strains, including clinical isolates of bacterial pathogens. We found that the lysis rate of a bacterial population linearly depends on the instantaneous growth rate of the population, regardless of how the latter is modulated. We further demonstrate that this predictive power at the population level can be explained by accounting for bacterial responses to the antibiotic treatment by single cells. This linear dependence of the lysis rate on the growth rate represents a dynamic signature associated with each bacterium–antibiotic pair and serves as the quantitative foundation for designing combination antibiotic therapy and predicting the population-structure change in a population with mixed phenotypes.",
                "authors": "A. J. Lee, Shangying Wang, Hannah R. Meredith, Bihan Zhuang, Zhuojun Dai, L. You",
                "citations": 113
            },
            {
                "title": "Giant kelp, Macrocystis pyrifera, increases faunal diversity through physical engineering",
                "abstract": "Foundation species define the ecosystems they live in, but ecologists have often characterized dominant plants as foundational without supporting evidence. Giant kelp has long been considered a marine foundation species due to its complex structure and high productivity; however, there is little quantitative evidence to evaluate this. Here, we apply structural equation modelling to a 15-year time series of reef community data to evaluate how giant kelp affects the reef community. Although species richness was positively associated with giant kelp biomass, most direct paths did not involve giant kelp. Instead, the foundational qualities of giant kelp were driven mostly by indirect effects attributed to its dominant physical structure and associated engineering influence on the ecosystem, rather than by its use as food by invertebrates and fishes. Giant kelp structure has indirect effects because it shades out understorey algae that compete with sessile invertebrates. When released from competition, sessile species in turn increase the diversity of mobile predators. Sea urchin grazing effects could have been misinterpreted as kelp effects, because sea urchins can overgraze giant kelp, understorey algae and sessile invertebrates alike. Our results confirm the high diversity and biomass associated with kelp forests, but highlight how species interactions and habitat attributes can be misconstrued as direct consequences of a foundation species like giant kelp.",
                "authors": "Robert J. Miller, K. Lafferty, T. Lamy, L. Kui, A. Rassweiler, D. Reed",
                "citations": 115
            },
            {
                "title": "Quantum Simulation Of The Quantum Rabi Model In A Trapped Ion",
                "abstract": "We thank Xiao Yuan, Xiongfeng Ma, Hyunchul Nha, Jiyong Park, Jaehak Lee, and M. S. Kim for useful discussions on the entanglement verification of the ground state. This work was supported by the National Key Research and Development Program of China under Grants No. 2016YFA0301900 and No. 2016YFA0301901 and the National Natural Science Foundation of China Grants No. 11374178, No. 11574002, and No. 11504197, MINECO/FEDER FIS2015-69983-P, Ramon y Cajal Grant No. RYC-2012-11391, and Basque Government IT986-16.",
                "authors": "Dingshun Lv, S. An, Zhen-Ya Liu, Jingning Zhang, J. S. Pedernales, L. Lamata, E. Solano, Kihwan Kim",
                "citations": 113
            },
            {
                "title": "A STUDY OF STUDENTS’ SENSE OF LEARNING COMMUNITY IN ONLINE ENVIRONMENTS",
                "abstract": "This paper looks first at some of the often unspoken epistemological, philosophical, and theoretical assumptions that are foundational to student-centered, interactive online pedagogical models. It is argued that these foundational assumptions point to the importance of learning community in the effectiveness of online learning environments. Next, a recent study of 2314 online students across thirty-two college campuses is presented. This study reports on learners’ sense of community and it is concluded through factor and regression analysis that elements of the Community of Inquiry model —specifically learners’ recognition of effective “directed facilitation” and effective instructional design and organization on the part of their instructor contributes to their sense of shared purpose, trust, connectedness, and learning—core elements of a community of learners. Gender also appears to play a small role in students’ sense of learning community with female students reporting higher levels than their male classmates. Implications for online learning environments design are discussed.",
                "authors": "P. Shea",
                "citations": 109
            },
            {
                "title": "Modelling for policy: The five principles of the Neglected Tropical Diseases Modelling Consortium",
                "abstract": "1 Neglected Tropical Diseases, Bill & Melinda Gates Foundation, Seattle, Washington, United States of America, 2 Blue Well 8, Seattle, Washington, United States of America, 3 MRC Centre for Global Infectious Disease Analysis and London Centre for Neglected Tropical Disease Research, Department of Infectious Disease Epidemiology, Imperial College London, London, United Kingdom, 4 Francis I. Proctor Foundation for Research in Ophthalmology, Department of Epidemiology and Biostatistics, and Department of Ophthalmology, University of California, San Francisco, United States of America, 5 Department of Public Health, Erasmus MC, University Medical Center Rotterdam, Rotterdam, the Netherlands, 6 London Centre for Neglected Tropical Disease Research, Department of Pathobiology and Population Sciences, Royal Veterinary College, Hatfield, Hertfordshire, United Kingdom, 7 London Centre for Neglected Tropical Disease Research and Department of Infectious Disease Epidemiology, Imperial College London, London, United Kingdom",
                "authors": "Matthew R Behrend, M. Basáñez, Jonathan I D Hamley, T. Porco, W. Stolk, M. Walker, S. D. de Vlas",
                "citations": 64
            },
            {
                "title": "Visualizing the secondary and tertiary architectural domains of lncRNA RepA.",
                "abstract": null,
                "authors": "Fei Liu, Srinivas Somarowthu, A. Pyle",
                "citations": 108
            },
            {
                "title": "Vehicular communication channel measurement, modelling, and application for beyond 5G and 6G",
                "abstract": ": As vehicular communications for beyond fifth-generation (B5G) and sixth-generation (6G) is picking up interests from academia and industry recently, more and more research and development have been devoted towards the establishment of vehicular communications for B5G and 6G that is capable of supporting the ever more intelligent transportation systems. One key facilitating the design and improvement of vehicular communications for B5G and 6G is channel modelling, which is widely regarded as the foundation of all communication and networking systems. In this paper, the authors focus on the research and analysis of B5G and 6G vehicular channel measurements and modelling. By emphasising the new requirements and challenges that the emerging B5G and 6G technologies and frequency bands bring to vehicular communication channel measurements and modelling, they present an overview of the existing work and identify the limitations therein, and provide guidelines on the channel model development and adoption for various system development and verification objectives. Finally, future challenges related to vehicular channel measurements, modelling, and their application for B5G and 6G are addressed.",
                "authors": "Xiang Cheng, Ziwei Huang, Shanzhi Chen",
                "citations": 60
            },
            {
                "title": "The Zebrafish Information Network: new support for non-coding genes, richer Gene Ontology annotations and the Alliance of Genome Resources",
                "abstract": "Abstract The Zebrafish Information Network (ZFIN) (https://zfin.org/) is the database for the model organism, zebrafish (Danio rerio). ZFIN expertly curates, organizes and provides a wide array of zebrafish genetic and genomic data, including genes, alleles, transgenic lines, gene expression, gene function, mutant phenotypes, orthology, human disease models, nomenclature and reagents. New features at ZFIN include increased support for genomic regions and for non-coding genes, and support for more expressive Gene Ontology annotations. ZFIN has recently taken over maintenance of the zebrafish reference genome sequence as part of the Genome Reference Consortium. ZFIN is also a founding member of the Alliance of Genome Resources, a collaboration of six model organism databases (MODs) and the Gene Ontology Consortium (GO). The recently launched Alliance portal (https://alliancegenome.org) provides a unified, comparative view of MOD, GO, and human data, and facilitates foundational and translational biomedical research.",
                "authors": "Leyla Ruzicka, D. Howe, S. Ramachandran, S. Toro, Ceri E. Van Slyke, Yvonne M. Bradford, Anne E. Eagle, D. Fashena, K. Frazer, Patrick Kalita, Prita Mani, Ryan Martin, S. Moxon, Holly Paddock, Christian Pich, Kevin Schaper, Xiang Shao, A. Singer, M. Westerfield",
                "citations": 125
            },
            {
                "title": "Layers of Bias: A Unified Approach for Understanding Problems With Risk Assessment",
                "abstract": "Scholars in several fields, including quantitative methodologists, legal scholars, and theoretically oriented criminologists, have launched robust debates about the fairness of quantitative risk assessment. As the Supreme Court considers addressing constitutional questions on the issue, we propose a framework for understanding the relationships among these debates: layers of bias. In the top layer, we identify challenges to fairness within the risk-assessment models themselves. We explain types of statistical fairness and the tradeoffs between them. The second layer covers biases embedded in data. Using data from a racially biased criminal justice system can lead to unmeasurable biases in both risk scores and outcome measures. The final layer engages conceptual problems with risk models: Is it fair to make criminal justice decisions about individuals based on groups? We show that each layer depends on the layers below it: Without assurances about the foundational layers, the fairness of the top layers is irrelevant.",
                "authors": "Laurel Eckhouse, K. Lum, Cynthia Conti-Cook, J. Ciccolini",
                "citations": 122
            },
            {
                "title": "Free vibration analysis of embedded nanosize FG plates using a new nonlocal trigonometric shear deformation theory",
                "abstract": "In this work, free vibration analysis of size-dependent functionally graded (FG) nanoplates resting on two-parameter elastic foundation is investigated based on a novel nonlocal refined trigonometric shear deformation theory for the first time. This theory includes undetermined integral variables and contains only four unknowns, with is even less than the conventional first shear deformation theory (FSDT). Mori–Tanaka model is employed to describe gradually distribution of material properties along the plate thickness. Size-dependency of nanosize FG plate is captured via the nonlocal elasticity theory of Eringen. By implementing Hamilton\\'s principle the equations of motion are obtained for a refined four-variable shear deformation plate theory and then solved analytically. To show the accuracy of the present theory, our research results in specific cases are compared with available results in the literature and a good agreement will be demonstrated. Finally, the influence of various parameters such as nonlocal parameter, power law indexes, elastic foundation parameters, aspect ratio, and the thickness ratio on the non-dimensional frequency of rectangular FG nanoscale plates are presented and discussed in detail.",
                "authors": "A. Besseghier, Mohammed Sid Ahmed Houari, A. Tounsi, S. R. Mahmoud",
                "citations": 115
            },
            {
                "title": "Flexural Wave Propagation Analysis of Embedded S-FGM Nanobeams Under Longitudinal Magnetic Field Based on Nonlocal Strain Gradient Theory",
                "abstract": null,
                "authors": "F. Ebrahimi, M. Barati",
                "citations": 116
            },
            {
                "title": "Equation of state for dense nucleonic matter from metamodeling. I. Foundational aspects",
                "abstract": "A metamodeling for the nucleonic equation of state (EOS), inspired from a Taylor expansion around the saturation density of symmetric nuclear matter, is proposed and parameterized in terms of the empirical parameters. The present knowledge of nuclear empirical parameters is first reviewed in order to estimate their average values and associated uncertainties, and thus defining the parameter space of the metamodeling. They are divided into isoscalar and isovector type, and ordered according to their power in the density expansion. The goodness of the metamodeling is analyzed against the predictions of the original models. In addition, since no correlation among the empirical parameters is assumed a priori, all arbitrary density dependences can be explored, which might not be accessible in existing functionals. Spurious correlations due to the assumed functional form are also removed. This meta-EOS allows direct relations between the uncertainties on the empirical parameters and the density dependence of the nuclear equation of state and its derivatives, and the mapping between the two can be done with standard Bayesian techniques. A sensitivity analysis shows that the more influential empirical parameters are the isovector parameters $L_{sym}$ and $K_{sym}$, and that laboratory constraints at super-saturation densities are essential to reduce the present uncertainties. The present metamodeling for the EOS for nuclear matter is proposed for further applications in neutron stars and supernova matter.",
                "authors": "J. Margueron, R. Casali, F. Gulminelli",
                "citations": 119
            },
            {
                "title": "Prior Distributions for Objective Bayesian Analysis",
                "abstract": "We provide a review of prior distributions for objective Bayesian analysis. We start by examining some foundational issues and then organize our exposition into priors for: i) estimation or prediction; ii) model selection; iii) highdimensional models. With regard to i), we present some basic notions, and then move to more recent contributions on discrete parameter space, hierarchical models, nonparametric models, and penalizing complexity priors. Point ii) is the focus of this paper: it discusses principles for objective Bayesian model comparison, and singles out some major concepts for building priors, which are subsequently illustrated in some detail for the classic problem of variable selection in normal linear models. We also present some recent contributions in the area of objective priors on model space. With regard to point iii) we only provide a short summary of some default priors for high-dimensional models, a rapidly growing area of research.",
                "authors": "G. Consonni, D. Fouskakis, B. Liseo, I. Ntzoufras",
                "citations": 105
            },
            {
                "title": "Performance of geotechnical seismic isolation system using rubber‐soil mixtures in centrifuge testing",
                "abstract": "Geotechnical seismic isolation (GSI) system involves the dynamic interaction between structure and low‐modulus foundation material, such as rubber‐soil mixtures (RSM). Whilst numerical studies have been carried out to demonstrate the potential benefits of GSI‐RSM system, experimental research is indispensable for confirming its isolation mechanism and effectiveness in reducing structural demand. In this regard, centrifuge modelling with an earthquake shaker under an acceleration field of 50 g adopted in this study can mimic the actual nonlinear dynamic response characteristics of RSM and subsoil in a coupled soil‐foundation‐structure system. This is the first time the performance of GSI‐RSM system was examined in a geotechnical centrifuge. It was found that an average of 40‐50% reduction of structural demand can be achieved. The increase in both the horizontal and rotation responses of the foundation was also evidenced. The unique augmented rocking mechanism with reversible foundation rotation was highlighted.",
                "authors": "H. Tsang, Duc-Phu Tran, W. Hung, K. Pitilakis, E. Gad",
                "citations": 55
            },
            {
                "title": "Urban Human Mobility: Data-Driven Modeling and Prediction",
                "abstract": "Human mobility is a multidisciplinary field of physics and computer science and has drawn a lot of attentions in recent years. Some representative models and prediction approaches have been proposed for modeling and predicting human mobility. However, multi-source heterogeneous data from handheld terminals, GPS, and social media, provides a new driving force for exploring urban human mobility patterns from a quantitative and microscopic perspective. The studies of human mobility modeling and prediction play a vital role in a series of applications such as urban planning, epidemic control, location-based services, and intelligent transportation management. In this survey, we review human mobility models based on a human-centric angle in a datadriven context. Specifically, we characterize human mobility patterns from individual, collective, and hybrid levels. Meanwhile, we survey human mobility prediction methods from four aspects and then describe recent development respectively. Finally, we discuss some open issues that provide a helpful reference for researchers' future direction. This review not only lays a solid foundation for beginners who want to acquire a quick understanding of human mobility but also provides helpful information for researchers on how to develop a unified human mobility model.",
                "authors": "Jinzhong Wang, Xiangjie Kong, Feng Xia, Lijun Sun",
                "citations": 83
            },
            {
                "title": "Software tools for business model innovation: current state and future challenges",
                "abstract": null,
                "authors": "Daniel Szopinski, Thorsten Schoormann, Thomas John, R. Knackstedt, Dennis Kundisch",
                "citations": 71
            },
            {
                "title": "Microscopic origin of the Drude-Smith model",
                "abstract": "The Drude-Smith model has been used extensively in fitting the THz conductivities of nanomaterials with carrier confinement on the mesoscopic scale. Here, we show that the conventional \"backscattering\" explanation for the suppression of low-frequency conductivities in the Drude-Smith model is not consistent with a confined Drude gas of classical noninteracting electrons and we derive a modified Drude-Smith conductivity formula based on a diffusive restoring current. We perform Monte Carlo simulations of a model system and show that the modifiedDrude-Smith model reproduces the extracted conductivitieswithout free parameters. This alternate route to the Drude-Smith model provides the popular formula with a more solid physical foundation and well-defined fit parameters.",
                "authors": "T. Cocker, D. Baillie, M. Buruma, L. Titova, R. Sydora, F. Marsiglio, F. Hegmann",
                "citations": 100
            },
            {
                "title": "Computational Modeling of Heterogeneity of Stress, Charge, and Cyclic Damage in Composite Electrodes of Li-Ion Batteries",
                "abstract": "P. L. is grateful for the China Scholarship Council for a visiting scholarship at Purdue University. R. X. and K. Z. acknowledge the support by the National Science Foundation through the grants DMR-1832707 and CBET-1603866. F. L. acknowledges the support from the National Science Foundation under grant No. DMR-1832613. Use of the Stanford Synchrotron Radiation Lightsource, SLAC National Accelerator Laboratory, is supported by the U.S. Department of Energy, Office of Science, Office of Basic Energy Sciences under Contract No. DE-AC02-76SF00515.",
                "authors": "Pengfei Liu, Rong Xu, Yijin Liu, Feng Lin, K. Zhao",
                "citations": 50
            },
            {
                "title": "Piecing the learning analytics puzzle: a consolidated model of a field of research and practice",
                "abstract": "ABSTRACT The field of learning analytics was founded with the goal to harness vast amounts of data about learning collected by the extensive use of technology. After the early formation, the field has now entered the next phase of maturation with a growing community who has an evident impact on research, practice, policy, and decision-making. Although learning analytics is a bricolage field borrowing from many related other disciplines, there is still no systematised model that shows how these different disciplines are pieced together. Existing models and frameworks of learning analytics are valuable in identifying elements and processes of learning analytics, but they insufficiently elaborate on the links with foundational disciplines. With this in mind, this paper proposes a consolidated model of the field of research and practice that is composed of three mutually connected dimensions – theory, design, and data science. The paper defines why and how each of the three dimensions along with their mutual relations is critical for research and practice of learning analytics. Finally, the paper stresses the importance of multi-perspective approaches to learning analytics based on its three core dimensions for a healthy development of the field and a sustainable impact on research and practice.",
                "authors": "D. Gašević, V. Kovanović, Srećko Joksimović",
                "citations": 106
            },
            {
                "title": "Dynamical stability of embedded spinning axially graded micro and nanotubes conveying fluid",
                "abstract": "ABSTRACT In this study, the dynamical stability of spinning axially graded micro and nanotubes transporting fluid rested on the Kerr foundation is analyzed. A detailed parametric study is performed to clarify the effect of various factors such as axial material gradation and size-dependent parameters on the divergence and flutter instability thresholds of the system. To model the micro and nanofluidic systems, modified couple stress theory (MCST) and nonlocal strain gradient theory (NSGT) are implemented, respectively. The backward and forward vibrational frequencies, as well as critical divergence spin and fluid velocities of the system, are obtained. It is concluded that the stability evolution of spinning micro and nanostructures containing fluid can be altered by fine-adjustment of axial material gradation. Besides, it is found that the enhancement of elastic modulus gradient, strain gradient, material length scale, and foundation parameters improve the dynamical stability of the small-scale structures.",
                "authors": "F. Zheng, Yundan Lu, A. Ebrahimi-Mamaghani",
                "citations": 47
            },
            {
                "title": "Strange metallicity in the doped Hubbard model",
                "abstract": "Looking for a strange metal In many materials, charge carriers are well described as noninteracting quasiparticles. However, in materials with strong correlations, this approximation can break down, leading to anomalous transport properties at high temperatures. Huang et al. used quantum Monte Carlo calculations to look for this so-called strange metal phase in the simplest two-dimensional model of interacting electrons, the Hubbard model. They found that the calculated resistivity had a linear temperature dependence when hole doping was introduced, as expected in the strange metal phase. This observation provides confidence that simplified models can be used to describe and understand the behavior of real materials, such as cuprate high-temperature superconductors. Science, this issue p. 987 Quantum Monte Carlo calculations indicate the presence of anomalous transport in the normal state of the 2D Hubbard model. Strange or bad metallic transport, defined by incompatibility with the conventional quasiparticle picture, is a theme common to many strongly correlated materials, including high-temperature superconductors. The Hubbard model represents a minimal starting point for modeling strongly correlated systems. Here we demonstrate strange metallic transport in the doped two-dimensional Hubbard model using determinantal quantum Monte Carlo calculations. Over a wide range of doping, we observe resistivities exceeding the Mott-Ioffe-Regel limit with linear temperature dependence. The temperatures of our calculations extend to as low as 1/40 of the noninteracting bandwidth, placing our findings in the degenerate regime relevant to experimental observations of strange metallicity. Our results provide a foundation for connecting theories of strange metals to models of strongly correlated materials.",
                "authors": "E. Huang, Ryan Sheppard, B. Moritz, T. Devereaux",
                "citations": 114
            },
            {
                "title": "Exploring the factors affecting learners’ continuance intention of MOOCs for online collaborative learning: An extended ECM perspective",
                "abstract": "The purpose of this paper was to investigate what factors influence learners’ continuance intention in massive open online courses (MOOCs) for online collaborative learning. An extended expectation confirmation model (ECM) was adopted as the theoretical foundation. A total of 435 valid samples were collected in mainland China and structural equation model (SEM) approach was adopted. The descriptive statistics show that platforms from abroad, such as Coursera and Khan, are more popular than native ones in mainland China. The empirical results show that the effects of three ECM factors (satisfaction with prior learning experience, confirmation with prior learning experience, and perceived usefulness) are significant. Different factors have different predicting power. Knowledge outcome is the first powerful indicator of learners’ continuance intention of MOOCs, followed by social influence, learners’ satisfaction with prior learning experience, and performance proficiency. The effects of knowledge outcome, performance proficiency, and social influence are significant, showing the success of extended ECM.",
                "authors": "Junjie Zhou",
                "citations": 91
            },
            {
                "title": "Toward a Framework for Islamic Psychology and Psychotherapy: An Islamic Model of the Soul",
                "abstract": null,
                "authors": "Abdallah Rothman, A. Coyle",
                "citations": 91
            },
            {
                "title": "A Parallel Surrogate Model Assisted Evolutionary Algorithm for Electromagnetic Design Optimization",
                "abstract": "Optimization efficiency is a major challenge for electromagnetic (EM) device, circuit, and machine design. Although both surrogate model-assisted evolutionary algorithms (SAEAs) and parallel computing are playing important roles in addressing this challenge, there is little research that investigates their integration to benefit from both techniques. In this paper, a new method, called parallel SAEA for electromagnetic design (PSAED), is proposed. A state-of-the-art SAEA framework, surrogate model-aware evolutionary search, is used as the foundation of PSAED. Considering the landscape characteristics of EM design problems, three differential evolution mutation operators are selected and organized in a particular way. A new SAEA framework is then proposed to make use of the selected mutation operators in a parallel computing environment. PSAED is tested by a micromirror and a dielectric resonator antenna as well as four mathematical benchmark problems of various complexity. Comparisons with state-of-the-art methods verify the advantages of PSAED in terms of efficiency and optimization capacity.",
                "authors": "Mobayode O. Akinsolu, Bo Liu, V. Grout, P. Lazaridis, M. E. Mognaschi, P. Barba",
                "citations": 77
            },
            {
                "title": "Following Policy: A Network Ethnography of the UK Character Education Policy Community",
                "abstract": "Over the past 15 years, there has been a growing interest and investment in ‘character’ education across the UK political landscape. Alongside the activities of central government, character education has been promoted by a range of non-government actors in the UK and beyond, including philanthropic foundations, think tanks, education entrepreneurs, and academics. It is the presence of these actors and their relationship to, and influence on, UK government policy that we examine in this article. Investigating character education from a perspective of policy formation and influence, we trace the key policy actors who have contributed to the adoption of character education in the UK, and their international connections, identifying the resources, activities and relationships through which they have achieved policy influence. A central and original contribution of this article is in identifying the financial and ideological influence of US Christian neoconservative philanthropic foundation the John Templeton Foundation on social science research and policy in the UK. Our analysis identifies academics in the UK and US who, through considerable John Templeton Foundation funding, have provided an evidence base that authorises character education as a policy solution. We also locate ‘policy entrepreneurs’ as key nodal actors, whose social capital and elite membership helps to lubricate network relations and facilitate policy influence. Finally, we consider the motivations and vested interests of policy actors, including the John Templeton Foundation’s particular model of philanthropy, and conclude that the character education agenda is underpinned by a set of ideas that promote a free-market, individualistic and socially conservative worldview.",
                "authors": "Kim Allen, A. Bull",
                "citations": 80
            },
            {
                "title": "Surface effects on the vibration behavior of flexoelectric nanobeams based on nonlocal elasticity theory",
                "abstract": null,
                "authors": "F. Ebrahimi, Mohammad Reza Barati",
                "citations": 86
            },
            {
                "title": "A model of listening engagement (MoLE)",
                "abstract": "Hearing impairment in older adulthood puts people at risk of communication difficulties, disengagement from listening, and social withdrawal. Here, we develop a model of listening engagement (MoLE) that provides a conceptual foundation to understand when people engage in listening and why some people disengage. We use the term \"listening engagement\" to describe the recruitment of executive and other cognitive resources in the service of a valued communication goal. Listening engagement, listening motivation, and listening experiences are closely interconnected: motivation and other factors determine the degree to which resources are recruited during listening, which in turn influences subjective listening experiences such as enjoyment, effort, frustration, and boredom. We anticipate that this model will help researchers assess more accurately whether a person with hearing difficulties is at risk of disengagement and social withdrawal. It is further useful to more comprehensively characterize a person's listening experiences in laboratory settings when rich, engaging stimulus materials, such as spoken stories, are used. We hope this model will allow new questions in applied and basic hearing science and auditory cognitive neuroscience to be asked and answered.",
                "authors": "Björn Herrmann, I. Johnsrude",
                "citations": 67
            },
            {
                "title": "Chemomechanical modeling of lithiation-induced failure in high-volume-change electrode materials for lithium ion batteries",
                "abstract": null,
                "authors": "Sulin Zhang",
                "citations": 100
            },
            {
                "title": "The research of regression model in machine learning field",
                "abstract": "The paper herein will analyze the sale of iced products affected by variation of temperature. Firstly, we will collect the data of the forecast temperature last year and the sale of iced products and then conduct data compilation and cleansing. Finally, we will set up the mathematical regression analysis model based on the cleansed data by means of data mining theory. Regression analysis refers to the method of studying the relationship between independent variable and dependent variable. Linear regression model that corresponds to the practical situation is proposed in the paper, which is to set up simple linear regression model based on practical problem and then to implement the following with the help of the latest and most popular Python3.6. Python3.6 boasts the features of pure object-oriented, platform independence and concise and elegant language. So we will call the corresponding library function to predict the sale of iced products according to the variation of temperature, which will provide the foundation for the company to adjust its production each month, or even each week and each day. As a result, the situation of overproduction can be avoided. Moreover, the other situation as the profit will be affected by the lack of production since the rise of temperature will also be avoided. So the regression model also has reference value for the other fields of marketing.",
                "authors": "Shen Rong, Bao-wen Zhang",
                "citations": 81
            },
            {
                "title": "A Standard Indoor Spatial Data Model - OGC IndoorGML and Implementation Approaches",
                "abstract": "With the recent progress in indoor spatial data modeling, indoor mapping and indoor positioning technologies, several spatial information services for indoor spaces have been provided like for outdoor spaces. In order to support interoperability between indoor spatial information services, IndoorGML was published by OGC (Open Geospatial Consortium) as a standard data model and XML-based exchange format. While the previous standards, such as IFC (Industrial Foundation Classes) and CityGML covering also indoor space, aim at feature modeling, the goal of IndoorGML is to establish a standard basis for the indoor space model. As IndoorGML defines a minimum data model for indoor space, more efforts are required to discover its potential aspects, which are not explicitly explained in the standard document. In this paper, we investigate the implications and potential aspects of IndoorGML and its basic concept of the cellular space model and discuss the implementation issues of IndoorGML for several purposes. In particular, we discuss the issues on cell determination, subspacing and the hierarchical structure of indoor space from the IndoorGML viewpoint. Additionally, we also focus on two important issues: computation of indoor distance and the implementation of indoor context-awareness services based on IndoorGML. We expect that this paper will serve as a technical document for better understanding of IndoorGML throughout these discussions.",
                "authors": "Hae-Kyong Kang, Ki-Joune Li",
                "citations": 91
            },
            {
                "title": "Tracing the Signature Dynamics of Language Teacher Immunity: A Retrodictive Qualitative Modeling Study",
                "abstract": "This article describes a validation study using Retrodictive Qualitative Modeling, a framework for conducting research from a dynamic and situated perspective, to establish an empirical foundation for a new phenomenological construct—language teacher immunity. Focus groups (N = 44) conducted with second language (L2) practitioners and teacher educators and a cluster analysis of questionnaire data with a larger sample (N = 293) of K–12 language teachers were used to identify and corroborate typical archetypes across the spectrum of language teacher immunity outcomes. Serial in‐depth interviews were then conducted with representative respondents from each archetype (N = 18) to trace developmental trajectories and investigate how these profiles manifested phenomenologically in teachers’ motivated thought and instructional practices. Results indicate that teacher immunity is associated with practitioners’ psychological, emotional, and cognitive functioning in the social setting of the L2 classroom. These findings contribute to the field's understanding of how language teachers sustain their adaptivity, openness to change, psychological well‐being, and their sense of purpose and investment in students’ learning. Thus, teacher immunity has the potential to bridge individual and situative concerns in second language teacher education and the psychology of language teaching and learning.",
                "authors": "Phil Hiver",
                "citations": 82
            },
            {
                "title": "Towards emotion recognition for virtual environments: an evaluation of eeg features on benchmark dataset",
                "abstract": null,
                "authors": "M. Menezes, A. Samara, L. Galway, A. Sant'Anna, A. Verikas, F. Alonso-Fernandez, Hui Wang, R. Bond",
                "citations": 80
            },
            {
                "title": "Wave propagation in double-walled carbon nanotube conveying fluid considering slip boundary condition and shell model based on nonlocal strain gradient theory",
                "abstract": null,
                "authors": "H. Zeighampour, Y. Beni, I. Karimipour",
                "citations": 85
            },
            {
                "title": "Grappling with racism as foundational practice of science teaching",
                "abstract": "Chicago, IL Correspondence Manali J. Sheth, PO Box 8464, Chicago, IL 60608 Email: mjsheth@wisc.edu Abstract While current science teacher education frameworks designed to support high-quality teaching have the potential to promote equitable science learning, they do not substantively engage with how racism organizes science teaching and learning. In this critical qualitative inquiry grounded in critical race theory and sociopolitical perspectives on teaching and learning, I analyzed the contradictions that emerged in science teaching practices that were both intended to support Student of Color science learning and engaged science-specific colorblind ideologies. The critical race theory analysis demonstrated how science teaching practices such as connecting to students’ experiences, creating interests in science, representing scientists as role models, and scaffolding doing science maintain unequal racialized power relations between students and science when historical and contemporary legacies of racism are not directly confronted. I also propose a science teaching practice of “grappling with racism” as a possible transformative solution to disrupt racism in and through science teaching.",
                "authors": "Manali J. Sheth",
                "citations": 77
            },
            {
                "title": "Wave propagation in double-walled carbon nanotube conveying fluid considering slip boundary condition and shell model based on nonlocal strain gradient theory",
                "abstract": null,
                "authors": "H. Zeighampour, Y. Beni, I. Karimipour",
                "citations": 85
            },
            {
                "title": "Grappling with racism as foundational practice of science teaching",
                "abstract": "Chicago, IL Correspondence Manali J. Sheth, PO Box 8464, Chicago, IL 60608 Email: mjsheth@wisc.edu Abstract While current science teacher education frameworks designed to support high-quality teaching have the potential to promote equitable science learning, they do not substantively engage with how racism organizes science teaching and learning. In this critical qualitative inquiry grounded in critical race theory and sociopolitical perspectives on teaching and learning, I analyzed the contradictions that emerged in science teaching practices that were both intended to support Student of Color science learning and engaged science-specific colorblind ideologies. The critical race theory analysis demonstrated how science teaching practices such as connecting to students’ experiences, creating interests in science, representing scientists as role models, and scaffolding doing science maintain unequal racialized power relations between students and science when historical and contemporary legacies of racism are not directly confronted. I also propose a science teaching practice of “grappling with racism” as a possible transformative solution to disrupt racism in and through science teaching.",
                "authors": "Manali J. Sheth",
                "citations": 77
            },
            {
                "title": "A novel nonlocal refined plate theory for stability response of orthotropic single-layer graphene sheet resting on elastic medium",
                "abstract": "This work presents the buckling investigation of embedded orthotropic nanoplates such as graphene by employing a new refined plate theory and nonlocal small-scale effects. The elastic foundation is modeled as two-parameter Pasternak foundation. The proposed two-variable refined plate theory takes account of transverse shear influences and parabolic variation of the transverse shear strains within the thickness of the plate by introducing undetermined integral terms, hence it is unnecessary to use shear correction factors. Nonlocal governing equations for the single layered graphene sheet are obtained from the principle of virtual displacements. The proposed theory is compared with other plate theories. Analytical solutions for buckling loads are obtained for single-layered graphene sheets with isotropic and orthotropic properties. The results presented in this study may provide useful guidance for design of orthotropic graphene based nanodevices that make use of the buckling properties of orthotropic nanoplates.",
                "authors": "M. Yazid, H. Heireche, A. Tounsi, A. A. Bousahla, Mohammed Sid Ahmed Houari",
                "citations": 82
            },
            {
                "title": "A size-dependent quasi-3D model for wave dispersion analysis of FG nanoplates",
                "abstract": "In this paper, a new size-dependent quasi-3D plate theory is presented for wave dispersion analysis of functionally graded nanoplates while resting on an elastic foundation and under the hygrothermaal environment. This quasi-3D plate theory considers both thickness stretching influences and shear deformation with the variations of displacements in the thickness direction as a parabolic function. Moreover, the stress-free boundary conditions on both sides of the plate are satisfied without using a shear correction factor. This theory includes five independent unknowns with results in only five governing equations. Size effects are obtained via a higher-order nonlocal strain gradient theory of elasticity. A variational approach is adopted to owning the governing equations employing Hamilton\\'s principle. Solving analytically via Fourier series, these equations gives wave frequencies and phase velocities as a function of wave numbers. The validity of the present results is examined by comparing them with those of the known data in the literature. Parametric studies are conducted for material composition, size dependency, two parametric elastic foundation, temperature and moisture differences, and wave number. Some conclusions are drawn from the parametric studies with respect to the wave characteristics.",
                "authors": "B. Karami, M. Janghorban, D. Shahsavari, A. Tounsi",
                "citations": 82
            },
            {
                "title": "AUS: Anisotropic undrained shear strength model for clays",
                "abstract": "A total stress model applicable to clays under undrained conditions is presented. The model involves three strength parameters: the undrained shear strengths in triaxial compression, triaxial extension, and simple shear. The amount of physical anisotropy implied by the model is a function of the relative magnitude of these three strengths assuming a Mises‐type plastic potential. Elastoplastic deformation characteristics below failure are accounted for by a hardening law requiring two additional parameters that can be related to the axial strains halfway to failure in triaxial compression and extension. Finally, elasticity is accounted for by Hooke law. The result is a relatively simple model whose parameters can all be inferred directly from a combination of in situ and standard undrained laboratory tests. The model is applied to a problem involving the horizontal loading of a monopile foundation for which full scale tests have been previously conducted. The model shows good agreement with the measured data.",
                "authors": "K. Krabbenhøft, S. Galindo‐Torres, Xue Zhang, J. Krabbenhøft",
                "citations": 62
            },
            {
                "title": "An integrative model of autonomous agent teammate-likeness",
                "abstract": "ABSTRACT Advancements in autonomy are beginning to allow humans to partner with machines in order to accomplish work tasks in various settings. As human–agent teaming (HAT) becomes more prevalent as a research topic, the need to understand humans’ psychological perceptions of the machine partner is increasingly important, especially in terms of its perceived role, which may ultimately impact trust and team effectiveness. Specifically, it remains unclear how humans perceive intelligent agents and how consistent these perceptions are with existing taxonomies found in the psychology of teams. The present paper presents a definition of the construct of autonomous agent teammate-likeness (AAT) and a conceptual model of its components, reviews related concepts and germane research and proffers a number of propositions to guide future research. The goal is to contribute to the nascent literature on HAT by establishing a theoretical foundation for the AAT construct, upon which researchers can advance research on HAT.",
                "authors": "Kevin T. Wynne, J. Lyons",
                "citations": 69
            },
            {
                "title": "Endurant Types in Ontology-Driven Conceptual Modeling: Towards OntoUML 2.0",
                "abstract": null,
                "authors": "G. Guizzardi, Claudenir M. Fonseca, A. Benevides, J. P. Almeida, Daniele Porello, T. P. Sales",
                "citations": 79
            },
            {
                "title": "Drosophila as a Model Organism in Host–Pathogen Interaction Studies",
                "abstract": "Owing to the genetic similarities and conserved pathways between a fruit fly and mammals, the use of the Drosophila model as a platform to unveil novel mechanisms of infection and disease progression has been justified and widely instigated. Gaining proper insight into host–pathogen interactions and identifying chief factors involved in host defense and pathogen virulence in Drosophila serves as a foundation to establish novel strategies for infectious disease prevention and control in higher organisms, including humans.",
                "authors": "S. Younes, A. Al-Sulaiti, E. A. Nasser, H. Najjar, L. Kamareddine",
                "citations": 42
            },
            {
                "title": "Colloid effect on clogging mechanism of hydraulic reclamation mud improved by vacuum preloading",
                "abstract": "Many cases of artificial soft clay foundation constructed by hydraulic filling and improved by the vacuum preloading show the clogging phenomenon in the surrounding soil, which compromises the improvement quality. To clarify the clogging mechanism and the formation of soil columns, the vacuum process was tracked from macro to micro by laboratory model tests. Results show that the soil column with higher strength (density) and lower water content was formed surrounding the prefabricated vertical drainage (PVD). Mercury intrusion porosimetry (MIP) tests revealed that the pore-entrance diameter of the soil column after 50 and 43 days of vacuum application ranged from 100 to 600 nm, and that at peak it is 300 nm. However, the mean diameter of the colloidal particles in tail water decreases from 1000 to 100 nm with continuous vacuum application, and then becomes stable at about 100 nm after 43 days. After re-visiting the vacuum process of the hydraulic reclamation mud, the pore-size distribution of surrounding soils and particle-size distribution of the tail water, the clogging was explained by the filling of the pores of the soil column by the colloidal particles in pore water. This mechanism differentiates the artificial foundation improved by vacuum preloading from the natural foundation for the presence of rich colloidal particles in pore water.",
                "authors": "Yongfeng Deng, Li Liu, Yu-Jun Cui, Qi Feng, Xianglong Chen, Ning He",
                "citations": 64
            },
            {
                "title": "A new model for transformation of ferrihydrite to hematite in soils and sediments",
                "abstract": "This study was supported by the National Key Research and Development Program of China (2016YFA0601903), and the National Natural Science Foundation of China (grants 41504055 and 41430962). Roberts acknowledges support from the Australian Research Council (grants DP110105419 and DP120103952).",
                "authors": "Zhaoxia Jiang, Qingsong Liu, A. Roberts, V. Barrón, J. Torrent, Qiang Zhang",
                "citations": 75
            },
            {
                "title": "Collective Intuition: Implications for Improved Decision Making and Organizational Learning",
                "abstract": "This article establishes the foundation for research on collective intuition through a study of decision making and organizational learning processes in police senior management teams. We conceptualize collective intuition as independently formed judgement based on domain-specific knowledge, experience and cognitive ability, shared and interpreted collectively. We contribute to intuition research, which has tended to focus its attention at the individual level, by studying intuition collectively in team settings. From a dual-process perspective, we investigate how expert intuition and deliberation affect decision making and learning at various levels of the organization. Furthermore, we contribute to organizational learning research by offering an empirically derived elaboration of the foundational 4I framework, identifying additional ‘feed-forward’ and ‘feedback’ loop processes, and thereby providing a more complete account of this organizational learning model. Bridging a variety of relevant but previously unconnected literatures via our focal concept of collective intuition, our research provides a foundation for future studies of this vitally important but under-researched organizational phenomenon. We offer theoretical and practical implications whereby expert intuitions can be developed and leveraged collectively as valuable sources of organizational knowledge and learning, and contribute to improved decision making in organizations.",
                "authors": "Cinla Akinci, E. Sadler‐Smith",
                "citations": 59
            },
            {
                "title": "Research on Power Load Forecasting Method Based on LSTM Model",
                "abstract": "Power load forecasting is an important part of power system planning and the foundation of power system economic operation. It is very important for power system planning and operation. The LSTM forecast model is used to Get more accurate power load prediction results. According to the time series rule of power load, the LSTM prediction model for load prediction is established in this paper. A verification experiment has been done to reflect the effect of this method. Experimental results show that accuracy of power load prediction is increased by using LSTM model.",
                "authors": "Can Cui, Ming He, Fangchun Di, Yi Lu, Yuhan Dai, Fengyi Lv",
                "citations": 36
            },
            {
                "title": "A new numerical approach and visco-refined zigzag theory for blast analysis of auxetic honeycomb plates integrated by multiphase nanocomposite facesheets in hygrothermal environment",
                "abstract": null,
                "authors": "M. Hajmohammad, A. Nouri, M. Zarei, R. Kolahchi",
                "citations": 68
            },
            {
                "title": "Why the North American Model of Wildlife Conservation is Problematic for Modern Wildlife Management",
                "abstract": "ABSTRACT The North American Model of Wildlife Conservation (NAM) is a slippery construct, used both to explain how North American wildlife conservation developed and as a prescriptive framework. We argue both applications of the NAM are problematic. The roots of wildlife conservation in North America are more complex than those associated with the NAM, and minimizing contributions from diverse sources makes building a diverse wildlife conservation community more difficult than it would otherwise be. The NAM is not inclusive enough of diversity among wildlife species or stakeholders. Principles labeled the bedrock foundation of the NAM exist in flux and at the whim of political systems. Belief that the NAM reflects a foundation of laws more stable than the milieu of governance structures shaping wildlife management can encourage complacency among wildlife conservation advocates. Wildlife management exists in systems too complex to be beneficially defined by a terse list of principles.",
                "authors": "M. Peterson, M. Nelson",
                "citations": 69
            },
            {
                "title": "Dynamics of Laminated Timoshenko Beams",
                "abstract": null,
                "authors": "B. Feng, T. Ma, R. N. Monteiro, C. Raposo",
                "citations": 62
            },
            {
                "title": "Vibration Analysis of Nano Beam Using Differential Transform Method Including Thermal Effect",
                "abstract": "In this paper, based on nonlocal Euler- Bernoulli beam model with arbitrary boundary conditions a simulation method called the Differential Transform Method (DTM) is employed to predict and to analysis the vibration of a single-walled carbon nanotube embedded in an elastic medium under thermal effect. A Winkler type elastic foundation is employed to model the interaction of carbon nanotube and the surrounding elastic medium. Ferstly, the Differential Transform Method is introduced. The research work reveals the significance of the small-scale coefficient, the vibrational mode number and the elastic medium on the non-dimensional natural frequency.",
                "authors": "R. Hamza-Cherif, Mustapha Meradjah, M. Zidour, A. Tounsi, Samir Belmahi, T. Bensattalah",
                "citations": 62
            },
            {
                "title": "Nonlocal piezo-hygrothermal analysis for vibration characteristics of a piezoelectric Kelvin–Voigt viscoelastic nanoplate embedded in a viscoelastic medium",
                "abstract": null,
                "authors": "A. Zenkour, M. Sobhy",
                "citations": 61
            },
            {
                "title": "New Automated BIM Object Classification Method to Support BIM Interoperability",
                "abstract": "AbstractIndustry Foundation Classes (IFC) is widely accepted as the future of building information modeling (BIM) to take on the challenge of BIM interoperability and enable its support of various ...",
                "authors": "Jin Wu, Jiansong Zhang",
                "citations": 50
            },
            {
                "title": "3D bearing capacity probabilistic analyses of footings on spatially variable c–φ soil",
                "abstract": null,
                "authors": "M. Kawa, W. Puła",
                "citations": 51
            },
            {
                "title": "Nutrient limitation, bioenergetics and stoichiometry: A new model to predict elemental fluxes mediated by fishes",
                "abstract": "BNP Paribas Foundation; Agence National de la Recherche, Grant/Award Number: ANR-17-CE32-0006; U.S. National Science Foundation, Grant/Award Number: OCE1547952",
                "authors": "Nina M. D. Schiettekatte, Diego R. Barneche, S. Villéger, Jacob E. Allgeier, D. Burkepile, S. Brandl, Jordan M. Casey, A. Mercière, Katrina S. Munsterman, F. Morat, V. Parravicini",
                "citations": 26
            },
            {
                "title": "Size-dependent vibration and bending analyses of the piezomagnetic three-layer nanobeams",
                "abstract": null,
                "authors": "M. Arefi, A. Zenkour",
                "citations": 63
            },
            {
                "title": "Generating correctness proofs with neural networks",
                "abstract": "Foundational verification allows programmers to build software which has been empirically shown to have high levels of assurance in a variety of important domains. However, the cost of producing foundationally verified software remains prohibitively high for most projects, as it requires significant manual effort by highly trained experts. In this paper we present Proverbot9001, a proof search system using machine learning techniques to produce proofs of software correctness in interactive theorem provers. We demonstrate Proverbot9001 on the proof obligations from a large practical proof project, the CompCert verified C compiler, and show that it can effectively automate what were previously manual proofs, automatically producing proofs for 28% of theorem statements in our test dataset, when combined with solver-based tooling. Without any additional solvers, we exhibit a proof completion rate that is a 4X improvement over prior state-of-the-art machine learning models for generating proofs in Coq.",
                "authors": "Alex Sanchez-Stern, Yousef Alhessi, L. Saul, Sorin Lerner",
                "citations": 51
            },
            {
                "title": "An integrated model of travelers’ pro-environmental decision-making process: the role of the New Environmental Paradigm",
                "abstract": "ABSTRACT This study integrated Value-Belief-Norm and Modified Norm Activation Model into a theoretical framework to identify the role of the New Environmental Paradigm (NEP) in tourists’ pro-environmental decision-making process. Results of the structural model from 484 sample travelers provide a comprehensive view of travelers’ pro-environmental decision-making process. Findings highlight that the integrated model captures a conceptual foundation by better-predicting travelers’ pro-environmental behaviors with the NEP playing a critical role in facilitating predictive power improvement. This study expands the literature on the NEP in the environment and tourism research and suggests destinations for incorporation of the concept into developing promotional campaigns.",
                "authors": "Eunkyoung Park, SoJung Lee, Choong‐Ki Lee, Jinok S. Kim, Nam‐Jo Kim",
                "citations": 53
            },
            {
                "title": "Demystifying Qualitative Data Analysis for Novice Qualitative Researchers",
                "abstract": "Qualitative research is a rich and diverse discipline, yet novice qualitative researchers may struggle in discerning how to approach their qualitative data analysis among the plethora of possibilities. This paper presents a foundational model that facilitates a comprehensive yet manageable approach to qualitative data analysis, and it can be applied within an array of qualitative methodologies. Based on an exhaustive review of expert qualitative methodologists, along with our own experience of teaching qualitative research, this model synthesises commonly-used analytic strategies and methods that are likewise applicable to novice qualitative researchers. This foundational model consists of four iterative cycles: The Inspection Cycle, Coding Cycle, Categorisation Cycle, and Modelling Cycle, and memo-writing is inherent to the entire analysis process. Our goal is to offer a solid foundation from which novice qualitative researchers may begin familiarising themselves with the craft of qualitative research and continue discovering methods for making sense of qualitative data.",
                "authors": "Neringa Kalpokaite, Iva Radivojevic",
                "citations": 53
            },
            {
                "title": "Strengthening divine values for self-regulation in religiosity: insights from Tawakkul (trust in God)",
                "abstract": "Reflections about fostering moral and spiritual qualities are the key point of view when considering the essence of religious beliefs in the theories about moral foundations. As a part of the spiritual values aimed at instructing human beings, mainly Muslims, tawakkul (trust in God) and tawhid (belief to God) are to be enhanced to situate the core religious foundation as the basic element for life at individual and social levels in the Muslim communities. This paper aims to critically examine tawakkul and tawhid to strengthening divine values as a foundation of self-regulation in religiosity.,This paper aims to critically examine tawakkul and tawhid to strengthening divine values as a foundation of self-regulation in religiosity. The literature review from referred journals was conducted using keywords on divine values, self-regulation in religiosity and tawakkul and tawhid. In order to obtain such literature, the critical analysis was conducted by organising substantive keywords. Then, extraction of data with deep literature analysis was carried out to interpret the findings. The key elements were analysed and synthesised into a new interpretation, conceptualisation and modelling of conceptualising tawakkul and tawhid concerns for sustainability of divine values for self-regulation in religiosity.,The finding reveals that the significance of conceptualising tawakkul and tawhid refers to as sustainability of divine involvement, as an emotionally religious commitment and as a consciously held religious discipline. Primarily, as the religious principle founded on a basic element transferred into individual and social levels, Islamic insights from tawakkul and tawhid offer valuable considerations for the understanding and amelioration of development by contributing a model that bases the “mental” and “spiritual” elements on a religious foundation, as an ultimate component for faith, a religious commitment and a belief in an authoritarian God to foster moral and spiritual qualities amidst the society.,Regulating tawakkul and tawhid to enhance dynamically constructive system for moral personality through critical examination as a foundation for religious-based self-regulation offers valuable considerations for the understanding and amelioration of development. Critical examination of tawakkul and tawhid as a foundation for religious self-regulation is considerably engaged to enhance the understanding and amelioration of development. It does so by contributing a model that bases mental and spiritual elements on a religious foundation, as an ultimate component for faith, religious commitment and belief in an authoritarian God to foster moral and spiritual qualities among human beings.",
                "authors": "M. Huda, A. Sudrajat, Razaleigh Muhamat, K. M. Teh, B. Jalal",
                "citations": 33
            },
            {
                "title": "Modeling of Powder Bed Manufacturing Defects",
                "abstract": null,
                "authors": "H. Mindt, O. Desmaison, M. Megahed, A. Peralta, J. Neumann",
                "citations": 63
            },
            {
                "title": "Assessment of the Carbonate Chemistry Seasonal Cycles in the Southern Ocean From Persistent Observational Platforms",
                "abstract": "U.S. National Science Foundation's Southern Ocean Carbon and Climate Observations and Modeling (SOCCOM) project under the NSF [PLR-1425989]; NASA [NNX14AP49G]; U.S. Argo through NOAA/JISAO [NA17RJ1232]; Pacific Marine Environmental Laboratory of NOAA; Ocean Observations and Monitoring Division, Climate Program Office, National Oceanic and Atmospheric Administration, United States; ARCS Foundation Oregon Chapter",
                "authors": "N. Williams, L. Juranek, R. Feely, J. Russell, K. Johnson, B. Hales",
                "citations": 47
            },
            {
                "title": "Wrinkle formation during steering in automated fiber placement: Modeling and experimental verification",
                "abstract": "Automated fiber placement is being widely applied in the aerospace industry due to its advantages. This technology has the capability to improve the efficiency of composite structures by steering where properties such as stiffness can vary within the same part. However, such steering is limited by process-induced defects such as out-of-plane wrinkles, which occur when the steering radius exceeds its critical limit. The present paper proposes a theoretical model for wrinkle formation during steering of the autoclave thermosetting prepreg. The Rayleigh–Ritz approach is used to model wrinkle formation based on the critical buckling load. The prepreg tape is considered an orthotropic plate resting on a Pasternak elastic foundation, which consists of one elastic spring layer connected to an elastic shear layer. Closed form solutions for predicting both critical steering radius and buckling wavelength is presented. The two foundation parameters and the required mechanical properties of the prepreg are experimentally characterized. The model-predicted results are validated by the experimental results. The results reveal good agreement between the predicted and experimental values. It is also found that a significant improvement in the model was achieved by adding the shear layer to the foundation.",
                "authors": "Muhsan Belhaj, M. Hojjati",
                "citations": 46
            },
            {
                "title": "Postbuckling of Curved Carbon Nanotubes Using Energy Equivalent Model",
                "abstract": "This paper presents a novel numerical procedure to predict nonlinear buckling and postbuckling stability of imperfect clamped–clamped single walled carbon nanotube (SWCNT) surrounded by nonlinear elastic foundation. Nanoscale effect of CNTs is included by using energy-equivalent model (EEM) which transferring the chemical energy between carbon atoms to mechanical strain energy. Young’s modulus and Poisson’s ratio for zigzag (n, 0), and armchair (n, n) carbon nanotubes (CNTs) are presented as functions of orientation and force constants by using energy-equivalent model (EEM). Nonlinear Euler-Bernoulli assumptions are proposed considering mid-plane stretching to exhibit a large deformation and a small strain. To simulate the interaction of CNTs with the surrounding elastic medium, nonlinear elastic foundation with cubic nonlinearity and shearing layer are employed. The governing nonlinear integro-partial-differential equations are derived in terms of only the lateral displacement. The modified differential quadrature method (DQM) is exploited to obtain numerical results of the nonlinear governing equations. The static problem is solved for critical buckling loads and the postbuckling deformation as a function of applied axial load, curved amplitude, CNT length, and orientations. Numerical results show that the effects of chirality angle and curved amplitude on static response of armchair and zigzag CNTs are significant. This model is helpful especially in mechanical design of NEMS manufactured from CNTs.",
                "authors": "M. A. Eltaher, N. Mohamed, S. Mohamed, L. F. Seddek",
                "citations": 38
            },
            {
                "title": "Nonlinear vibration of nonlocal four-variable graded plates with porosities implementing homotopy perturbation and Hamiltonian methods",
                "abstract": null,
                "authors": "M. Barati, H. Shahverdi",
                "citations": 39
            },
            {
                "title": "Direct finite element method for nonlinear earthquake analysis of concrete dams: Simplification, modeling, and practical application",
                "abstract": "A direct finite element (FE) method for nonlinear response history analysis of semi‐unbounded dam‐water‐foundation systems has recently been presented. The analysis procedure employs standard viscous‐damper absorbing boundaries to model the semi‐unbounded foundation and fluid domains and specifies the seismic input as effective earthquake forces—determined from a control motion defined at the foundation surface—at these boundaries. Presented in this paper are several simplifications to this direct FE method that greatly facilitates its implementation in commercial FE software. Also addressed is the modeling of the principal nonlinear mechanisms for concrete dams, calibration of damping in the numerical model to ensure consistency with values measured at actual dams, and practical procedures for implementation of the direct FE method with a commercial FE program.",
                "authors": "A. Løkke, A. Chopra",
                "citations": 32
            },
            {
                "title": "Oil spill modeling",
                "abstract": "Oil spill modeling is fundamental for planning and preparing for, as well as responding to and mitigating, actual spill events. As a result, significant research effort has been directed toward developing analytical approaches for deepening our understanding of spill risk, community vulnerability, oil behavior, spill outcomes, and impacts. The purpose of this paper is to provide a synthesis of the oil spill risk assessment and impact modeling literature, with a focus on the vulnerability of local environmental, ecological, and community systems, as well as the geographic processes associated with modeling spills and transforming these data into a robust and meaningful impact assessments. The results of this progress report reveal a number of methodological and substantive commonalities across the scientific literature. Moreover, the synthesis of this literature should provide researchers with a strong foundation for pursuing future work in this domain.",
                "authors": "J. Nelson, T. Grubesic",
                "citations": 51
            },
            {
                "title": "A classification system for zebrafish adipose tissues",
                "abstract": "ABSTRACT The zebrafish model system offers significant utility for in vivo imaging of adipose tissue (AT) dynamics and for screening to identify chemical and genetic modifiers of adiposity. In particular, AT can be quantified accurately in live zebrafish using fluorescent lipophilic dyes. Although this methodology offers considerable promise, the comprehensive identification and classification of zebrafish ATs has not been performed. Here, we use fluorescent lipophilic dyes and in vivo imaging systematically to identify, classify and quantify the zebrafish AT pool. We identify 34 regionally distinct zebrafish ATs, including five visceral ATs and 22 subcutaneous ATs. For each of these ATs, we describe detailed morphological characteristics to aid their identification in future studies. Furthermore, we quantify the areas for each AT and construct regression models to allow prediction of expected AT size and variation across a range of developmental stages. Finally, we demonstrate the utility of this resource for identifying effects of strain variation and high-fat diet on AT growth. Altogether, this resource provides foundational information on the identity, dynamics and expected quantities of zebrafish ATs for use as a reference for future studies. Summary: A standardized nomenclature and classification system for zebrafish adipose tissues and regression models to predict expected adipose size during the course of zebrafish development.",
                "authors": "James E. N. Minchin, J. Rawls",
                "citations": 62
            },
            {
                "title": "Fractional Dynamics of Fluid-Conveying Pipes Made of Polymer-Like Materials",
                "abstract": null,
                "authors": "Ye Tang, Tianzhi Yang, B. Fang",
                "citations": 46
            },
            {
                "title": "Nonlocal elasticity and shear deformation effects on thermal buckling of a CNT embedded in a viscoelastic medium",
                "abstract": null,
                "authors": "A. Zenkour",
                "citations": 40
            },
            {
                "title": "Liquefaction analysis and damage evaluation of embankment-type structures",
                "abstract": null,
                "authors": "I. Rapti, F. Lopez-Caballero, A. Modaressi-Farahmand-Razavi, A. Foucault, F. Voldoire",
                "citations": 40
            },
            {
                "title": "Events as Entities in Ontology-Driven Conceptual Modeling",
                "abstract": null,
                "authors": "J. P. Almeida, R. Falbo, G. Guizzardi",
                "citations": 38
            },
            {
                "title": "Centrifuge modeling of disconnected piled raft using vertical pushover tests",
                "abstract": null,
                "authors": "Heon-Joon Park, K. Ko, Young-Hun Song, Myungjun Song, Seokwoo Jin, J. Ha, Dong‐Soo Kim",
                "citations": 22
            },
            {
                "title": "A lumped parameter model for time‐domain inertial soil‐structure interaction analysis of structures on pile foundations",
                "abstract": "The paper presents a lumped parameter model for the approximation of the frequency‐dependent dynamic stiffness of pile group foundations. The model can be implemented in commercial software to perform linear or nonlinear dynamic analyses of structures founded on piles taking into account the frequency‐dependent coupled roto‐translational, vertical, and torsional behaviour of the soil‐foundation system. Closed‐form formulas for estimating parameters of the model are proposed with reference to pile groups embedded in homogeneous soil deposits. These are calibrated with a nonlinear least square procedure, based on data provided by an extensive non‐dimensional parametric analysis performed with a model previously developed by the authors. Pile groups with square layout and different number of piles embedded in soft and stiff soils are considered. Formulas are overall well capable to reproduce parameters of the proposed lumped system that can be straightforwardly incorporated into inertial structural analyses to account for the dynamic behaviour of the soil‐foundation system. Some applications on typical bridge piers are finally presented to show examples of practical use of the proposed model. Results demonstrate the capability of the proposed lumped system as well as the formulas efficiency in approximating impedances of pile groups and the relevant effect on the response of the superstructure.",
                "authors": "S. Carbonari, M. Morici, F. Dezi, G. Leoni",
                "citations": 35
            },
            {
                "title": "Modeling and",
                "abstract": "- This paper presented the control modeling and analysis of the prototype of Multiple Launch Rocket System (MLRS) using Matlab Simulink. The mathematical model was determined using physical law and theoretical foundation of the turning mechanism control system and the elevating mechanism control system. Then, the model was identified in Matlab Simulink and analyzed in open-loop frequency characteristic analysis, closed-loop frequency characteristic analysis, step response characteristic analysis, and sinusoidal response analysis. The simulation results showed a good stability performance for both open-loop control and closed-loop control systems. We observed the fast response of tracking and controlling with the overshoot less than 3% and the steady-state error less than 0.2 Mil.",
                "authors": "Narongkorn Dernlugkam, Rungtawan Chodsirisak",
                "citations": 34
            },
            {
                "title": "Dynamic modeling of embedded nanoplate systems incorporating flexoelectricity and surface effects",
                "abstract": null,
                "authors": "F. Ebrahimi, M. Barati",
                "citations": 27
            },
            {
                "title": "A non-classical model for an orthotropic Kirchhoff plate embedded in a viscoelastic medium",
                "abstract": null,
                "authors": "G. Y. Zhang, X.-L. Gao, Z. Guo",
                "citations": 34
            },
            {
                "title": "Centrifuge modeling of geotechnical mitigation measures for shallow foundations subjected to reverse faulting",
                "abstract": "Surface fault ruptures are particularly damaging to buildings, lifelines, and bridges located across or adjacent to active faults. These structures should be designed in consideration of surface fault rupture hazards or strategies should be adopted to protect the structures from fault-induced damage. Geotechnical mitigation strategies such as diversion of the fault rupture away from the structure and diffusion of the rupture over a wide zone are possible strategies. The effectiveness of these geotechnical mitigation measures for reverse faulting on shallow embedded foundations was investigated using a series of centrifuge tests. These measures included excavation of a vertical trench adjacent to the foundation and installation of geogrid layers beneath the foundation. The trench was shown to be effective for a range of foundation positions depending on the magnitude of the fault offset, dip angle of the fault, depth of the trench, embedment depth of the foundation, and the number of trenches used. The geogrid layers prevented a distinct fault rupture from reaching the surface and spread fault displacement over a wider zone, but were unable to mitigate the surface fault rupture hazard for shallow embedded foundations.",
                "authors": "M. Ashtiani, A. Ghalandarzadeh, M. Mahdavi, M. Hedayati",
                "citations": 30
            },
            {
                "title": "Mathematical modeling of groundwaters pressure distribution in the underground structures by cylindrical form zone",
                "abstract": "A mathematical model is developed for studying the distribution of groundwater pressure and its variation in the zone of underground structures of a cylindrical shape. Based on the created model, the influence of the thickness of the aquifer, the soil porosity, the filtration coefficient, the viscosity coefficient and the piezoelectric conductivity coefficient on the pressure that groundwater exerts on the lower part of the underground structure is investigated. The analysis of the possibility of pushing the structure and breaking the foundation under the influence of pressure caused by groundwater is analyzed. Analytical formulas are obtained for estimating the stresses in the foundation and predicting the possibility of its destruction.",
                "authors": "V. Telichenko, V. Rimshin, V. Eremeev, V. Kurbatov",
                "citations": 30
            },
            {
                "title": "The challenges and opportunities of global neurosurgery in East Africa: the Neurosurgery Education and Development model.",
                "abstract": "OBJECTIVE\nThe objective of this study was to describe the experience of a volunteering neurosurgeon during an 18-week stay at the Neurosurgery Education and Development (NED) Institute and to report the general situation regarding the development of neurosurgery in Zanzibar, identifying the challenges and opportunities and explaining the NED Foundation's model for safe practice and sustainability.\n\n\nMETHODS\nThe NED Foundation deployed the volunteer neurosurgeon coordinator (NC) for an 18-week stay at the NED Institute at the Mnazi Mmoja Hospital, Stonetown, Zanzibar. The main roles of the NC were as follows: management of patients, reinforcement of weekly academic activities, coordination of international surgical camps, and identification of opportunities for improvement. The improvement opportunities were categorized as clinical, administrative, and sociocultural and were based on observations made by the NC as well as on interviews with local doctors, administrators, and government officials.\n\n\nRESULTS\nDuring the 18-week period, the NC visited 460 patients and performed 85 surgical procedures. Four surgical camps were coordinated on-site. Academic activities were conducted weekly. The most significant challenges encountered were an intense workload, deficient infrastructure, lack of self-confidence among local physicians, deficiencies in technical support and repairs of broken equipment, and lack of guidelines. Through a series of interviews, the sociocultural factors influencing the NED Foundation's intervention were determined. Factors identified for success were the activity of neurosurgical societies in East Africa; structured pan-African neurosurgical training; the support of the Foundation for International Education in Neurological Surgery (FIENS) and the College of Surgeons of East, Central and Southern Africa (COSECSA); motivated personnel; and the Revolutionary Government of Zanzibar's willingness to collaborate with the NED Foundation.\n\n\nCONCLUSIONS\nInternational collaboration programs should balance local challenges and opportunities in order to effectively promote the development of neurosurgery in East Africa. Support and endorsement should be sought to harness shared resources and experience. Determining the caregiving and educational objectives within the logistic, administrative, social, and cultural framework of the target hospital is paramount to success.",
                "authors": "A. Leidinger, P. Extremera, Eliana E. Kim, Mahmood M. Qureshi, P. Young, J. Piquer",
                "citations": 30
            },
            {
                "title": "Seismic isolation of small modular reactors using metamaterials",
                "abstract": "Adaptation of metamaterials at micro- to nanometer scales to metastructures at much larger scales offers a new alternative for seismic isolation systems. These new isolation systems, known as periodic foundations, function both as a structural foundation to support gravitational weight of the superstructure and also as a seismic isolator to isolate the superstructure from incoming seismic waves. Here we describe the application of periodic foundations for the seismic protection of nuclear power plants, in particular small modular reactors (SMR). For this purpose, a large-scale shake table test on a one-dimensional (1D) periodic foundation supporting an SMR building model was conducted. The 1D periodic foundation was designed and fabricated using reinforced concrete and synthetic rubber (polyurethane) materials. The 1D periodic foundation structural system was tested under various input waves, which include white noise, stepped sine and seismic waves in the horizontal and vertical directions as well as in the torsional mode. The shake table test results show that the 1D periodic foundation can reduce the acceleration response (transmissibility) of the SMR building up to 90%. In addition, the periodic foundation-isolated structure also exhibited smaller displacement than the non-isolated SMR building. This study indicates that the challenge faced in developing metastructures can be overcome and the periodic foundations can be applied to isolating vibration response of engineering structures.",
                "authors": "W. Witarto, Shiang‐Jung Wang, C. Yang, X. Nie, Y. Mo, Kuo-Chun Chang, Yu Tang, R. Kassawara",
                "citations": 31
            },
            {
                "title": "Investigation of seismic performances of unconnected pile foundations using dynamic centrifuge tests",
                "abstract": null,
                "authors": "J. Ha, K. Ko, S. Jo, Heon-Joon Park, Dong‐Soo Kim",
                "citations": 25
            },
            {
                "title": "Representing a reference foundational ontology of events in SROIQ",
                "abstract": "In recent years, there has been a growing interest in the application of foundational ontologies, i.e., formal ontological theories in the philosophical sense, to provide a theoretically sound foundation for improving the theory and practice of conceptual modeling and knowledge representation. This paper addresses one particular foundational theory of events termed UFO-B, which has been successfully employed as a reference model for addressing problems from complex media management, enterprise architecture, software engineering, and modeling of events in petroleum exploration. Despite its success, there is still no formalization of UFO-B in a decidable knowledge representation language that could support reasoning about complex events and event relations. We address this gap by proposing a number of alternative translations from UFO-B’s original axiomatization (in first-order logic and in the Alloy formal language) to the description logic SROIQ, which is the formal underpinning of OWL 2 DL. Additionally, to support practical applications, we translated these SROIQ theories to OWL 2 DL TBoxes, which were validated by showing that all the intended models of UFO-B (the logical models of the UFO-B specification in Alloy) that we generated are consistent with these UFO-B TBoxes. In a sense, the specification in Alloy implements the specification in first-order logic, while the OWL 2 TBoxes implement the SROIQ specifications. Incidentally, the methodology that we designed for the translation from UFO-B’s original axiomatization in FOL and Alloy to SROIQ came to be a key contribution of this work by providing us evidence of the inadequacy of DLs for the specification of comprehensive foundational ontologies.",
                "authors": "A. Benevides, Jean-Rémi Bourguet, G. Guizzardi, R. Peñaloza, J. P. Almeida",
                "citations": 25
            },
            {
                "title": "Gangdese culmination model: Oligocene–Miocene duplexing along the India-Asia suture zone, Lazi region, southern Tibet",
                "abstract": "U.S. National Science Foundation Continental Dynamics Program (EAR-1008527); U.S. National Science Foundation Instrumentation and Facilities program (EAR-1338583) to the Arizona LaserChron Center; China National Science Foundation (41490610); Geological Society of America (student research grant).",
                "authors": "A. Laskowski, P. Kapp, Fulong Cai",
                "citations": 27
            },
            {
                "title": "Numerical Analysis of Existing Foundations Underpinned by Micropiles",
                "abstract": "AbstractMicropiles of small diameter have been used in practice to increase load capacities of existing foundations on soft soil, which may resist additional loads from the vertical expansion of structures. Load transfer from the existing foundation to the micropiles is an important mechanism to consider when designing an existing foundation underpinned by micropiles. However, this mechanism has not been well investigated or understood. This paper presents a numerical study on an existing foundation underpinned by micropiles using three-dimensional (3D) finite-difference software. Verification of the numerical model was first achieved by comparing the results of the numerical model to those obtained from the full-scale loading test. In this verification, the numerical model was used to simulate an existing footing initially constructed on a natural soil to support a structure and later subjected to additional loads under two different conditions. Under the first condition, micropiles were installed withou...",
                "authors": "W. E. Kamash, Jie Han",
                "citations": 28
            },
            {
                "title": "A Geometric Model of Opinion Polarization",
                "abstract": "We introduce a simple geometric model of opinion polarization. It is a model of political persuasion as well as marketing and advertising, utilizing social values. It focuses on the interplay between different topics and persuasion efforts. We demonstrate that societal opinion polarization often arises as an unintended by-product of influencers attempting to promote a product or idea. We discuss a number of mechanisms for the emergence of polarization involving one or more influencers, sending messages strategically, heuristically, or randomly. We also examine some computational aspects of choosing the most effective means of influencing agents and the effects of those strategic considerations on polarization. Funding: J. Hązła was partially supported by the National Science Foundation [Grant DMS-1737944] and later, the Alexander von Humboldt Foundation [African Institute for Mathematical Sciences Rwanda research chair funding] as well as the German Academic Exchange Service (DAAD) [grant in cooperation with Goethe University in Frankfurt]. Y. Jin was partially supported by the Department of Defense [Grant ARO MURI W911NF1910217]. E. Mossel was partially supported by the Simons Investigator in Mathematics [Award 622132], the National Science Foundation [Awards DMS-1737944 and CCF-1918421] and the Department of Defense [Grant ARO MURI W911NF1910217] and Vannevar Bush [Faculty Fellowship ONR-N00014-20-1-2826]. G. Ramnarayan was partially supported by the National Science Foundation [Grant DMS-1737944] and the Department of Defense [Grant ARO MURI W911NF1910217].",
                "authors": "Jan Hązła, Yan Jin, Elchanan Mossel, Govind Ramnarayan",
                "citations": 18
            },
            {
                "title": "Countermeasures for enhancing the stability of composite breakwater under earthquake and subsequent tsunami",
                "abstract": null,
                "authors": "B. Chaudhary, H. Hazarika, A. Murakami, K. Fujisawa",
                "citations": 23
            },
            {
                "title": "Safe and just Earth system boundaries",
                "abstract": null,
                "authors": "J. Rockström, J. Gupta, Dahe Qin, S. Lade, J. Abrams, L. Andersen, D. A. Armstrong McKay, Xuemei Bai, G. Bala, S. Bunn, Daniel Ciobanu, F. DeClerck, K. Ebi, L. Gifford, C. Gordon, Syezlin Hasan, N. Kanie, T. Lenton, S. Loriani, D. Liverman, Awaz Mohamed, N. Nakicenovic, D. Obura, D. Ospina, K. Prodani, C. Rammelt, B. Sakschewski, J. Scholtens, B. Stewart‐Koster, Thejna Tharammal, D. V. van Vuuren, P. Verburg, R. Winkelmann, C. Zimm, E. Bennett, S. Bringezu, Wendy Broadgate, P. Green, Lei Huang, L. Jacobson, C. Ndehedehe, Simona Pedde, J. Rocha, M. Scheffer, L. Schulte-Uebbing, W. de Vries, C. Xiao, Chi Xu, Xinwu Xu, Noelia Zafra‐Calvo, Xin Zhang",
                "citations": 410
            },
            {
                "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)",
                "abstract": "For a long time, different recommendation tasks require designing task-specific architectures and training objectives. As a result, it is hard to transfer the knowledge and representations from one task to another, thus restricting the generalization ability of existing recommendation approaches. To deal with such issues, considering that language can describe almost anything and language grounding is a powerful medium to represent various problems or tasks, we present a flexible and unified text-to-text paradigm called “Pretrain, Personalized Prompt, and Predict Paradigm” (P5) for recommendation, which unifies various recommendation tasks in a shared framework. In P5, all data such as user-item interactions, user descriptions, item metadata, and user reviews are converted to a common format — natural language sequences. The rich information from natural language assists P5 to capture deeper semantics for personalization and recommendation. Specifically, P5 learns different tasks with the same language modeling objective during pretraining. Thus, it serves as the foundation model for various downstream recommendation tasks, allows easy integration with other modalities, and enables instruction-based recommendation. P5 advances recommender systems from shallow model to deep model to big model, and will revolutionize the technical form of recommender systems towards universal recommendation engine. With adaptive personalized prompt for different users, P5 is able to make predictions in a zero-shot or few-shot manner and largely reduces the necessity for extensive fine-tuning. On several benchmarks, we conduct experiments to show the effectiveness of P5. To help advance future research on Recommendation as Language Processing (RLP), Personalized Foundation Models (PFM), and Universal Recommendation Engine (URE), we release the source code, dataset, prompts, and pretrained P5 model at https://github.com/jeykigung/P5.",
                "authors": "Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang",
                "citations": 362
            },
            {
                "title": "Decomposing NeRF for Editing via Feature Field Distillation",
                "abstract": "Emerging neural radiance fields (NeRF) are a promising scene representation for computer graphics, enabling high-quality 3D reconstruction and novel view synthesis from image observations. However, editing a scene represented by a NeRF is challenging, as the underlying connectionist representations such as MLPs or voxel grids are not object-centric or compositional. In particular, it has been difficult to selectively edit specific regions or objects. In this work, we tackle the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes. We propose to distill the knowledge of off-the-shelf, self-supervised 2D image feature extractors such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the radiance field. Given a user-specified query of various modalities such as text, an image patch, or a point-and-click selection, 3D feature fields semantically decompose 3D space without the need for re-training and enable us to semantically select and edit regions in the radiance field. Our experiments validate that the distilled feature fields (DFFs) can transfer recent progress in 2D vision and language foundation models to 3D scene representations, enabling convincing 3D segmentation and selective editing of emerging neural graphics representations.",
                "authors": "Sosuke Kobayashi, Eiichi Matsumoto, Vincent Sitzmann",
                "citations": 289
            },
            {
                "title": "Estimating distances from parallaxes. V: Geometric and photogeometric distances to 1.47 billion stars in Gaia Early Data Release 3.",
                "abstract": "Stellar distances constitute a foundational pillar of astrophysics. The publication of 1.47 billion stellar parallaxes from Gaia is a major contribution to this. Yet despite Gaia's precision, the majority of these stars are so distant or faint that their fractional parallax uncertainties are large, thereby precluding a simple inversion of parallax to provide a distance. Here we take a probabilistic approach to estimating stellar distances that uses a prior constructed from a three-dimensional model of our Galaxy. This model includes interstellar extinction and Gaia's variable magnitude limit. We infer two types of distance. The first, geometric, uses the parallax together with a direction-dependent prior on distance. The second, photogeometric, additionally uses the colour and apparent magnitude of a star, by exploiting the fact that stars of a given colour have a restricted range of probable absolute magnitudes (plus extinction). Tests on simulated data and external validations show that the photogeometric estimates generally have higher accuracy and precision for stars with poor parallaxes. We provide a catalogue of 1.47 billion geometric and 1.35 billion photogeometric distances together with asymmetric uncertainty measures. Our estimates are quantiles of a posterior probability distribution, so they transform invariably and can therefore also be used directly in the distance modulus (5log10(r)-5). The catalogue may be downloaded or queried using ADQL at various sites (see this http URL) where it can also be cross-matched with the Gaia catalogue.",
                "authors": "C. Bailer-Jones, J. Rybizki, M. Fouesneau, M. Demleitner, R. M. J. I. O. Astronomy, Heidelberg, A. Rechen-Institut",
                "citations": 646
            },
            {
                "title": "Shifting the limits in wheat research and breeding using a fully annotated reference genome",
                "abstract": "Insights from the annotated wheat genome Wheat is one of the major sources of food for much of the world. However, because bread wheat's genome is a large hybrid mix of three separate subgenomes, it has been difficult to produce a high-quality reference sequence. Using recent advances in sequencing, the International Wheat Genome Sequencing Consortium presents an annotated reference genome with a detailed analysis of gene content among subgenomes and the structural organization for all the chromosomes. Examples of quantitative trait mapping and CRISPR-based genome modification show the potential for using this genome in agricultural research and breeding. Ramírez-González et al. exploited the fruits of this endeavor to identify tissue-specific biased gene expression and coexpression networks during development and exposure to stress. These resources will accelerate our understanding of the genetic basis of bread wheat. Science, this issue p. eaar7191; see also p. eaar6089 The 21 annotated chromosomes of bread wheat provide a foundation for innovation in research and breeding. INTRODUCTION Wheat (Triticum aestivum L.) is the most widely cultivated crop on Earth, contributing about a fifth of the total calories consumed by humans. Consequently, wheat yields and production affect the global economy, and failed harvests can lead to social unrest. Breeders continuously strive to develop improved varieties by fine-tuning genetically complex yield and end-use quality parameters while maintaining stable yields and adapting the crop to regionally specific biotic and abiotic stresses. RATIONALE Breeding efforts are limited by insufficient knowledge and understanding of wheat biology and the molecular basis of central agronomic traits. To meet the demands of human population growth, there is an urgent need for wheat research and breeding to accelerate genetic gain as well as to increase and protect wheat yield and quality traits. In other plant and animal species, access to a fully annotated and ordered genome sequence, including regulatory sequences and genome-diversity information, has promoted the development of systematic and more time-efficient approaches for the selection and understanding of important traits. Wheat has lagged behind, primarily owing to the challenges of assembling a genome that is more than five times as large as the human genome, polyploid, and complex, containing more than 85% repetitive DNA. To provide a foundation for improvement through molecular breeding, in 2005, the International Wheat Genome Sequencing Consortium set out to deliver a high-quality annotated reference genome sequence of bread wheat. RESULTS An annotated reference sequence representing the hexaploid bread wheat genome in the form of 21 chromosome-like sequence assemblies has now been delivered, giving access to 107,891 high-confidence genes, including their genomic context of regulatory sequences. This assembly enabled the discovery of tissue- and developmental stage–related gene coexpression networks using a transcriptome atlas representing all stages of wheat development. The dynamics of change in complex gene families involved in environmental adaptation and end-use quality were revealed at subgenome resolution and contextualized to known agronomic single-gene or quantitative trait loci. Aspects of the future value of the annotated assembly for molecular breeding and research were exemplarily illustrated by resolving the genetic basis of a quantitative trait locus conferring resistance to abiotic stress and insect damage as well as by serving as the basis for genome editing of the flowering-time trait. CONCLUSION This annotated reference sequence of wheat is a resource that can now drive disruptive innovation in wheat improvement, as this community resource establishes the foundation for accelerating wheat research and application through improved understanding of wheat biology and genomics-assisted breeding. Importantly, the bioinformatics capacity developed for model-organism genomes will facilitate a better understanding of the wheat genome as a result of the high-quality chromosome-based genome assembly. By necessity, breeders work with the genome at the whole chromosome level, as each new cross involves the modification of genome-wide gene networks that control the expression of complex traits such as yield. With the annotated and ordered reference genome sequence in place, researchers and breeders can now easily access sequence-level information to precisely define the necessary changes in the genomes for breeding programs. This will be realized through the implementation of new DNA marker platforms and targeted breeding technologies, including genome editing. Wheat genome deciphered, assembled, and ordered. Seeds, or grains, are what counts with respect to wheat yields (left panel), but all parts of the plant contribute to crop performance. With complete access to the ordered sequence of all 21 wheat chromosomes, the context of regulatory sequences, and the interaction network of expressed genes—all shown here as a circular plot (right panel) with concentric tracks for diverse aspects of wheat genome composition—breeders and researchers now have the ability to rewrite the story of wheat crop improvement. Details on value ranges underlying the concentric heatmaps of the right panel are provided in the full article online. An annotated reference sequence representing the hexaploid bread wheat genome in 21 pseudomolecules has been analyzed to identify the distribution and genomic context of coding and noncoding elements across the A, B, and D subgenomes. With an estimated coverage of 94% of the genome and containing 107,891 high-confidence gene models, this assembly enabled the discovery of tissue- and developmental stage–related coexpression networks by providing a transcriptome atlas representing major stages of wheat development. Dynamics of complex gene families involved in environmental adaptation and end-use quality were revealed at subgenome resolution and contextualized to known agronomic single-gene or quantitative trait loci. This community resource establishes the foundation for accelerating wheat research and application through improved understanding of wheat biology and genomics-assisted breeding.",
                "authors": "R. Appels, K. Eversole, N. Stein, C. Feuillet, B. Keller, J. Rogers, C. Pozniak, F. Choulet, A. Distelfeld, J. Poland, G. Ronen, A. Sharpe, Omer Barad, Kobi Baruch, G. Keeble-Gagnère, M. Mascher, Gil Ben-Zvi, A. Josselin, A. Himmelbach, F. Balfourier, Juan J. Gutierrez-Gonzalez, M. Hayden, C. Koh, G. Muehlbauer, R. Pasam, E. Paux, P. Rigault, J. Tibbits, V. Tiwari, M. Spannagl, D. Lang, H. Gundlach, G. Haberer, K. Mayer, D. Ormanbekova, Verena M. Prade, H. Šimková, T. Wicker, D. Swarbreck, Hélène Rimbert, Marius Felder, N. Guilhot, G. Kaithakottil, J. Keilwagen, P. Leroy, Thomas M. Lux, S. Twardziok, Luca Venturini, A. Juhász, M. Abrouk, I. Fischer, C. Uauy, P. Borrill, R. Ramirez-Gonzalez, D. Arnaud, Smahane Chalabi, B. Chalhoub, A. Cory, R. Datla, M. Davey, J. Jacobs, S. J. Robinson, B. Steuernagel, F. van Ex, B. Wulff, M. Benhamed, A. Bendahmane, L. Concia, D. Latrasse, J. Bartoš, A. Bellec, H. Bergès, J. Doležel, Z. Frenkel, B. Gill, A. Korol, T. Letellier, O. Olsen, Kuldeep Singh, M. Valárik, E. V. D. van der Vossen, S. Vautrin, S. Weining, T. Fahima, Vladimir Glikson, Dina Raats, J. Číhalíková, Helena Toegelová, J. Vrána, P. Sourdille, Benoît Darrier, D. Barabaschi, L. Cattivelli, P. Hernández, Sergio Gálvez, H. Budak, Jonathan D. G. Jones, Kamil Witek, Guotai Yu, I. Small, J. Melonek, Ruonan Zhou, T. Belova, K. Kanyuka, R. King, K. Nilsen, S. Walkowiak, R. Cuthbert, R. Knox, Krystalee Wiebe, D. Xiang, A. Rohde, T. Golds, J. Čížková, B. A. Akpınar, Sezgi Biyiklioglu, Liangliang Gao, Amidou N'Daiye, M. Kubaláková, J. Šafář, Françoise Alfama, A. Adam-Blondon, Raphael Flores, Claire Guerche, M. Loaec, H. Quesneville, J. Condie, J. Ens, Ron Maclachlan, Yifang Tan, A. Alberti, J. Aury, V. Barbe, A. Couloux, C. Cruaud, K. Labadie, S. Mangenot, P. Wincker, G. Kaur, M. Luo, S. Sehgal, P. Chhuneja, O. Gupta, S. Jindal, Parampreet Kaur, Palvi Malik, Priti Sharma, B. Yadav, N. Singh, J. Khurana, Chanderkant Chaudhary, P. Khurana, Vinod Kumar, A. Mahato, S. Mathur, A. Sevanthi, N. Sharma, Ram Sewak Singh Tomar, Kateřina Holušová, O. Plíhal, M. Clark, D. Heavens, George Kettleborough, Jonathan Wright, Barbora Balcárková, Yuqin Hu, E. Salina, N. Ravin, K. Skryabin, A. Beletsky, V. Kadnikov, A. Mardanov, Michail A. Nesterov, A. Rakitin, E. Sergeeva, H. Handa, H. Kanamori, S. Katagiri, F. Kobayashi, S. Nasuda, Tsuyoshi Tanaka, Jianzhon Wu, F. Cattonaro, Min Jiumeng, Karl G. Kugler, M. Pfeifer, S. Sandve, Xu Xun, Bujie Zhan, J. Batley, P. Bayer, D. Edwards, S. Hayashi, Zuzana Tulpová, Paul Visendi, Licao Cui, Xianghong Du, Kewei Feng, Xiaojun Nie, W. Tong, Le Wang",
                "citations": 2361
            },
            {
                "title": "Normalizing Flows for Probabilistic Modeling and Inference",
                "abstract": "Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.",
                "authors": "G. Papamakarios, Eric T. Nalisnick, Danilo Jimenez Rezende, S. Mohamed, Balaji Lakshminarayanan",
                "citations": 1463
            },
            {
                "title": "A quantum engineer's guide to superconducting qubits",
                "abstract": "The aim of this review is to provide quantum engineers with an introductory guide to the central concepts and challenges in the rapidly accelerating field of superconducting quantum circuits. Over the past twenty years, the field has matured from a predominantly basic research endeavor to one that increasingly explores the engineering of larger-scale superconducting quantum systems. Here, we review several foundational elements -- qubit design, noise properties, qubit control, and readout techniques -- developed during this period, bridging fundamental concepts in circuit quantum electrodynamics (cQED) and contemporary, state-of-the-art applications in gate-model quantum computation.",
                "authors": "P. Krantz, M. Kjaergaard, F. Yan, T. Orlando, S. Gustavsson, W. Oliver",
                "citations": 1339
            },
            {
                "title": "Rethinking some of the rethinking of partial least squares",
                "abstract": "Partial least squares structural equation modeling (PLS-SEM) is an important statistical technique in the toolbox of methods that researchers in marketing and other social sciences disciplines frequently use in their empirical analyses. The purpose of this paper is to shed light on several misconceptions that have emerged as a result of the proposed “new guidelines” for PLS-SEM. The authors discuss various aspects related to current debates on when or when not to use PLS-SEM, and which model evaluation metrics to apply. In addition, this paper summarizes several important methodological extensions of PLS-SEM researchers can use to improve the quality of their analyses, results and findings.The paper merges literature from various disciplines, including marketing, strategic management, information systems, accounting and statistics, to present a state-of-the-art review of PLS-SEM. Based on these findings, the paper offers a point of orientation on how to consider and apply these latest developments when executing or assessing PLS-SEM-based research.This paper offers guidance regarding situations that favor the use of PLS-SEM and discusses the need to consider certain model evaluation metrics. It also summarizes how to deal with endogeneity in PLS-SEM, and critically comments on the recent proposal to adjust PLS-SEM estimates to mimic common factor models that are the foundation of covariance-based SEM. Finally, this paper opposes characterizing common concepts and practices of PLS-SEM as “out-of-date” without providing well-substantiated alternatives and solutions.The paper paves the way for future discussions and suggests a way forward to reach consensus regarding situations that favor PLS-SEM use and its application.This paper offers guidance on how to consider the latest methodological developments when executing or assessing PLS-SEM-based research.This paper complements recently proposed “new guidelines” with the aim of offering a counter perspective on some strong claims made in the latest literature on PLS-SEM. It also clarifies some misconceptions regarding the application of PLS-SEM.",
                "authors": "Joseph F. Hair, M. Sarstedt, C. Ringle",
                "citations": 880
            },
            {
                "title": "Video-based AI for beat-to-beat assessment of cardiac function",
                "abstract": null,
                "authors": "David Ouyang, B. He, Amirata Ghorbani, N. Yuan, J. Ebinger, C. Langlotz, P. Heidenreich, R. Harrington, D. Liang, E. Ashley, J. Zou",
                "citations": 585
            },
            {
                "title": "The walkthrough method: An approach to the study of apps",
                "abstract": "Software applications (apps) are now prevalent in the digital media environment. They are the site of significant sociocultural and economic transformations across many domains, from health and relationships to entertainment and everyday finance. As relatively closed technical systems, apps pose new methodological challenges for sociocultural digital media research. This article describes a method, grounded in a combination of science and technology studies with cultural studies, through which researchers can perform a critical analysis of a given app. The method involves establishing an app’s environment of expected use by identifying and describing its vision, operating model and modes of governance. It then deploys a walkthrough technique to systematically and forensically step through the various stages of app registration and entry, everyday use and discontinuation of use. The walkthrough method establishes a foundational corpus of data upon which can be built a more detailed analysis of an app’s intended purpose, embedded cultural meanings and implied ideal users and uses. The walkthrough also serves as a foundation for further user-centred research that can identify how users resist these arrangements and appropriate app technology for their own purposes.",
                "authors": "B. Light, J. Burgess, Stefanie Duguay",
                "citations": 621
            },
            {
                "title": "Squeeze-and-Excitation Networks",
                "abstract": "The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of <inline-formula><tex-math notation=\"LaTeX\">${\\sim }$</tex-math><alternatives><mml:math><mml:mo>∼</mml:mo></mml:math><inline-graphic xlink:href=\"shen-ieq1-2913372.gif\"/></alternatives></inline-formula>25 percent. Models and code are available at <uri>https://github.com/hujie-frank/SENet</uri>.",
                "authors": "Jie Hu, Li Shen, Samuel Albanie, Gang Sun, E. Wu",
                "citations": 896
            },
            {
                "title": "Technology acceptance model in educational context: A systematic literature review",
                "abstract": "A respectable amount of work dealing with Technology Acceptance Model (TAM) clearly indicates a popularity of TAM in the field of technology acceptance in general. Nevertheless, there is still a gap in existing knowledge regarding representative academic literature that underlie research on TAM in educational context. The main objective of this systematic literature review is to provide an overview of the current state of research efforts on TAM application in the field of learning and teaching for a variety of learning domains, learning technologies and types of users. Through systematic search by the use of EBSCO Discovery Service, the review has identified 71 relevant studies ranged between 2003 and 2018. The main findings indicate that TAM and its many different versions represent a credible model for facilitating assessment of diverse learning technologies. TAM's core variables, perceived ease of use and perceived usefulness, have been proven to be antecedent factors affecting acceptance of learning with technology. The paper identifies some gaps in current work and suggests areas for further investigation. The results of this systematic review provide a better understanding of TAM acceptance studies in educational context and create a firm foundation for advancing knowledge in the field. Practitioner NotesWhat is already known about this topic Technology acceptance research in teaching and learning context has become an attractive trend.A number of reviews and meta‐analysis focused on specific topics related to technology acceptance in education have been conducted.The Technology Acceptance Model (TAM) is the key model in understanding predictors of human behaviour towards potential acceptance or rejection of the technology.What this paper adds The state of current research on Technology Acceptance Model application in educational context lacks comprehensive reviews addressing variety of learning domains, learning technologies and types of users.The paper presents systematic review of relevant academic literature on Technology Acceptance Model (TAM) in the field of learning and teaching.The paper provides empirical evidence on the predictive validity of the models based on TAM presented in selected literature.The findings revealed that TAM, along with its many different versions called TAM++, is a leading scientific paradigm and credible model for facilitating assessment of diverse technological deployments in educational context.TAM's core variables, perceived ease of use and perceived usefulness, have been proven to be antecedent factors that have affected acceptance of learning with technology.Implications for practice and/or policy The systematic review adds to the body of knowledge and creates a firm foundation for advancing knowledge in the field.By following the most common research objectives and/or by filling current gaps in applied research methods, chosen sample groups and types of result analysis, an own study could be conducted.Future research may well focus on identifying additional external factors that could further explain acceptance and usage of various learning technologies. [ABSTRACT FROM AUTHOR] uracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",
                "authors": "A. Granić, Nikola Marangunic",
                "citations": 612
            },
            {
                "title": "Blockchain for Supply Chain Traceability: Business Requirements and Critical Success Factors",
                "abstract": "We seek to guide operations management (OM) research on the implementation of supply chain traceability systems by identifying business requirements and the factors critical to successful implementation. We first motivate the need for implementing traceability systems in two very different industries—cobalt mining and pharmaceuticals—and present business requirements and critical success factors for implementation. Next, we describe how we carried out thematic analysis of practitioner and scholarly articles on implementing blockchain for supply chain traceability. Finally, we present our results pertaining to the needs of different stakeholders such as suppliers, consumers, and regulators. The business requirements for traceability systems are curbing illegal practices; improving sustainability performance; increasing operational efficiency; enhancing supply‐chain coordination; and sensing market trends. Critical success factors for implementation are companies’ capabilities; collaboration; technology maturity; supply chain practices; leadership; and governance of the traceability efforts. These findings provide a nascent measurement model for empirical work and a foundation for descriptive and normative research on blockchain applications for supply chain traceability.",
                "authors": "Gabriella M. Hastig, M. Sodhi",
                "citations": 532
            },
            {
                "title": "The Supply Chain Has No Clothes: Technology Adoption of Blockchain for Supply Chain Transparency",
                "abstract": "Blockchain technology, popularized by Bitcoin cryptocurrency, is characterized as an open-source, decentralized, distributed database for storing transaction information. Rather than relying on centralized intermediaries (e.g., banks) this technology allows two parties to transact directly using duplicate, linked ledgers called blockchains. This makes transactions considerably more transparent than those provided by centralized systems. As a result, transactions are executed without relying on explicit trust [of a third party], but on the distributed trust based on the consensus of the network (i.e., other blockchain users). Applying this technology to improve supply chain transparency has many possibilities. Every product has a long and storied history. However, much of this history is presently obscured. Often, when negative practices are exposed, they quickly escalate to scandalous, and financially crippling proportions. There are many recent examples, such as the exposure of child labor upstream in the manufacturing process and the unethical use of rainforest resources. Blockchain may bring supply chain transparency to a new level, but presently academic and managerial adoption of blockchain technologies is limited by our understanding. To address this issue, this research uses the Unified Theory of Acceptance and Use of Technology (UTAUT) and the concept of technology innovation adoption as a foundational framework for supply chain traceability. A conceptual model is developed and the research culminates with supply chain implications of blockchain that are inspired by theory and literature review.",
                "authors": "Kristoffer Francisco, David Swanson",
                "citations": 687
            },
            {
                "title": "Global land use / land cover with Sentinel 2 and deep learning",
                "abstract": "Land use/land cover (LULC) maps are foundational geospatial data products needed by analysts and decision makers across governments, civil society, industry, and finance to monitor global environmental change and measure risk to sustainable livelihoods and development. There is a strong need for high-level, automated geospatial analysis products that turn these pixels into actionable insights for non-geospatial experts. The Sentinel 2 satellites, first launched in mid-2015, are excellent candidates for LULC mapping due to their high spatial, spectral, and temporal resolution. Advances in deep learning and scalable cloud-based compute now provide the analysis capability required to unlock the value in global satellite imagery observations. Based on a novel, very large dataset of over 5 billion human-labeled Sentinel-2 pixels, we developed and deployed a deep learning segmentation model on Sentinel-2 data to create a global LULC map at 10m resolution that achieves state-of-the-art accuracy and enables automated LULC mapping from time series observations.",
                "authors": "Krishna Karra, C. Kontgis, Zoe Statman-Weil, Joseph C. Mazzariello, M. Mathis, S. Brumby",
                "citations": 461
            },
            {
                "title": "Global, Regional, and National Burden of Rheumatic Heart Disease, 1990–2015",
                "abstract": "BACKGROUND Rheumatic heart disease remains an important preventable cause of cardiovascular death and disability, particularly in low‐income and middle‐income countries. We estimated global, regional, and national trends in the prevalence of and mortality due to rheumatic heart disease as part of the 2015 Global Burden of Disease study. METHODS We systematically reviewed data on fatal and nonfatal rheumatic heart disease for the period from 1990 through 2015. Two Global Burden of Disease analytic tools, the Cause of Death Ensemble model and DisMod‐MR 2.1, were used to produce estimates of mortality and prevalence, including estimates of uncertainty. RESULTS We estimated that there were 319,400 (95% uncertainty interval, 297,300 to 337,300) deaths due to rheumatic heart disease in 2015. Global age‐standardized mortality due to rheumatic heart disease decreased by 47.8% (95% uncertainty interval, 44.7 to 50.9) from 1990 to 2015, but large differences were observed across regions. In 2015, the highest age‐standardized mortality due to and prevalence of rheumatic heart disease were observed in Oceania, South Asia, and central sub‐Saharan Africa. We estimated that in 2015 there were 33.4 million (95% uncertainty interval, 29.7 million to 43.1 million) cases of rheumatic heart disease and 10.5 million (95% uncertainty interval, 9.6 million to 11.5 million) disability‐adjusted life‐years due to rheumatic heart disease globally. CONCLUSIONS We estimated the global disease prevalence of and mortality due to rheumatic heart disease over a 25‐year period. The health‐related burden of rheumatic heart disease has declined worldwide, but high rates of disease persist in some of the poorest regions in the world. (Funded by the Bill and Melinda Gates Foundation and the Medtronic Foundation.)",
                "authors": "D. Watkins, C. Johnson, Samantha M. Colquhoun, G. Karthikeyan, A. Beaton, G. Bukhman, M. Forouzanfar, C. Longenecker, B. Mayosi, G. Mensah, B. Nascimento, A. Ribeiro, C. Sable, A. Steer, M. Naghavi, A. Mokdad, C. Murray, T. Vos, J. Carapetis, Gregory A. Roth",
                "citations": 830
            },
            {
                "title": "Recent Advances in the Catalytic Oxidation of Volatile Organic Compounds: A Review Based on Pollutant Sorts and Sources.",
                "abstract": "It is well known that urbanization and industrialization have resulted in the rapidly increasing emissions of volatile organic compounds (VOCs), which are a major contributor to the formation of secondary pollutants (e.g., tropospheric ozone, PAN (peroxyacetyl nitrate), and secondary organic aerosols) and photochemical smog. The emission of these pollutants has led to a large decline in air quality in numerous regions around the world, which has ultimately led to concerns regarding their impact on human health and general well-being. Catalytic oxidation is regarded as one of the most promising strategies for VOC removal from industrial waste streams. This Review systematically documents the progresses and developments made in the understanding and design of heterogeneous catalysts for VOC oxidation over the past two decades. It addresses in detail how catalytic performance is often drastically affected by the pollutant sources and reaction conditions. It also highlights the primary routes for catalyst deactivation and discusses protocols for their subsequent reactivation. Kinetic models and proposed oxidation mechanisms for representative VOCs are also provided. Typical catalytic reactors and oxidizers for industrial VOC destruction are further discussed. We believe that this Review will provide a great foundation and reference point for future design and development in this field.",
                "authors": "Chi He, Jie Cheng, Xin Zhang, Mark Douthwaite, Samuel Pattisson, Z. Hao",
                "citations": 1375
            },
            {
                "title": "Testing the nature of dark compact objects: a status report",
                "abstract": null,
                "authors": "V. Cardoso, P. Pani",
                "citations": 641
            },
            {
                "title": "Defining, Conceptualising and Measuring the Digital Economy",
                "abstract": "The digital economy is growing fast, especially in developing countries. Yet the meaning and metrics of the digital economy are both limited and divergent. The aim of this paper is to review what is currently known in order to develop a definition of the digital economy, and an estimate of its size. The paper argues there are three scopes of relevance. The core of the digital economy is the ‘digital sector’: the IT/ICT sector producing foundational digital goods and services. The true ‘digital economy’ - defined as “that part of economic output derived solely or primarily from digital technologies with a business model based on digital goods or services” - consists of the digital sector plus emerging digital and platform services. The widest scope - use of ICTs in all economic fields - is here referred to as the ‘digitalised economy’. Following a review of measurement challenges, the paper estimates the digital economy as defined here to make up around 5% of global GDP and 3% of global employment. Behind this lies significant unevenness: the global North has had the lion’s share of the digital economy to date, but growth rates are fastest in the global South. Yet potential growth could be much higher: further research to understand more about the barriers to and impacts of the digital economy in developing countries is therefore a priority.",
                "authors": "Rumana Bukht, Richard Heeks",
                "citations": 608
            },
            {
                "title": "The system of professions",
                "abstract": "What is an ‘information professional’? For that matter, what is a ‘professional’? These questions are awkward because they are difficult to answer with precision or certainty. ‘The professions’ have been an object of study by sociologists for many decades, but only one author has meaningfully ventured into the information professions. The System of Professions is now somewhat elderly, but the foundational theory developed here is very well-suited to a field like Information – weakly delineated and continually evolving. The purpose of this review is to explore whether this classic sociological work, following its thirtieth birthday, might have new relevance to students and practitioners of both traditional and emerging information professions, given their rapidly changing work environments. Andrew Abbott is a professor of Sociology at the University of Chicago, and he served as the long-time editor of the American Journal of Sociology (2000–2016). This was his first book, notably winning the prestigious American Sociology Association’s Distinguished Scholarly Book Award (1991). Abbott’s own doctoral research had focused on the emergence of psychiatry as a profession; it was from this work that he abstracted his general conceptual model of professions as a ‘survival-of-thefittest’ ecology, and one that is set in a frequently changing social, epistemological, and technical landscape. Abbott’s distinctive contribution to the discourse is to methodically define professions wholly in terms of an elbows-out application of expertise; professions compete with each other for expertise-based jurisdiction over solvable problems. Competition",
                "authors": "Colin D. Furness",
                "citations": 408
            },
            {
                "title": "The role of digital technologies for the service transformation of industrial companies",
                "abstract": "The role of digital technologies in service business transformation is under-investigated. This paper contributes to filling this gap by addressing how the Internet of things (IoT), cloud computing (CC) and predictive analytics (PA) facilitate service transformation in industrial companies. Through the Data–Information–Knowledge–Wisdom (DIKW) model, we discuss how the abovementioned technologies transform low-level entities such as data into information and knowledge to support the service transformation of manufacturers. We propose a set of digital capabilities, based on the extant literature and the findings from four case studies. Then, we discuss how these capabilities support the service transformation trajectories of manufacturers. We find that IoT is foundational to any service transformation, although it is mostly needed to become an availability provider. PA is essential for moving to the performance provider profile. Besides providing scalability in all profiles, CC is specifically used to implement an industrialiser strategy, therefore leading to standardised, repeatable and productised offerings.",
                "authors": "M. Ardolino, M. Rapaccini, N. Saccani, Paolo Gaiardelli, Giovanni Crespi, Carlo Ruggeri",
                "citations": 511
            },
            {
                "title": "Explainable Deep Learning: A Field Guide for the Uninitiated",
                "abstract": "Deep neural networks (DNNs) are an indispensable machine learning tool despite the difficulty of diagnosing what aspects of a model’s input drive its decisions. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a DNN’s decisions has thus blossomed into an active and broad area of research. The field’s complexity is exacerbated by competing definitions of what it means “to explain” the actions of a DNN and to evaluate an approach’s “ability to explain”. This article offers a field guide to explore the space of explainable deep learning for those in the AI/ML field who are uninitiated. The field guide: i) Introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) discusses user-oriented explanation design and future directions. We hope the guide is seen as a starting point for those embarking on this research field.",
                "authors": "Ning Xie, Gabrielle Ras, M. Gerven, Derek Doran",
                "citations": 340
            },
            {
                "title": "A supply chain transparency and sustainability technology appraisal model for blockchain technology",
                "abstract": "Blockchain technology is a technology that can effectively support supply chain transparency. An important initial managerial activity is for organisations in supply chains to evaluate and select the most suitable blockchain technology. However, uncertainty and emphasis on sustainable transparency has made this appraisal more complex. This paper: (1) introduces blockchain technology performance measures incorporating various sustainable supply chain transparency and technical attributes; and (2) introduces a new hybrid group decision method, integrated hesitant fuzzy set and regret theory, for blockchain technology evaluation and selection. This method emphasises decision maker psychological characteristics and variation in decision maker opinions. An illustrative application and sensitivity analysis is introduced to aid supply chain managers and researchers understand the blockchain technology selection decision. Methodological and managerial implications associated with the decision tool and application are introduced. This research sets the foundation for significant future research in blockchain technologies evaluation in a supply chain environment.",
                "authors": "Chunguang Bai, J. Sarkis",
                "citations": 422
            },
            {
                "title": "Simulation of Growth Trajectories of Childhood Obesity into Adulthood",
                "abstract": "BACKGROUND Although the current obesity epidemic has been well documented in children and adults, less is known about long‐term risks of adult obesity for a given child at his or her present age and weight. We developed a simulation model to estimate the risk of adult obesity at the age of 35 years for the current population of children in the United States. METHODS We pooled height and weight data from five nationally representative longitudinal studies totaling 176,720 observations from 41,567 children and adults. We simulated growth trajectories across the life course and adjusted for secular trends. We created 1000 virtual populations of 1 million children through the age of 19 years that were representative of the 2016 population of the United States and projected their trajectories in height and weight up to the age of 35 years. Severe obesity was defined as a body‐mass index (BMI, the weight in kilograms divided by the square of the height in meters) of 35 or higher in adults and 120% or more of the 95th percentile in children. RESULTS Given the current level of childhood obesity, the models predicted that a majority of today's children (57.3%; 95% uncertainly interval [UI], 55.2 to 60.0) will be obese at the age of 35 years, and roughly half of the projected prevalence will occur during childhood. Our simulations indicated that the relative risk of adult obesity increased with age and BMI, from 1.17 (95% UI, 1.09 to 1.29) for overweight 2‐year‐olds to 3.10 (95% UI, 2.43 to 3.65) for 19‐year‐olds with severe obesity. For children with severe obesity, the chance they will no longer be obese at the age of 35 years fell from 21.0% (95% UI, 7.3 to 47.3) at the age of 2 years to 6.1% (95% UI, 2.1 to 9.9) at the age of 19 years. CONCLUSIONS On the basis of our simulation models, childhood obesity and overweight will continue to be a major health problem in the United States. Early development of obesity predicted obesity in adulthood, especially for children who were severely obese. (Funded by the JPB Foundation and others.)",
                "authors": "Z. Ward, M. Long, S. Resch, C. Giles, A. Cradock, S. Gortmaker",
                "citations": 507
            },
            {
                "title": "MolTrans: Molecular Interaction Transformer for drug–target interaction prediction",
                "abstract": "Abstract Motivation Drug–target interaction (DTI) prediction is a foundational task for in-silico drug discovery, which is costly and time-consuming due to the need of experimental search over large drug compound space. Recent years have witnessed promising progress for deep learning in DTI predictions. However, the following challenges are still open: (i) existing molecular representation learning approaches ignore the sub-structural nature of DTI, thus produce results that are less accurate and difficult to explain and (ii) existing methods focus on limited labeled data while ignoring the value of massive unlabeled molecular data. Results We propose a Molecular Interaction Transformer (MolTrans) to address these limitations via: (i) knowledge inspired sub-structural pattern mining algorithm and interaction modeling module for more accurate and interpretable DTI prediction and (ii) an augmented transformer encoder to better extract and capture the semantic relations among sub-structures extracted from massive unlabeled biomedical data. We evaluate MolTrans on real-world data and show it improved DTI prediction performance compared to state-of-the-art baselines. Availability and implementation The model scripts are available at https://github.com/kexinhuang12345/moltrans. Supplementary information Supplementary data are available at Bioinformatics online.",
                "authors": "Kexin Huang, Cao Xiao, Lucas Glass, Jimeng Sun",
                "citations": 259
            },
            {
                "title": "Modeling the ACMG/AMP Variant Classification Guidelines as a Bayesian Classification Framework",
                "abstract": null,
                "authors": "S. Tavtigian, M. Greenblatt, S. Harrison, R. Nussbaum, Snehit Prabhu, K. Boucher, L. Biesecker",
                "citations": 377
            },
            {
                "title": "Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids",
                "abstract": "Real-life control tasks involve matters of various substances---rigid or soft bodies, liquid, gas---each with distinct physical behaviors. This poses challenges to traditional rigid-body physics engines. Particle-based simulators have been developed to model the dynamics of these complex scenes; however, relying on approximation techniques, their simulation often deviates from real-world physics, especially in the long term. In this paper, we propose to learn a particle-based simulator for complex control tasks. Combining learning with particle-based systems brings in two major benefits: first, the learned simulator, just like other particle-based systems, acts widely on objects of different materials; second, the particle-based representation poses strong inductive bias for learning: particles of the same type have the same dynamics within. This enables the model to quickly adapt to new environments of unknown dynamics within a few observations. We demonstrate robots achieving complex manipulation tasks using the learned simulator, such as manipulating fluids and deformable foam, with experiments both in simulation and in the real world. Our study helps lay the foundation for robot learning of dynamic scenes with particle-based representations.",
                "authors": "Yunzhu Li, Jiajun Wu, Russ Tedrake, J. Tenenbaum, A. Torralba",
                "citations": 361
            },
            {
                "title": "Fast and accurate view classification of echocardiograms using deep learning",
                "abstract": null,
                "authors": "Ali Madani, R. Arnaout, M. Mofrad, Rima Arnaout",
                "citations": 381
            },
            {
                "title": "Cell-free gene expression: an expanded repertoire of applications",
                "abstract": null,
                "authors": "A. Silverman, Ashty S. Karim, M. Jewett",
                "citations": 398
            },
            {
                "title": "How do stomata respond to water status?",
                "abstract": "Stomatal responses to humidity, soil moisture and other factors that influence plant water status are critical drivers of photosynthesis, productivity, water yield, ecohydrology and climate forcing, yet we still lack a thorough mechanistic understanding of these responses. Here I review historical and recent advances in stomatal water relations. Clear evidence now implicates a metabolically mediated response to leaf water status ('hydroactive feedback') in stomatal responses to evaporative demand and soil drought, possibly involving abscisic acid production in leaves. Other hypothetical mechanisms involving vapor and heat transport within leaves may contribute to humidity, light and temperature responses, but require further theoretical clarification and experimental validation. Variation and dynamics in hydraulic conductance, particularly within leaves, may contribute to water status responses. Continuing research to fully resolve mechanisms of stomatal responses to water status should focus on several areas: validating and quantifying the mechanism of leaf-based hydroactive feedback, identifying where in leaves water status is actively sensed, clarifying the role of leaf vapor and energy transport in humidity and temperature responses, and verifying foundational but minimally replicated results of stomatal hydromechanics across species. Clarity on these matters promises to deliver modelers with a tractable and reliable mechanistic model of stomatal responses to water status.",
                "authors": "T. Buckley",
                "citations": 337
            },
            {
                "title": "A Formal Security Analysis of the Signal Messaging Protocol",
                "abstract": null,
                "authors": "Katriel Cohn-Gordon, Cas J. F. Cremers, Benjamin Dowling, L. Garratt, D. Stebila",
                "citations": 294
            },
            {
                "title": "Revisiting aspiration and ability in international migration",
                "abstract": "ABSTRACT It is a refreshingly simple thought that migration is the combined result of two factors: the aspiration to migrate and the ability to migrate. Without having to resort to overly structural or individualistic explanations, this analytical distinction helps disentangle complex questions around why some people migrate but others do not. Still, aspiration and ability raise their own thorny theoretical and methodological questions. To begin with, what does it mean to have migration aspirations? How can such concepts be objects of empirical research? And is it meaningful to say that individuals possess the ability to migrate if their preference is to stay? The aspiration/ability model was originally proposed in this journal and has since been diversely applied and adapted. In this article, we look back at more than a decade of research to examine a series of theoretical and empirical developments related to the aspiration/ability model and its extensions. We identify two-step approaches as a class of analytical frameworks that share the basic logic of the aspiration/ability model. Covering expansive theoretical, methodological and empirical ground, we seek to lay a foundation for new research on global migration in its diverse forms. Video abstract Read the transcript Watch the video on Vimeo",
                "authors": "J. Carling, K. Schewel",
                "citations": 337
            },
            {
                "title": "Development of Foundational Movement Skills: A Conceptual Model for Physical Activity Across the Lifespan",
                "abstract": null,
                "authors": "Ryan M. Hulteen, P. Morgan, L. Barnett, D. Stodden, D. Lubans",
                "citations": 308
            },
            {
                "title": "Explaining by Removing: A Unified Framework for Model Explanation",
                "abstract": "Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We establish a new class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's influence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 25 existing methods, including several of the most widely used approaches (SHAP, LIME, Meaningful Perturbations, permutation tests). This new class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.",
                "authors": "Ian Covert, Scott M. Lundberg, Su-In Lee",
                "citations": 221
            },
            {
                "title": "Big data and business analytics ecosystems: paving the way towards digital transformation and sustainable societies",
                "abstract": null,
                "authors": "I. Pappas, Patrick Mikalef, M. Giannakos, J. Krogstie, George Lekakos",
                "citations": 289
            },
            {
                "title": "Three Concerns With Applying a Bifactor Model as a Structure of Psychopathology",
                "abstract": "Recently, bifactor modeling applications in clinical measurement have proliferated (e.g., Caspi et al., 2014; Lahey et al., 2012; Simms, Grös, Watson, & O’Hara, 2008; Vanheule, Desmet, Groenvynck, Rosseel, & Fontaine, 2008). It is critical, however, to distinguish between two types of applications. The first focuses on using the bifactor model as a tool for understanding the psychometrics of an assessment scale (see Rodriguez, Reise, & Haviland, 2016a, 2016b). This type has proven invaluable for informing the degree to which a measure yields an univocal total score (Reise, Moore, & Haviland, 2010) and, relatedly, the extent to which subscales representing theoretically distinct constructs (i.e., group factors) yield reliable scores after accounting for the general factor (Reise, Bonifay, & Haviland, 2013). The second is far more ambitious and leverages a bifactor model to represent the general and group factor structure of an entire domain of psychological functioning. This is the type in question in the target article ( Snyder, Young, & Hankin, 2016, this issue), wherein correlations among psychopathology items are modeled as reflecting a single psychopathology dimension, p, as well as orthogonal internalizing/externalizing group factors. In this and related studies, it is the structure, rather than the psychometric properties, that is of paramount theoretical interest. We raise three issues with bifactor modeling as it is applied to the “structure of psychopathology”— interpretability, model fit, and validation—and we point to recent psychometric tools for bifactor model evaluation. In the initial development of the bifactor method, Holzinger and Swineford (1937) stated that group factors are derived from the residual correlations that remain after extracting the general factor. Although estimation methods have changed radically since 1937, interpretation of group factors that are orthogonal to a general factor remains challenging. In certain applications (e.g., Cho, Cohen, & Kim, 2014), group factors are not viewed as meaningful subconstructs of a test but rather as methodological “nuisances” that impede measurement of the primary construct of interest. Assuming, however, that group factors are meaningful, how should they be interpreted? They must be construed as substantively unique, measuring subconstructs exclusive to the general factor. Snyder et al. (2016, this issue) and others conclude that the structure of psychopathology includes internalizing/ externalizing group factors; it is unclear whether, or to what degree, these factors can be interpreted as traits orthogonal to the p factor. It may also be difficult to interpret the general dimension in a bifactor model. Previous research has noted that a positive manifold does not imply a single general causal structure (e.g., a single neuropsychobiological structure that causes variation across content-diverse indicators; van der Maas et al., 2006). Although strong correlations among measures may suggest a bifactor structure, that does not imply that such a structure exists at the genotypic level (e.g., Cohen, Cohen, Teresi, Marchi, & Velez, 1990). Thus, it could be that the internalizing/ externalizing factors identified by Snyder et al. (2016, this issue) are interpreted correctly but that the emergence of a general p factor, rather than being generated by a single general latent trait, is the result of some different process altogether. Researchers must carefully investigate such issues before considering the bifactor model seriously as a foundational structure for clinical research. Of particular concern is the bifactor model’s tendency to show superior goodness of fit in model comparison studies. In Snyder et al. (2016, this issue), the bifactor structure outperformed both the unidimensional and the two (correlated) factor alternatives regarding goodness of fit and was thereby selected as the best representation of psychopathology. However, the superior performance of the bifactor model may be a symptom of “overfitting”— that is, modeling not only the important trends in the 657069 CPXXXX10.1177/2167702616657069Bonifay et al.Bifactor Model as a Structure of Psychopathology research-article2016",
                "authors": "Wes Bonifay, S. Lane, S. Reise",
                "citations": 315
            },
            {
                "title": "Chemical",
                "abstract": "Address: Casilla de Correo 16, Sucursal 4, 1900 La Plata, Argentina. Phone: (+54 21) 257430; 257291; Telex: BULAP AR 31151; CESLA AR 31216; Fax: (+54 21) 254642; E-mail: ajarvia@inifta.edu.ar. Director/Head: Alejandro Jorge Arvia. Number of Research Scientists: 93; Number of Staff: 22. Scientific Fields of Interest: Energy; Materials; Chemistry; Environment. Main Lines of Research and Training Activities: Gas phase and solution reaction kinetics; photochemistry; physical and chemical adsorption; surface chemistry related to heterogeneous catalysis; electrochemistry; electrocatalysis; spectroelectrochemistry; metal electrodeposition; phase change phenomena and growth patterns; metallic corrosion and passivity; electroless coatings; bioelectrochemistry; electrochemical energy conversion and storage; physical chemistry of natural and synthetic polymers; polymerization reactions; modelling of chemical and physical processes. Major Scientific Results or Products: Each year, more than 60 articles published in refereed international journals; since 1946, supervised 150 doctoral approved by University of La Plata and other universities in Argentina and foreign countries. Main Research Facilities Available: Auger spectrometer; mass spectrometer; IR and UV spectrometers for bulk and surface reflection studies; transmission electronic microscope; atomic force microscopy; scanning tunnelling microscope; X-ray diffractometer; nuclear magnetic resonance equipment; electron paramagnetic resonance equipment; electronic equipment and facilities for electrochemical studies; vacuum equipment for thin metal film studies; advanced equipment for polymer science research; 30 PCs; glass-blowing facilities; mechanical workshop; library. Future Development Plans: To continue projects in main research lines. Cooperation Arrangements with Developing Countries: Cooperation agreements and scientific exchange programmes with universities and research centres in Uruguay, Brazil, Chile, Mexico, Colombia, Venezuela, Bolivia, Panama, Peru and Saudi Arabia. Participant in TWAS Associate Membership Scheme in Centres of Excellence in South. Other International Cooperation Arrangements: Joint research programmes and scientific exchange with institutes and centres in US, Germany, France, England, Japan, Italy and Spain through Argentine National Research Council (CONICET), National Science Foundation, US, CNRS, France, British Council, Royal Society, England, DAAD, von Humboldt Foundation, Volkswagenwerk Foundation, Germany, JICA, Japan, CSIC, Spain, TWAS, Italy, and Organization of the American States (OAS).",
                "authors": "A. J. Arvia",
                "citations": 229
            },
            {
                "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization",
                "abstract": "Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.",
                "authors": "Zijie J. Wang, Robert Turko, Omar Shaikh, Haekyu Park, Nilaksh Das, Fred Hohman, Minsuk Kahng, Duen Horng Chau",
                "citations": 213
            },
            {
                "title": "Ensemble machine learning approach for classification of IoT devices in smart home",
                "abstract": null,
                "authors": "Ivan Cvitić, Dragan Peraković, Marko Periša, B. Gupta",
                "citations": 151
            },
            {
                "title": "An Overview of Dynamic-Linearization-Based Data-Driven Control and Applications",
                "abstract": "A brief overview on the model-based control and data-driven control methods is presented. The data-driven equivalent dynamic linearization, as a foundational analysis tool of data-driven control methods for discrete-time nonlinear systems, is introduced in detail with motivations and distinct features. The prototype model-free adaptive control schemes by using the dynamic linearization to an unknown nonlinear plant model, as well as the alternative model-free adaptive control methods by using the dynamic linearization to an unknown ideal nonlinear controller, are discussed. Furthermore, the extensions of the dynamic linearization to unknown nonlinear repetitive systems and the corresponding model-free adaptive iterative learning control methods are also overviewed and summarized. This work highlights the characteristics and comments of the different model-free adaptive control schemes in detail to facilitate the understanding of the readers. Finally, some perspectives on data-driven control methods in information-rich age are given.",
                "authors": "Z. Hou, R. Chi, Huijun Gao",
                "citations": 316
            },
            {
                "title": "A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging: From the 2018 NIH/RSNA/ACR/The Academy Workshop.",
                "abstract": "Imaging research laboratories are rapidly creating machine learning systems that achieve expert human performance using open-source methods and tools. These artificial intelligence systems are being developed to improve medical image reconstruction, noise reduction, quality assurance, triage, segmentation, computer-aided detection, computer-aided classification, and radiogenomics. In August 2018, a meeting was held in Bethesda, Maryland, at the National Institutes of Health to discuss the current state of the art and knowledge gaps and to develop a roadmap for future research initiatives. Key research priorities include: 1, new image reconstruction methods that efficiently produce images suitable for human interpretation from source data; 2, automated image labeling and annotation methods, including information extraction from the imaging report, electronic phenotyping, and prospective structured image reporting; 3, new machine learning methods for clinical imaging data, such as tailored, pretrained model architectures, and federated machine learning methods; 4, machine learning methods that can explain the advice they provide to human users (so-called explainable artificial intelligence); and 5, validated methods for image de-identification and data sharing to facilitate wide availability of clinical imaging data sets. This research roadmap is intended to identify and prioritize these needs for academic research laboratories, funding agencies, professional societies, and industry.",
                "authors": "C. Langlotz, Bibb Allen, B. Erickson, Jayashree Kalpathy-Cramer, K. Bigelow, T. Cook, A. Flanders, M. Lungren, David S. Mendelson, J. Rudie, Ge Wang, K. Kandarpa",
                "citations": 269
            },
            {
                "title": "The Refined Consensus Model of Pedagogical Content Knowledge in Science Education",
                "abstract": null,
                "authors": "Janet F. Carlson, Kirsten R. Daehler, Alicia C. Alonzo, E. Barendsen, A. Berry, Andreas Borowski, Jared Carpendale, K. Chan, Rebecca Cooper, Patricia J. Friedrichsen, Julie Gess-Newsome, Ineke Henze-Rietveld, A. Hume, Sophie Kirschner, Sven Liepertz, J. Loughran, Elizabeth Mavhunga, K. Neumann, P. Nilsson, Soonhye Park, Marissa Rollnick, Aaron J. Sickel, J. Suh, Rebecca Schneider, J. Driel, Christopher D. Wilson",
                "citations": 208
            },
            {
                "title": "The Neotoma Paleoecology Database, a multiproxy, international, community-curated data resource",
                "abstract": "Abstract The Neotoma Paleoecology Database is a community-curated data resource that supports interdisciplinary global change research by enabling broad-scale studies of taxon and community diversity, distributions, and dynamics during the large environmental changes of the past. By consolidating many kinds of data into a common repository, Neotoma lowers costs of paleodata management, makes paleoecological data openly available, and offers a high-quality, curated resource. Neotoma’s distributed scientific governance model is flexible and scalable, with many open pathways for participation by new members, data contributors, stewards, and research communities. The Neotoma data model supports, or can be extended to support, any kind of paleoecological or paleoenvironmental data from sedimentary archives. Data additions to Neotoma are growing and now include >3.8 million observations, >17,000 datasets, and >9200 sites. Dataset types currently include fossil pollen, vertebrates, diatoms, ostracodes, macroinvertebrates, plant macrofossils, insects, testate amoebae, geochronological data, and the recently added organic biomarkers, stable isotopes, and specimen-level data. Multiple avenues exist to obtain Neotoma data, including the Explorer map-based interface, an application programming interface, the neotoma R package, and digital object identifiers. As the volume and variety of scientific data grow, community-curated data resources such as Neotoma have become foundational infrastructure for big data science.",
                "authors": "John W. Williams, Eric C. Grimm, J. Blois, Donald F. Charles, Edward B. Davis, S. Goring, Russell W. Graham, Alison J. Smith, Michael Anderson, J. Arroyo‐Cabrales, Allan C. Ashworth, Julio L. Betancourt, B. Bills, Robert K. Booth, Philip I. Buckland, B. Brandon Curry, T. Giesecke, Stephen T. Jackson, C. Latorre, J. Nichols, T. Purdum, Robert E. Roth, Michael Stryker, H. Takahara",
                "citations": 246
            },
            {
                "title": "Identification of phytochemical inhibitors against main protease of COVID-19 using molecular modeling approaches",
                "abstract": "Abstract Severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) is a novel corona virus that causes corona virus disease 2019 (COVID-19). The COVID-19 rapidly spread across the nations with high mortality rate even as very little is known to contain the virus at present. In the current study, we report novel natural metabolites namely, ursolic acid, carvacrol and oleanolic acid as the potential inhibitors against main protease (Mpro) of COVID-19 by using integrated molecular modeling approaches. From a combination of molecular docking and molecular dynamic (MD) simulations, we found three ligands bound to protease during 50 ns of MD simulations. Furthermore, the molecular mechanic/generalized/Born/Poisson-Boltzmann surface area (MM/G/P/BSA) free energy calculations showed that these chemical molecules have stable and favourable energies causing strong binding with binding site of Mpro protein. All these three molecules, namely, ursolic acid, carvacrol and oleanolic acid, have passed the ADME (Absorption, Distribution, Metabolism, and Excretion) property as well as Lipinski’s rule of five. The study provides a basic foundation and suggests that the three phytochemicals, viz. ursolic acid, carvacrol and oleanolic acid could serve as potential inhibitors in regulating the Mpro protein’s function and controlling viral replication. Communicated by Ramaswamy H. Sarma",
                "authors": "Anuj Kumar, G. Choudhir, S. Shukla, Mansi M. Sharma, P. Tyagi, Arvind Bhushan, M. Rathore",
                "citations": 172
            },
            {
                "title": "Service Ecosystem Design: Propositions, Process Model, and Future Research Agenda",
                "abstract": "While service design has been highlighted as a promising approach for driving innovation, there are often struggles in realizing lasting change in practice. The issues with long-term implementation reveal a reductionist view of service design that ignores the institutional arrangements and other interdependencies that influence design efforts within multi-actor service systems. The purpose of this article is to build a systemic understanding of service design to inform actors’ efforts aimed at intentional, long-term change in service systems. To achieve this aim, we inform the conceptual building blocks of service design by applying service-dominant logic’s service ecosystems perspective. Through this process, we develop four core propositions and a multilevel process model of service ecosystem design. The conceptualization of service ecosystem design advances service design theory by illuminating previously taken for granted aspects; explaining how intentional, long-term change emerges; and expanding the scope of service design beyond projects. Furthermore, this research offers a foundation for future research on service design that involves extending the systemic conceptualization of service design, conducting more holistic empirical investigations, and developing practical methods and approaches for the embedded, collective processes of designing.",
                "authors": "J. Vink, Kaisa Koskela-Huotari, B. Tronvoll, B. Edvardsson, Katarina Wetter-Edman",
                "citations": 169
            },
            {
                "title": "A Test of the Cosmological Principle with Quasars",
                "abstract": "We study the large-scale anisotropy of the universe by measuring the dipole in the angular distribution of a flux-limited, all-sky sample of 1.36 million quasars observed by the Wide-field Infrared Survey Explorer (WISE). This sample is derived from the new CatWISE2020 catalog, which contains deep photometric measurements at 3.4 and 4.6 μm from the cryogenic, post-cryogenic, and reactivation phases of the WISE mission. While the direction of the dipole in the quasar sky is similar to that of the cosmic microwave background (CMB), its amplitude is over twice as large as expected, rejecting the canonical, exclusively kinematic interpretation of the CMB dipole with a p-value of 5 × 10−7 (4.9σ for a normal distribution, one-sided), the highest significance achieved to date in such studies. Our results are in conflict with the cosmological principle, a foundational assumption of the concordance ΛCDM model.",
                "authors": "N. Secrest, S. V. Hausegger, M. Rameez, R. Mohayaee, S. Sarkar, J. Colin",
                "citations": 168
            },
            {
                "title": "Unsupervised Machine Learning on a Hybrid Quantum Computer",
                "abstract": "Machine learning techniques have led to broad adoption of a statistical model of computing. The statistical distributions natively available on quantum processors are a superset of those available classically. Harnessing this attribute has the potential to accelerate or otherwise improve machine learning relative to purely classical performance. A key challenge toward that goal is learning to hybridize classical computing resources and traditional learning techniques with the emerging capabilities of general purpose quantum processors. Here, we demonstrate such hybridization by training a 19-qubit gate model processor to solve a clustering problem, a foundational challenge in unsupervised learning. We use the quantum approximate optimization algorithm in conjunction with a gradient-free Bayesian optimization to train the quantum machine. This quantum/classical hybrid algorithm shows robustness to realistic noise, and we find evidence that classical optimization can be used to train around both coherent and incoherent imperfections.",
                "authors": "J. Otterbach, R. Manenti, N. Alidoust, A. Bestwick, M. Block, B. Bloom, S. Caldwell, N. Didier, E. Fried, S. Hong, Peter J. Karalekas, C. Osborn, A. Papageorge, E. C. Peterson, G. Prawiroatmodjo, N. Rubin, C. Ryan, D. Scarabelli, M. Scheer, E. Sete, P. Sivarajah, Robert S. Smith, A. Staley, N. Tezak, W. Zeng, A. Hudson, Blake R. Johnson, M. Reagor, M. Silva, C. Rigetti",
                "citations": 277
            },
            {
                "title": "A Goal Congruity Model of Role Entry, Engagement, and Exit: Understanding Communal Goal Processes in STEM Gender Gaps",
                "abstract": "The goal congruity perspective provides a theoretical framework to understand how motivational processes influence and are influenced by social roles. In particular, we invoke this framework to understand communal goal processes as proximal motivators of decisions to engage in science, technology, engineering, and mathematics (STEM). STEM fields are not perceived as affording communal opportunities to work with or help others, and understanding these perceived goal affordances can inform knowledge about differences between (a) STEM and other career pathways and (b) women’s and men’s choices. We review the patterning of gender disparities in STEM that leads to a focus on communal goal congruity (Part I), provide evidence for the foundational logic of the perspective (Part II), and explore the implications for research and policy (Part III). Understanding and transmitting the opportunities for communal goal pursuit within STEM can reap widespread benefits for broadening and deepening participation.",
                "authors": "A. Diekman, Mia Steinberg, Elizabeth R. Brown, A. Belanger, E. Clark",
                "citations": 249
            },
            {
                "title": "Yi: Open Foundation Models by 01.AI",
                "abstract": "We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.",
                "authors": "01.AI Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",
                "citations": 379
            },
            {
                "title": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics",
                "abstract": "Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learning between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, ranging from 50M up to 2.5B parameters and integrating information from 3,202 diverse human genomes, as well as 850 genomes selected across diverse phyla, including both model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the developed models can be fine-tuned at low cost and despite low available data regime to solve a variety of genomics applications. Despite no supervision, the transformer models learned to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence. Code and weights available on GitHub in Jax and HuggingFace in Pytorch. Example notebooks to apply these models to any downstream task are available on HuggingFace.",
                "authors": "Hugo Dalla-torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolás López Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin J. Skwark, Karim Beguir, Marie Lopez, Thomas Pierrot",
                "citations": 143
            },
            {
                "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                "abstract": "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",
                "authors": "Chameleon Team",
                "citations": 117
            },
            {
                "title": "MOMENT: A Family of Open Time-series Foundation Models",
                "abstract": "We introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on Huggingface.",
                "authors": "Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, Artur Dubrawski",
                "citations": 60
            },
            {
                "title": "OpenEQA: Embodied Question Answering in the Era of Foundation Models",
                "abstract": "We present a modern formulation of Embodied Question Answering (EQA) as the task of understanding an environment well enough to answer questions about it in natural language. An agent can achieve such an understanding by either drawing upon episodic memory, exemplified by agents on smart glasses, or by actively exploring the environment, as in the case of mobile robots. We accompany our formulation with OpenEQA - the first open-vocabulary benchmark dataset for EQA supporting both episodic memory and active exploration use cases. OpenEQA contains over 1600 high-quality human generated questions drawn from over 180 real-world environments. In addition to the dataset, we also provide an automatic LLM-powered evaluation protocol that has excellent correlation with human judgement. Using this dataset and evaluation protocol, we evaluate several state-of-the-art foundation models including GPT-4V, and find that they significantly lag behind human-level performance. Consequently, OpenEQA stands out as a straightforward, measurable, and practically rele-vant benchmark that poses a considerable challenge to current generation offoundation models. We hope this inspires and stimulates future research at the intersection of Embod-ied AI, conversational agents, and world models.",
                "authors": "Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent-Pierre Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Alexander Sax, A. Rajeswaran",
                "citations": 59
            },
            {
                "title": "Movie Gen: A Cast of Media Foundation Models",
                "abstract": "We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",
                "authors": "Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Ki-ran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam S. Tsai, S. Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali K. Thabet, A. Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, Yuming Du",
                "citations": 46
            },
            {
                "title": "Probing the 3D Awareness of Visual Foundation Models",
                "abstract": "Recent advances in large-scale pretraining have yielded visual foundation models with strong capabilities. Not only can recent models generalize to arbitrary images for their training task, their intermediate representations are useful for other visual tasks such as detection and segmentation. Given that such models can classify, delineate, and local- ize objects in 2D, we ask whether they also represent their 3D structure? In this work, we analyze the 3D awareness of visual foundation models. We posit that 3D awareness implies that representations (1) encode the 3D structure of the scene and (2) consistently represent the surface across views. We conduct a series of experiments using task-specific probes and zero-shot inference procedures on frozen fea- tures. Our experiments reveal several limitations of the current models. Our code and analysis can be found at https://github.com/mbanani/probe3d.",
                "authors": "Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas J. Guibas, Justin Johnson, Varun Jampani",
                "citations": 49
            },
            {
                "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
                "abstract": "We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.",
                "authors": "Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen",
                "citations": 39
            },
            {
                "title": "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models",
                "abstract": "Foundation models (FMs) adapt surprisingly well to downstream tasks with fine-tuning. However, their colossal parameter space prohibits their training on resource-constrained edge-devices. For federated fine-tuning, we need to consider the smaller FMs of few billion parameters at most, namely on-device FMs (ODFMs), which can be deployed on-device. Federated fine-tuning of ODFMs has unique challenges non-present in standard fine-tuning: i) ODFMs poorly generalize to downstream tasks due to their limited sizes making proper fine-tuning imperative to their performance, and ii) devices have limited and heterogeneous system capabilities and data that can deter the performance of fine-tuning.Tackling these challenges, we propose HetLoRA, a feasible and effective federated fine-tuning method for ODFMs that leverages the system and data heterogeneity at the edge. HetLoRA allows heterogeneous LoRA ranks across clients for their individual system resources, and efficiently aggregates and distributes these LoRA modules in a data-aware manner by applying rank self-pruning locally and sparsity-weighted aggregation at the server. It combines the advantages of high and low-rank LoRAs, achieving improved convergence speed and final performance compared to homogeneous LoRA. Furthermore, HetLoRA has enhanced computation and communication efficiency compared to full fine-tuning making it more feasible for the edge.",
                "authors": "Yae Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, Gauri Joshi",
                "citations": 25
            },
            {
                "title": "CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models",
                "abstract": "Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object’s grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: copa-2024.github.io",
                "authors": "Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, Yang Gao",
                "citations": 26
            },
            {
                "title": "Blind Baselines Beat Membership Inference Attacks for Foundation Models",
                "abstract": "Membership inference (MI) attacks try to determine if a data sample was used to train a machine learning model. For foundation models trained on unknown Web data, MI attacks can be used to detect copyrighted training materials, measure test set contamination, or audit machine unlearning. Unfortunately, we find that evaluations of MI attacks for foundation models are flawed, because they sample members and non-members from different distributions. For 8 published MI evaluation datasets, we show that blind attacks -- that distinguish the member and non-member distributions without looking at any trained model -- outperform state-of-the-art MI attacks. Existing evaluations thus tell us nothing about membership leakage of a foundation model's training data.",
                "authors": "Debeshee Das, Jie Zhang, F. Tramèr",
                "citations": 19
            },
            {
                "title": "InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding",
                "abstract": "data level, we prioritize the spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions. This improves the alignment between video and text. We scale both data and model size for our InternVideo2 . Through extensive experiments, we validate our designs and demonstrate the state-of-the-art performance on over 60 video and audio tasks. Notably, our model outperforms others on various video-related captioning, dialogue, and long video understanding benchmarks, highlighting its ability to reason and comprehend long temporal contexts.",
                "authors": "Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang",
                "citations": 78
            },
            {
                "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models",
                "abstract": "Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.",
                "authors": "Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, Xuanzhe Liu",
                "citations": 51
            },
            {
                "title": "Foundation Models for Time Series Analysis: A Tutorial and Survey",
                "abstract": "Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advances in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored for time series analysis. This survey aims to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either application or pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future exploration.",
                "authors": "Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, Qingsong Wen",
                "citations": 51
            },
            {
                "title": "On the Societal Impact of Open Foundation Models",
                "abstract": "Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on open foundation models, defined here as those with broadly available model weights (e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties (e.g. greater customizability, poor monitoring) of open foundation models that lead to both their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their marginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work helps support a more grounded assessment of the societal impact of open foundation models by outlining what research is needed to empirically validate their theoretical benefits and risks.",
                "authors": "Sayash Kapoor, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Peter Cihon, Aspen Hopkins, Kevin Bankston, Stella Biderman, Miranda Bogen, Rumman Chowdhury, Alex Engler, Peter Henderson, Yacine Jernite, Seth Lazar, Stefano Maffulli, Alondra Nelson, Joelle Pineau, Aviya Skowron, Dawn Song, Victor Storchan, Daniel Zhang, Daniel E. Ho, Percy Liang, Arvind Narayanan",
                "citations": 40
            },
            {
                "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
                "abstract": "Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such\"in-the-wild\"data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.",
                "authors": "Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, K. Gopalakrishnan, Karol Hausman, Brian Ichter, A. Irpan, Nikhil J. Joshi, Ryan C. Julian, Sean Kirmani, Isabel Leal, T. Lee, Sergey Levine, Yao Lu, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag R. Sanketi, P. Sermanet, Q. Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu",
                "citations": 33
            },
            {
                "title": "Real-World Robot Applications of Foundation Models: A Review",
                "abstract": "Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.",
                "authors": "Kento Kawaharazuka, T. Matsushima, Andrew Gambardella, Jiaxian Guo, Chris Paxton, Andy Zeng",
                "citations": 29
            },
            {
                "title": "Developing Generalist Foundation Models from a Multimodal Dataset for 3D Computed Tomography",
                "abstract": "While computer vision has achieved tremendous success with multimodal encoding and direct textual interaction with images via chat-based large language models, similar advancements in medical imaging AI, particularly in 3D imaging, have been limited due to the scarcity of comprehensive datasets. To address this critical gap, we introduce CT-RATE, the first dataset that pairs 3D medical images with corresponding textual reports. CT-RATE comprises 25,692 non-contrast 3D chest CT scans from 21,304 unique patients. Through various reconstructions, these scans are expanded to 50,188 volumes, totaling over 14.3 million 2D slices. Each scan is accompanied by its corresponding radiology report. Leveraging CT-RATE, we develop CT-CLIP, a CT-focused contrastive language-image pretraining framework designed for broad applications without the need for task-specific training. We demonstrate how CT-CLIP can be used in two tasks: multi-abnormality detection and case retrieval. Remarkably, in multi-abnormality detection, CT-CLIP outperforms state-of-the-art fully supervised models across all key metrics, effectively eliminating the need for manual annotation. In case retrieval, it efficiently retrieves relevant cases using either image or textual queries, thereby enhancing knowledge dissemination. By combining CT-CLIP's vision encoder with a pretrained large language model, we create CT-CHAT, a vision-language foundational chat model for 3D chest CT volumes. Finetuned on over 2.7 million question-answer pairs derived from the CT-RATE dataset, CT-CHAT surpasses other multimodal AI assistants, underscoring the necessity for specialized methods in 3D medical imaging. Collectively, the open-source release of CT-RATE, CT-CLIP, and CT-CHAT not only addresses critical challenges in 3D medical imaging but also lays the groundwork for future innovations in medical AI and improved patient care.",
                "authors": "Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, Ayse Gulnihan Simsek, S. Esirgun, Irem Dogan, M. F. Dasdelen, Bastian Wittmann, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, A. Sekuboyina, Berkan Lafci, M. K. Ozdemir, Bjoern H Menze",
                "citations": 17
            },
            {
                "title": "OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning",
                "abstract": "Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as One Tracker. One- Tracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker. This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object. Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker. Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter- efficient finetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our One- Tracker outperforms other models and achieves state-of-the-art performance.",
                "authors": "Lingyi Hong, Shilin Yan, Renrui Zhang, Wanyun Li, Xinyu Zhou, Pinxue Guo, Kaixun Jiang, Yiting Chen, Jinglun Li, Zhaoyu Chen, Wenqiang Zhang",
                "citations": 22
            },
            {
                "title": "Why Tabular Foundation Models Should Be a Research Priority",
                "abstract": "Recent text and image foundation models are incredibly impressive, and these models are attracting an ever-increasing portion of research resources. In this position piece we aim to shift the ML research community's priorities ever so slightly to a different modality: tabular data. Tabular data is the dominant modality in many fields, yet it is given hardly any research attention and significantly lags behind in terms of scale and power. We believe the time is now to start developing tabular foundation models, or what we coin a Large Tabular Model (LTM). LTMs could revolutionise the way science and ML use tabular data: not as single datasets that are analyzed in a vacuum, but contextualized with respect to related datasets. The potential impact is far-reaching: from few-shot tabular models to automating data science; from out-of-distribution synthetic data to empowering multidisciplinary scientific discovery. We intend to excite reflections on the modalities we study, and convince some researchers to study large tabular models.",
                "authors": "B. V. Breugel, M. Schaar",
                "citations": 19
            },
            {
                "title": "Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models",
                "abstract": "\n We propose masked particle modeling (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capability of the model, showing that it can be adapted to tasks such as supervised and weakly supervised jet classification, and that the model can transfer efficiently with small fine-tuning data sets to new classes and new data domains.",
                "authors": "L. Heinrich, T. Golling, M. Kagan, Samuel Klein, Matthew Leigh, Margarita Osadchy, J. A. Raine",
                "citations": 15
            },
            {
                "title": "Asymmetry in Low-Rank Adapters of Foundation Models",
                "abstract": "Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training $B$ improves the bound. We support our conclusions with experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.",
                "authors": "Jiacheng Zhu, K. Greenewald, Kimia Nadjahi, Haitz S'aez de Oc'ariz Borde, Rickard Brüel Gabrielsson, Leshem Choshen, Marzyeh Ghassemi, M. Yurochkin, Justin Solomon",
                "citations": 18
            },
            {
                "title": "DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models",
                "abstract": "We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its endeffector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.",
                "authors": "Norman Di Palo, Edward Johns",
                "citations": 16
            },
            {
                "title": "A Survey for Foundation Models in Autonomous Driving",
                "abstract": "The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the methods employed in current research. It identifies the gaps between existing foundation models and cutting-edge AD approaches, thereby charting future research directions and proposing a roadmap for bridging these gaps.",
                "authors": "Haoxiang Gao, Yaqian Li, Kaiwen Long, Ming Yang, Yiqing Shen",
                "citations": 16
            },
            {
                "title": "Sora-Based Parallel Vision for Smart Sensing of Intelligent Vehicles: From Foundation Models to Foundation Intelligence",
                "abstract": "There are a large number of functional sensors installed on the modern intelligent vehicles. Many Artificial Intelligence based foundation models have been proposed for smart sensing to recognize the known object classes in the new but similar scenarios. However, it is still challenging for the foundation models of smart sensing to detect all the object classes in both seen and unseen scenarios. This letter aims at pushing the boundary of smart sensing research for intelligent vehicles. We first summarize the current widely-used foundation models and the foundation intelligence needed for smart sensing of intelligent vehicles. We then explain Sora-based Parallel Vision to boost the foundation models of smart sensing from basic intelligence (1.0) to enhanced intelligence (2.0) and final generalized intelligence (3.0). Several representative case studies are discussed to show the potential usages of Sora-based Parallel Vision, followed by its future research direction.",
                "authors": "Hongkai Yu, Xinyu Liu, Yonglin Tian, Yutong Wang, Chao Gou, Fei-Yue Wang",
                "citations": 17
            },
            {
                "title": "On the Opportunities and Challenges of Foundation Models for GeoAI (Vision Paper)",
                "abstract": "Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have not yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial domains, including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality, such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, the task-agnostic large learning models (LLMs) can outperform task-specific fully supervised models in a zero-shot or few-shot learning setting. However, on other geospatial tasks, especially tasks that involve multiple data modalities (e.g., POI-based urban function classification, street view image–based urban noise intensity classification, and remote sensing image scene classification), existing FMs still underperform task-specific models. Based on these observations, we propose that one of the major challenges of developing an FM for GeoAI is to address the multimodal nature of geospatial tasks. After discussing the distinct challenges of each geospatial data modality, we suggest the possibility of a multimodal FM that can reason over various types of geospatial data through geospatial alignments. We conclude this article by discussing the unique risks and challenges to developing such a model for GeoAI.",
                "authors": "Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao Liu, Song Gao, Tianming Liu, Gao Cong, Yingjie Hu, Chris Cundy, Ziyuan Li, Rui Zhu, Ni Lao",
                "citations": 18
            },
            {
                "title": "Data-Centric Foundation Models in Computational Healthcare: A Survey",
                "abstract": "The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare-related foundation models and datasets at https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .",
                "authors": "Yunkun Zhang, Jin Gao, Zheling Tan, Lingfeng Zhou, Kexin Ding, Mu Zhou, Shaoting Zhang, Dequan Wang",
                "citations": 17
            },
            {
                "title": "Foundation Models",
                "abstract": null,
                "authors": "Johannes Schneider, Christian Meske, P. Kuss",
                "citations": 54
            },
            {
                "title": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence",
                "abstract": "Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use.",
                "authors": "Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, Manish Sethi, Xuan-Hong Dang, Pengyuan Li, Kun-Lung Wu, Syed Zawad, Andrew Coleman, Matthew White, Mark Lewis, Raju Pavuluri, Y. Koyfman, Boris Lublinsky, M. D. Bayser, Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Yi Zhou, Chris Johnson, Aanchal Goyal, Hima Patel, Yousaf Shah, Petros Zerfos, Heiko Ludwig, Asim Munawar, M. Crouse, P. Kapanipathi, Shweta Salaria, Bob Calio, Sophia Wen, Seetharami R. Seelam, Brian M. Belgodere, Carlos Fonseca, Amith Singhee, Nirmit Desai, David D. Cox, Ruchir Puri, Rameswar Panda",
                "citations": 34
            },
            {
                "title": "Multimodal artificial intelligence foundation models: Unleashing the power of remote sensing big data in earth observation",
                "abstract": null,
                "authors": "D. Hong, Chenyu Li, Bing Zhang, N. Yokoya, J. Benediktsson, J. Chanussot",
                "citations": 41
            },
            {
                "title": "Poseidon: Efficient Foundation Models for PDEs",
                "abstract": "We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.",
                "authors": "Maximilian Herde, Bogdan Raoni'c, Tobias Rohner, R. Käppeli, Roberto Molinaro, Emmanuel de B'ezenac, Siddhartha Mishra",
                "citations": 15
            },
            {
                "title": "FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs",
                "abstract": "This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual speech recognition, emotion recognition, and audio event detection; and CosyVoice, which facilitates natural speech generation with control over multiple languages, timbre, speaking style, and speaker identity. SenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and SenseVoice-Large supports high-precision ASR for over 50 languages, while CosyVoice excels in multi-lingual voice generation, zero-shot in-context learning, cross-lingual voice cloning, and instruction-following capabilities. The models related to SenseVoice and CosyVoice have been open-sourced on Modelscope and Huggingface, along with the corresponding training, inference, and fine-tuning codes released on GitHub. By integrating these models with LLMs, FunAudioLLM enables applications such as speech-to-speech translation, emotional voice chat, interactive podcasts, and expressive audiobook narration, thereby pushing the boundaries of voice interaction technology. Demos are available at https://fun-audio-llm.github.io, and the code can be accessed at https://github.com/FunAudioLLM.",
                "authors": "Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, Shengpeng Ji, Yabin Li, Zerui Li, Heng Lu, Xiang Lv, Bin Ma, Ziyang Ma, Chongjia Ni, Changhe Song, Jiaqi Shi, Xian Shi, Hao Wang, Wen Wang, Yuxuan Wang, Zhangyu Xiao, Zhijie Yan, Yexin Yang, Bin Zhang, Qingling Zhang, Shi-Ya Zhang, Nan Zhao, Siqi Zheng",
                "citations": 17
            },
            {
                "title": "Many-Shot In-Context Learning in Multimodal Foundation Models",
                "abstract": "Large language models are effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have enabled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 14 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (image classification, visual QA, and object localization). We observe that many-shot ICL, including up to almost 2,000 demonstrating examples, leads to substantial improvements compared to few-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly up to the maximum number of tested examples on many datasets. We also find open-weights multimodal foundation models like Llama 3.2-Vision do not benefit from the demonstrating examples, highlighting an important gap between open and closed multimodal foundation models. Given the high inference costs required for many-shot ICL, we also explore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and many-shot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro learns more quickly than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Our codebase is publicly available at https://github.com/stanfordmlgroup/ManyICL .",
                "authors": "Yixing Jiang, Jeremy Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan H. Chen, Andrew Y. Ng",
                "citations": 14
            },
            {
                "title": "Few-shot Adaptation of Multi-modal Foundation Models: A Survey",
                "abstract": "Multi-modal (vision-language) models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of visual foundation models. These models with robust and aligned semantic representations learned from billions of internet image-text pairs and can be applied to various downstream tasks in a zero-shot manner. However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired. Consequently, many researchers have begun to explore few-shot adaptation methods for these models, gradually deriving three main technical approaches: 1) prompt-based methods, 2) adapter-based methods, and 3) external knowledge-based methods. Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress. Therefore, in this survey, we introduce and analyze the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods. In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models. The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size. Based on this, we propose three possible solutions from the following aspects: 1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive knowledge utilization.",
                "authors": "Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai Xiaocong Zhou, Delong Chen",
                "citations": 14
            },
            {
                "title": "Position: Graph Foundation Models Are Already Here",
                "abstract": "Graph Foundation Models (GFMs) are emerging as a significant research topic in the graph domain, aiming to develop graph models trained on extensive and diverse data to enhance their applicability across various tasks and domains. Developing GFMs presents unique challenges over traditional Graph Neural Networks (GNNs), which are typically trained from scratch for specific tasks on particular datasets. The primary challenge in constructing GFMs lies in effectively leveraging vast and diverse graph data to achieve positive transfer. Drawing inspiration from existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, expressiveness, and stability. Such a vocabulary perspective can potentially advance the future GFM design in line with the neural scaling laws. All relevant resources with GFM design can be found here.",
                "authors": "Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Mikhail Galkin, Jiliang Tang",
                "citations": 14
            },
            {
                "title": "Large language models and multimodal foundation models for precision oncology",
                "abstract": null,
                "authors": "Daniel Truhn, Jan-Niklas Eckardt, Dyke Ferber, J. N. Kather",
                "citations": 23
            },
            {
                "title": "Surgical-DINO: adapter learning of foundation models for depth estimation in endoscopic surgery",
                "abstract": null,
                "authors": "Beilei Cui, Mobarakol Islam, Long Bai, Hongliang Ren",
                "citations": 27
            },
            {
                "title": "OpenGraph: Towards Open Graph Foundation Models",
                "abstract": "Graph learning has become essential in various domains, including recommendation systems and social network analysis. Graph Neural Networks (GNNs) have emerged as promising techniques for encoding structural information and improving performance in tasks like link prediction and node classification. However, a key challenge remains: the difficulty of generalizing to unseen graph data with different properties. In this work, we propose a novel graph foundation model, called OpenGraph, to address this challenge. Our approach tackles several technical obstacles. Firstly, we enhance data augmentation using a large language model (LLM) to overcome data scarcity in real-world scenarios. Secondly, we introduce a unified graph tokenizer that enables the model to generalize effectively to diverse graph data, even when encountering unseen properties during training. Thirdly, our developed scalable graph transformer captures node-wise dependencies within the global topological context. Extensive experiments validate the effectiveness of our framework. By adapting OpenGraph to new graph characteristics and comprehending diverse graphs, our approach achieves remarkable zero-shot graph learning performance across various settings. We release the model implementation at https://github.com/HKUDS/OpenGraph.",
                "authors": "Lianghao Xia, Ben Kao, Chao Huang",
                "citations": 19
            },
            {
                "title": "Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware",
                "abstract": "Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified ten key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. In this paper, we discuss these challenges in detail and state the path for innovation that we envision. Next, we present FMArts , which is our long-term effort towards creating a cradle-to-grave platform for the engineering of trustworthy FMware. Finally, we (i) show how the unique properties of FMArts enabled us to design and develop a complex FMware for a large customer in a timely manner and (ii) discuss the lessons that we learned in doing so. We hope that the disclosure of the aforementioned challenges and our associated efforts to tackle them will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions across the software engineering discipline.",
                "authors": "Ahmed E. Hassan, Dayi Lin, Gopi Krishnan Rajbahadur, Keheliya Gallaba, F. Côgo, Boyuan Chen, Haoxiang Zhang, Kishanthan Thangarajah, G. Oliva, Jiahuei Lin, Wali Mohammad Abdullah, Zhen Ming Jiang",
                "citations": 10
            },
            {
                "title": "Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models",
                "abstract": "To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.",
                "authors": "Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Schölkopf, Pradeep Ravikumar",
                "citations": 13
            },
            {
                "title": "Heterogeneous Contrastive Learning for Foundation Models and Beyond",
                "abstract": "In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how contrastive learning is applied to train and fine-tune the multi-view foundation models. Then, we move to contrastive learning methods for task heterogeneity, including pretraining tasks and downstream tasks, and show how different tasks are combined with contrastive learning loss for different purposes. Finally, we conclude this survey by discussing the open challenges and shedding light on the future directions of contrastive learning.",
                "authors": "Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, Jingrui He",
                "citations": 13
            },
            {
                "title": "A Survey on Robotics with Foundation Models: toward Embodied AI",
                "abstract": "While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.",
                "authors": "Zhiyuan Xu, Kun Wu, Junjie Wen, Jinming Li, Ning Liu, Zhengping Che, Jian Tang",
                "citations": 13
            },
            {
                "title": "A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model",
                "abstract": "Time series data are ubiquitous across various domains, making time series analysis critically important. Traditional time series models are task-specific, featuring singular functionality and limited generalization capacity. Recently, large language foundation models have unveiled their remarkable capabilities for cross-task transferability, zero-shot/few-shot learning, and decision-making explainability. This success has sparked interest in the exploration of foundation models to solve multiple time series challenges simultaneously. There are two main research lines, namely pre-training foundation models from scratch for time series and adapting large language foundation models for time series. They both contribute to the development of a unified model that is highly generalizable, versatile, and comprehensible for time series analysis. This survey offers a 3E analytical framework for comprehensive examination of related research. Specifically, we examine existing works from three dimensions, namely Effectiveness, Efficiency and Explainability. In each dimension, we focus on discussing how related works devise tailored solution by considering unique challenges in the realm of time series. Furthermore, we provide a domain taxonomy to help followers keep up with the domain-specific advancements. In addition, we introduce extensive resources to facilitate the field's development, including datasets, open-source, time series libraries. A GitHub repository is also maintained for resource updates (https://github.com/start2020/Awesome-TimeSeries-LLM-FM).",
                "authors": "Jiexia Ye, Weiqi Zhang, Ke Yi, Yongzi Yu, Ziyue Li, Jia Li, F. Tsung",
                "citations": 14
            },
            {
                "title": "Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities",
                "abstract": "The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving.",
                "authors": "Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu",
                "citations": 13
            },
            {
                "title": "Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems",
                "abstract": "Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6 G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the unique needs of next-generation wireless systems, thereby paving the way towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network adaptation thanks to logical and mathematical reasoning facilitated by neuro-symbolic AI. In essence, these properties enable the proposed LMM framework to build universal capabilities that cater to various cross-layer networking tasks and alignment of intents across different domains. Preliminary results from experimental evaluation demonstrate the efficacy of grounding using RAG in LMMs, and showcase the alignment of LMMs with wireless system designs. Furthermore, the enhanced rationale exhibited in the responses to mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the logical and mathematical reasoning capabilities inherent in LMMs. Building on those results, we present a sequel of open questions and challenges for LMMs. We then conclude with a set of recommendations that ignite the path towards LMM-empowered AI-native systems.",
                "authors": "Shengzhe Xu, Christo Kurisummoottil Thomas, Omar Hashash, N. Muralidhar, Walid Saad, Naren Ramakrishnan",
                "citations": 12
            },
            {
                "title": "MeshXL: Neural Coordinate Field for Generative 3D Foundation Models",
                "abstract": "The polygon mesh representation of 3D data exhibits great flexibility, fast rendering speed, and storage efficiency, which is widely preferred in various applications. However, given its unstructured graph representation, the direct generation of high-fidelity 3D meshes is challenging. Fortunately, with a pre-defined ordering strategy, 3D meshes can be represented as sequences, and the generation process can be seamlessly treated as an auto-regressive problem. In this paper, we validate the Neural Coordinate Field (NeurCF), an explicit coordinate representation with implicit neural embeddings, is a simple-yet-effective representation for large-scale sequential mesh modeling. After that, we present MeshXL, a family of generative pre-trained auto-regressive models, which addresses the process of 3D mesh generation with modern large language model approaches. Extensive experiments show that MeshXL is able to generate high-quality 3D meshes, and can also serve as foundation models for various down-stream applications.",
                "authors": "Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Yanru Wang, Zhibin Wang, C. Zhang, Jingyi Yu, Gang Yu, Bin Fu, Tao Chen",
                "citations": 12
            },
            {
                "title": "Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning",
                "abstract": "Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our theoretical claims with extensive empirical evidence. Further, we present results affirming our task selection algorithm adeptly chooses related finetuning tasks, providing advantages to the model performance on target tasks. We believe our study shed new light on the effective adaptation of foundation models to new tasks that lack abundant labels. Our code is available at https://github.com/OliverXUZY/Foudation-Model_Multitask.",
                "authors": "Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang",
                "citations": 12
            },
            {
                "title": "On the Foundations of Earth and Climate Foundation Models",
                "abstract": "Foundation models have enormous potential in advancing Earth and climate sciences, however, current approaches may not be optimal as they focus on a few basic features of a desirable Earth and climate foundation model. Crafting the ideal Earth foundation model, we define eleven features which would allow such a foundation model to be beneficial for any geoscientific downstream application in an environmental- and human-centric manner.We further shed light on the way forward to achieve the ideal model and to evaluate Earth foundation models. What comes after foundation models? Energy efficient adaptation, adversarial defenses, and interpretability are among the emerging directions.",
                "authors": "Xiao Xiang Zhu, Zhitong Xiong, Yi Wang, Adam J. Stewart, Konrad Heidler, Yuanyuan Wang, Zhenghang Yuan, Thomas Dujardin, Qingsong Xu, Yilei Shi",
                "citations": 12
            },
            {
                "title": "OV-NeRF: Open-Vocabulary Neural Radiance Fields With Vision and Language Foundation Models for 3D Semantic Understanding",
                "abstract": "The development of Neural Radiance Fields (NeRFs) has provided a potent representation for encapsulating the geometric and appearance characteristics of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D semantic perception tasks has been a recent focus. However, current methods that extract semantics directly from Contrastive Language-Image Pretraining (CLIP) for semantic field learning encounter difficulties due to noisy and view-inconsistent semantics provided by CLIP. To tackle these limitations, we propose OV-NeRF, which exploits the potential of pre-trained vision and language foundation models to enhance semantic field learning through proposed single-view and cross-view strategies. First, from the single-view perspective, we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask proposals derived from Segment Anything (SAM) to rectify the noisy semantics of each training view, facilitating accurate semantic field learning. Second, from the cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy to address the challenge raised by view-inconsistent semantics. Rather than invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the 3D consistent semantics generated from the well-trained semantic field itself for semantic field training, aiming to reduce ambiguity and enhance overall semantic consistency across different views. Extensive experiments validate our OV-NeRF outperforms current state-of-the-art methods, achieving a significant improvement of 20.31% and 18.42% in mIoU metric on Replica and ScanNet, respectively. Furthermore, our approach exhibits consistent superior results across various CLIP configurations, further verifying its robustness. Codes are available at: https://github.com/pcl3dv/OV-NeRF.",
                "authors": "Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, Qing Li",
                "citations": 12
            },
            {
                "title": "VLM2Scene: Self-Supervised Image-Text-LiDAR Learning with Foundation Models for Autonomous Driving Scene Understanding",
                "abstract": "Vision and language foundation models (VLMs) have showcased impressive capabilities in 2D scene understanding. However, their latent potential in elevating the understanding of 3D autonomous driving scenes remains untapped. In this paper, we propose VLM2Scene, which exploits the potential of VLMs to enhance 3D self-supervised representation learning through our proposed image-text-LiDAR contrastive learning strategy. Specifically, in the realm of autonomous driving scenes, the inherent sparsity of LiDAR point clouds poses a notable challenge for point-level contrastive learning methods. This method often grapples with limitations tied to a restricted receptive field and the presence of noisy points. To tackle this challenge, our approach emphasizes region-level learning, leveraging regional masks without semantics derived from the vision foundation model. This approach capitalizes on valuable contextual information to enhance the learning of point cloud representations. First, we introduce Region Caption Prompts to generate fine-grained language descriptions for the corresponding regions, utilizing the language foundation model. These region prompts then facilitate the establishment of positive and negative text-point pairs within the contrastive loss framework. Second, we propose a Region Semantic Concordance Regularization, which involves a semantic-filtered region learning and a region semantic assignment strategy. The former aims to filter the false negative samples based on the semantic distance, and the latter mitigates potential inaccuracies in pixel semantics, thereby enhancing overall semantic consistency. Extensive experiments on representative autonomous driving datasets demonstrate that our self-supervised method significantly outperforms other counterparts. Codes are available at https://github.com/gbliao/VLM2Scene.",
                "authors": "Guibiao Liao, Jiankun Li, Xiaoqing Ye",
                "citations": 12
            },
            {
                "title": "Q-Bench<inline-formula><tex-math notation=\"LaTeX\">$^+$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3445770.gif\"/></alternatives></inline-formula>: A Benchmark for Multi-Modal Foundation Models on Low-Level Visi",
                "abstract": "The rapid development of Multi-modality Large Language Models (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile foundational models. However, evaluating MLLMs in <italic>low-level visual perception and understanding</italic> remains a yet-to-explore domain. To this end, we design benchmark settings to <italic>emulate human language responses</italic> related to low-level vision: the low-level visual <italic>perception</italic> (<underline>A1</underline>) <italic>via</italic> visual question answering related to low-level attributes (<italic>e.g. clarity, lighting</italic>); and the low-level visual <italic>description</italic> (<underline>A2</underline>), on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related question-answering and description evaluations of MLLMs from single images to <italic>image pairs</italic>. Specifically, for <italic>perception</italic> (A1), we carry out the LLVisionQA<inline-formula><tex-math notation=\"LaTeX\">$^{+}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3445770.gif\"/></alternatives></inline-formula> dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended question about its low-level features; for <bold/><italic>description</italic><bold/> (A2), we propose the LLDescribe<inline-formula><tex-math notation=\"LaTeX\">$^{+}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3445770.gif\"/></alternatives></inline-formula> dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on <bold/><italic>assessment</italic><bold/> (A3) ability, <italic>i.e.</italic> predicting score, by employing a softmax-based approach to enable all MLLMs to generate <italic>quantifiable</italic> quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level visual competencies on single images, but only GPT-4V exhibits higher accuracy on pairwise comparisons than single image evaluations (<italic>like humans</italic>). We hope that our benchmark will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs.",
                "authors": "Zicheng Zhang, Haoning Wu, Erli Zhang, Guangtao Zhai, Weisi Lin",
                "citations": 14
            },
            {
                "title": "A Large-Scale Evaluation of Speech Foundation Models",
                "abstract": "The foundation model paradigm leverages a shared foundation model to achieve state-of-the-art (SOTA) performance for various tasks, requiring minimal downstream-specific data collection and modeling. This approach has proven crucial in the field of Natural Language Processing (NLP). However, the speech processing community lacks a similar setup to explore the paradigm systematically. To bridge this gap, we establish the Speech processing Universal PERformance Benchmark (SUPERB). SUPERB represents an ecosystem designed to evaluate foundation models across a wide range of speech processing tasks, facilitating the sharing of results on an online leaderboard and fostering collaboration through a community-driven benchmark database that aids in new development cycles. We present a unified learning framework for solving the speech processing tasks in SUPERB with the frozen foundation model followed by task-specialized lightweight prediction heads. Combining our results with community submissions, we verify that the framework is simple yet effective, as the best-performing foundation model shows competitive generalizability across most SUPERB tasks. Finally, we conduct a series of analyses to offer an in-depth understanding of SUPERB and speech foundation models, including information flows across tasks inside the models and the statistical significance and robustness of the benchmark.",
                "authors": "Shu-Wen Yang, Heng-Jui Chang, Zili Huang, Andy T. Liu, Cheng-I Lai, Haibin Wu, Jiatong Shi, Xuankai Chang, Hsiang-Sheng Tsai, Wen-Chin Huang, Tzu-hsun Feng, Po-Han Chi, Yist Y. Lin, Yung-Sung Chuang, Tzu-hsien Huang, Wei-Cheng Tseng, Kushal Lakhotia, Shang-Wen Li, Abdelrahman Mohamed, Shinji Watanabe, Hung-yi Lee",
                "citations": 11
            },
            {
                "title": "Distributed Foundation Models for Multi-Modal Learning in 6G Wireless Networks",
                "abstract": "Benefiting from the ability to process and integrate data from various modalities, multi-modal foundation models (FMs) facilitate potential applications across a range of fields, including computer vision (CV), natural language processing (NLP), and diverse multi-modal applications such as imagetext retrieval. Currently, FMs are deployed on computing clusters for training and inference to meet their considerable computational demands. In the foreseeable future, the parameter size of FMs is expected to evolve further, posing challenges to both computation resources and energy supply. Fortunately, leveraging the next-generation wireless networks (6G) to aggregate substantial computation resources and multi-modal data from myriad wireless devices holds promise for handling the aforementioned challenges. In this work, we delve into state-of-the-art artificial intelligence (AI) techniques, specifically focusing on pipeline parallelism, data parallelism, and multi-modal learning, with the aim of supporting the sustainable development of distributed multi-modal FMs in the 6G era. In the context of pipeline parallelism, compressing activations and gradients while intelligently allocating communication resources can overcome communication bottlenecks caused by unstable wireless links. For data parallelism, federated learning (FL) with over-the-air computation (AirComp) seamlessly integrates communication and computation, significantly expediting gradient aggregation. Furthermore, by following the recent success of large language models (LLMs) and incorporating multi-modal learning into FMs, we can seamlessly integrate NLP and CV, along with the broader AI community, establishing the cornerstone for the intrinsic AI within 6G wireless networks.",
                "authors": "Jun Du, Tianyi Lin, Chunxiao Jiang, Qianqian Yang, Faouzi Bader, Zhu Han",
                "citations": 11
            },
            {
                "title": "Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",
                "abstract": "science (CS) domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs",
                "authors": "Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan Yin, Zhao Li",
                "citations": 13
            },
            {
                "title": "Foundation Models for Video Understanding: A Survey",
                "abstract": "Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks. Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data. This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories. Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks. We categorize ViFMs into three categories: 1) Image-based ViFMs, which adapt existing image models for video tasks, 2) Video-Based ViFMs, which utilize video-specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework. By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding. Our analysis surprisingly reveals that image-based foundation models consistently outperform video-based models on most video understanding tasks. Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks. We share the comprehensive list of ViFMs studied in this work at: \\url{https://github.com/NeeluMadan/ViFM_Survey.git}",
                "authors": "Neelu Madan, Andreas Møgelmose, Rajat Modi, Y. S. Rawat, T. Moeslund",
                "citations": 11
            },
            {
                "title": "A Survey of Deep Learning and Foundation Models for Time Series Forecasting",
                "abstract": "Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available. Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to utilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling techniques are reviewed, and suggestions for further work are provided.",
                "authors": "John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Arpinar, Ninghao Liu",
                "citations": 10
            },
            {
                "title": "Considerations for governing open foundation models",
                "abstract": "Different policy proposals may disproportionately affect the innovation ecosystem Foundation models (e.g., GPT-4 and Llama 3.1) are at the epicenter of artificial intelligence (AI), driving technological innovation and billions of dollars in investment. This has sparked widespread demands for regulation. Central to the debate about how to regulate foundation models is the process by which foundation models are released (1)—whether they are made available only to the model developers, fully open to the public, or somewhere in between. Open foundation models can benefit society by promoting competition, accelerating innovation, and distributing power. However, an emerging concern is whether open foundation models pose distinct risks to society (2). In general, although most policy proposals and regulations do not mention open foundation models by name, they may have an uneven impact on open and closed foundation models. We illustrate tensions that surface—and that policy-makers should consider—regarding different policy proposals that may disproportionately damage the innovation ecosystem around open foundation models.",
                "authors": "Rishi Bommasani, Sayash Kapoor, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Daniel Zhang, Marietje Schaake, Daniel E. Ho, Arvind Narayanan, Percy Liang",
                "citations": 10
            },
            {
                "title": "On Catastrophic Inheritance of Large Foundation Models",
                "abstract": "Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims to unite both the machine learning and social sciences communities for more responsible and promising AI development and deployment.",
                "authors": "Hao Chen, Bhiksha Raj, Xing Xie, Jindong Wang",
                "citations": 10
            },
            {
                "title": "One for All: Toward Unified Foundation Models for Earth Vision",
                "abstract": "Foundation models characterized by extensive parameters and trained on large-scale datasets have demonstrated remarkable efficacy across various downstream tasks for remote sensing data. Current remote sensing foundation models typically specialize in a single modality or a specific spatial resolution range, limiting their versatility for downstream datasets. While there have been attempts to develop multi-modal remote sensing foundation models, they typically employ separate vision encoders for each modality or spatial resolution, necessitating a switch in backbones contingent upon the input data. To address this issue, we introduce a simple yet effective method, termed OFA-Net (One-For-All Network): employing a single, shared Transformer backbone for multiple data modalities with different spatial resolutions. Using the masked image modeling mechanism, we pre-train a single Transformer backbone on a curated multi-modal dataset with this simple design. Then the backbone model can be used in different downstream tasks, thus forging a path towards a unified foundation backbone model in Earth vision. The proposed method is evaluated on 12 distinct downstream tasks and demonstrates promising performance.",
                "authors": "Zhitong Xiong, Yi Wang, Fahong Zhang, Xiao Xiang Zhu",
                "citations": 11
            },
            {
                "title": "From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models",
                "abstract": "Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models, have revolutionized various natural language processing tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. We review fundamental building blocks crucial for studying chart understanding tasks. Additionally, we explore various tasks and their evaluation metrics and sources of both charts and textual inputs. Various modeling strategies are then examined, encompassing both classification-based and generation-based approaches, along with tool augmentation techniques that enhance chart understanding performance. Furthermore, we discuss the state-of-the-art performance of each task and discuss how we can improve the performance. Challenges and future directions are addressed, highlighting the importance of several topics, such as domain-specific charts, lack of efforts in developing evaluation metrics, and agent-oriented settings. This survey paper serves as a comprehensive resource for researchers and practitioners in the fields of natural language processing, computer vision, and data analysis, providing valuable insights and directions for future research in chart understanding leveraging large foundation models. The studies mentioned in this paper, along with emerging new research, will be continually updated at: https://github.com/khuangaf/Awesome-Chart-Understanding.",
                "authors": "Kung-Hsiang Huang, Hou Pong Chan, Y. Fung, Haoyi Qiu, Mingyang Zhou, Shafiq R. Joty, Shih-Fu Chang, Heng Ji",
                "citations": 11
            },
            {
                "title": "Theia: Distilling Diverse Vision Foundation Models for Robot Learning",
                "abstract": "Vision-based robot policy learning, which maps visual inputs to actions, necessitates a holistic understanding of diverse visual tasks beyond single-task needs like classification or segmentation. Inspired by this, we introduce Theia, a vision foundation model for robot learning that distills multiple off-the-shelf vision foundation models trained on varied vision tasks. Theia's rich visual representations encode diverse visual knowledge, enhancing downstream robot learning. Extensive experiments demonstrate that Theia outperforms its teacher models and prior robot learning models using less training data and smaller model sizes. Additionally, we quantify the quality of pre-trained visual representations and hypothesize that higher entropy in feature norm distributions leads to improved robot learning performance. Code, models, and demo are available at https://theia.theaiinstitute.com.",
                "authors": "Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, M. Minniti, Tarik Kelestemur, David Watkins, Laura Herlant",
                "citations": 9
            },
            {
                "title": "Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models",
                "abstract": "Recent advancements in large language models have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment. In this context, our paper delves into the realm of vision foundation models, focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter's capabilities beyond the former's limits. We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision. Our comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings. The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of fine-tuning strong models with whole datasets. This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision foundation models. The code is available at https://github.com/ggjy/vision_weak_to_strong.",
                "authors": "Jianyuan Guo, Hanting Chen, Chengcheng Wang, Kai Han, Chang Xu, Yunhe Wang",
                "citations": 9
            },
            {
                "title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models",
                "abstract": "Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness. Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments.",
                "authors": "Yuxuan Kuang, Hai Lin, Meng Jiang",
                "citations": 12
            },
            {
                "title": "Evaluating the Utilities of Foundation Models in Single-cell Data Analysis",
                "abstract": "Foundation Models (FMs) have made significant strides in both industrial and scientific domains. In this paper, we evaluate the performance of FMs for single-cell sequencing data analysis through comprehensive experiments across eight downstream tasks pertinent to single-cell data. Overall, the top FMs include scGPT, Geneformer, and CellPLM by considering model performances and user accessibility among ten single-cell FMs. However, by comparing these FMs with task-specific methods, we found that single-cell FMs may not consistently excel than task-specific methods in all tasks, which challenges the necessity of developing foundation models for single-cell analysis. In addition, we evaluated the effects of hyper-parameters, initial settings, and stability for training single-cell FMs based on a proposed scEval framework, and provide guidelines for pre-training and fine-tuning, to enhance the performances of single-cell FMs. Our work summarizes the current state of single-cell FMs, points to their constraints and avenues for future development, and offers a freely available evaluation pipeline to benchmark new models and improve method development.",
                "authors": "Tianyu Liu, Kexing Li, Yuge Wang, Hongyu Li, Hongyu Zhao",
                "citations": 9
            },
            {
                "title": "Participation in the age of foundation models",
                "abstract": "Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of services, from banking to healthcare. Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to historically marginalized groups. The larger scale and domain-agnostic manner in which these models operate further heightens the stakes: any errors or harms are liable to reoccur across use cases. In AI & ML more broadly, participatory approaches hold promise to lend agency and decision-making power to marginalized stakeholders, leading to systems that better benefit justice through equitable and distributed governance. But existing approaches in participatory AI/ML are typically grounded in a specific application and set of relevant stakeholders, and it is not straightforward how to apply these lessons to the context of foundation models. Our paper aims to fill this gap. First, we examine existing attempts at incorporating participation into foundation models. We highlight the tension between participation and scale, demonstrating that it is intractable for impacted communities to meaningfully shape a foundation model that is intended to be universally applicable. In response, we develop a blueprint for participatory foundation models that identifies more local, application-oriented opportunities for meaningful participation. In addition to the “foundation” layer, our framework proposes the “subfloor” layer, in which stakeholders develop shared technical infrastructure, norms and governance for a grounded domain such as clinical care, journalism, or finance, and the “surface” (or application) layer, in which affected communities shape the use of a foundation model for a specific downstream task. The intermediate “subfloor” layer scopes the range of potential harms to consider, and affords communities more concrete avenues for deliberation and intervention. At the same time, it avoids duplicative effort by scaling input across relevant use cases. Through three case studies in clinical care, financial services, and journalism, we illustrate how this multi-layer model can create more meaningful opportunities for participation than solely intervening at the foundation layer.",
                "authors": "Harini Suresh, Emily Tseng, Meg Young, Mary L. Gray, Emma Pierson, Karen Levy",
                "citations": 9
            },
            {
                "title": "PhilEO Bench: Evaluating Geo-Spatial Foundation Models",
                "abstract": "Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at multiple n-shots and convergence rates.",
                "authors": "Casper Fibaek, Luke Camilleri, Andreas Luyts, Nikolaos Dionelis, B. L. Saux",
                "citations": 9
            },
            {
                "title": "AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models",
                "abstract": "AutoGluon-Multimodal (AutoMM) is introduced as an open-source AutoML library designed specifically for multimodal learning. Distinguished by its exceptional ease of use, AutoMM enables fine-tuning of foundation models with just three lines of code. Supporting various modalities including image, text, and tabular data, both independently and in combination, the library offers a comprehensive suite of functionalities spanning classification, regression, object detection, semantic matching, and image segmentation. Experiments across diverse datasets and tasks showcases AutoMM's superior performance in basic classification and regression tasks compared to existing AutoML tools, while also demonstrating competitive results in advanced tasks, aligning with specialized toolboxes designed for such purposes.",
                "authors": "Zhiqiang Tang, Haoyang Fang, Su Zhou, Taojiannan Yang, Zihan Zhong, Tony Hu, Katrin Kirchhoff, George Karypis",
                "citations": 9
            },
            {
                "title": "Weaver: Foundation Models for Creative Writing",
                "abstract": "This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage). We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs.",
                "authors": "Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Z. Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yichen Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamujiang Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yu-Jie Ye, Yihan Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyuan Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, Wangchunshu Zhou",
                "citations": 12
            },
            {
                "title": "Acceptable Use Policies for Foundation Models",
                "abstract": "As foundation models have accumulated hundreds of millions of users, developers have begun to take steps to prevent harmful types of uses. One salient intervention that foundation model developers adopt is acceptable use policies—legally binding policies that prohibit users from using a model for specific purposes. This paper identifies acceptable use policies from 30 foundation model developers, analyzes the use restrictions they contain, and argues that acceptable use policies are an important lens for understanding the regulation of foundation models. Taken together, developers’ acceptable use policies include 127 distinct use restrictions; the wide variety in the number and type of use restrictions may create fragmentation across the AI supply chain. Companies also employ acceptable use policies to prevent competitors or specific industries from making use of their models. Developers alone decide what constitutes acceptable use, and rarely provide transparency about how they enforce their policies. In practice, acceptable use policies are difficult to enforce, and scrupulous enforcement can act as a barrier to researcher access and limit beneficial uses of foundation models. Acceptable use policies for foundation models are an early example of self-regulation that have a significant impact on the market for foundation models and the AI ecosystem.",
                "authors": "Kevin Klyman",
                "citations": 6
            },
            {
                "title": "Low-Resource Finetuning of Foundation Models Beats State-of-the-Art in Histopathology",
                "abstract": "To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature quality and generalizability for a variety of tasks. In this study, we benchmark the most popular vision foundation models as feature extractors for histopathology data. We evaluate the models in two settings: slide-level classification and patch-level classification. We show that foundation models are a strong baseline. Our experiments demonstrate that by finetuning a foundation model on a single GPU for only two hours or three days depending on the dataset, we can match or outperform state-of-the-art feature extractors for computational pathology. These findings imply that even with little resources one can finetune a feature extractor tailored towards a specific downstream task and dataset. This is a considerable shift from the current state, where only few institutions with large amounts of resources and datasets are able to train a feature extractor. We publish all code used for training and evaluation as well as the finetuned models1.",
                "authors": "Benedikt Roth, Valentin Koch, S. Wagner, Julia A. Schnabel, Carsten Marr, Tingying Peng",
                "citations": 7
            },
            {
                "title": "Nucleotide Transformer: building and evaluating robust foundation models for human genomics.",
                "abstract": null,
                "authors": "Hugo Dalla-torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, B. P. de Almeida, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir, Marie Lopez, Thomas Pierrot",
                "citations": 6
            },
            {
                "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model",
                "abstract": "We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code and weights for our models for the LLaVA-Gemma models.",
                "authors": "Musashi Hinck, M. L. Olson, David Cobbley, Shao-Yen Tseng, Vasudev Lal",
                "citations": 8
            },
            {
                "title": "A comprehensive survey on pretrained foundation models: a history from BERT to ChatGPT",
                "abstract": null,
                "authors": "Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jianfeng Pei, Philip S. Yu, Lichao Sun",
                "citations": 12
            },
            {
                "title": "Collaborative Consortium of Foundation Models for Open-World Few-Shot Learning",
                "abstract": "Open-World Few-Shot Learning (OFSL) is a crucial research field dedicated to accurately identifying target samples in scenarios where data is limited and labels are unreliable. This research holds significant practical implications and is highly relevant to real-world applications. Recently, the advancements in foundation models like CLIP and DINO have showcased their robust representation capabilities even in resource-constrained settings with scarce data. This realization has brought about a transformative shift in focus, moving away from “building models from scratch” towards “effectively harnessing the potential of foundation models to extract pertinent prior knowledge suitable for OFSL and utilizing it sensibly”. Motivated by this perspective, we introduce the Collaborative Consortium of Foundation Models (CO3), which leverages CLIP, DINO, GPT-3, and DALL-E to collectively address the OFSL problem. CO3 comprises four key blocks: (1) the Label Correction Block (LC-Block) corrects unreliable labels, (2) the Data Augmentation Block (DA-Block) enhances available data, (3) the Feature Extraction Block (FE-Block) extracts multi-modal features, and (4) the Text-guided Fusion Adapter (TeFu-Adapter) integrates multiple features while mitigating the impact of noisy labels through semantic constraints. Only the adapter's parameters are adjustable, while the others remain frozen. Through collaboration among these foundation models, CO3 effectively unlocks their potential and unifies their capabilities to achieve state-of-the-art performance on multiple benchmark datasets. https://github.com/The-Shuai/CO3.",
                "authors": "Shuai Shao, Yu Bai, Yan Wang, Baodi Liu, Bin Liu",
                "citations": 6
            },
            {
                "title": "Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models",
                "abstract": "Vision-and-Language Navigation (VLN) has gained increasing attention over recent years and many approaches have emerged to advance their development. The remarkable achievements of foundation models have shaped the challenges and proposed methods for VLN research. In this survey, we provide a top-down review that adopts a principled framework for embodied planning and reasoning, and emphasizes the current methods and future opportunities leveraging foundation models to address VLN challenges. We hope our in-depth discussions could provide valuable resources and insights: on one hand, to milestone the progress and explore opportunities and potential roles for foundation models in this field, and on the other, to organize different challenges and solutions in VLN to foundation model researchers.",
                "authors": "Yue Zhang, Ziqiao Ma, Jialu Li, Yanyuan Qiao, Zun Wang, Joyce Chai, Qi Wu, Mohit Bansal, Parisa Kordjamshidi",
                "citations": 8
            },
            {
                "title": "A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models",
                "abstract": "In recent years, the rapid evolution of computer vision has seen the emergence of various foundation models, each tailored to specific data types and tasks. In this study, we explore the adaptation of these models for few-shot semantic segmentation. Specifically, we conduct a comprehensive comparative analysis of four prominent foundation models: DINO V2, Segment Anything, CLIP, Masked AutoEncoders, and of a straightforward ResNet50 pre-trained on the COCO dataset. We also include 5 adaptation methods, ranging from linear probing to fine tuning. Our findings show that DINO V2 outperforms other models by a large margin, across various datasets and adaptation methods. On the other hand, adaptation methods provide little discrepancy in the obtained results, suggesting that a simple linear probing can compete with advanced, more computationally intensive, alternatives",
                "authors": "Reda Bensaid, Vincent Gripon, Franccois Leduc-Primeau, Lukas Mauch, G. B. Hacene, Fabien Cardinaux",
                "citations": 7
            },
            {
                "title": "Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees",
                "abstract": "Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values. For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making. This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion. It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution. Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor. It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy. Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data. En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor.",
                "authors": "Yu Gui, Ying Jin, Zhimei Ren",
                "citations": 8
            },
            {
                "title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations",
                "abstract": "Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 points worse. Finally, we present two prompting techniques, $\\textit{IsoCombination}$ and $\\textit{IsoScratchPad}$, which improve model performance by considering combinations of, and translations between, different input representations.",
                "authors": "Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, W. Neiswanger",
                "citations": 8
            },
            {
                "title": "Configurable Foundation Models: Building LLMs from a Modular Perspective",
                "abstract": "Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, we highlight several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.",
                "authors": "Chaojun Xiao, Zhengyan Zhang, Chenyang Song, Dazhi Jiang, Feng Yao, Xu Han, Xiaozhi Wang, Shuo Wang, Yufei Huang, Guanyu Lin, Yingfa Chen, Weilin Zhao, Yuge Tu, Zexuan Zhong, Ao Zhang, Chenglei Si, Khai Hao Moo, Chenyang Zhao, Huimin Chen, Yankai Lin, Zhiyuan Liu, Jingbo Shang, Maosong Sun",
                "citations": 7
            },
            {
                "title": "Foundation Models for Music: A Survey",
                "abstract": "In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm.",
                "authors": "Ying-Chao Ma, Anders Oland, Anton Ragni, B. M. D. Sette, C. Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elio Quinton, Elona Shatri, Fabio Morreale, Ge Zhang, Gyorgy Fazekas, Gus G. Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin, Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B. Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shu-Yuan Dai, Shunwei Lei, Shiyin Kang, Simon Dixon, Wenhu Chen, Wehhao Huang, Xin Du, Xingwei Qu, Xu Tan, Yizhi Li, Zeyue Tian, Zhi-Xin Wu, Zhizheng Wu, Ziyang Ma, Ziyu Wang",
                "citations": 8
            },
            {
                "title": "Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives",
                "abstract": "Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS",
                "authors": "Sheng Luo, Wei Chen, Wanxin Tian, Rui Liu, Luanxuan Hou, Xiubao Zhang, Haifeng Shen, Ruiqi Wu, Shuyi Geng, Yi Zhou, Ling Shao, Yi Yang, Bojun Gao, Qun Li, Guobin Wu",
                "citations": 7
            },
            {
                "title": "Foundation models in ophthalmology",
                "abstract": "Foundation models represent a paradigm shift in artificial intelligence (AI), evolving from narrow models designed for specific tasks to versatile, generalisable models adaptable to a myriad of diverse applications. Ophthalmology as a specialty has the potential to act as an exemplar for other medical specialties, offering a blueprint for integrating foundation models broadly into clinical practice. This review hopes to serve as a roadmap for eyecare professionals seeking to better understand foundation models, while equipping readers with the tools to explore the use of foundation models in their own research and practice. We begin by outlining the key concepts and technological advances which have enabled the development of these models, providing an overview of novel training approaches and modern AI architectures. Next, we summarise existing literature on the topic of foundation models in ophthalmology, encompassing progress in vision foundation models, large language models and large multimodal models. Finally, we outline major challenges relating to privacy, bias and clinical validation, and propose key steps forward to maximise the benefit of this powerful technology.",
                "authors": "Mark A. Chia, F. Antaki, Yukun Zhou, A. Turner, Aaron Y. Lee, P. Keane",
                "citations": 6
            },
            {
                "title": "Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?",
                "abstract": "The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.",
                "authors": "Marco Gaido, Sara Papi, Matteo Negri, L. Bentivogli",
                "citations": 7
            },
            {
                "title": "Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks",
                "abstract": "Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.",
                "authors": "Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto",
                "citations": 6
            },
            {
                "title": "Visual Foundation Models Boost Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation",
                "abstract": "Unsupervised domain adaptation (UDA) is vital for alleviating the workload of labeling 3D point cloud data and mitigating the absence of labels when facing a newly defined domain. Various methods of utilizing images to enhance the performance of cross-domain 3D segmentation have recently emerged. However, the pseudo labels, which are generated from models trained on the source domain and provide additional supervised signals for the unseen domain, are inadequate when utilized for 3D segmentation due to their inherent noisiness and consequently restrict the accuracy of neural networks. With the advent of 2D visual foundation models (VFMs) and their abundant knowledge prior, we propose a novel pipeline VFMSeg to further enhance the cross-modal unsupervised domain adaptation framework by leveraging these models. In this work, we study how to harness the knowledge priors learned by VFMs to produce more accurate labels for unlabeled target domains and improve overall performance. We first utilize a multi-modal VFM, which is pre-trained on large scale image-text pairs, to provide supervised labels (VFM-PL) for images and point clouds from the target domain. Then, another VFM trained on fine-grained 2D masks is adopted to guide the generation of semantically augmented images and point clouds to enhance the performance of neural networks, which mix the data from source and target domains like view frustums (FrustumMixing). Finally, we merge class-wise prediction across modalities to produce more accurate annotations for unlabeled target domains. Our method is evaluated on various autonomous driving datasets and the results demonstrate a significant improvement for 3D segmentation task.",
                "authors": "Jingyi Xu, Weidong Yang, Lingdong Kong, You-Chen Liu, Rui Zhang, Qingyuan Zhou, Ben Fei",
                "citations": 7
            },
            {
                "title": "A Clinical Benchmark of Public Self-Supervised Pathology Foundation Models",
                "abstract": "The use of self-supervised learning (SSL) to train pathology foundation models has increased substantially in the past few years. Notably, several models trained on large quantities of clinical data have been made publicly available in recent months. This will significantly enhance scientific research in computational pathology and help bridge the gap between research and clinical deployment. With the increase in availability of public foundation models of different sizes, trained using different algorithms on different datasets, it becomes important to establish a benchmark to compare the performance of such models on a variety of clinically relevant tasks spanning multiple organs and diseases. In this work, we present a collection of pathology datasets comprising clinical slides associated with clinically relevant endpoints including cancer diagnoses and a variety of biomarkers generated during standard hospital operation from two medical centers. We leverage these datasets to systematically assess the performance of public pathology foundation models and provide insights into best practices for training new foundation models and selecting appropriate pretrained models.",
                "authors": "Gabriele Campanella, Shengjia Chen, Ruchika Verma, Jennifer Zeng, A. Stock, Matthew Croken, Brandon Veremis, Abdulkadir Elmas, Kuan-lin Huang, Ricky Kwan, Jane Houldsworth, Adam J. Schoenfeld, Chad M. Vanderbilt",
                "citations": 7
            },
            {
                "title": "State Space Models as Foundation Models: A Control Theoretic Overview",
                "abstract": "In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.",
                "authors": "Carmen Amo Alonso, Jerome Sieber, M. Zeilinger",
                "citations": 8
            },
            {
                "title": "How to build the best medical image segmentation algorithm using foundation models: a comprehensive empirical study with Segment Anything Model",
                "abstract": "Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or\"best-practice\"guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at https://github.com/mazurowski-lab/finetune-SAM.",
                "authors": "Han Gu, Haoyu Dong, Jichen Yang, M. Mazurowski",
                "citations": 7
            },
            {
                "title": "OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine",
                "abstract": "The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas. However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages. It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data. The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner. In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models. It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data. Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks. We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab.",
                "authors": "Xiaosong Wang, Xiaofan Zhang, Guotai Wang, Junjun He, Zhongyu Li, Wentao Zhu, Yi Guo, Qi Dou, Xiaoxiao Li, Dequan Wang, Liang Hong, Qicheng Lao, Tong Ruan, Yukun Zhou, Yixue Li, Jie Zhao, Kang Li, Xin Sun, Lifeng Zhu, Shaoting Zhang",
                "citations": 6
            },
            {
                "title": "Foundation models are platform models: Prompting and the political economy of AI",
                "abstract": "A recent innovation in the field of machine learning has been the creation of very large pre-trained models, also referred to as ‘foundation models’, that draw on much larger and broader sets of data than typical deep learning systems and can be applied to a wide variety of tasks. Underpinning text-based systems such as OpenAI's ChatGPT and image generators such as Midjourney, these models have received extraordinary amounts of public attention, in part due to their reliance on prompting as the main technique to direct and apply them. This paper thus uses prompting as an entry point into the critical study of foundation models and their implications. The paper proceeds as follows: In the first section, we introduce foundation models in more detail, outline some of the main critiques, and present our general approach. We then discuss prompting as an algorithmic technique, show how it makes foundation models programmable, and explain how it enables different audiences to use these models as (computational) platforms. In the third section, we link the material properties of the technologies under scrutiny to questions of political economy, discussing, in turn, deep user interactions, reordered cost structures, and centralization and lock-in. We conclude by arguing that foundation models and prompting further strengthen Big Tech's dominance over the field of computing and, through their broad applicability, many other economic sectors, challenging our capacities for critical appraisal and regulatory response.",
                "authors": "Sarah Burkhardt, Bernhard Rieder",
                "citations": 8
            },
            {
                "title": "Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models",
                "abstract": "The integration of machine learning techniques has become a cornerstone in the development of intelligent urban services, significantly contributing to the enhancement of urban efficiency, sustainability, and overall livability. Recent advancements in foundational models, such as ChatGPT, have introduced a paradigm shift within the fields of machine learning and artificial intelligence. These models, with their exceptional capacity for contextual comprehension, problem-solving, and task adaptability, present a transformative opportunity to reshape the future of smart cities and drive progress toward Urban General Intelligence (UGI). Despite increasing attention to Urban Foundation Models (UFMs), this rapidly evolving field faces critical challenges, including the lack of clear definitions, systematic reviews, and universalizable solutions. To address these issues, this paper first introduces the definition and concept of UFMs and highlights the distinctive challenges involved in their development. Furthermore, we present a data-centric taxonomy that classifies existing research on UFMs according to the various urban data modalities and types. In addition, we propose a prospective framework designed to facilitate the realization of versatile UFMs, aimed at overcoming the identified challenges and driving further progress in this field. Finally, this paper explores the wide-ranging applications of UFMs within urban contexts, illustrating their potential to significantly impact and transform urban systems. A comprehensive collection of relevant research papers and open-source resources have been collated and are continuously updated at: https://github.com/usail-hkust/Awesome-Urban-Foundation-Models.",
                "authors": "Weijiao Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, Hui Xiong",
                "citations": 8
            },
            {
                "title": "SegmentNT: annotating the genome at single-nucleotide resolution with DNA foundation models",
                "abstract": "Foundation models have achieved remarkable success in several fields such as natural language processing, computer vision and more recently biology. DNA foundation models in particular are emerging as a promising approach for genomics. However, so far no model has delivered granular, nucleotide-level predictions across a wide range of genomic and regulatory elements, limiting their practical usefulness. In this paper, we build on our previous work on the Nucleotide Transformer (NT) to develop a segmentation model, SegmentNT, that processes input DNA sequences up to 30kb-long to predict 14 different classes of genomic elements at single nucleotide resolution. By utilizing pre-trained weights from NT, SegmentNT surpasses the performance of several ablation models, including convolution networks with one-hot encoded nucleotide sequences and models trained from scratch. SegmentNT can process multiple sequence lengths with zero-shot generalization for sequences of up to 50kb. We show improved performance on the detection of splice sites throughout the genome and demonstrate strong nucleotide-level precision. Because it evaluates all gene elements simultaneously, SegmentNT can predict the impact of sequence variants not only on splice site changes but also on exon and intron rearrangements in transcript isoforms. Finally, we show that a SegmentNT model trained on human genomic elements can generalize to elements of different human and plant species and that a trained multispecies SegmentNT model achieves stronger generalization for all genic elements on unseen species. In summary, SegmentNT demonstrates that DNA foundation models can tackle complex, granular tasks in genomics at a single-nucleotide resolution. SegmentNT can be easily extended to additional genomic elements and species, thus representing a new paradigm on how we analyze and interpret DNA. We make our SegmentNT-30kb human and multispecies models available on our github repository in Jax and HuggingFace space in Pytorch.",
                "authors": "B. P. de Almeida, Hugo Dalla-torre, Guillaume Richard, Christopher Blum, Lorenz Hexemer, Maxence Gélard, Javier Mendoza-Revilla, Priyanka Pandey, Stefan Laurent, Marie Lopez, Alexandre Laterre, Maren Lang, U. Sahin, Karim Beguir, Thomas Pierrot",
                "citations": 8
            },
            {
                "title": "Dual-Personalizing Adapter for Federated Foundation Models",
                "abstract": "Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning diverse instruction data. Notably, federated foundation models (FedFM) emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to FedFM for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications, and conventional methods for test-time distribution shifts in personalized FL are less effective for FedFM due to their failure to adapt to complex distribution shift scenarios and the requirement to train all parameters. To bridge this gap, we refine the setting in FedFM, termed test-time personalization, which aims to learn personalized federated foundation models on clients while effectively handling test-time distribution shifts simultaneously. To address challenges in this setting, we explore a simple yet effective solution, a Federated Dual-Personalizing Adapter (FedDPA) architecture. By co-working with a foundation model, a global adapter and a local adapter jointly tackle the test-time distribution shifts and client-specific personalization. Additionally, we introduce an instance-wise dynamic weighting mechanism that dynamically integrates the global and local adapters for each test instance during inference, facilitating effective test-time personalization. The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks.",
                "authors": "Yiyuan Yang, Guodong Long, Taoshu Shen, Jing Jiang, Michael Blumenstein",
                "citations": 6
            },
            {
                "title": "Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models",
                "abstract": "Low-Rank Adaptation (LoRA) emerges as a popular parameter-efficient fine-tuning (PEFT) method, which proposes to freeze pretrained model weights and update an additive low-rank trainable matrix. In this work, we study the enhancement of LoRA training by introducing an $r \\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. We theoretically verify that the proposed preconditioner stabilizes feature learning with LoRA under infinite-width NN setting. Empirically, the implementation of this new preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with this new preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. The new preconditioner can be derived from a novel Riemannian metric in low-rank matrix field. Code can be accessed at https://github.com/pilancilab/Riemannian_Preconditioned_LoRA.",
                "authors": "Fangzhao Zhang, Mert Pilanci",
                "citations": 10
            },
            {
                "title": "Augmenting research methods with foundation models and generative AI",
                "abstract": null,
                "authors": "Sippo Rossi, Matti Rossi, R. Mukkamala, J. Thatcher, Yogesh K. Dwivedi",
                "citations": 10
            },
            {
                "title": "TrafficGPT: Viewing, processing and interacting with traffic foundation models",
                "abstract": null,
                "authors": "Siyao Zhang, Daocheng Fu, Wenzhe Liang, Zhao Zhang, Bin Yu, Pinlong Cai, Baozhen Yao",
                "citations": 10
            },
            {
                "title": "FairMedFM: Fairness Benchmarking for Medical Imaging Foundation Models",
                "abstract": "The advent of foundation models (FMs) in healthcare offers unprecedented opportunities to enhance medical diagnostics through automated classification and segmentation tasks. However, these models also raise significant concerns about their fairness, especially when applied to diverse and underrepresented populations in healthcare applications. Currently, there is a lack of comprehensive benchmarks, standardized pipelines, and easily adaptable libraries to evaluate and understand the fairness performance of FMs in medical imaging, leading to considerable challenges in formulating and implementing solutions that ensure equitable outcomes across diverse patient populations. To fill this gap, we introduce FairMedFM, a fairness benchmark for FM research in medical imaging.FairMedFM integrates with 17 popular medical imaging datasets, encompassing different modalities, dimensionalities, and sensitive attributes. It explores 20 widely used FMs, with various usages such as zero-shot learning, linear probing, parameter-efficient fine-tuning, and prompting in various downstream tasks -- classification and segmentation. Our exhaustive analysis evaluates the fairness performance over different evaluation metrics from multiple perspectives, revealing the existence of bias, varied utility-fairness trade-offs on different FMs, consistent disparities on the same datasets regardless FMs, and limited effectiveness of existing unfairness mitigation methods. Checkout FairMedFM's project page and open-sourced codebase, which supports extendible functionalities and applications as well as inclusive for studies on FMs in medical imaging over the long term.",
                "authors": "Ruinan Jin, Zikang Xu, Yuan Zhong, Qiongsong Yao, Qi Dou, S. K. Zhou, Xiaoxiao Li",
                "citations": 4
            },
            {
                "title": "Automating the Enterprise with Foundation Models",
                "abstract": "Automating enterprise workflows could unlock $4 trillion/year in productivity gains. Despite being of interest to the data management community for decades, the ultimate vision of end-to-end workflow automation has remained elusive. Current solutions rely on process mining and robotic process automation (RPA), in which a bot is hard-coded to follow a set of predefined rules for completing a workflow. Through case studies of a hospital and large B2B enterprise, we find that the adoption of RPA has been inhibited by high set-up costs (12--18 months), unreliable execution (60% initial accuracy), and burdensome maintenance (requiring multiple FTEs). Multimodal foundation models (FMs) such as GPT-4 offer a promising new approach for end-to-end workflow automation given their generalized reasoning and planning abilities. To study these capabilities we propose ECLAIR, a system to automate enterprise workflows with minimal human supervision. We conduct initial experiments showing that multimodal FMs can address the limitations of traditional RPA with (1) near-human-level understanding of workflows (93% accuracy on a workflow understanding task) and (2) instant set-up with minimal technical barrier (based solely on a natural language description of a workflow, ECLAIR achieves end-to-end completion rates of 40%). We identify human-AI collaboration, validation, and self-improvement as open challenges, and suggest ways they can be solved with data management techniques.",
                "authors": "Michael Wornow, A. Narayan, Krista Opsahl-Ong, Quinn McIntyre, Nigam H. Shah, Christopher Ré",
                "citations": 4
            },
            {
                "title": "Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking",
                "abstract": "Respiratory audio, such as coughing and breathing sounds, has predictive power for a wide range of healthcare applications, yet is currently under-explored. The main problem for those applications arises from the difficulty in collecting large labeled task-specific data for model development. Generalizable respiratory acoustic foundation models pretrained with unlabeled data would offer appealing advantages and possibly unlock this impasse. However, given the safety-critical nature of healthcare applications, it is pivotal to also ensure openness and replicability for any proposed foundation model solution. To this end, we introduce OPERA, an OPEn Respiratory Acoustic foundation model pretraining and benchmarking system, as the first approach answering this need. We curate large-scale respiratory audio datasets (~136K samples, over 400 hours), pretrain three pioneering foundation models, and build a benchmark consisting of 19 downstream respiratory health tasks for evaluation. Our pretrained models demonstrate superior performance (against existing acoustic models pretrained with general audio on 16 out of 19 tasks) and generalizability (to unseen datasets and new respiratory audio modalities). This highlights the great promise of respiratory acoustic foundation models and encourages more studies using OPERA as an open resource to accelerate research on respiratory audio for health. The system is accessible from https://github.com/evelyn0414/OPERA.",
                "authors": "Yuwei Zhang, Tong Xia, Jing Han, Y. Wu, Georgios Rizos, Yang Liu, Mohammed Mosuily, Jagmohan Chauhan, Cecilia Mascolo",
                "citations": 4
            },
            {
                "title": "Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology",
                "abstract": "Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. In response, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval. By benchmarking these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify weakly-supervised models as superior, achieving a P@1 of up to 0.594. This performance not only competes with a specialized model but does so without the need for fine-tuning. Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty. Despite these challenges, our research underscores the vast potential of foundation models for CBIR in radiology, proposing a shift towards versatile, general-purpose medical image retrieval systems that do not require specific tuning.",
                "authors": "Stefan Denner, David Zimmerer, Dimitrios Bounias, Markus Bujotzek, Shuhan Xiao, Lisa Kausch, Philipp Schader, Tobias Penzkofer, Paul F. Jäger, K. Maier-Hein",
                "citations": 4
            },
            {
                "title": "Speech Foundation Models on Intelligibility Prediction for Hearing-Impaired Listeners",
                "abstract": "Speech foundation models (SFMs) have been benchmarked on many speech processing tasks, often achieving state-of-the-art performance with minimal adaptation. However, the SFM paradigm has been significantly less explored for applications of interest to the speech perception community. In this paper we present a systematic evaluation of 10 SFMs on one such application: Speech intelligibility prediction. We focus on the non-intrusive setup of the Clarity Prediction Challenge 2 (CPC2), where the task is to predict the percentage of words correctly perceived by hearing-impaired listeners from speech-in-noise recordings. We propose a simple method that learns a lightweight specialized prediction head on top of frozen SFMs to approach the problem. Our results reveal statistically significant differences in performance across SFMs. Our method resulted in the winning submission in the CPC2, demonstrating its promise for speech perception applications.",
                "authors": "Santiago Cuervo, R. Marxer",
                "citations": 5
            },
            {
                "title": "Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology",
                "abstract": "Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. In response, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval. By benchmarking these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify weakly-supervised models as superior, achieving a P@1 of up to 0.594. This performance not only competes with a specialized model but does so without the need for fine-tuning. Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty. Despite these challenges, our research underscores the vast potential of foundation models for CBIR in radiology, proposing a shift towards versatile, general-purpose medical image retrieval systems that do not require specific tuning.",
                "authors": "Stefan Denner, David Zimmerer, Dimitrios Bounias, Markus Bujotzek, Shuhan Xiao, Lisa Kausch, Philipp Schader, Tobias Penzkofer, Paul F. Jäger, K. Maier-Hein",
                "citations": 4
            },
            {
                "title": "Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation",
                "abstract": "LLM-based agents have demonstrated impressive zero-shot performance in vision-language navigation (VLN) task. However, existing LLM-based methods often focus only on solving high-level task planning by selecting nodes in predefined navigation graphs for movements, overlooking low-level control in navigation scenarios. To bridge this gap, we propose AO-Planner, a novel Affordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates various foundation models to achieve affordances-oriented low-level motion planning and high-level decision-making, both performed in a zero-shot setting. Specifically, we employ a Visual Affordances Prompting (VAP) approach, where the visible ground is segmented by SAM to provide navigational affordances, based on which the LLM selects potential candidate waypoints and plans low-level paths towards selected waypoints. We further propose a high-level PathAgent which marks planned paths into the image input and reasons the most probable path by comprehending all environmental information. Finally, we convert the selected path into 3D coordinates using camera intrinsic parameters and depth information, avoiding challenging 3D predictions for LLMs. Experiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner achieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our method can also serve as a data annotator to obtain pseudo-labels, distilling its waypoint prediction ability into a learning-based predictor. This new predictor does not require any waypoint data from the simulator and achieves 47% SR competing with supervised methods. We establish an effective connection between LLM and 3D world, presenting novel prospects for employing foundation models in low-level motion control.",
                "authors": "Jiaqi Chen, Bingqian Lin, Xinmin Liu, Xiaodan Liang, Kwan-Yee K. Wong",
                "citations": 4
            },
            {
                "title": "A Comprehensive Survey of Foundation Models in Medicine",
                "abstract": "Foundation models (FMs) are large-scale deep learning models trained on massive datasets, often using self-supervised learning techniques. These models serve as a versatile base for a wide range of downstream tasks, including those in medicine and healthcare. FMs have demonstrated remarkable success across multiple healthcare domains. However, existing surveys in this field do not comprehensively cover all areas where FMs have made significant strides. In this survey, we present a comprehensive review of FMs in medicine, focusing on their evolution, learning strategies, flagship models, applications, and associated challenges. We examine how prominent FMs, such as the BERT and GPT families, are transforming various aspects of healthcare, including clinical large language models, medical image analysis, and omics research. Additionally, we provide a detailed taxonomy of FM-enabled healthcare applications, spanning clinical natural language processing, medical computer vision, graph learning, and other biology- and omics- related tasks. Despite the transformative potentials of FMs, they also pose unique challenges. This survey delves into these challenges and highlights open research questions and lessons learned to guide researchers and practitioners. Our goal is to provide valuable insights into the capabilities of FMs in health, facilitating responsible deployment and mitigating associated risks.",
                "authors": "Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang",
                "citations": 4
            },
            {
                "title": "Benchmarking foundation models as feature extractors for weakly-supervised computational pathology",
                "abstract": "Advancements in artificial intelligence have driven the development of numerous pathology foundation models capable of extracting clinically relevant information. However, there is currently limited literature independently evaluating these foundation models on truly external cohorts and clinically-relevant tasks to uncover adjustments for future improvements. In this study, we benchmarked 19 histopathology foundation models on 13 patient cohorts with 6,818 patients and 9,528 slides from lung, colorectal, gastric, and breast cancers. The models were evaluated on weakly-supervised tasks related to biomarkers, morphological properties, and prognostic outcomes. We show that a vision-language foundation model, CONCH, yielded the highest performance when compared to vision-only foundation models, with Virchow2 as close second. The experiments reveal that foundation models trained on distinct cohorts learn complementary features to predict the same label, and can be fused to outperform the current state of the art. An ensemble combining CONCH and Virchow2 predictions outperformed individual models in 55% of tasks, leveraging their complementary strengths in classification scenarios. Moreover, our findings suggest that data diversity outweighs data volume for foundation models. Our work highlights actionable adjustments to improve pathology foundation models.",
                "authors": "Peter Neidlinger, O. S. E. Nahhas, H. Muti, Tim Lenz, M. Hoffmeister, Hermann Brenner, M. Treeck, Rupert Langer, B. Dislich, Hans-Michael Behrens, Christoph Röcken, S. Foersch, Daniel Truhn, Antonio Marra, O. Saldanha, J. N. Kather",
                "citations": 4
            },
            {
                "title": "Advances and Open Challenges in Federated Learning with Foundation Models",
                "abstract": "—The integration of Foundation Models (FMs) with Federated Learning (FL) presents a transformative paradigm in Artificial Intelligence (AI), offering enhanced capabilities while addressing concerns of privacy, data decentralization, and computational efficiency. This paper provides a comprehensive survey of the emerging field of Federated Foundation Models ( FedFM ), elucidating their synergistic relationship and exploring novel methodologies, challenges, and future directions that the FL research field needs to focus on in order to thrive in the age of foundation models. A systematic multi-tiered taxonomy is proposed, categorizing existing FedFM approaches for model training, aggregation, trustworthiness, and incentivization. Key challenges, including how to enable FL to deal with high complexity of computational demands, privacy considerations, contribution evaluation, and communication efficiency, are thoroughly discussed. Moreover, the paper explores the intricate challenges of communication, scalability and security inherent in training/fine-tuning FMs via FL, highlighting the potential of quantum computing to revolutionize the training, inference, optimization and data encryption processes. This survey underscores the importance of further research to propel innovation in FedFM , emphasizing the need for developing trustworthy solutions. It serves as a foundational guide for researchers and practitioners interested in contributing to this interdisciplinary and rapidly advancing field.",
                "authors": "Chao Ren, Han Yu, Hongyi Peng, Xiaoli Tang, Anran Li, Yulan Gao, A. Tan, Bo Zhao, Xiaoxiao Li, Zengxiang Li, Qiang Yang",
                "citations": 5
            },
            {
                "title": "FedPFT: Federated Proxy Fine-Tuning of Foundation Models",
                "abstract": "Adapting Foundation Models (FMs) for down- stream tasks through Federated Learning (FL) emerges a promising strategy for protecting data privacy and valuable FMs. Existing methods fine- tune FM by allocating sub-FM to clients in FL, however, leading to suboptimal performance due to insufficient tuning and inevitable error accumula- tions of gradients. In this paper, we propose Feder- ated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation in downstream tasks through FL by two key modules. First, the sub-FM construction module employs a layer-wise com- pression approach, facilitating comprehensive FM fine-tuning across all layers by emphasizing those crucial neurons. Second, the sub-FM alignment module conducts a two-step distillations—layer- level and neuron-level—before and during FL fine- tuning respectively, to reduce error of gradient by accurately aligning sub-FM with FM under theo- retical guarantees. Experimental results on seven commonly used datasets (i.e., four text and three vi- sion) demonstrate the superiority of FedPFT. Our code is available at https://github.com/pzp-dzd/FedPFT.",
                "authors": "Zhaopeng Peng, Xiaoliang Fan, Yufan Chen, Zheng Wang, Shirui Pan, Chenglu Wen, Ruisheng Zhang, Cheng Wang",
                "citations": 4
            },
            {
                "title": "Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models",
                "abstract": "Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at https://github.com/RL4M/MED-PEFT.",
                "authors": "Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang",
                "citations": 4
            },
            {
                "title": "Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models",
                "abstract": "Go-Explore is a powerful family of algorithms designed to solve hard-exploration problems built on the principle of archiving discovered states, and iteratively returning to and exploring from the most promising states. This approach has led to superhuman performance across a wide variety of challenging problems including Atari games and robotic control, but requires manually designing heuristics to guide exploration (i.e., determine which states to save and explore from, and what actions to consider next), which is time-consuming and infeasible in general. To resolve this, we propose Intelligent Go-Explore (IGE) which greatly extends the scope of the original Go-Explore by replacing these handcrafted heuristics with the intelligence and internalized human notions of interestingness captured by giant pretrained foundation models (FMs). This provides IGE with a human-like ability to instinctively identify how interesting or promising any new state is (e.g., discovering new objects, locations, or behaviors), even in complex environments where heuristics are hard to define. Moreover, IGE offers the exciting opportunity to recognize and capitalize on serendipitous discoveries-states encountered during exploration that are valuable in terms of exploration, yet where what makes them interesting was not anticipated by the human user. We evaluate our algorithm on a diverse range of language and vision-based tasks that require search and exploration. Across these tasks, IGE strongly exceeds classic reinforcement learning and graph search baselines, and also succeeds where prior state-of-the-art FM agents like Reflexion completely fail. Overall, Intelligent Go-Explore combines the tremendous strengths of FMs and the powerful Go-Explore algorithm, opening up a new frontier of research into creating more generally capable agents with impressive exploration capabilities.",
                "authors": "Cong Lu, Shengran Hu, Jeff Clune",
                "citations": 4
            },
            {
                "title": "Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models",
                "abstract": "Speech foundation models (SFMs) have achieved state-of-the-art results for various speech tasks in supervised (e.g. Whisper) or self-supervised systems (e.g. WavLM). However, the performance of SFMs for child ASR has not been systematically studied. In addition, there is no benchmark for child ASR with standard evaluations, making the comparisons of novel ideas difficult. In this paper, we initiate and present a comprehensive benchmark on several child speech databases based on various SFMs (Whisper, Wav2vec2.0, HuBERT, and WavLM). Moreover, we investigate finetuning strategies by comparing various data augmentation and parameter-efficient finetuning (PEFT) methods. We observe that the behaviors of these methods are different when the model size increases. For example, PEFT matches the performance of full finetuning for large models but worse for small models. To stabilize finetuning using augmented data, we propose a perturbation invariant finetuning (PIF) loss as a regularization.",
                "authors": "Ruchao Fan, Natarajan Balaji Shankar, Abeer Alwan",
                "citations": 4
            },
            {
                "title": "Towards a Framework for Openness in Foundation Models: Proceedings from the Columbia Convening on Openness in Artificial Intelligence",
                "abstract": "Over the past year, there has been a robust debate about the benefits and risks of open sourcing foundation models. However, this discussion has often taken place at a high level of generality or with a narrow focus on specific technical attributes. In part, this is because defining open source for foundation models has proven tricky, given its significant differences from traditional software development. In order to inform more practical and nuanced decisions about opening AI systems, including foundation models, this paper presents a framework for grappling with openness across the AI stack. It summarizes previous work on this topic, analyzes the various potential reasons to pursue openness, and outlines how openness varies in different parts of the AI stack, both at the model and at the system level. In doing so, its authors hope to provide a common descriptive framework to deepen a nuanced and rigorous understanding of openness in AI and enable further work around definitions of openness and safety in AI.",
                "authors": "Adrien Basdevant, Camille François, Victor Storchan, Kevin Bankston, Ayah Bdeir, Brian Behlendorf, M. Debbah, Sayash Kapoor, Yann LeCun, Mark Surman, Helen King-Turvey, Nathan Lambert, Stefano Maffulli, Nik Marda, Govind Shivkumar, Justine Tunney",
                "citations": 4
            },
            {
                "title": "Training and Serving System of Foundation Models: A Comprehensive Survey",
                "abstract": "Foundation models (e.g., ChatGPT, DALL-E, PengCheng Mind, PanGu-$\\Sigma$) have demonstrated extraordinary performance in key technological areas, such as natural language processing and visual recognition, and have become the mainstream trend of artificial general intelligence. This has led more and more major technology giants to dedicate significant human and financial resources to actively develop their foundation model systems, which drives continuous growth of these models' parameters. As a result, the training and serving of these models have posed significant challenges, including substantial computing power, memory consumption, bandwidth demands, etc. Therefore, employing efficient training and serving strategies becomes particularly crucial. Many researchers have actively explored and proposed effective methods. So, a comprehensive survey of them is essential for system developers and researchers. This paper extensively explores the methods employed in training and serving foundation models from various perspectives. It provides a detailed categorization of these state-of-the-art methods, including finer aspects such as network, computing, and storage. Additionally, the paper summarizes the challenges and presents a perspective on the future development direction of foundation model systems. Through comprehensive discussion and analysis, it hopes to provide a solid theoretical basis and practical guidance for future research and applications, promoting continuous innovation and development in foundation model systems.",
                "authors": "Jiahang Zhou, Yanyu Chen, Zicong Hong, Wuhui Chen, Yue Yu, Tao Zhang, Hui Wang, Chuan-fu Zhang, Zibin Zheng",
                "citations": 4
            },
            {
                "title": "Benchmark Early and Red Team Often: A Framework for Assessing and Managing Dual-Use Hazards of AI Foundation Models",
                "abstract": "A concern about cutting-edge or “frontier” AI foundation models is that an adversary may use the models for preparing chemical, biological, radiological, nuclear (CBRN), cyber, or other attacks. At least two methods can identify foundation models with potential dual-use capability; each method has advantages and disadvantages:\nA. Open benchmarks (based on openly available questions and answers), which are low-cost but accuracy-limited by the need to omit security-sensitive details, and\nB. Closed red team evaluations (based on private evaluation by CBRN and cyber experts), which are higher in cost but can achieve higher accuracy by incorporating sensitive details.\nWe propose a research and risk-management approach using a combination of methods including both open benchmarks and closed red team evaluations, in a way that leverages advantages of both methods.\nWe recommend that one or more groups of researchers with sufficient resources and access to a range of near-frontier and frontier foundation models:\n1. Run a set of foundation models through dual-use capability evaluation benchmarks and red teamevaluations, then2. Analyze the resulting sets of models’ scores on benchmark and red team evaluations to see howcorrelated those are.\nIf, as we expect, there is substantial correlation between the dual-use potential benchmark scores and the red team evaluation scores, then implications include the following:\n• The open benchmarks should be used frequently during foundation model development as a quick,low-cost measure of a model’s dual-use potential; and• If a particular model gets a high score on the dual-use potential benchmark, then more in-depth redteam assessments of that model’s dual-use capability should be performed.\nWe also discuss limitations and mitigations for our approach, e.g., if model developers try to game benchmarks by including a version of benchmark test data in a model’s training data.",
                "authors": "Anthony M. Barrett, Krystal Jackson, Evan R. Murphy, Nada Madkour, Jessica Newman",
                "citations": 4
            },
            {
                "title": "An Integrated Data Processing Framework for Pretraining Foundation Models",
                "abstract": "The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code and demonstration videos are accessible on GitHub.",
                "authors": "Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, Jiaxin Mao",
                "citations": 4
            },
            {
                "title": "Pruning Foundation Models for High Accuracy without Retraining",
                "abstract": "Despite the superior performance, it is challenging to deploy foundation models or large language models (LLMs) due to their massive parameters and computations. While pruning is a promising technique to reduce model size and accelerate the inference, the traditional pruning techniques can hardly be applied for LLMs as they need to finetune the model on the full dataset with multiple epochs consuming massive data and hardware resources. To deal with this problem, post-training pruning methods are proposed to prune LLMs in one-shot without retraining. However, their accuracy after pruning may suffer from certain performance degradation due to the lack of retraining with massive data. To address this issue, in this paper, we first formulate the post-training problem for layer-wise LLM compression to simultaneously prune multiple weights in LLMs. Next, we provide an optimal solution for this problem and design our post-training pruning algorithm for both unstructured and semi-structured sparsity. Our extensive experiments demonstrate the superior performance of the proposed methods in comparison to SOTA baselines across various LLM families including transformer-based LLMs and Mamba-based LLMs. Code link: https://github.com/piuzha/APT",
                "authors": "Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Yanzhi Wang, Xue Lin",
                "citations": 5
            },
            {
                "title": "Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning",
                "abstract": "Semi-supervised learning (SSL) has witnessed remarkable progress, resulting in the emergence of numerous method variations. However, practitioners often encounter challenges when attempting to deploy these methods due to their subpar performance. In this paper, we present a novel SSL approach named FineSSL that significantly addresses this limitation by adapting pre-trained foundation models. We identify the aggregated biases and cognitive deviation problems inherent in foundation models, and propose a simple yet effective solution by imposing balanced margin softmax and decoupled label smoothing. Through extensive experiments, we demonstrate that FineSSL sets a new state of the art for SSL on multiple benchmark datasets, reduces the training cost by over six times, and can seamlessly integrate various fine-tuning and modern SSL algorithms. The source code is available at https://github.com/Gank0078/FineSSL.",
                "authors": "Kai Gan, Tong Wei",
                "citations": 4
            },
            {
                "title": "How to Benchmark Vision Foundation Models for Semantic Segmentation?",
                "abstract": "Recent vision foundation models (VFMs) have demonstrated proficiency in various tasks but require supervised fine-tuning to perform the task of semantic segmentation effectively. Benchmarking their performance is essential for selecting current models and guiding future model developments for this task. The lack of a standardized benchmark complicates comparisons. Therefore, the primary objective of this paper is to study how VFMs should be benchmarked for semantic segmentation. To do so, various VFMs are fine-tuned under various settings, and the impact of individual settings on the performance ranking and training time is assessed. Based on the results, the recommendation is to fine-tune the ViT-B variants of VFMs with a 16 × 16 patch size and a linear decoder, as these settings are representative of using a larger model, more advanced decoder and smaller patch size, while reducing training time by more than 13 times. Using multiple datasets for training and evaluation is also recommended, as the performance ranking across datasets and domain shifts varies. Linear probing, a common practice for some VFMs, is not recommended, as it is not representative of end-to-end fine-tuning. The benchmarking setup recommended in this paper enables a performance analysis of VFMs for semantic segmentation. The findings of such an analysis reveal that pretraining with promptable segmentation is not beneficial, whereas masked image modeling (MIM) with abstract representations is crucial, even more important than the type of supervision used. The code for efficiently fine-tuning VFMs for semantic segmentation can be accessed through the project page 1.",
                "authors": "Tommie Kerssies, Daan de Geus, Gijs Dubbelman",
                "citations": 5
            },
            {
                "title": "Semantic-Aware Power Allocation for Generative Semantic Communications with Foundation Models",
                "abstract": "Recent advancements in diffusion models have made a significant breakthrough in generative modeling. The combination of the generative model and semantic communication (SemCom) enables high-fidelity semantic information exchange at ultra-low rates. A novel generative SemCom framework for image tasks is proposed, wherein pre-trained foundation models serve as semantic encoders and decoders for semantic feature extractions and image regenerations, respectively. The mathematical relationship between the transmission reliability and the perceptual quality of the regenerated image and the semantic values of semantic features are modeled, which are obtained by conducting numerical simulations on the Kodak dataset. We also investigate the semantic-aware power allocation problem, with the objective of minimizing the total power consumption while guaranteeing semantic performance. To solve this problem, two semanticaware power allocation methods are proposed by constraint decoupling and bisection search, respectively. Numerical results show that the proposed semantic-aware methods demonstrate superior performance compared to the conventional one in terms of total power consumption.",
                "authors": "Chunmei Xu, Mahdi Boloursaz Mashhadi, Yi Ma, Rahim Tafazolli",
                "citations": 5
            },
            {
                "title": "Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability",
                "abstract": "Research on geospatial foundation models (GFMs) has become a trending topic in geospatial artificial intelligence (AI) research due to their potential for achieving high generalizability and domain adaptability, reducing model training costs for individual researchers. Unlike large language models, such as ChatGPT, constructing visual foundation models for image analysis, particularly in remote sensing, encountered significant challenges such as formulating diverse vision tasks into a general problem framework. This paper evaluates the recently released NASA-IBM GFM Prithvi for its predictive performance on high-level image analysis tasks across multiple benchmark datasets. Prithvi was selected because it is one of the first open-source GFMs trained on time-series of high-resolution remote sensing imagery. A series of experiments were designed to assess Prithvi's performance as compared to other pre-trained task-specific AI models in geospatial image analysis. New strategies, including band adaptation, multi-scale feature generation, and fine-tuning techniques, are introduced and integrated into an image analysis pipeline to enhance Prithvi's domain adaptation capability and improve model performance. In-depth analyses reveal Prithvi's strengths and weaknesses, offering insights for both improving Prithvi and developing future visual foundation models for geospatial tasks.",
                "authors": "Chia-Yu Hsu, Wenwen Li, Sizhe Wang",
                "citations": 5
            },
            {
                "title": "GenBench: A Benchmarking Suite for Systematic Evaluation of Genomic Foundation Models",
                "abstract": "The Genomic Foundation Model (GFM) paradigm is expected to facilitate the extraction of generalizable representations from massive genomic data, thereby enabling their application across a spectrum of downstream applications. Despite advancements, a lack of evaluation framework makes it difficult to ensure equitable assessment due to experimental settings, model intricacy, benchmark datasets, and reproducibility challenges. In the absence of standardization, comparative analyses risk becoming biased and unreliable. To surmount this impasse, we introduce GenBench, a comprehensive benchmarking suite specifically tailored for evaluating the efficacy of Genomic Foundation Models. GenBench offers a modular and expandable framework that encapsulates a variety of state-of-the-art methodologies. Through systematic evaluations of datasets spanning diverse biological domains with a particular emphasis on both short-range and long-range genomic tasks, firstly including the three most important DNA tasks covering Coding Region, Non-Coding Region, Genome Structure, etc. Moreover, We provide a nuanced analysis of the interplay between model architecture and dataset characteristics on task-specific performance. Our findings reveal an interesting observation: independent of the number of parameters, the discernible difference in preference between the attention-based and convolution-based models on short- and long-range tasks may provide insights into the future design of GFM.",
                "authors": "Zicheng Liu, Jiahui Li, Siyuan Li, Z. Zang, Cheng Tan, Yufei Huang, Yajing Bai, Stan Z. Li",
                "citations": 4
            },
            {
                "title": "Low-Resource Vision Challenges for Foundation Models",
                "abstract": "Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for deep learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we address this gap and explore the challenges of low-resource image tasks with vision foundation models. We first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share three challenges: data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on our three low-resource tasks demonstrate our proposals already provide a better baseline than transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. Project page: https://xiaobai1217.github.io/Low-Resource-Vision/.",
                "authors": "Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek",
                "citations": 4
            },
            {
                "title": "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models",
                "abstract": "The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount. This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities. By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners. Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research in this pivotal area.",
                "authors": "Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, Aman Chadha",
                "citations": 4
            },
            {
                "title": "On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models",
                "abstract": "The Open Whisper-style Speech Model (OWSM) series was introduced to achieve full transparency in building advanced speech-to-text (S2T) foundation models. To this end, OWSM models are trained on 25 public speech datasets, which are heterogeneous in multiple ways. In this study, we advance the OWSM series by introducing OWSM v3.2, which improves on prior models by investigating and addressing the impacts of this data heterogeneity. Our study begins with a detailed analysis of each dataset, from which we derive two key strategies: data filtering with proxy task to enhance data quality, and the incorporation of punctuation and true-casing using an open large language model (LLM). With all other configurations staying the same, OWSM v3.2 improves performance over the OWSM v3.1 baseline while using 15% less training data.",
                "authors": "Jinchuan Tian, Yifan Peng, William Chen, Kwanghee Choi, Karen Livescu, Shinji Watanabe",
                "citations": 4
            },
            {
                "title": "Federated Fine-Tuning for Pre-Trained Foundation Models Over Wireless Networks",
                "abstract": "Pre-trained foundation models (FMs), with extensive number of neurons, are key to advancing next-generation intelligence services, where personalizing these models requires massive amount of task-specific data and computational resources. The prevalent solution involves centralized processing at the edge server, which, however, raises privacy concerns due to the transmission of raw data. Instead, federated fine-tuning (FedFT) is an emerging privacy-preserving fine-tuning (FT) paradigm for personalized pre-trained foundation models. In particular, by integrating low-rank adaptation (LoRA) with federated learning (FL), federated LoRA enables the collaborative FT of a global model with edge devices, achieving comparable learning performance to full FT while training fewer parameters over distributed data and preserving raw data privacy. However, the limited radio resources and computation capabilities of edge devices pose significant challenges for deploying federated LoRA over wireless networks. To this paper, we propose a split federated LoRA framework, which deploys the computationally-intensive encoder of a pre-trained model at the edge server, while keeping the embedding and task modules at the edge devices. Building on this split framework, the paper provides a rigorous analysis of the upper bound of the convergence gap for the wireless federated LoRA system. This analysis motivates the formulation of a long-term upper bound minimization problem, where we decompose the formulated long-term mixed-integer programming (MIP) problem into sequential sub-problems using the Lyapunov technique. We then develop an online algorithm for effective device scheduling and bandwidth allocation. Simulation results demonstrate the effectiveness of the proposed online algorithm in enhancing learning performance.",
                "authors": "Zixin Wang, Yong Zhou, Yuanming Shi, K. Letaief",
                "citations": 4
            },
            {
                "title": "InternVideo2: Scaling Foundation Models for Multimodal Video Understanding",
                "abstract": null,
                "authors": "Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Jilan Xu, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang",
                "citations": 5
            },
            {
                "title": "Fast Imitation via Behavior Foundation Models",
                "abstract": "Imitation learning (IL) aims at producing agents that can imitate any behavior given a few expert demonstrations. Yet existing approaches require many demonstrations and/or running (online or offline) reinforcement learning (RL) algorithms for each new imitation task. Here we show that recent RL foundation models based on successor measures can imitate any expert behavior almost instantly with just a few demonstrations and no need for RL or fine-tuning, while accommodating several IL principles (behavioral cloning, feature matching, reward-based",
                "authors": "Matteo Pirotta, Andrea Tirinzoni, Ahmed Touati, A. Lazaric, Yann Ollivier",
                "citations": 5
            },
            {
                "title": "Mapping the individual, social and biospheric impacts of Foundation Models",
                "abstract": "Responding to the rapid roll-out and large-scale commercialization of foundation models, large language models, and generative AI, an emerging body of work is shedding light on the myriad impacts these technologies are having across society. Such research is expansive, ranging from the production of discriminatory, fake and toxic outputs, and privacy and copyright violations, to the unjust extraction of labor and natural resources. The same has not been the case in some of the most prominent AI governance initiatives in the global north like the UK’s AI Safety Summit and the G7’s Hiroshima process, which have influenced much of the international dialogue around AI governance. Despite the wealth of cautionary tales and evidence of algorithmic harm, there has been an ongoing over-emphasis within the AI governance discourse on technical matters of safety and global catastrophic or existential risks. This narrowed focus has tended to draw attention away from very pressing social and ethical challenges posed by the current brute-force industrialization of AI applications. To address such a visibility gap between real-world consequences and speculative risks, this paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI. Drawing on a review of the literature on the harms and risks of foundations models, and insights from critical data studies, science and technology studies, and environmental justice scholarship, we identify 14 categories of risks and harms and map them according to their individual, social, and biospheric impacts. We argue that this novel typology offers an integrative perspective to address the most urgent negative impacts of foundation models and their downstream applications. We conclude with recommendations on how this typology could be used to inform technical and normative interventions to advance responsible AI.",
                "authors": "Andrés Domínguez Hernández, Shyam Krishna, A. Perini, Michael Katell, SJ Bennett, Ann Borda, Youmna Hashem, Semeli Hadjiloizou, Sabeehah Mahomed, Smera Jayadeva, Mhairi Aitken, David Leslie",
                "citations": 5
            },
            {
                "title": "What Foundation Models can Bring for Robot Learning in Manipulation : A Survey",
                "abstract": "The realization of universal robots is an ultimate goal of researchers. However, a key hurdle in achieving this goal lies in the robots' ability to manipulate objects in their unstructured surrounding environments according to different tasks. The learning-based approach is considered an effective way to address generalization. The impressive performance of foundation models in the fields of computer vision and natural language suggests the potential of embedding foundation models into manipulation tasks as a viable path toward achieving general manipulation capability. However, we believe achieving general manipulation capability requires an overarching framework akin to auto driving. This framework should encompass multiple functional modules, with different foundation models assuming distinct roles in facilitating general manipulation capability. This survey focuses on the contributions of foundation models to robot learning for manipulation. We propose a comprehensive framework and detail how foundation models can address challenges in each module of the framework. What's more, we examine current approaches, outline challenges, suggest future research directions, and identify potential risks associated with integrating foundation models into this domain.",
                "authors": "Dingzhe Li, Yixiang Jin, Yuhao Sun, A. Yong, Hongze Yu, Jun Shi, Xiaoshuai Hao, Peng Hao, Huaping Liu, Fuchun Sun, Jianwei Zhang, Bin Fang",
                "citations": 5
            },
            {
                "title": "Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning",
                "abstract": "Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, we aim to develop a generic anomaly detection model applicable across multiple scenarios. To achieve this, we customize generic visual-language foundation models that possess extensive knowledge and robust reasoning abilities into anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers multi-modal prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling multi-modal anomaly detection and reasoning. Our preliminary studies demonstrate that combining visual and language prompts as conditions for customizing the models enhances anomaly detection performance. The customized models showcase the ability to detect anomalies across different data modalities such as images and point clouds. Qualitative case studies further highlight the anomaly detection and reasoning capabilities, particularly for multi-object scenes and temporal data. Our code is available at https://github.com/Xiaohao-Xu/Customizable-VLM.",
                "authors": "Xiaohao Xu, Yunkang Cao, Yongqi Chen, Weiming Shen, Xiaonan Huang",
                "citations": 4
            },
            {
                "title": "Recommendations for designing conversational companion robots with older adults through foundation models",
                "abstract": "Companion robots are aimed to mitigate loneliness and social isolation among older adults by providing social and emotional support in their everyday lives. However, older adults’ expectations of conversational companionship might substantially differ from what current technologies can achieve, as well as from other age groups like young adults. Thus, it is crucial to involve older adults in the development of conversational companion robots to ensure that these devices align with their unique expectations and experiences. The recent advancement in foundation models, such as large language models, has taken a significant stride toward fulfilling those expectations, in contrast to the prior literature that relied on humans controlling robots (i.e., Wizard of Oz) or limited rule-based architectures that are not feasible to apply in the daily lives of older adults. Consequently, we conducted a participatory design (co-design) study with 28 older adults, demonstrating a companion robot using a large language model (LLM), and design scenarios that represent situations from everyday life. The thematic analysis of the discussions around these scenarios shows that older adults expect a conversational companion robot to engage in conversation actively in isolation and passively in social settings, remember previous conversations and personalize, protect privacy and provide control over learned data, give information and daily reminders, foster social skills and connections, and express empathy and emotions. Based on these findings, this article provides actionable recommendations for designing conversational companion robots for older adults with foundation models, such as LLMs and vision-language models, which can also be applied to conversational robots in other domains.",
                "authors": "Bahar Irfan, Sanna Kuoppamäki, Gabriel Skantze",
                "citations": 5
            },
            {
                "title": "On the Evaluation of Speech Foundation Models for Spoken Language Understanding",
                "abstract": "The Spoken Language Understanding Evaluation (SLUE) suite of benchmark tasks was recently introduced to address the need for open resources and benchmarking of complex spoken language understanding (SLU) tasks, including both classification and sequence generation tasks, on natural speech. The benchmark has demonstrated preliminary success in using pre-trained speech foundation models (SFM) for these SLU tasks. However, the community still lacks a fine-grained understanding of the comparative utility of different SFMs. Inspired by this, we ask: which SFMs offer the most benefits for these complex SLU tasks, and what is the most effective approach for incorporating these SFMs? To answer this, we perform an extensive evaluation of multiple supervised and self-supervised SFMs using several evaluation protocols: (i) frozen SFMs with a lightweight prediction head, (ii) frozen SFMs with a complex prediction head, and (iii) fine-tuned SFMs with a lightweight prediction head. Although the supervised SFMs are pre-trained on much more speech recognition data (with labels), they do not always outperform self-supervised SFMs; the latter tend to perform at least as well as, and sometimes better than, supervised SFMs, especially on the sequence generation tasks in SLUE. While there is no universally optimal way of incorporating SFMs, the complex prediction head gives the best performance for most tasks, although it increases the inference time. We also introduce an open-source toolkit and performance leaderboard, SLUE-PERB, for these tasks and modeling strategies.",
                "authors": "Siddhant Arora, Ankita Pasad, Chung-Ming Chien, Jionghao Han, Roshan S. Sharma, Jee-weon Jung, Hira Dhamyal, William Chen, Suwon Shon, Hung-yi Lee, Karen Livescu, Shinji Watanabe",
                "citations": 4
            },
            {
                "title": "Leveraging Foundation Models for Crafting Narrative Visualization: A Survey",
                "abstract": "Narrative visualization effectively transforms data into engaging stories, making complex information accessible to a broad audience. Foundation models, essential for narrative visualization, inherently facilitate this process through their superior ability to handle natural language queries and answers, generate cohesive narratives, and enhance visual communication. Inspired by previous work in narrative visualization and recent advances in foundation models, we synthesized potential tasks and opportunities for foundation models at various stages of narrative visualization. In our study, we surveyed 77 papers to explore the role of foundation models in automatingnarrative visualization creation. We propose a reference model that leverages foundation models for crafting narrative visualization, categorizing the reviewed literature into four essential phases: Analysis, Narration, Visualization, and Interaction. Additionally, we identifyeight specific tasks where foundation models are applied across these stages. This study maps out the landscape of challenges and opportunities, providing insightful directions for future research and valuable guidance for scholars in the field.",
                "authors": "Yi He, Shixiong Cao, Yang Shi, Qing Chen, Ke Xu, Nan Cao",
                "citations": 5
            },
            {
                "title": "Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation",
                "abstract": "Open-vocabulary semantic segmentation (OVS) aims to segment images of arbitrary categories specified by class labels or captions. However, most previous best-performing methods, whether pixel grouping methods or region recognition methods, suffer from false matches between image features and category labels. We attribute this to the natural gap between the textual features and visual features. In this work, we rethink how to mitigate false matches from the perspective of image-to-image matching and propose a novel relation-aware intra-modal matching (RIM) framework for OVS based on visual foundation models. RIM achieves robust region classification by firstly constructing diverse image-modal reference features and then matching them with region features based on relation-aware ranking distribution. The proposed RIM enjoys several merits. First, the intra-modal reference features are better aligned, circumventing potential ambiguities that may arise in cross-modal matching. Second, the ranking-based matching process harnesses the structure information implicit in the inter-class relationships, making it more robust than comparing individually. Extensive experiments on three benchmarks demonstrate that RIM outperforms previous state-of-the-art methods by large margins, obtaining a lead of more than 10% in mIoU on PASCAL VOC benchmark.",
                "authors": "Yuan Wang, Rui Sun, Naisong Luo, Yuwen Pan, Tianzhu Zhang",
                "citations": 5
            },
            {
                "title": "Measuring Geographic Diversity of Foundation Models with a Natural Language-based Geo-guessing Experiment on GPT-4",
                "abstract": "Abstract. Generative AI based on foundation models provides a first glimpse into the world represented by machines trained on vast amounts of multimodal data ingested by these models during training. If we consider the resulting models as knowledge bases in their own right, this may open up new avenues for understanding places through the lens of machines. In this work, we adopt this thinking and select GPT-4, a state-of-the-art representative in the family of multimodal large language models, to study its geographic diversity regarding how well geographic features are represented. Using DBpedia abstracts as a ground-truth corpus for probing, our natural language–based geo-guessing experiment shows that GPT-4 may currently encode insufficient knowledge about several geographic feature types on a global level. On a local level, we observe not only this insufficiency but also inter-regional disparities in GPT-4’s geo-guessing performance on UNESCO World Heritage Sites that carry significance to both local and global populations, and the inter-regional disparities may become smaller as the geographic scale increases. Morever, whether assessing the geo-guessing performance on a global or local level, we find inter-model disparities in GPT-4’s geo-guessing performance when comparing its unimodal and multimodal variants. We hope this work can initiate a discussion on geographic diversity as an ethical principle within the GIScience community in the face of global socio-technical challenges.\n",
                "authors": "Zilong Liu, Krzysztof Janowicz, Kitty Currier, Meilin Shi",
                "citations": 4
            },
            {
                "title": "FoundationGrasp: Generalizable Task-Oriented Grasping with Foundation Models",
                "abstract": "Task-oriented grasping (TOG), which refers to synthesizing grasps on an object that are configurationally compatible with the downstream manipulation task, is the first milestone towards tool manipulation. Analogous to the activation of two brain regions responsible for semantic and geometric reasoning during cognitive processes, modeling the intricate relationship between objects, tasks, and grasps necessitates rich semantic and geometric prior knowledge about these elements. Existing methods typically restrict the prior knowledge to a closed-set scope, limiting their generalization to novel objects and tasks out of the training set. To address such a limitation, we propose FoundationGrasp, a foundation model-based TOG framework that leverages the open-ended knowledge from foundation models to learn generalizable TOG skills. Extensive experiments are conducted on the contributed Language and Vision Augmented TaskGrasp (LaViA-TaskGrasp) dataset, demonstrating the superiority of FoundationGrasp over existing methods when generalizing to novel object instances, object classes, and tasks out of the training set. Furthermore, the effectiveness of FoundationGrasp is validated in real-robot grasping and manipulation experiments on a 7-DoF robotic arm. Our code, data, appendix, and video are publicly available at https://sites.google.com/view/foundationgrasp.",
                "authors": "Chao Tang, Dehao Huang, Wenlong Dong, Ruinian Xu, Hong Zhang",
                "citations": 4
            },
            {
                "title": "Foundation Models for Education: Promises and Prospects",
                "abstract": "With the advent of foundation models like ChatGPT, educators are excited about the transformative role that artificial intelligence (AI) might play in propelling the next education revolution. The developing speed and the profound impact of foundation models in various industries force us to think deeply about the changes they will make to education, a domain that is critically important for the future of humans. In this article, we discuss the strengths of foundation models, such as personalized learning, education inequality, and reasoning capabilities, as well as the development of agent architecture tailored for education, which integrates AI agents with pedagogical frameworks to create adaptive learning environments. Furthermore, we highlight the risks and opportunities of AI overreliance and creativity. Finally, we envision a future where foundation models in education harmonize human and AI capabilities, fostering a dynamic, inclusive, and adaptive educational ecosystem.",
                "authors": "Tianlong Xu, Richard Tong, Jing Liang, Xing Fan, Haoyang Li, Qingsong Wen, Guansong Pang",
                "citations": 4
            },
            {
                "title": "OpenCity: Open Spatio-Temporal Foundation Models for Traffic Prediction",
                "abstract": "Accurate traffic forecasting is crucial for effective urban planning and transportation management, enabling efficient resource allocation and enhanced travel experiences. However, existing models often face limitations in generalization, struggling with zero-shot prediction on unseen regions and cities, as well as diminished long-term accuracy. This is primarily due to the inherent challenges in handling the spatial and temporal heterogeneity of traffic data, coupled with the significant distribution shift across time and space. In this work, we aim to unlock new possibilities for building versatile, resilient and adaptive spatio-temporal foundation models for traffic prediction. To achieve this goal, we introduce a novel foundation model, named OpenCity, that can effectively capture and normalize the underlying spatio-temporal patterns from diverse data characteristics, facilitating zero-shot generalization across diverse urban environments. OpenCity integrates the Transformer architecture with graph neural networks to model the complex spatio-temporal dependencies in traffic data. By pre-training OpenCity on large-scale, heterogeneous traffic datasets, we enable the model to learn rich, generalizable representations that can be seamlessly applied to a wide range of traffic forecasting scenarios. Experimental results demonstrate that OpenCity exhibits exceptional zero-shot predictive performance. Moreover, OpenCity showcases promising scaling laws, suggesting the potential for developing a truly one-for-all traffic prediction solution that can adapt to new urban contexts with minimal overhead. We made our proposed OpenCity model open-source and it is available at the following link: https://github.com/HKUDS/OpenCity.",
                "authors": "Zhonghang Li, Long Xia, Lei Shi, Yong Xu, Dawei Yin, Chao Huang",
                "citations": 4
            },
            {
                "title": "HEMM: Holistic Evaluation of Multimodal Foundation Models",
                "abstract": "Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models.",
                "authors": "P. Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei Yu, R. Salakhutdinov, Louis-Philippe Morency",
                "citations": 3
            },
            {
                "title": "CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models",
                "abstract": "Multimodal foundation models are prone to hallucination, generating outputs that either contradict the input or are not grounded by factual information. Given the diversity in architectures, training data and instruction tuning techniques, there can be large variations in systems' susceptibility to hallucinations. To assess system hallucination robustness, hallucination ranking approaches have been developed for specific tasks such as image captioning, question answering, summarization, or biography generation. However, these approaches typically compare model outputs to gold-standard references or labels, limiting hallucination benchmarking for new domains. This work proposes\"CrossCheckGPT\", a reference-free universal hallucination ranking for multimodal foundation models. The core idea of CrossCheckGPT is that the same hallucinated content is unlikely to be generated by different independent systems, hence cross-system consistency can provide meaningful and accurate hallucination assessment scores. CrossCheckGPT can be applied to any model or task, provided that the information consistency between outputs can be measured through an appropriate distance metric. Focusing on multimodal large language models that generate text, we explore two information consistency measures: CrossCheck-explicit and CrossCheck-implicit. We showcase the applicability of our method for hallucination ranking across various modalities, namely the text, image, and audio-visual domains. Further, we propose the first audio-visual hallucination benchmark,\"AVHalluBench\", and illustrate the effectiveness of CrossCheckGPT, achieving correlations of 98% and 89% with human judgements on MHaluBench and AVHalluBench, respectively.",
                "authors": "Guangzhi Sun, Potsawee Manakul, Adian Liusie, Kunat Pipatanakul, Chao Zhang, P. Woodland, Mark J. F. Gales",
                "citations": 3
            },
            {
                "title": "Competition between AI foundation models: dynamics and policy recommendations",
                "abstract": "\n Generative AI is set to become a critical technology for our modern economies. If we are currently experiencing a strong, dynamic competition between the underlying foundation models, legal institutions have an important role to play in ensuring that the spring of foundation models does not turn into a winter with an ecosystem frozen by a handful of players.",
                "authors": "Thibault Schrepel, Alex Pentland",
                "citations": 3
            },
            {
                "title": "Smart Mining With Autonomous Driving in Industry 5.0: Architectures, Platforms, Operating Systems, Foundation Models, and Applications",
                "abstract": "The increasing importance of mineral resources in contemporary society is becoming more prominent, playing an indispensable and crucial role in the global economy. These resources not only provide essential raw materials for the global economic system but also play an irreplaceable role in supporting the development of modern industry, technology, and infrastructure. With the rapid development of intelligent technologies such as Industry 5.0 and advanced Large Language Models (LLMs), the mining industry is facing unprecedented opportunities and challenges. The development of smart mines has become a crucial direction for industry progress. This article aims to explore the strategic requirements for the development of smart mines by combining advanced products or technologies such as Chat-GPT (one of the successful applications of LLMs), digital twins, and scenario engineering. We propose a comprehensive architecture consisting of three different levels, the mining industrial Internet of Things (IoT) platform, mining operating systems, and foundation models. The systems and models empower the mining equipment for transportation. The architecture delivers a comprehensive solution that aligns perfectly with the demands of Industry 5.0. The application and validation outcomes of this intelligent solution showcase a noteworthy enhancement in mining efficiency and a reduction in safety risks, thereby laying a sturdy groundwork for the advent of Mining 5.0.",
                "authors": "Long Chen, Yuchen Li, Wushour Silamu, Qingquan Li, Sirong Ge, Fei-Yue Wang",
                "citations": 9
            },
            {
                "title": "Personalized Adapter for Large Meteorology Model on Devices: Towards Weather Foundation Models",
                "abstract": "This paper demonstrates that pre-trained language models (PLMs) are strong foundation models for on-device meteorological variables modeling. We present LM-Weather, a generic approach to taming PLMs, that have learned massive sequential knowledge from the universe of natural language databases, to acquire an immediate capability to obtain highly customized models for heterogeneous meteorological data on devices while keeping high efficiency. Concretely, we introduce a lightweight personalized adapter into PLMs and endows it with weather pattern awareness. During communication between clients and the server, low-rank-based transmission is performed to effectively fuse the global knowledge among devices while maintaining high communication efficiency and ensuring privacy. Experiments on real-wold dataset show that LM-Weather outperforms the state-of-the-art results by a large margin across various tasks (e.g., forecasting and imputation at different scales). We provide extensive and in-depth analyses experiments, which verify that LM-Weather can (1) indeed leverage sequential knowledge from natural language to accurately handle meteorological sequence, (2) allows each devices obtain highly customized models under significant heterogeneity, and (3) generalize under data-limited and out-of-distribution (OOD) scenarios.",
                "authors": "Shengchao Chen, Guodong Long, Jing Jiang, Chengqi Zhang",
                "citations": 3
            },
            {
                "title": "Foundation Models for Recommender Systems: A Survey and New Perspectives",
                "abstract": "Recently, Foundation Models (FMs), with their extensive knowledge bases and complex architectures, have offered unique opportunities within the realm of recommender systems (RSs). In this paper, we attempt to thoroughly examine FM-based recommendation systems (FM4RecSys). We start by reviewing the research background of FM4RecSys. Then, we provide a systematic taxonomy of existing FM4RecSys research works, which can be divided into four different parts including data characteristics, representation learning, model type, and downstream tasks. Within each part, we review the key recent research developments, outlining the representative models and discussing their characteristics. Moreover, we elaborate on the open problems and opportunities of FM4RecSys aiming to shed light on future research directions in this area. In conclusion, we recap our findings and discuss the emerging trends in this field.",
                "authors": "Chengkai Huang, Tong Yu, Kaige Xie, Shuai Zhang, Lina Yao, Julian McAuley",
                "citations": 3
            },
            {
                "title": "Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging",
                "abstract": "Advances in artificial intelligence (AI) have achieved expert-level performance in medical imaging applications. Notably, self-supervised vision-language foundation models can detect a broad spectrum of pathologies without relying on explicit training annotations. However, it is crucial to ensure that these AI models do not mirror or amplify human biases, thereby disadvantaging historically marginalized groups such as females or Black patients. The manifestation of such biases could systematically delay essential medical care for certain patient subgroups. In this study, we investigate the algorithmic fairness of state-of-the-art vision-language foundation models in chest X-ray diagnosis across five globally-sourced datasets. Our findings reveal that compared to board-certified radiologists, these foundation models consistently underdiagnose marginalized groups, with even higher rates seen in intersectional subgroups, such as Black female patients. Such demographic biases present over a wide range of pathologies and demographic attributes. Further analysis of the model embedding uncovers its significant encoding of demographic information. Deploying AI systems with these biases in medical imaging can intensify pre-existing care disparities, posing potential challenges to equitable healthcare access and raising ethical questions about their clinical application.",
                "authors": "Yuzhe Yang, Yujia Liu, Xin Liu, A. Gulhane, Domenico Mastrodicasa, Wei Wu, Edward J Wang, Dushyant W. Sahani, Shwetak N. Patel",
                "citations": 3
            },
            {
                "title": "Can Visual Foundation Models Achieve Long-term Point Tracking?",
                "abstract": "Large-scale vision foundation models have demonstrated remarkable success across various tasks, underscoring their robust generalization capabilities. While their proficiency in two-view correspondence has been explored, their effectiveness in long-term correspondence within complex environments remains unexplored. To address this, we evaluate the geometric awareness of visual foundation models in the context of point tracking: (i) in zero-shot settings, without any training; (ii) by probing with low-capacity layers; (iii) by fine-tuning with Low Rank Adaptation (LoRA). Our findings indicate that features from Stable Diffusion and DINOv2 exhibit superior geometric correspondence abilities in zero-shot settings. Furthermore, DINOv2 achieves performance comparable to supervised models in adaptation settings, demonstrating its potential as a strong initialization for correspondence learning.",
                "authors": "Görkay Aydemir, Weidi Xie, Fatma Güney",
                "citations": 3
            },
            {
                "title": "AI Foundation Models in Remote Sensing: A Survey",
                "abstract": "Artificial Intelligence (AI) technologies have profoundly transformed the field of remote sensing, revolutionizing data collection, processing, and analysis. Traditionally reliant on manual interpretation and task-specific models, remote sensing has been significantly enhanced by the advent of foundation models--large-scale, pre-trained AI models capable of performing a wide array of tasks with unprecedented accuracy and efficiency. This paper provides a comprehensive survey of foundation models in the remote sensing domain, covering models released between June 2021 and June 2024. We categorize these models based on their applications in computer vision and domain-specific tasks, offering insights into their architectures, pre-training datasets, and methodologies. Through detailed performance comparisons, we highlight emerging trends and the significant advancements achieved by these foundation models. Additionally, we discuss the technical challenges, practical implications, and future research directions, addressing the need for high-quality data, computational resources, and improved model generalization. Our research also finds that pre-training methods, particularly self-supervised learning techniques like contrastive learning and masked autoencoders, significantly enhance the performance and robustness of foundation models in remote sensing tasks such as scene classification, object detection, and other applications. This survey aims to serve as a resource for researchers and practitioners by providing a panorama of advances and promising pathways for continued development and application of foundation models in remote sensing.",
                "authors": "Siqi Lu, Junlin Guo, James Zimmer-Dauphinee, Jordan M Nieusma, Xiao Wang, P. VanValkenburgh, Steven A. Wernke, Yuankai Huo",
                "citations": 3
            },
            {
                "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts",
                "abstract": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.",
                "authors": "X. Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin",
                "citations": 3
            },
            {
                "title": "Learning with Noisy Foundation Models",
                "abstract": "Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning.",
                "authors": "Hao Chen, Jindong Wang, Zihan Wang, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj",
                "citations": 3
            },
            {
                "title": "Open-Vocabulary 3D Semantic Segmentation with Foundation Models",
                "abstract": "In dynamic 3D environments, the ability to recognize a diverse range of objects without the constraints of predefined categories is indispensable for real-world applications. In response to this need, we introduce OV3D, an innovative framework designed for open-vocabulary 3D semantic segmentation. OV3D leverages the broad open-world knowledge embedded in vision and language foundation models to establish a fine-grained correspondence between 3D points and textual entity descriptions. These entity descriptions are enriched with contextual information, enabling a more open and comprehensive understanding. By seamlessly aligning 3D point features with entity text features, OV3D empowers open-vocabulary recognition in the 3D domain, achieving state-of-the-art open-vocabulary semantic segmentation performance across multiple datasets, including ScanNet, Matterport3D, and nuScenes.",
                "authors": "Li Jiang, Shaoshuai Shi, B. Schiele",
                "citations": 3
            },
            {
                "title": "Low-Rank Knowledge Decomposition for Medical Foundation Models",
                "abstract": "The popularity of large-scale pretraining has promoted the development of medical foundation models. However, some studies have shown that although foundation models exhibit strong general feature extraction capabilities, their performance on specific tasks is still inferior to task-specific methods. In this paper, we explore a new perspective called “Knowledge Decomposition” to improve the performance on specific medical tasks, which deconstruct the foundation model into multiple lightweight expert models, each dedicated to a particular task, with the goal of improving specialization while concurrently mitigating resource expenditure. To accomplish the above objective, we design a novel framework named Low-Rank Knowledge De-composition (LoRKD), which explicitly separates graidents by incorporating low-rank expert modules and the efficient knowledge separation convolution. Extensive experimental results demonstrate that the decomposed models perform well in terms of performance and transferability, even surpassing the original foundation models. Source code is available at: https://github.com/MediaBrain-SJTU/LoRKD",
                "authors": "Yuhang Zhou, Haolin Li, Siyuan Du, Jiangchao Yao, Ya Zhang, Yanfeng Wang",
                "citations": 3
            },
            {
                "title": "MAKEN: Improving Medical Report Generation with Adapter Tuning and Knowledge Enhancement in Vision-Language Foundation Models",
                "abstract": "Medical report generation demands automatic creation of coherent and precise descriptions for medical images. However, the scarcity of labelled medical image-report pairs poses formidable challenges in developing large-scale neural networks capable of harnessing the potential of artificial intelligence, exemplified by large language models. This study builds upon the state-of-the-art vision-language pre-training and fine-tuning approach, BLIP-2, to customize general large-scale foundation models. Integrating adapter tuning and a medical knowledge enhancement loss, our model significantly improves accuracy and coherence. Validation on the dataset of ImageCLEFmedical 2023 demonstrates our model’s prowess, achieving the best-averaged results against several state-of-the-art methods. Significant improvements in ROUGE and CIDEr underscore our method’s efficacy, highlighting promising outcomes for the rapid medical-domain adaptation of the vision-language foundation models in addressing challenges posed by data scarcity. Our codes and model weights are available at https://openi.pcl.ac.cn/OpenMedIA/MAKEN.",
                "authors": "Shibin Wu, Bang Yang, Zhiyu Ye, Haoqian Wang, Hairong Zheng, Tong Zhang",
                "citations": 3
            },
            {
                "title": "Eureka: Evaluating and Understanding Large Foundation Models",
                "abstract": "Rigorous and reproducible evaluation is critical for assessing the state of the art and for guiding scientific advances in Artificial Intelligence. Evaluation is challenging in practice due to several reasons, including benchmark saturation, lack of transparency in methods used for measurement, development challenges in extracting measurements for generative tasks, and, more generally, the extensive number of capabilities required for a well-rounded comparison across models. We make three contributions to alleviate the above challenges. First, we present Eureka, an open-source framework for standardizing evaluations of large foundation models beyond single-score reporting and rankings. Second, we introduce Eureka-Bench as an extensible collection of benchmarks testing capabilities that (i) are still challenging for state-of-the-art models and (ii) represent fundamental but overlooked language and multimodal capabilities. The inherent space for improvement in non-saturated benchmarks enables us to discover meaningful differences between models at a capability level. Third, using Eureka, we conduct an analysis of 12 state-of-the-art models, providing in-depth insights into failure understanding and model comparison, which can be leveraged to plan targeted improvements. In contrast to recent trends in reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for some capabilities. Despite the recent improvements, current models still struggle with several fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.",
                "authors": "Vidhisha Balachandran, Jingya Chen, Neel Joshi, Besmira Nushi, Hamid Palangi, Eduardo Salinas, Vibhav Vineet, James Woffinden-Luey, Safoora Yousefi",
                "citations": 3
            },
            {
                "title": "As Firm As Their Foundations: Can open-sourced foundation models be used to create adversarial examples for downstream tasks?",
                "abstract": "Foundation models pre-trained on web-scale vision-language data, such as CLIP, are widely used as cornerstones of powerful machine learning systems. While pre-training offers clear advantages for downstream learning, it also endows downstream models with shared adversarial vulnerabilities that can be easily identified through the open-sourced foundation model. In this work, we expose such vulnerabilities in CLIP's downstream models and show that foundation models can serve as a basis for attacking their downstream systems. In particular, we propose a simple yet effective adversarial attack strategy termed Patch Representation Misalignment (PRM). Solely based on open-sourced CLIP vision encoders, this method produces adversaries that simultaneously fool more than 20 downstream models spanning 4 common vision-language tasks (semantic segmentation, object detection, image captioning and visual question-answering). Our findings highlight the concerning safety risks introduced by the extensive usage of public foundational models in the development of downstream systems, calling for extra caution in these scenarios.",
                "authors": "Anjun Hu, Jindong Gu, Francesco Pinto, Konstantinos Kamnitsas, Philip Torr",
                "citations": 3
            },
            {
                "title": "Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models",
                "abstract": "We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic speech recognition (ASR) systems in diverse target domains, such as noise and accents. STAR is developed for prevalent speech foundation models based on Transformer-related architecture with auto-regressive decoding (e.g., Whisper, Canary). Specifically, we propose a novel indicator that empirically integrates step-wise information during decoding to assess the token-level quality of pseudo labels without ground truth, thereby guiding model updates for effective unsupervised adaptation. Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation. Surprisingly, we also observe that STAR prevents the adapted model from the common catastrophic forgetting problem without recalling source-domain data. Furthermore, STAR exhibits high data efficiency that only requires less than one-hour unlabeled data, and seamless generality to alternative large speech models and speech translation tasks. Our code aims to open source to the research communities.",
                "authors": "Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Chengwei Qin, Pin-Yu Chen, Chng Eng Siong, Chao Zhang",
                "citations": 3
            },
            {
                "title": "Investigating the performance of foundation models on human 3’UTR sequences",
                "abstract": "Foundation models, such as DNABERT and Nucleotide Transformer have recently shaped a new direction in DNA research. Trained in an unsupervised manner on a vast quantity of genomic data, they can be used for a variety of downstream tasks, such as promoter prediction, DNA methylation prediction, gene network prediction or functional variant prioritization. However, these models are often trained and evaluated on entire genomes, neglecting genome partitioning into different functional regions. In our study, we investigate the efficacy of various unsupervised approaches, including genome-wide and 3’UTR-specific foundation models on human 3’UTR regions. Our evaluation includes downstream tasks specific for RNA biology, such as recognition of binding motifs of RNA binding proteins, detection of functional genetic variants, prediction of expression levels in massively parallel reporter assays, and estimation of mRNA half-life. Remarkably, models specifically trained on 3’UTR sequences demonstrate superior performance when compared to the established genome-wide foundation models in three out of four downstream tasks. Our results underscore the importance of considering genome partitioning into functional regions when training and evaluating foundation models.",
                "authors": "Sergey Vilov, M. Heinig",
                "citations": 3
            },
            {
                "title": "Medical image foundation models in assisting diagnosis of brain tumors: a pilot study.",
                "abstract": null,
                "authors": "Mengyao Chen, Meng Zhang, Lijuan Yin, Lu Ma, Renxin Ding, Tao Zheng, Qiang Yue, Su Lui, Huaiqiang Sun",
                "citations": 3
            },
            {
                "title": "Few-Shot Object Detection with Foundation Models",
                "abstract": "Few-shot object detection (FSOD) aims to detect objects with only a few training examples. Visual feature extraction and query-support similarity learning are the two critical components. Existing works are usually developed based on ImageNet pre-trained vision backbones and design sophis-ticated metric-learning networks for few-shot learning, but still have inferior accuracy. In this work, we study few-shot object detection using modern foundation models. First, vision-only contrastive pre-trained DINOv2 model is used for the vision backbone, which shows strong transferable performance without tuning the parameters. Second, Large Language Model (LLM) is employed for contextualized few-shot learning with the input of all classes and query image proposals. Language instructions are carefully designed to prompt the LLM to classify each proposal in context. The contextual information include proposal-proposal relations, proposal-class relations, and class-class relations, which can largely promote few-shot learning. We comprehensively evaluate the proposed model (FM-FSOD) in multiple FSOD benchmarks, achieving state-of-the-arts performance.",
                "authors": "Guangxing Han, Ser-Nam Lim",
                "citations": 3
            },
            {
                "title": "Uncertainty-Aware Pre-Trained Foundation Models for Patient Risk Prediction via Gaussian Process",
                "abstract": "Patient risk prediction models are crucial as they enable healthcare providers to proactively identify and address potential health risks. Large pre-trained foundation models offer remarkable performance in risk prediction tasks by analyzing multimodal patient data. However, a notable limitation of pre-trained foundation models lies in their deterministic predictions (i.e., lacking the ability to acknowledge uncertainty). We propose Gaussian Process-based foundation models to enable the generation of accurate predictions with instance-level uncertainty quantification, thus allowing healthcare professionals to make more informed and cautious decisions. Our proposed approach is principled and architecture-agnostic. Experimental results show that our proposed approach achieves competitive performance on classical classification metrics. Moreover, we observe that the accuracy of certain predictions is much higher than that of the uncertain ones, which validates the uncertainty awareness of our proposed method. Therefore, healthcare providers can trust low-uncertainty predictions and conduct more comprehensive investigations on high-uncertainty predictions, ultimately enhancing patient outcomes with less expert intervention.",
                "authors": "Jiaying Lu, Shifan Zhao, Wenjing Ma, Hui Shao, Xiao Hu, Yuanzhe Xi, Carl Yang",
                "citations": 3
            },
            {
                "title": "The devil is in the object boundary: towards annotation-free instance segmentation using Foundation Models",
                "abstract": "Foundation models, pre-trained on a large amount of data have demonstrated impressive zero-shot capabilities in various downstream tasks. However, in object detection and instance segmentation, two fundamental computer vision tasks heavily reliant on extensive human annotations, foundation models such as SAM and DINO struggle to achieve satisfactory performance. In this study, we reveal that the devil is in the object boundary, \\textit{i.e.}, these foundation models fail to discern boundaries between individual objects. For the first time, we probe that CLIP, which has never accessed any instance-level annotations, can provide a highly beneficial and strong instance-level boundary prior in the clustering results of its particular intermediate layer. Following this surprising observation, we propose $\\textbf{Zip}$ which $\\textbf{Z}$ips up CL$\\textbf{ip}$ and SAM in a novel classification-first-then-discovery pipeline, enabling annotation-free, complex-scene-capable, open-vocabulary object detection and instance segmentation. Our Zip significantly boosts SAM's mask AP on COCO dataset by 12.5% and establishes state-of-the-art performance in various settings, including training-free, self-training, and label-efficient finetuning. Furthermore, annotation-free Zip even achieves comparable performance to the best-performing open-vocabulary object detecters using base annotations. Code is released at https://github.com/ChengShiest/Zip-Your-CLIP",
                "authors": "Cheng Shi, Sibei Yang",
                "citations": 3
            },
            {
                "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases",
                "abstract": "Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.",
                "authors": "Zian Su, Xiangzhe Xu, Ziyang Huang, Kaiyuan Zhang, Xiangyu Zhang",
                "citations": 3
            },
            {
                "title": "Chameleon: Foundation Models for Fairness-aware Multi-modal Data Augmentation to Enhance Coverage of Minorities",
                "abstract": "Potential harms from the under-representation of minorities in data, particularly in multi-modal settings, is a well-recognized concern. While there has been extensive effort in detecting such under-representation, resolution has remained a challenge.\n With recent generative AI advancements, large language and foundation models have emerged as versatile tools across various domains. In this paper, we propose Chameleon, a system that efficiently utilizes these tools to augment a dataset with minimal addition of synthetically generated tuples to enhance the coverage of the under-represented groups. Our system applies quality and outlier-detection tests to ensure the quality and semantic integrity of the generated tuples. In order to minimize the rejection chance of the generated tuples, we propose multiple strategies to provide a guide for the foundation model. Our experiment results, in addition to confirming the efficiency of our proposed algorithms, illustrate our approach's effectiveness, as the model's unfairness in a downstream task significantly dropped after data repair using Chameleon.",
                "authors": "Mahdi Erfanian, H. V. Jagadish, Abolfazl Asudeh",
                "citations": 3
            },
            {
                "title": "Parametric Feature Transfer: One-shot Federated Learning with Foundation Models",
                "abstract": "In one-shot federated learning (FL), clients collaboratively train a global model in a single round of communication. Existing approaches for one-shot FL enhance communication efficiency at the expense of diminished accuracy. This paper introduces FedPFT (Federated Learning with Parametric Feature Transfer), a methodology that harnesses the transferability of foundation models to enhance both accuracy and communication efficiency in one-shot FL. The approach involves transferring per-client parametric models (specifically, Gaussian mixtures) of features extracted from foundation models. Subsequently, each parametric model is employed to generate synthetic features for training a classifier head. Experimental results on eight datasets demonstrate that FedPFT enhances the communication-accuracy frontier in both centralized and decentralized FL scenarios, as well as across diverse data-heterogeneity settings such as covariate shift and task shift, with improvements of up to 20.6%. Additionally, FedPFT adheres to the data minimization principle of FL, as clients do not send real features. We demonstrate that sending real features is vulnerable to potent reconstruction attacks. Moreover, we show that FedPFT is amenable to formal privacy guarantees via differential privacy, demonstrating favourable privacy-accuracy tradeoffs.",
                "authors": "Mahdi Beitollahi, Alex Bie, S. Hemati, Leo Maxime Brunswic, Xu Li, Xi Chen, Guojun Zhang",
                "citations": 3
            },
            {
                "title": "Foundation models for bioinformatics",
                "abstract": "Transformer‐based foundation models such as ChatGPTs have revolutionized our daily life and affected many fields including bioinformatics. In this perspective, we first discuss about the direct application of textual foundation models on bioinformatics tasks, focusing on how to make the most out of canonical large language models and mitigate their inherent flaws. Meanwhile, we go through the transformer‐based, bioinformatics‐tailored foundation models for both sequence and non‐sequence data. In particular, we envision the further development directions as well as challenges for bioinformatics foundation models.",
                "authors": "Ziyu Chen, Lin Wei, Ge Gao",
                "citations": 3
            },
            {
                "title": "The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning",
                "abstract": "Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI systems, as interpretable symbolic techniques provide formal behaviour guarantees. The challenge is how to effectively integrate neural and symbolic computation, to enable learning and reasoning from raw data. Existing pipelines that train the neural and symbolic components sequentially require extensive labelling, whereas end-to-end approaches are limited in terms of scalability, due to the combinatorial explosion in the symbol grounding problem. In this paper, we leverage the implicit knowledge within foundation models to enhance the performance in NeSy tasks, whilst reducing the amount of data labelling and manual engineering. We introduce a new architecture, called NeSyGPT, which fine-tunes a vision-language foundation model to extract symbolic features from raw data, before learning a highly expressive answer set program to solve a downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has superior accuracy over various baselines, and can scale to complex NeSy tasks. Finally, we highlight the effective use of a large language model to generate the programmatic interface between the neural and symbolic components, significantly reducing the amount of manual engineering required.",
                "authors": "Daniel Cunnington, Mark Law, Jorge Lobo, Alessandra Russo",
                "citations": 3
            },
            {
                "title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
                "abstract": "Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry. The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time. The rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide common sense reasoning that existing planners are missing. Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like how humans adapt to unexpected scenarios. Large language models have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment. While this application direction is very promising empirically, foundation models are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor. We argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model's decision, and detect when it may be hallucinating. In this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, and explore areas for further research in this exciting field.",
                "authors": "Neeloy Chakraborty, Melkior Ornik, K. Driggs-Campbell",
                "citations": 3
            },
            {
                "title": "Vision foundation models: can they be applied to astrophysics data?",
                "abstract": "Vision foundation models, which have demonstrated significant potential in multimedia applications, are often underutilized in the natural sciences. This is primarily due to mismatches between the nature of domain-specific scientific data and the typical training data used for foundation models, leading to distribution shift. Scientific images can have unique structure and characteristics, and researchers often face the challenge of optimizing model performance with only a few hundred or thousand labeled examples. Furthermore, the choice of vision foundation model can be non-trivial, as each model exhibits unique strengths and limitations, influenced by differences in architecture, training procedures, and the datasets used for training. In this work, we evaluate the application of various vision foundation models to images from optical and radio astronomy. Our results show that use of certain foundation models improves the classification accuracy of optical galaxy images compared to conventional supervised training. Similarly, we achieve equivalent or better performance in object detection tasks with radio images. However, their performance in classifying radio galaxy images is generally poor and often inferior to traditional supervised training results. These findings suggest that selecting suitable vision foundation models for astrophysics applications requires careful consideration of the model characteristics and alignment with the specific requirements of the downstream tasks.",
                "authors": "E. Lastufka, M. Drozdova, Vitaliy Kinakh, S. Voloshynovskiy",
                "citations": 2
            },
            {
                "title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs",
                "abstract": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant attention due to the prevalence of free-text node features in real-world applications and the advancements in Large Language Models (LLMs) that bolster TAG methodologies. However, current TAG approaches face two primary challenges: (i) Heavy reliance on label information and (ii) Limited cross-domain zero/few-shot transferability. These issues constrain the scaling of both data and model size, owing to high labor costs and scaling laws, complicating the development of graph foundation models with strong transferability. In this work, we propose the GraphCLIP framework to address these challenges by learning graph foundation models with strong cross-domain zero/few-shot transferability through a self-supervised contrastive graph-summary pretraining method. Specifically, we generate and curate large-scale graph-summary pair data with the assistance of LLMs, and introduce a novel graph-summary pretraining method, combined with invariant learning, to enhance graph foundation models with strong cross-domain zero-shot transferability. For few-shot learning, we propose a novel graph prompt tuning technique aligned with our pretraining objective to mitigate catastrophic forgetting and minimize learning costs. Extensive experiments show the superiority of GraphCLIP in both zero-shot and few-shot settings, while evaluations across various downstream tasks confirm the versatility of GraphCLIP. Our code is available at: https://github.com/ZhuYun97/GraphCLIP",
                "authors": "Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, Siliang Tang",
                "citations": 2
            },
            {
                "title": "A Survey of Foundation Models for Music Understanding",
                "abstract": "Music is essential in daily life, fulfilling emotional and entertainment needs, and connecting us personally, socially, and culturally. A better understanding of music can enhance our emotions, cognitive skills, and cultural connections. The rapid advancement of artificial intelligence (AI) has introduced new ways to analyze music, aiming to replicate human understanding of music and provide related services. While the traditional models focused on audio features and simple tasks, the recent development of large language models (LLMs) and foundation models (FMs), which excel in various fields by integrating semantic information and demonstrating strong reasoning abilities, could capture complex musical features and patterns, integrate music with language and incorporate rich musical, emotional and psychological knowledge. Therefore, they have the potential in handling complex music understanding tasks from a semantic perspective, producing outputs closer to human perception. This work, to our best knowledge, is one of the early reviews of the intersection of AI techniques and music understanding. We investigated, analyzed, and tested recent large-scale music foundation models in respect of their music comprehension abilities. We also discussed their limitations and proposed possible future directions, offering insights for researchers in this field.",
                "authors": "Wenjun Li, Ying Cai, Ziyang Wu, Wenyi Zhang, Yifan Chen, Rundong Qi, Mengqi Dong, Peigen Chen, Xiao Dong, Fenghao Shi, Lei Guo, Junwei Han, Bao Ge, Tianming Liu, Lin Gan, Tuo Zhang",
                "citations": 1
            },
            {
                "title": "On the Efficiency and Robustness of Vibration-Based Foundation Models for IoT Sensing: A Case Study",
                "abstract": "This paper demonstrates the potential of vibration-based Foundation Models (FMs), pre-trained with unlabeled sensing data, to improve the robustness of run-time inference in (a class of) IoT applications. A case study is presented featuring a vehicle classification application using acoustic and seismic sensing. The work is motivated by the success of foundation models in the areas of natural language processing and computer vision, leading to generalizations of the FM concept to other domains as well, where significant amounts of unlabeled data exist that can be used for self-supervised pre-training. One such domain is IoT applications. Foundation models for selected sensing modalities in the IoT domain can be pre-trained in an environment-agnostic fashion using available unlabeled sensor data and then fine-tuned to the deployment at hand using a small amount of labeled data. The paper shows that the pre-training/fine-tuning approach improves the robustness of downstream inference and facilitates adaptation to different environmental conditions. More specifically, we present a case study in a real-world setting to evaluate a simple (vibration-based) FM-like model, called FOCAL, demonstrating its superior robustness and adaptation, compared to conventional supervised deep neural networks (DNNs). We also demonstrate its superior convergence over supervised solutions. Our findings highlight the advantages of vibration-based FMs (and FM-inspired self-supervised models in general) in terms of inference robustness, runtime efficiency, and model adaptation (via fine-tuning) in resource-limited IoT settings.",
                "authors": "Tomoyoshi Kimura, Jinyang Li, Tianshi Wang, Denizhan Kara, Yizhuo Chen, Yigong Hu, Ruijie Wang, Maggie B. Wigness, Shengzhong Liu, Mani B. Srivastava, Suhas N. Diggavi, Tarek F. Abdelzaher",
                "citations": 1
            },
            {
                "title": "Multi-Spectral Remote Sensing Image Retrieval Using Geospatial Foundation Models",
                "abstract": "Image retrieval enables an efficient search through vast amounts of satellite imagery and returns similar images to a query. Deep learning models can identify images across various semantic concepts without the need for annotations. This work proposes to use Geospatial Foundation Models, like Prithvi, for remote sensing image retrieval with multiple benefits: i) the models encode multi-spectral satellite data and ii) generalize without further fine-tuning. We introduce two datasets to the retrieval task and observe a strong performance: Prithvi processes six bands and achieves a mean Average Precision of 97.62% on BigEarthNet-43 and 44.51% on ForestNet-12, outperforming other RGB-based models. Further, we evaluate three compression methods with binarized embeddings balancing retrieval speed and accuracy. They match the retrieval speed of much shorter hash codes while maintaining the same accuracy as floating-point embeddings but with a 32-fold compression. The code is available at https://github.com/IBM/remote-sensing-image-retrieval.",
                "authors": "Benedikt Blumenstiel, Viktoria Moor, Romeo Kienzler, Thomas Brunschwiler",
                "citations": 2
            },
            {
                "title": "Evaluating and Benchmarking Foundation Models for Earth Observation and Geospatial AI",
                "abstract": "When we are primarily interested in solving several problems jointly with a given prescribed high performance accuracy for each target application, then Foundation Models should for most cases be used rather than problem-specific models. We focus on the specific Computer Vision application of Foundation Models for Earth Observation (EO) and geospatial AI. These models can solve important problems we are tackling, including for example land cover classification, crop type mapping, flood segmentation, building density estimation, and road regression segmentation. In this paper, we show that for a limited number of labelled data, Foundation Models achieve improved performance compared to problem-specific models. In this work, we also present our proposed evaluation benchmark for Foundation Models for EO. Benchmarking the generalization performance of Foundation Models is important as it has become difficult to standardize a fair comparison across the many different models that have been proposed recently. We present the results using our evaluation benchmark for EO Foundation Models and show that Foundation Models are label efficient in the downstream tasks and help us solve problems we are tackling in EO and remote sensing.",
                "authors": "Nikolaos Dionelis, Casper Fibaek, Luke Camilleri, Andreas Luyts, Jente Bosmans, B. L. Saux",
                "citations": 2
            },
            {
                "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding",
                "abstract": "Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks. Code: https://github.com/YunzeMan/Lexicon3D",
                "authors": "Yunze Man, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Liangyan Gui, Yu-Xiong Wang",
                "citations": 2
            },
            {
                "title": "PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models",
                "abstract": "Recent success of vision foundation models have shown promising performance for the 2D perception tasks. However, it is difficult to train a 3D foundation network directly due to the limited dataset and it remains under explored whether existing foundation models can be lifted to 3D space seamlessly. In this paper, we present PointSeg, a novel training-free paradigm that leverages off-the-shelf vision foundation models to address 3D scene perception tasks. PointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to align their corresponding pixels across frames. Concretely, we design a two-branch prompts learning structure to construct the 3D point-box prompts pairs, combining with the bidirectional matching strategy for accurate point and proposal prompts generation. Then, we perform the iterative post-refinement adaptively when cooperated with different vision foundation models. Moreover, we design a affinity-aware merging algorithm to improve the final ensemble masks. PointSeg demonstrates impressive segmentation performance across various datasets, all without training. Specifically, our approach significantly surpasses the state-of-the-art specialist training-free model by 14.1$\\%$, 12.3$\\%$, and 12.6$\\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively. On top of that, PointSeg can incorporate with various foundation models and even surpasses the specialist training-based methods by 3.4$\\%$-5.4$\\%$ mAP across various datasets, serving as an effective generalist model.",
                "authors": "Qingdong He, Jinlong Peng, Zhengkai Jiang, Xiaobin Hu, Jiangning Zhang, Qiang Nie, Yabiao Wang, Chengjie Wang",
                "citations": 2
            },
            {
                "title": "Supervised Fine-tuning in turn Improves Visual Foundation Models",
                "abstract": "Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and vision-linguistic scenarios.",
                "authors": "Xiaohu Jiang, Yixiao Ge, Yuying Ge, Chun Yuan, Ying Shan",
                "citations": 2
            },
            {
                "title": "Enhancing Representation in Medical Vision-Language Foundation Models Via Multi-Scale Information Extraction Techniques",
                "abstract": "The development of medical vision-language foundation models has attracted significant attention in the field of medicine and healthcare due to their promising prospect in various clinical applications. While previous studies have commonly focused on feature learning at a single learning scale, investigation on integrating multi-scale information is lacking, which may hinder the potential for mutual reinforcement among these features. This paper aims to bridge this gap by proposing a method that effectively exploits multi-scale information to enhance the performance of medical foundation models. The proposed method simultaneously exploits features at the local, instance, modality and global aspects, facilitating comprehensive representation learning within the models. We evaluate the effectiveness of the proposed method on six open-source datasets across different clinical tasks, demonstrating its ability to enhance the performance of medical foundation models.",
                "authors": "Weijian Huang, Cheng Li, Hong-Yu Zhou, Jiarun Liu, Hao Yang, Yong Liang, Guangming Shi, Hairong Zheng, Shanshan Wang",
                "citations": 2
            },
            {
                "title": "TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models",
                "abstract": "One key challenge in backdoor attacks against large foundation models is the resource limits. Backdoor attacks usually require retraining the target model, which is impractical for very large foundation models. Existing backdoor attacks are mainly designed for supervised classifiers or small foundation models (e.g., BERT). None of these attacks has successfully compromised a very large foundation model, such as Llama-3-70B, especially with limited computational resources. In this paper, we propose TrojFM, a novel backdoor attack tailored for very large foundation models. Our primary technical contribution is the development of a novel backdoor injection method. This method forces a backdoored model to generate similar hidden representations for poisoned inputs regardless of their actual semantics. Our approach injects such backdoors by fine-tuning only a very small proportion of model parameters. This enables TrojFM to efficiently launch downstream task-agnostic backdoor attacks against very large foundation models under limited computational resources. Moreover, we optimize the fine-tuning process with our customized QLoRA technique, enabling launching our attack via only~\\textit{one A100 GPU}. Furthermore, we design a new trigger injection method to ensure our attack stealthiness. Through extensive experiments, we first demonstrate that TrojFM can launch effective backdoor attacks against widely used large GPT-style models without jeopardizing their normal functionalities (and outperforming existing attacks on BERT-style models). Furthermore, we show that TrojFM is resilient to SOTA defenses and is insensitive to changes in key hyper-parameters. Finally, we conduct a resource analysis to quantify that our method can significantly save computational and memory costs compared to existing backdoor attacks.",
                "authors": "Yuzhou Nie, Yanting Wang, Jinyuan Jia, Michael J. De Lucia, Nathaniel D. Bastian, Wenbo Guo, D. Song",
                "citations": 2
            },
            {
                "title": "Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT",
                "abstract": "Multiple Object Tracking (MOT) is a computer vision task that has been employed in a variety of sectors. Some common limitations in MOT are varying object appearances, occlusions, or crowded scenes. To address these challenges, machine learning methods have been extensively deployed, leveraging large datasets, sophisticated models, and substantial computational resources. Due to practical limitations, access to the above is not always an option. However, with the recent release of foundation models by prominent AI companies, pretrained models have been trained on vast datasets and resources using state-of-the-art methods. This work tries to leverage one such foundation model, called DINOv2, through using knowledge distillation. The proposed method uses a teacher-student architecture, where DINOv2 is the teacher and the FairMOT backbone HRNetv2 W18 is the student. The results imply that although the proposed method shows improvements in certain scenarios, it does not consistently outperform the original FairMOT model. These findings highlight the potential and limitations of applying foundation models in knowledge",
                "authors": "Niels G. Faber, Seyed Sahand Mohamadi Ziabari, F. Karimi Nejadasl",
                "citations": 2
            },
            {
                "title": "Harnessing Foundation Models for Image Anonymization",
                "abstract": "Traditional deep learning pipelines involve multiple intricate steps, from data acquisition to model training, fine-tuning, and deployment. However, recent advancements in foundation models, particularly in text-to-image generation, offer a paradigm shift in addressing tasks without the need for these conventional processes. In this paper, we explore how foundation models can be leveraged to solve tasks, specifically focusing on anonymization, without the requirement for training or fine-tuning. By bypassing traditional pipelines, we demonstrate the efficiency and effectiveness of this approach in achieving anonymization objectives directly from the foundation model's inherent knowledge. Our findings underscore the transformative potential of foundation models in simplifying and accelerating deep learning tasks, paving the way for novel applications in various domains.",
                "authors": "Luca Piano, Pietro Basci, Fabrizio Lamberti, Lia Morra",
                "citations": 1
            },
            {
                "title": "Foundation models for cardiovascular disease detection via biosignals from digital stethoscopes",
                "abstract": null,
                "authors": "George Mathew, Daniel Barbosa, John Prince, Subramaniam Venkatraman",
                "citations": 2
            },
            {
                "title": "Exploring Speech Foundation Models for Speaker Diarization in Child-Adult Dyadic Interactions",
                "abstract": "Speech foundation models, trained on vast datasets, have opened unique opportunities in addressing challenging low-resource speech understanding, such as child speech. In this work, we explore the capabilities of speech foundation models on child-adult speaker diarization. We show that exemplary foundation models can achieve 39.5% and 62.3% relative reductions in Diarization Error Rate and Speaker Confusion Rate, respectively, compared to previous speaker diarization methods. In addition, we benchmark and evaluate the speaker diarization results of the speech foundation models with varying the input audio window size, speaker demographics, and training data ratio. Our results highlight promising pathways for understanding and adopting speech foundation models to facilitate child speech understanding.",
                "authors": "Anfeng Xu, Kevin Huang, Tiantian Feng, Lue Shen, H. Tager-Flusberg, Shrikanth S. Narayanan",
                "citations": 2
            },
            {
                "title": "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models",
                "abstract": "Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FedKIM, designed to scale the medical foundation model within a federated learning framework. FedKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M^3OE) module. This method not only preserves privacy but also enhances the model’s ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FedKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data. Source codes are available at https://github.com/XiaochenWang-PSU/FedKIM.",
                "authors": "Xiaochen Wang, Jiaqi Wang, Houping Xiao, Jinghui Chen, Fenglong Ma",
                "citations": 2
            },
            {
                "title": "Foundation Models and Information Retrieval in Digital Pathology",
                "abstract": "The paper reviews the state-of-the-art of foundation models, LLMs, generative AI, information retrieval and CBIR in digital pathology",
                "authors": "H. R. Tizhoosh",
                "citations": 2
            },
            {
                "title": "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models",
                "abstract": "Recent developments in large speech foundation models like Whisper have led to their widespread use in many automatic speech recognition (ASR) applications. These systems incorporate `special tokens' in their vocabulary, such as $\\texttt{<|endoftext|>}$, to guide their language generation process. However, we demonstrate that these tokens can be exploited by adversarial attacks to manipulate the model's behavior. We propose a simple yet effective method to learn a universal acoustic realization of Whisper's $\\texttt{<|endoftext|>}$ token, which, when prepended to any speech signal, encourages the model to ignore the speech and only transcribe the special token, effectively `muting' the model. Our experiments demonstrate that the same, universal 0.64-second adversarial audio segment can successfully mute a target Whisper ASR model for over 97\\% of speech samples. Moreover, we find that this universal adversarial audio segment often transfers to new datasets and tasks. Overall this work demonstrates the vulnerability of Whisper models to `muting' adversarial attacks, where such attacks can pose both risks and potential benefits in real-world settings: for example the attack can be used to bypass speech moderation systems, or conversely the attack can also be used to protect private speech data.",
                "authors": "Vyas Raina, Rao Ma, Charles McGhee, Kate Knill, Mark J. F. Gales",
                "citations": 2
            },
            {
                "title": "Past, Present and Future Challenges in Sharing Science: From PhysioNet to Foundation Models",
                "abstract": "Over the last 25 years, the sharing of data and models for research in cardiology has evolved from sneaker-net to the internet - from mailing tapes and compact discs of a handful of well-curated recordings of an array of arrhythmias, to the high-speed download of an entire hospital database. Yet, bandwidth and local computation has not kept pace with the rate at which we can strip mine our data archives. Recently, the trend towards the development of large foundational models has required enormous computing power, making large-scale local clusters or centralized cloud instances of data and compute the only viable solution for developing such models. Instead of democ-ratizing access to data and compute, as was the intent of the pioneers in this field, this trend is leading to the balka-nization of innovation in the hands of the rich and powerful. Moreover, claims of foundation models in cardiology, physiology, and medical data in general, are largely premature because foundation models must be trained on a broad enough range of data so that they may be applied across a wide range of use cases. To do so requires broad representation, of both individuals and medical conditions. This article examines these trends and provides a discussion of the most promising future directions, together with potential solutions to the concentration of power and lack of diversity in foundation models.",
                "authors": "Gari Clifford",
                "citations": 2
            },
            {
                "title": "PCLmed: Champion Solution for ImageCLEFmedical 2024 Caption Prediction Challenge via Medical Vision-Language Foundation Models",
                "abstract": "Automatically generating captions and reports for medical images has become increasingly important due to the growing workload of radiologists in hospitals. To tackle this challenging task with limited annotation data, there is a rising interest in developing medical vision-language foundation models (Med-VLFMs). These models leverage the capabilities of vision foundation models and large language models (LLMs) and often utilize the parameter-efficient fine-tuning (PEFT) technique. However, current Med-VLFMs face two critical issues: (1) relying on a single vision model to represent the semantics of medical images, and (2) adapting LLMs with PEFT without considering the interference between vision and text modalities. This work presents a novel Med-VLFM with vision encoder ensembling (VEE) and modality-aware adaptation (MAA) to address these limitations. VEE combines the strengths of general and medical specialist vision foundation models to produce a more holistic representation of medical images. MAA introduces two small sets of trainable parameters into LLMs to calibrate vision and text features, respectively. Our proposed Med-VLFM ranked 1 𝑠𝑡 on most of the automatic evaluation metrics, including BERTScore, ROUGE-1, BLEU-1, BLEURT, METEOR, CIDEr and RefCLIPScore, in the ImageCLEFmedical 2024 caption prediction challenge. Our code and models are available at https://openi.pcl.ac.cn/OpenMedIA/PCLmed24.",
                "authors": "Bang Yang, Yue Yu, Yuexian Zou, Tong Zhang",
                "citations": 2
            },
            {
                "title": "Scaling Wearable Foundation Models",
                "abstract": "Wearable sensors have become ubiquitous thanks to a variety of health tracking features. The resulting continuous and longitudinal measurements from everyday life generate large volumes of data; however, making sense of these observations for scientific and actionable insights is non-trivial. Inspired by the empirical success of generative modeling, where large neural networks learn powerful representations from vast amounts of text, image, video, or audio data, we investigate the scaling properties of sensor foundation models across compute, data, and model size. Using a dataset of up to 40 million hours of in-situ heart rate, heart rate variability, electrodermal activity, accelerometer, skin temperature, and altimeter per-minute data from over 165,000 people, we create LSM, a multimodal foundation model built on the largest wearable-signals dataset with the most extensive range of sensor modalities to date. Our results establish the scaling laws of LSM for tasks such as imputation, interpolation and extrapolation, both across time and sensor modalities. Moreover, we highlight how LSM enables sample-efficient downstream learning for tasks like exercise and activity recognition.",
                "authors": "Girish Narayanswamy, Xin Liu, Kumar Ayush, Yuzhe Yang, Xuhai Xu, Shun Liao, Jake Garrison, Shyam Tailor, Jacob Sunshine, Yun Liu, Tim Althoff, Shrikanth Narayanan, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak N. Patel, Samy Abdel-Ghaffar, Daniel McDuff",
                "citations": 1
            },
            {
                "title": "Navigating the Future of Federated Recommendation Systems with Foundation Models",
                "abstract": "In recent years, the integration of federated learning (FL) and recommendation systems (RS), known as Federated Recommendation Systems (FRS), has attracted attention for preserving user privacy by keeping private data on client devices. However, FRS faces inherent limitations such as data heterogeneity and scarcity, due to the privacy requirements of FL and the typical data sparsity issues of RSs. Models like ChatGPT are empowered by the concept of transfer learning and self-supervised learning, so they can be easily applied to the downstream tasks after fine-tuning or prompting. These models, so-called Foundation Models (FM), fouce on understanding the human's intent and perform following their designed roles in the specific tasks, which are widely recognized for producing high-quality content in the image and language domains. Thus, the achievements of FMs inspire the design of FRS and suggest a promising research direction: integrating foundation models to address the above limitations. In this study, we conduct a comprehensive review of FRSs with FMs. Specifically, we: 1) summarise the common approaches of current FRSs and FMs; 2) review the challenges posed by FRSs and FMs; 3) discuss potential future research directions; and 4) introduce some common benchmarks and evaluation metrics in the FRS field. We hope that this position paper provides the necessary background and guidance to explore this interesting and emerging topic.",
                "authors": "Zhiwei Li, Guodong Long",
                "citations": 2
            },
            {
                "title": "Foundation Models for Aerial Robotics",
                "abstract": "Developing machine intelligence abilities in robots and autonomous systems is an expensive and time-consuming process. Existing solutions are tailored to specific applications and are harder to generalize. Furthermore, scarcity of training data adds a layer of complexity in deploying deep machine learning models. We present a new platform for General Robot Intelligence Development (GRID) to address both of these issues. The platform enables robots to learn, compose and adapt skills to their physical capabilities, environmental constraints and goals. One of the components of GRID is a state-of-the-art simulation system that models robot physics and the machine intelligence processes. This system in turn is tightly coupled with a multitude of Foundation Models that enables rapid prototyping, design, debugging and refinement of robot AI models. GRID is designed from the ground up to be extensible to accommodate new types of robots, vehicles, hardware platforms and software protocols. In addition, the modular design enables various deep ML components and existing foundation models to be easily usable in a wider variety of robot-centric problems. We demonstrate the platform in various aerial robotics scenarios and demonstrate how the platform dramatically accelerates development of machine intelligent robots.",
                "authors": "Ashish Kapoor",
                "citations": 1
            },
            {
                "title": "Synergizing Foundation Models and Federated Learning: A Survey",
                "abstract": "The recent development of Foundation Models (FMs), represented by large language models, vision transformers, and multimodal models, has been making a significant impact on both academia and industry. Compared with small-scale models, FMs have a much stronger demand for high-volume data during the pre-training phase. Although general FMs can be pre-trained on data collected from open sources such as the Internet, domain-specific FMs need proprietary data, posing a practical challenge regarding the amount of data available due to privacy concerns. Federated Learning (FL) is a collaborative learning paradigm that breaks the barrier of data availability from different participants. Therefore, it provides a promising solution to customize and adapt FMs to a wide range of domain-specific tasks using distributed datasets whilst preserving privacy. This survey paper discusses the potentials and challenges of synergizing FL and FMs and summarizes core techniques, future directions, and applications. A periodically updated paper collection on FM-FL is available at https://github.com/lishenghui/awesome-fm-fl.",
                "authors": "Shenghui Li, Fanghua Ye, Meng Fang, Jiaxu Zhao, Yun-Hin Chan, Edith C. H. Ngai, Thiemo Voigt",
                "citations": 2
            },
            {
                "title": "Creative and Correct: Requesting Diverse Code Solutions from AI Foundation Models",
                "abstract": "AI foundation models have the capability to produce a wide array of responses to a single prompt, a feature that is highly beneficial in software engineering to generate diverse code solutions. However, this advantage introduces a significant trade-off between diversity and correctness. In software engineering tasks, diversity is key to exploring design spaces and fostering creativity, but the practical value of these solutions is heavily dependent on their correctness. Our study systematically investigates this trade-off using experiments with HumanEval tasks, exploring various parameter settings and prompting strategies. We assess the diversity of code solutions using similarity metrics from the code clone community. The study identifies combinations of parameters and strategies that strike an optimal balance between diversity and correctness, situated on the Pareto front of this trade-off space. These findings offer valu-able insights for software engineers on how to effectively use AI foundation models to generate code solutions that are diverse and accurate.",
                "authors": "Scott Blyth, Christoph Treude, Markus Wagner",
                "citations": 1
            },
            {
                "title": "A Survey on Trustworthiness in Foundation Models for Medical Image Analysis",
                "abstract": "The rapid advancement of foundation models in medical imaging represents a significant leap toward enhancing diagnostic accuracy and personalized treatment. However, the deployment of foundation models in healthcare necessitates a rigorous examination of their trustworthiness, encompassing privacy, robustness, reliability, explainability, and fairness. The current body of survey literature on foundation models in medical imaging reveals considerable gaps, particularly in the area of trustworthiness. Additionally, existing surveys on the trustworthiness of foundation models do not adequately address their specific variations and applications within the medical imaging domain. This survey aims to fill that gap by presenting a novel taxonomy of foundation models used in medical imaging and analyzing the key motivations for ensuring their trustworthiness. We review current research on foundation models in major medical imaging applications, focusing on segmentation, medical report generation, medical question and answering (Q\\&A), and disease diagnosis. These areas are highlighted because they have seen a relatively mature and substantial number of foundation models compared to other applications. We focus on literature that discusses trustworthiness in medical image analysis manuscripts. We explore the complex challenges of building trustworthy foundation models for each application, summarizing current concerns and strategies for enhancing trustworthiness. Furthermore, we examine the potential of these models to revolutionize patient care. Our analysis underscores the imperative for advancing towards trustworthy AI in medical image analysis, advocating for a balanced approach that fosters innovation while ensuring ethical and equitable healthcare delivery.",
                "authors": "Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, Xiaoxiao Li",
                "citations": 2
            },
            {
                "title": "Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models",
                "abstract": "A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce NILS: Natural language Instruction Labeling for Scalability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention. NILS combines pretrained vision-language foundation models in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets. Evaluations on BridgeV2, Fractal, and a kitchen play dataset show that NILS can autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets while alleviating several shortcomings of crowdsourced human annotations, such as low data quality and diversity. We use NILS to label over 115k trajectories obtained from over 430 hours of robot data. We open-source our auto-labeling code and generated annotations on our website: http://robottasklabeling.github.io.",
                "authors": "Nils Blank, Moritz Reuss, Marcel Rühle, Ömer Erdinç Yagmurlu, Fabian Wenzel, Oier Mees, Rudolf Lioutikov",
                "citations": 2
            },
            {
                "title": "Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models",
                "abstract": "Vision transformers (ViTs) have emerged as a significant area of focus, particularly for their capacity to be jointly trained with large language models and to serve as robust vision foundation models. Yet, the development of trustworthy explanation methods for ViTs has lagged, particularly in the context of post-hoc interpretations of ViT predictions. Existing sub-image selection approaches, such as feature-attribution and conceptual models, fall short in this regard. This paper proposes five desiderata for explaining ViTs -- faithfulness, stability, sparsity, multi-level structure, and parsimony -- and demonstrates the inadequacy of current methods in meeting these criteria comprehensively. We introduce a variational Bayesian explanation framework, dubbed ProbAbilistic Concept Explainers (PACE), which models the distributions of patch embeddings to provide trustworthy post-hoc conceptual explanations. Our qualitative analysis reveals the distributions of patch-level concepts, elucidating the effectiveness of ViTs by modeling the joint distribution of patch embeddings and ViT's predictions. Moreover, these patch-level explanations bridge the gap between image-level and dataset-level explanations, thus completing the multi-level structure of PACE. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that PACE surpasses state-of-the-art methods in terms of the defined desiderata.",
                "authors": "Hengyi Wang, Shiwei Tan, Hao Wang",
                "citations": 2
            },
            {
                "title": "Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models",
                "abstract": "As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential.",
                "authors": "Yunpeng Huang, Yaonan Gu, Jingwei Xu, Zhihong Zhu, Zhaorun Chen, Xiaoxing Ma",
                "citations": 2
            },
            {
                "title": "Foundation Models for the Digital Twin Creation of Cyber-Physical Systems",
                "abstract": "Foundation models are trained on a large amount of data to learn generic patterns. Consequently, these models can be used and fine-tuned for various purposes. Naturally, studying such models' use in the context of digital twins for cyber-physical systems (CPSs) is a relevant area of investigation. To this end, we provide perspectives on various aspects within the context of developing digital twins for CPSs, where foundation models can be used to increase the efficiency of creating digital twins, improve the effectiveness of the capabilities they provide, and used as specialized fine-tuned foundation models acting as digital twins themselves. We also discuss challenges in using foundation models in a more generic context. We use the case of an autonomous driving system as a representative CPS to give examples. Finally, we provide discussions and open research directions that we believe are valuable for the digital twin community.",
                "authors": "Shaukat Ali, Paolo Arcaini, Aitor Arrieta",
                "citations": 2
            },
            {
                "title": "Foundation Models for Geophysics: Review and Perspective",
                "abstract": "Recently, large models, or foundation models, have exhibited remarkable performance, profoundly impacting research paradigms in diverse domains. Foundation models, trained on extensive and diverse datasets, provide exceptional generalization abilities, allowing for their straightforward application across various use cases and domains. Exploration geophysics is the study of the Earth's subsurface to find natural resources and help with environmental and engineering projects. It uses methods like analyzing seismic, magnetic, and electromagnetic data, which presents unique challenges and opportunities for the development of geophysical foundation models (GeoFMs). This perspective explores the potential applications and future research directions of GeoFMs in exploration geophysics. We also review the development of foundation models, including large language models, large vision models, and large multimodal models, as well as their advancement in the field of geophysics. Furthermore, we discuss the hierarchy of GeoFMs for exploration geophysics and the critical techniques employed, providing a foundational research workflow for their development. Lastly, we summarize the challenges faced in developing GeoFMs, along with future trends and their potential impact on the field. In conclusion, this perspective provides a comprehensive overview of the development, hierarchy, applications, development workflow, and challenges of foundation models, highlighting their transformative potential in advancing exploration geophysics.",
                "authors": "Qi Liu, Jianwei Ma",
                "citations": 1
            },
            {
                "title": "DeLF: Designing Learning Environments with Foundation Models",
                "abstract": "Reinforcement learning (RL) offers a capable and intuitive structure for the fundamental sequential decision-making problem. Despite impressive breakthroughs, it can still be difficult to employ RL in practice in many simple applications. In this paper, we try to address this issue by introducing a method for designing the components of the RL environment for a given, user-intended application. We provide an initial formalization for the problem of RL component design, that concentrates on designing a good representation for observation and action space. We propose a method named DeLF: Designing Learning Environments with Foundation Models, that employs large language models to design and codify the user's intended learning scenario. By testing our method on four different learning environments, we demonstrate that DeLF can obtain executable environment codes for the corresponding RL problems.",
                "authors": "Aida Afshar, Wenchao Li",
                "citations": 2
            },
            {
                "title": "On the Utility of Speech and Audio Foundation Models for Marmoset Call Analysis",
                "abstract": "Marmoset monkeys encode vital information in their calls and serve as a surrogate model for neuro-biologists to understand the evolutionary origins of human vocal communication. Traditionally analyzed with signal processing-based features, recent approaches have utilized self-supervised models pre-trained on human speech for feature extraction, capitalizing on their ability to learn a signal's intrinsic structure independently of its acoustic domain. However, the utility of such foundation models remains unclear for marmoset call analysis in terms of multi-class classification, bandwidth, and pre-training domain. This study assesses feature representations derived from speech and general audio domains, across pre-training bandwidths of 4, 8, and 16 kHz for marmoset call-type and caller classification tasks. Results show that models with higher bandwidth improve performance, and pre-training on speech or general audio yields comparable results, improving over a spectral baseline.",
                "authors": "Eklavya Sarkar, Mathew Magimai.-Doss",
                "citations": 2
            },
            {
                "title": "Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare",
                "abstract": "We are currently facing unprecedented cyber warfare with the rapid evolution of tactics, increasing asymmetry of intelligence, and the growing accessibility of hacking tools. In this landscape, cyber deception emerges as a critical component of our defense strategy against increasingly sophisticated attacks. This chapter aims to highlight the pivotal role of game-theoretic models and foundation models (FMs) in analyzing, designing, and implementing cyber deception tactics. Game models (GMs) serve as a foundational framework for modeling diverse adversarial interactions, allowing us to encapsulate both adversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the building blocks for creating tailored machine learning models suited to given applications. By leveraging the synergy between GMs and FMs, we can advance proactive and automated cyber defense mechanisms by not only securing our networks against attacks but also enhancing their resilience against well-planned operations. This chapter discusses the games at the tactical, operational, and strategic levels of warfare, delves into the symbiotic relationship between these methodologies, and explores relevant applications where such a framework can make a substantial impact in cybersecurity. The chapter discusses the promising direction of the multi-agent neurosymbolic conjectural learning (MANSCOL), which allows the defender to predict adversarial behaviors, design adaptive defensive deception tactics, and synthesize knowledge for the operational level synthesis and adaptation. FMs serve as pivotal tools across various functions for MANSCOL, including reinforcement learning, knowledge assimilation, formation of conjectures, and contextual representation. This chapter concludes with a discussion of the challenges associated with FMs and their application in the domain of cybersecurity.",
                "authors": "Tao Li, Quanyan Zhu",
                "citations": 2
            },
            {
                "title": "Towards Large-Scale Training of Pathology Foundation Models",
                "abstract": "Driven by the recent advances in deep learning methods and, in particular, by the development of modern self-supervised learning algorithms, increased interest and efforts have been devoted to build foundation models (FMs) for medical images. In this work, we present our scalable training pipeline for large pathology imaging data, and a comprehensive analysis of various hyperparameter choices and training techniques for building pathology FMs. We release and make publicly available the first batch of our pathology FMs (https://github.com/kaiko-ai/towards_large_pathology_fms) trained on open-access TCGA whole slide images, a commonly used collection of pathology images. The experimental evaluation shows that our models reach state-of-the-art performance on various patch-level downstream tasks, ranging from breast cancer subtyping to colorectal nuclear segmentation. Finally, to unify the evaluation approaches used in the field and to simplify future comparisons of different FMs, we present an open-source framework (https://github.com/kaiko-ai/eva) designed for the consistent evaluation of pathology FMs across various downstream tasks.",
                "authors": "kaiko.ai, N. Aben, Edwin D. de Jong, Ioannis Gatopoulos, Nicolas Kanzig, Mikhail Karasikov, Axel Lagr'e, Roman Moser, J. Doorn, Fei Tang",
                "citations": 2
            },
            {
                "title": "Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare",
                "abstract": "We are currently facing unprecedented cyber warfare with the rapid evolution of tactics, increasing asymmetry of intelligence, and the growing accessibility of hacking tools. In this landscape, cyber deception emerges as a critical component of our defense strategy against increasingly sophisticated attacks. This chapter aims to highlight the pivotal role of game-theoretic models and foundation models (FMs) in analyzing, designing, and implementing cyber deception tactics. Game models (GMs) serve as a foundational framework for modeling diverse adversarial interactions, allowing us to encapsulate both adversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the building blocks for creating tailored machine learning models suited to given applications. By leveraging the synergy between GMs and FMs, we can advance proactive and automated cyber defense mechanisms by not only securing our networks against attacks but also enhancing their resilience against well-planned operations. This chapter discusses the games at the tactical, operational, and strategic levels of warfare, delves into the symbiotic relationship between these methodologies, and explores relevant applications where such a framework can make a substantial impact in cybersecurity. The chapter discusses the promising direction of the multi-agent neurosymbolic conjectural learning (MANSCOL), which allows the defender to predict adversarial behaviors, design adaptive defensive deception tactics, and synthesize knowledge for the operational level synthesis and adaptation. FMs serve as pivotal tools across various functions for MANSCOL, including reinforcement learning, knowledge assimilation, formation of conjectures, and contextual representation. This chapter concludes with a discussion of the challenges associated with FMs and their application in the domain of cybersecurity.",
                "authors": "Tao Li, Quanyan Zhu",
                "citations": 2
            },
            {
                "title": "Past, Present and Future Challenges in Sharing Science: From PhysioNet to Foundation Models",
                "abstract": "Over the last 25 years, the sharing of data and models for research in cardiology has evolved from sneaker-net to the internet - from mailing tapes and compact discs of a handful of well-curated recordings of an array of arrhythmias, to the high-speed download of an entire hospital database. Yet, bandwidth and local computation has not kept pace with the rate at which we can strip mine our data archives. Recently, the trend towards the development of large foundational models has required enormous computing power, making large-scale local clusters or centralized cloud instances of data and compute the only viable solution for developing such models. Instead of democ-ratizing access to data and compute, as was the intent of the pioneers in this field, this trend is leading to the balka-nization of innovation in the hands of the rich and powerful. Moreover, claims of foundation models in cardiology, physiology, and medical data in general, are largely premature because foundation models must be trained on a broad enough range of data so that they may be applied across a wide range of use cases. To do so requires broad representation, of both individuals and medical conditions. This article examines these trends and provides a discussion of the most promising future directions, together with potential solutions to the concentration of power and lack of diversity in foundation models.",
                "authors": "Gari Clifford",
                "citations": 2
            },
            {
                "title": "Foundation Models for Aerial Robotics",
                "abstract": "Developing machine intelligence abilities in robots and autonomous systems is an expensive and time-consuming process. Existing solutions are tailored to specific applications and are harder to generalize. Furthermore, scarcity of training data adds a layer of complexity in deploying deep machine learning models. We present a new platform for General Robot Intelligence Development (GRID) to address both of these issues. The platform enables robots to learn, compose and adapt skills to their physical capabilities, environmental constraints and goals. One of the components of GRID is a state-of-the-art simulation system that models robot physics and the machine intelligence processes. This system in turn is tightly coupled with a multitude of Foundation Models that enables rapid prototyping, design, debugging and refinement of robot AI models. GRID is designed from the ground up to be extensible to accommodate new types of robots, vehicles, hardware platforms and software protocols. In addition, the modular design enables various deep ML components and existing foundation models to be easily usable in a wider variety of robot-centric problems. We demonstrate the platform in various aerial robotics scenarios and demonstrate how the platform dramatically accelerates development of machine intelligent robots.",
                "authors": "Ashish Kapoor",
                "citations": 1
            },
            {
                "title": "Scaling Wearable Foundation Models",
                "abstract": "Wearable sensors have become ubiquitous thanks to a variety of health tracking features. The resulting continuous and longitudinal measurements from everyday life generate large volumes of data; however, making sense of these observations for scientific and actionable insights is non-trivial. Inspired by the empirical success of generative modeling, where large neural networks learn powerful representations from vast amounts of text, image, video, or audio data, we investigate the scaling properties of sensor foundation models across compute, data, and model size. Using a dataset of up to 40 million hours of in-situ heart rate, heart rate variability, electrodermal activity, accelerometer, skin temperature, and altimeter per-minute data from over 165,000 people, we create LSM, a multimodal foundation model built on the largest wearable-signals dataset with the most extensive range of sensor modalities to date. Our results establish the scaling laws of LSM for tasks such as imputation, interpolation and extrapolation, both across time and sensor modalities. Moreover, we highlight how LSM enables sample-efficient downstream learning for tasks like exercise and activity recognition.",
                "authors": "Girish Narayanswamy, Xin Liu, Kumar Ayush, Yuzhe Yang, Xuhai Xu, Shun Liao, Jake Garrison, Shyam Tailor, Jacob Sunshine, Yun Liu, Tim Althoff, Shrikanth Narayanan, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak N. Patel, Samy Abdel-Ghaffar, Daniel McDuff",
                "citations": 1
            },
            {
                "title": "Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models",
                "abstract": "A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce NILS: Natural language Instruction Labeling for Scalability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention. NILS combines pretrained vision-language foundation models in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets. Evaluations on BridgeV2, Fractal, and a kitchen play dataset show that NILS can autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets while alleviating several shortcomings of crowdsourced human annotations, such as low data quality and diversity. We use NILS to label over 115k trajectories obtained from over 430 hours of robot data. We open-source our auto-labeling code and generated annotations on our website: http://robottasklabeling.github.io.",
                "authors": "Nils Blank, Moritz Reuss, Marcel Rühle, Ömer Erdinç Yagmurlu, Fabian Wenzel, Oier Mees, Rudolf Lioutikov",
                "citations": 2
            },
            {
                "title": "Reprogramming Distillation for Medical Foundation Models",
                "abstract": "Medical foundation models pre-trained on large-scale datasets have demonstrated powerful versatile capabilities for various tasks. However, due to the gap between pre-training tasks (or modalities) and downstream tasks (or modalities), the real-world computation and speed constraints, it might not be straightforward to apply medical foundation models in the downstream scenarios. Previous methods, such as parameter efficient fine-tuning (PEFT) methods and knowledge distillation (KD) methods, are unable to simultaneously address the task (or modality) inconsistency and achieve personalized lightweight deployment under diverse real-world demands. To address the above issues, we propose a novel framework called Reprogramming Distillation (RD). On one hand, RD reprograms the original feature space of the foundation model so that it is more relevant to downstream scenarios, aligning tasks and modalities. On the other hand, through a co-training mechanism and a shared classifier, connections are established between the reprogrammed knowledge and the knowledge of student models, ensuring that the reprogrammed feature space can be smoothly mimic by the student model of different structures. Further, to reduce the randomness under different training conditions, we design a Centered Kernel Alignment (CKA) distillation to promote robust knowledge transfer. Empirically, we show that on extensive datasets, RD consistently achieve superior performance compared with previous PEFT and KD methods.",
                "authors": "Yuhang Zhou, Siyuan Du, Haolin Li, Jiangchao Yao, Ya Zhang, Yanfeng Wang",
                "citations": 1
            },
            {
                "title": "Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models",
                "abstract": "Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks. However, SSL strategies must be adapted to the type of training data and downstream tasks required. We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning. By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator. Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discrimination of a variety of objects and uncertainty mitigation. In addition to our results, we make the RS3L dataset publicly available for further studies on how to improve SSL strategies.",
                "authors": "Philip Harris, Michael Kagan, J. Krupa, B. Maier, Nathaniel S. Woodward",
                "citations": 7
            },
            {
                "title": "Protein Design Using Structure-Prediction Networks: AlphaFold and RoseTTAFold as Protein Structure Foundation Models.",
                "abstract": "Designing proteins with tailored structures and functions is a long-standing goal in bioengineering. Recently, deep learning advances have enabled protein structure prediction at near-experimental accuracy, which has catalyzed progress in protein design as well. We review recent studies that use structure-prediction neural networks to design proteins, via approaches such as activation maximization, inpainting, or denoising diffusion. These methods have led to major improvements over previous methods in wet-lab success rates for designing protein binders, metalloproteins, enzymes, and oligomeric assemblies. These results show that structure-prediction models are a powerful foundation for developing protein-design tools and suggest that continued improvement of their accuracy and generality will be key to unlocking the full potential of protein design.",
                "authors": "Jue Wang, Joseph L. Watson, S. Lisanza",
                "citations": 6
            },
            {
                "title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization",
                "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.",
                "authors": "Chris Cummins, Volker Seeker, Dejan Grubisic, Baptiste Rozière, Jonas Gehring, Gabriele Synnaeve, Hugh Leather",
                "citations": 8
            },
            {
                "title": "Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection",
                "abstract": "Open-vocabulary human-object interaction (HOI) detection, which is concerned with the problem of detecting novel HOIs guided by natural language, is crucial for understanding human-centric scenes. However, prior zero-shot HOI detectors often employ the same levels of feature maps to model HOIs with varying distances, leading to suboptimal performance in scenes containing human-object pairs with a wide range of distances. In addition, these detectors primarily rely on category names and over-look the rich contextual information that language can provide, which is essential for capturing open vocabulary concepts that are typically rare and not well-represented by category names alone. In this paper, we introduce a novel end-to-end open vocabulary HOI detection framework with conditional multi-level decoding and fine-grained semantic enhancement (CMD-SE), harnessing the potential of Visual-Language Models (VLMs). Specifically, we propose to model human-object pairs with different distances with different levels of feature maps by incorporating a soft constraint during the bipartite matching process. Furthermore, by leveraging large language models (LLMs) such as GPT models, we exploit their extensive world knowledge to generate descriptions of human body part states for various interactions. Then we integrate the generalizable and fine- grained semantics of human body parts to improve interaction recognition. Experimental results on two datasets, SWIG-HOI and HICO-DET, demonstrate that our proposed method achieves state-of-the-art results in open vocabulary HOI detection. The code and models are available at https://github.com/ltttpku/CMD-SE-release.",
                "authors": "Ting Lei, Shaofeng Yin, Yang Liu",
                "citations": 7
            },
            {
                "title": "Bridging Remote Sensors with Multisensor Geospatial Foundation Models",
                "abstract": "In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. ms-GFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. ms-GFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, under-scoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities. Code can be found at https://github.com/boranhan/Geospatial_Foundation_Models",
                "authors": "Boran Han, Shuai Zhang, Xingjian Shi, M. Reichstein",
                "citations": 7
            },
            {
                "title": "Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation",
                "abstract": "Geolocating precise locations from images presents a challenging problem in computer vision and information retrieval.Traditional methods typically employ either classification, which dividing the Earth surface into grid cells and classifying images accordingly, or retrieval, which identifying locations by matching images with a database of image-location pairs. However, classification-based approaches are limited by the cell size and cannot yield precise predictions, while retrieval-based systems usually suffer from poor search quality and inadequate coverage of the global landscape at varied scale and aggregation levels. To overcome these drawbacks, we present Img2Loc, a novel system that redefines image geolocalization as a text generation task. This is achieved using cutting-edge large multi-modality models like GPT4V or LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based representations to generate an image-based coordinate query database. It then uniquely combines query results with images itself, forming elaborate prompts customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art models but does so without any model training.",
                "authors": "Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, Gengchen Mai",
                "citations": 7
            },
            {
                "title": "Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning",
                "abstract": "Foundation models (FMs) such as large language models have revolutionized the field of AI by showing remarkable performance in various tasks. However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability. Since FMs are trained using loss functions aimed at reconstructing the training corpus in a self-supervised manner, there is no guarantee that the model's output aligns with users' preferences for a specific task at hand. In this survey paper, we propose a conceptual framework that encapsulates different modes by which agents could interact with FMs and guide them suitably for a set of tasks, particularly through knowledge augmentation and reasoning. Our framework elucidates agent role categories such as updating the underlying FM, assisting with prompting the FM, and evaluating the FM output. We also categorize several state-of-the-art approaches into agent interaction protocols, highlighting the nature and extent of involvement of the various agent roles. The proposed framework provides guidance for future directions to further realize the power of FMs in practical AI systems.",
                "authors": "D. Bhattacharjya, Junkyu Lee, Don Joven Agravante, Balaji Ganesan, Radu Marinescu",
                "citations": 0
            },
            {
                "title": "NetBench: A Large-Scale and Comprehensive Network Traffic Benchmark Dataset for Foundation Models",
                "abstract": "In computer networking, network traffic refers to the amount of data transmitted in the form of packets between internetworked computers or Cyber- Physical Systems. Monitoring and analyzing network traffic is crucial for ensuring the performance, security, and reliability of a network. However, a significant challenge in network traffic analysis is to process diverse data packets including both ciphertext and plaintext. While many methods have been adopted to analyze network traffic, they often rely on different datasets for performance evaluation. This inconsistency results in substantial manual data processing efforts and unfair comparisons. Moreover, some data processing methods may cause data leakage due to improper separation of training and testing data. To address these issues, we introduce the NetBench, a large-scale and comprehensive bench-mark dataset for assessing machine learning models, especially foundation models, in both network traffic classification and generation tasks. NetBench is built upon seven publicly available datasets and encompasses a broad spectrum of 20 tasks, including 15 classification tasks and 5 generation tasks. Furthermore, we evaluate eight State-Of-The-Art (SOTA) classification models (including two foundation models) and two generative models using our benchmark. The results show that foundation models significantly outperform the traditional deep learning methods in traffic classification. We believe NetBench will facilitate fair comparisons among various approaches and advance the development of foundation models for network traffic. Our benchmark is available at https://github.com/WM-JayLab/NetBench.",
                "authors": "Chen Qian, Xiaochang Li, Qineng Wang, Gang Zhou, Huajie Shao",
                "citations": 0
            },
            {
                "title": "Pathology Foundation Models",
                "abstract": "Pathology has played a crucial role in the diagnosis and evaluation of patient tissue samples obtained from surgeries and biopsies for many years. The advent of Whole Slide Scanners and the development of deep learning technologies have significantly advanced the field, leading to extensive research and development in pathology AI (Artificial Intelligence). These advancements have contributed to reducing the workload of pathologists and supporting decision-making in treatment plans. Recently, large-scale AI models known as Foundation Models (FMs), which are more accurate and applicable to a wide range of tasks compared to traditional AI, have emerged, and expanded their application scope in the healthcare field. Numerous FMs have been developed in pathology, and there are reported cases of their application in various tasks, such as disease diagnosis, rare cancer diagnosis, patient survival prognosis prediction, biomarker expression prediction, and the scoring of immunohistochemical expression intensity. However, several challenges remain for the clinical application of FMs, which healthcare professionals, as users, must be aware of. Research is ongoing to address these challenges. In the future, it is expected that the development of Generalist Medical AI, which integrates pathology FMs with FMs from other medical domains, will progress, leading to the effective utilization of AI in real clinical settings to promote precision and personalized medicine.",
                "authors": "Mieko Ochi, D. Komura, Shumpei Ishikawa",
                "citations": 0
            },
            {
                "title": "Text-Free Multi-domain Graph Pre-training: Toward Graph Foundation Models",
                "abstract": "Given the ubiquity of graph data, it is intriguing to ask: Is it possible to train a graph foundation model on a broad range of graph data across diverse domains? A major hurdle toward this goal lies in the fact that graphs from different domains often exhibit profoundly divergent characteristics. Although there have been some initial efforts in integrating multi-domain graphs for pre-training, they primarily rely on textual descriptions to align the graphs, limiting their application to text-attributed graphs. Moreover, different source domains may conflict or interfere with each other, and their relevance to the target domain can vary significantly. To address these issues, we propose MDGPT, a text free Multi-Domain Graph Pre-Training and adaptation framework designed to exploit multi-domain knowledge for graph learning. First, we propose a set of domain tokens to to align features across source domains for synergistic pre-training. Second, we propose a dual prompts, consisting of a unifying prompt and a mixing prompt, to further adapt the target domain with unified multi-domain knowledge and a tailored mixture of domain-specific knowledge. Finally, we conduct extensive experiments involving six public datasets to evaluate and analyze MDGPT, which outperforms prior art by up to 37.9%.",
                "authors": "Xingtong Yu, Chang Zhou, Yuan Fang, Xinming Zhang",
                "citations": 6
            },
            {
                "title": "Foundation Models for Biomedical Image Segmentation: A Survey",
                "abstract": "Recent advancements in biomedical image analysis have been significantly driven by the Segment Anything Model (SAM). This transformative technology, originally developed for general-purpose computer vision, has found rapid application in medical image processing. Within the last year, marked by over 100 publications, SAM has demonstrated its prowess in zero-shot learning adaptations for medical imaging. The fundamental premise of SAM lies in its capability to segment or identify objects in images without prior knowledge of the object type or imaging modality. This approach aligns well with tasks achievable by the human visual system, though its application in non-biological vision contexts remains more theoretically challenging. A notable feature of SAM is its ability to adjust segmentation according to a specified resolution scale or area of interest, akin to semantic priming. This adaptability has spurred a wave of creativity and innovation in applying SAM to medical imaging. Our review focuses on the period from April 1, 2023, to September 30, 2023, a critical first six months post-initial publication. We examine the adaptations and integrations of SAM necessary to address longstanding clinical challenges, particularly in the context of 33 open datasets covered in our analysis. While SAM approaches or achieves state-of-the-art performance in numerous applications, it falls short in certain areas, such as segmentation of the carotid artery, adrenal glands, optic nerve, and mandible bone. Our survey delves into the innovative techniques where SAM's foundational approach excels and explores the core concepts in translating and applying these models effectively in diverse medical imaging scenarios.",
                "authors": "Ho Hin Lee, Yu Gu, Theodore Zhao, Yanbo Xu, Jianwei Yang, Naoto Usuyama, Cliff Wong, Mu-Hsin Wei, Bennett A. Landman, Yuankai Huo, Alberto Santamaría-Pang, Hoifung Poon",
                "citations": 8
            },
            {
                "title": "Molecular causality in the advent of foundation models",
                "abstract": null,
                "authors": "Sebastian Lobentanzer, P. Rodríguez-Mier, Stefan Bauer, Julio Saez-Rodriguez",
                "citations": 6
            },
            {
                "title": "SMILES-Mamba: Chemical Mamba Foundation Models for Drug ADMET Prediction",
                "abstract": "In drug discovery, predicting the absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties of small-molecule drugs is critical for ensuring safety and efficacy. However, the process of accurately predicting these properties is often resource-intensive and requires extensive experimental data. To address this challenge, we propose SMILES-Mamba, a two-stage model that leverages both unlabeled and labeled data through a combination of self-supervised pretraining and fine-tuning strategies. The model first pre-trains on a large corpus of unlabeled SMILES strings to capture the underlying chemical structure and relationships, before being fine-tuned on smaller, labeled datasets specific to ADMET tasks. Our results demonstrate that SMILES-Mamba exhibits competitive performance across 22 ADMET datasets, achieving the highest score in 14 tasks, highlighting the potential of self-supervised learning in improving molecular property prediction. This approach not only enhances prediction accuracy but also reduces the dependence on large, labeled datasets, offering a promising direction for future research in drug discovery.",
                "authors": "Bohao Xu, Yingzhou Lu, Chenhao Li, Ling Yue, Xiao Wang, Nan Hao, Tianfan Fu, Jim Chen",
                "citations": 8
            },
            {
                "title": "Finetuning foundation models for joint analysis optimization in High Energy Physics",
                "abstract": "In this work we demonstrate that significant gains in performance and data efficiency can be achieved in High Energy Physics (HEP) by moving beyond the standard paradigm of sequential optimization or reconstruction and analysis components. We conceptually connect HEP reconstruction and analysis to modern machine learning workflows such as pretraining, finetuning, domain adaptation and high-dimensional embedding spaces and quantify the gains in the example usecase of searches of heavy resonances decaying via an intermediate di-Higgs system to four b-jets. To our knowledge this is the first example of a low-level feature extraction network finetuned for a downstream HEP analysis objective.",
                "authors": "M. Vigl, N. Hartman, L. Heinrich",
                "citations": 8
            },
            {
                "title": "Verifiably Following Complex Robot Instructions with Foundation Models",
                "abstract": "Enabling mobile robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction grounding for Motion Planning (LIMP), an approach that enables robots to verifiably follow expressive and complex open-ended instructions in real-world environments without prebuilt semantic maps. LIMP constructs a symbolic instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that are correct-by-construction. We perform a large scale evaluation and demonstrate our approach on 150 instructions in five real-world environments showing the generality of our approach and the ease of deployment in novel unstructured domains. In our experiments, LIMP performs comparably with state-of-the-art LLM task planners and LLM code-writing planners on standard open vocabulary tasks and additionally achieves 79\\% success rate on complex spatiotemporal instructions while LLM and Code-writing planners both achieve 38\\%. See supplementary materials and demo videos at https://robotlimp.github.io",
                "authors": "Benedict Quartey, Eric Rosen, Stefanie Tellex, G. Konidaris",
                "citations": 7
            },
            {
                "title": "The Promises and Perils of Foundation Models in Dermatology.",
                "abstract": null,
                "authors": "Haiwen Gui, J. Omiye, Crystal T. Chang, Roxana Daneshjou",
                "citations": 6
            },
            {
                "title": "The new paradigm in machine learning – foundation models, large language models and beyond: a primer for physicians",
                "abstract": "Foundation machine learning models are deep learning models capable of performing many different tasks using different data modalities such as text, audio, images and video. They represent a major shift from traditional task‐specific machine learning prediction models. Large language models (LLM), brought to wide public prominence in the form of ChatGPT, are text‐based foundational models that have the potential to transform medicine by enabling automation of a range of tasks, including writing discharge summaries, answering patients questions and assisting in clinical decision‐making. However, such models are not without risk and can potentially cause harm if their development, evaluation and use are devoid of proper scrutiny. This narrative review describes the different types of LLM, their emerging applications and potential limitations and bias and likely future translation into clinical practice.",
                "authors": "Ian A. Scott, Guido Zuccon",
                "citations": 5
            },
            {
                "title": "Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights",
                "abstract": "Given the ubiquity of graph data and its applications in diverse domains, building a Graph Foundation Model (GFM) that can work well across different graphs and tasks with a unified backbone has recently garnered significant interests. A major obstacle to achieving this goal stems from the fact that graphs from different domains often exhibit diverse node features. Inspired by multi-modal models that align different modalities with natural language, the text has recently been adopted to provide a unified feature space for diverse graphs. Despite the great potential of these text-space GFMs, current research in this field is hampered by two problems. First, the absence of a comprehensive benchmark with unified problem settings hinders a clear understanding of the comparative effectiveness and practical value of different text-space GFMs. Second, there is a lack of sufficient datasets to thoroughly explore the methods' full potential and verify their effectiveness across diverse settings. To address these issues, we conduct a comprehensive benchmark providing novel text-space datasets and comprehensive evaluation under unified problem settings. Empirical results provide new insights and inspire future research directions. Our code and data are publicly available from \\url{https://github.com/CurryTang/TSGFM}.",
                "authors": "Zhikai Chen, Haitao Mao, Jingzhe Liu, Yu Song, Bingheng Li, Wei-dong Jin, Bahare Fatemi, Anton Tsitsulin, Bryan Perozzi, Hui Liu, Jiliang Tang",
                "citations": 4
            },
            {
                "title": "Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction",
                "abstract": "We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate time series forecasting.",
                "authors": "Cheng Feng, Long Huang, Denis Krompass",
                "citations": 4
            },
            {
                "title": "Exploiting Foundation Models and Speech Enhancement for Parkinson's Disease Detection from Speech in Real-World Operative Conditions",
                "abstract": "This work is concerned with devising a robust Parkinson's (PD) disease detector from speech in real-world operating conditions using (i) foundational models, and (ii) speech enhancement (SE) methods. To this end, we first fine-tune several foundational-based models on the standard PC-GITA (s-PC-GITA) clean data. Our results demonstrate superior performance to previously proposed models. Second, we assess the generalization capability of the PD models on the extended PC-GITA (e-PC-GITA) recordings, collected in real-world operative conditions, and observe a severe drop in performance moving from ideal to real-world conditions. Third, we align training and testing conditions applaying off-the-shelf SE techniques on e-PC-GITA, and a significant boost in performance is observed only for the foundational-based models. Finally, combining the two best foundational-based models trained on s-PC-GITA, namely WavLM Base and Hubert Base, yielded top performance on the enhanced e-PC-GITA.",
                "authors": "Moreno La Quatra, Maria Francesca Turco, Torbjørn Svendsen, G. Salvi, J. Orozco-Arroyave, Sabato Marco Siniscalchi",
                "citations": 4
            },
            {
                "title": "ChatEarthNet: A Global-Scale Image-Text Dataset Empowering Vision-Language Geo-Foundation Models",
                "abstract": "An in-depth comprehension of global land cover is essential in Earth observation, forming the foundation for a multitude of applications. Although remote sensing technology has advanced rapidly, leading to a proliferation of satellite imagery, the inherent complexity of these images often makes them difficult for non-expert users to understand. Natural language, as a carrier of human knowledge, can be a bridge between common users and complicated satellite imagery. In this context, we introduce a global-scale, high-quality image-text dataset for remote sensing, providing natural language descriptions for Sentinel-2 data to facilitate the understanding of satellite imagery for common users. Specifically, we utilize Sentinel-2 data for its global coverage as the foundational image source, employing semantic segmentation labels from the European Space Agency's (ESA) WorldCover project to enrich the descriptions of land covers. By conducting in-depth semantic analysis, we formulate detailed prompts to elicit rich descriptions from ChatGPT. To enhance the dataset's quality, we introduce the manual verification process. This step involves manual inspection and correction to refine the dataset, thus significantly improving its accuracy and quality. Finally, we offer the community ChatEarthNet, a large-scale image-text dataset characterized by global coverage, high quality, wide-ranging diversity, and detailed descriptions. ChatEarthNet consists of 163,488 image-text pairs with captions generated by ChatGPT-3.5 and an additional 10,000 image-text pairs with captions generated by ChatGPT-4V(ision). This dataset has significant potential for training vision-language geo-foundation models and evaluating large vision-language models for remote sensing. The dataset will be made publicly available.",
                "authors": "Zhenghang Yuan, Zhitong Xiong, Lichao Mou, Xiao Xiang Zhu",
                "citations": 4
            },
            {
                "title": "RouteFinder: Towards Foundation Models for Vehicle Routing Problems",
                "abstract": "This paper introduces RouteFinder, a comprehensive foundation model framework to tackle different Vehicle Routing Problem (VRP) variants. Our core idea is that a foundation model for VRPs should be able to represent variants by treating each as a subset of a generalized problem equipped with different attributes. We propose a unified VRP environment capable of efficiently handling any attribute combination. The RouteFinder model leverages a modern transformer-based encoder and global attribute embeddings to improve task representation. Additionally, we introduce two reinforcement learning techniques to enhance multi-task performance: mixed batch training, which enables training on different variants at once, and multi-variant reward normalization to balance different reward scales. Finally, we propose efficient adapter layers that enable fine-tuning for new variants with unseen attributes. Extensive experiments on 24 VRP variants show RouteFinder achieves competitive results. Our code is openly available at https://github.com/ai4co/routefinder.",
                "authors": "Federico Berto, Chuanbo Hua, Nayeli Gast Zepeda, André Hottung, Niels A. Wouda, Leon Lan, Kevin Tierney, Jinkyoo Park",
                "citations": 4
            },
            {
                "title": "CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for Foundation Models",
                "abstract": "Due to the rapid advancements in multimodal large language models, evaluating their multimodal mathematical capabilities continues to receive wide attention. Despite the datasets like MathVista proposed benchmarks for assessing mathematical capabilities in multimodal scenarios, there is still a lack of corresponding evaluation tools and datasets for fine-grained assessment in the context of K12 education in Chinese language. To systematically evaluate the capability of multimodal large models in solving Chinese multimodal mathematical problems, we propose a Chinese Multi-modal Math Skill Evaluation Benchmark, named CMMaTH, contraining 23k multimodal K12 math related questions, forming the largest Chinese multimodal mathematical problem benchmark to date. CMMaTH questions from elementary to high school levels, provide increased diversity in problem types, solution objectives, visual elements, detailed knowledge points, and standard solution annotations. We have constructed an open-source tool GradeGPT integrated with the CMMaTH dataset, facilitating stable, rapid, and cost-free model evaluation. Our data and code are available.",
                "authors": "Zhongzhi Li, Ming-Liang Zhang, Fei Yin, Zhi-Long Ji, Jin-Feng Bai, Zhenru Pan, Fan-Hu Zeng, Jian Xu, Jiaxin Zhang, Cheng-Lin Liu",
                "citations": 5
            },
            {
                "title": "Autonomous Improvement of Instruction Following Skills via Foundation Models",
                "abstract": "Intelligent instruction-following robots capable of improving from autonomously collected experience have the potential to transform robot learning: instead of collecting costly teleoperated demonstration data, large-scale deployment of fleets of robots can quickly collect larger quantities of autonomous data that can collectively improve their performance. However, autonomous improvement requires solving two key problems: (i) fully automating a scalable data collection procedure that can collect diverse and semantically meaningful robot data and (ii) learning from non-optimal, autonomous data with no human annotations. To this end, we propose a novel approach that addresses these challenges, allowing instruction-following policies to improve from autonomously collected data without human supervision. Our framework leverages vision-language models to collect and evaluate semantically meaningful experiences in new environments, and then utilizes a decomposition of instruction following tasks into (semantic) language-conditioned image generation and (non-semantic) goal reaching, which makes it significantly more practical to improve from this autonomously collected data without any human annotations. We carry out extensive experiments in the real world to demonstrate the effectiveness of our approach, and find that in a suite of unseen environments, the robot policy can be improved 2x with autonomously collected data. We open-source the code for our semantic autonomous improvement pipeline, as well as our autonomous dataset of 30.5K trajectories collected across five tabletop environments.",
                "authors": "Zhiyuan Zhou, P. Atreya, Abraham Lee, Homer Walke, Oier Mees, Sergey Levine",
                "citations": 5
            },
            {
                "title": "UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation",
                "abstract": "We present Unified PDE Solvers (UPS), a data- and compute-efficient approach to developing unified neural operators for diverse families of spatiotemporal PDEs from various domains, dimensions, and resolutions. UPS embeds different PDEs into a shared representation space and processes them using a FNO-transformer architecture. Rather than training the network from scratch, which is data-demanding and computationally expensive, we warm-start the transformer from pretrained LLMs and perform explicit alignment to reduce the modality gap while improving data and compute efficiency. The cross-modal UPS achieves state-of-the-art results on a wide range of 1D and 2D PDE families from PDEBench, outperforming existing unified models using 4 times less data and 26 times less compute. Meanwhile, it is capable of few-shot transfer to unseen PDE families and coefficients.",
                "authors": "Junhong Shen, Tanya Marwah, Ameet Talwalkar",
                "citations": 4
            },
            {
                "title": "Parametric Graph Representations in the Era of Foundation Models: A Survey and Position",
                "abstract": "Graphs have been widely used in the past decades of big data and AI to model comprehensive relational data. When analyzing a graph's statistical properties, graph laws serve as essential tools for parameterizing its structure. Identifying meaningful graph laws can significantly enhance the effectiveness of various applications, such as graph generation and link prediction. Facing the large-scale foundation model developments nowadays, the study of graph laws reveals new research potential, e.g., providing multi-modal information for graph neural representation learning and breaking the domain inconsistency of different graph data. In this survey, we first review the previous study of graph laws from multiple perspectives, i.e., macroscope and microscope of graphs, low-order and high-order graphs, static and dynamic graphs, different observation spaces, and newly proposed graph parameters. After we review various real-world applications benefiting from the guidance of graph laws, we conclude the paper with current challenges and future research directions.",
                "authors": "Dongqi Fu, Liri Fang, Zihao Li, Hanghang Tong, Vetle I. Torvik, Jingrui He",
                "citations": 4
            },
            {
                "title": "Sample, estimate, aggregate: A recipe for causal discovery foundation models",
                "abstract": "Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, causal discovery algorithms over larger sets of variables tend to be brittle against misspecification or when data are limited. To mitigate these challenges, we train a supervised model that learns to predict a larger causal graph from the outputs of classical causal discovery algorithms run over subsets of variables, along with other statistical hints like inverse covariance. Our approach is enabled by the observation that typical errors in the outputs of classical methods remain comparable across datasets. Theoretically, we show that this model is well-specified, in the sense that it can recover a causal graph consistent with graphs over subsets. Empirically, we train the model to be robust to erroneous estimates using diverse synthetic data. Experiments on real and synthetic data demonstrate that this model maintains high accuracy in the face of misspecification or distribution shift, and can be adapted at low cost to different discovery algorithms or choice of statistics.",
                "authors": "Menghua Wu, Yujia Bao, R. Barzilay, T. Jaakkola",
                "citations": 4
            },
            {
                "title": "Foundation models in shaping the future of ecology",
                "abstract": null,
                "authors": "Albert Morera",
                "citations": 4
            },
            {
                "title": "Foundation models for the electric power grid",
                "abstract": null,
                "authors": "Hendrik F. Hamann, Thomas Brunschwiler, B. Gjorgiev, Leonardo S. A. Martins, Alban Puech, Anna Varbella, Jonas Weiss, Juan Bernabé-Moreno, Alexandre Blondin Mass'e, Seong Choi, Ian Foster, B. Hodge, Rishabh Jain, Kibaek Kim, Vincent Mai, Franccois Miralles, M. D. Montigny, Octavio Ramos-Leaños, Hussein Suprême, Le Xie, El-Nasser S. Youssef, Arnaud Zinflou, Alexander J. Belyi, Ricardo J. Bessa, B. Bhattarai, J. Schmude, Stanislav Sobolevsky",
                "citations": 5
            },
            {
                "title": "Evolution and Prospects of Foundation Models: From Large Language Models to Large Multimodal Models",
                "abstract": null,
                "authors": "Zheyi Chen, Liuchang Xu, Hongting Zheng, Luyao Chen, Amr Tolba, Liang Zhao, Keping Yu, Hailin Feng",
                "citations": 4
            },
            {
                "title": "Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance",
                "abstract": "Large, general-purpose robotic policies trained on diverse demonstration datasets have been shown to be remarkably effective both for controlling a variety of robots in a range of different scenes, and for acquiring broad repertoires of manipulation skills. However, the data that such policies are trained on is generally of mixed quality -- not only are human-collected demonstrations unlikely to perform the task perfectly, but the larger the dataset is, the harder it is to curate only the highest quality examples. It also remains unclear how optimal data from one embodiment is for training on another embodiment. In this paper, we present a general and broadly applicable approach that enhances the performance of such generalist robot policies at deployment time by re-ranking their actions according to a value function learned via offline RL. This approach, which we call Value-Guided Policy Steering (V-GPS), is compatible with a wide range of different generalist policies, without needing to fine-tune or even access the weights of the policy. We show that the same value function can improve the performance of five different state-of-the-art policies with different architectures, even though they were trained on distinct datasets, attaining consistent performance improvement on multiple robotic platforms across a total of 12 tasks. Code and videos can be found at: https://nakamotoo.github.io/V-GPS",
                "authors": "Mitsuhiko Nakamoto, Oier Mees, Aviral Kumar, Sergey Levine",
                "citations": 5
            },
            {
                "title": "Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification",
                "abstract": "Traditional deep learning models implicity encode knowledge limiting their transparency and ability to adapt to data changes. Yet, this adaptability is vital for addressing user data privacy concerns. We address this limitation by storing embeddings of the underlying training data independently of the model weights, enabling dynamic data modifications without retraining. Specifically, our approach integrates the k-Nearest Neighbor (k-NN) classifier with a vision-based foundation model, pre-trained self-supervised on natural images, enhancing interpretability and adaptability. We share open-source implementations of a previously unpublished baseline method as well as our performance-improving contributions. Quantitative experiments confirm improved classification across established benchmark datasets and the method’s applicability to distinct medical image classification tasks. Additionally, we assess the method’s robustness in continual learning and data removal scenarios. The approach exhibits great promise for bridging the gap between foundation models’ performance and challenges tied to data privacy. The source code is available at github.com/TobArc.",
                "authors": "Sebastian Doerrich, Tobias Archut, Francesco Di Salvo, Christian Ledig",
                "citations": 2
            },
            {
                "title": "A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models",
                "abstract": "In this paper, we build upon two major recent developments in the field, Diffusion Policies for visuomotor manipulation and large pre-trained multimodal foundational models to obtain a robotic skill learning system. The system can obtain new skills via the behavioral cloning approach of visuomotor diffusion policies given teleoperated demonstrations. Foundational models are being used to perform skill selection given the user’s prompt in natural language. Before executing a skill the foundational model performs a precondition check given an observation of the workspace. We compare the performance of different foundational models to this end and give a detailed experimental evaluation of the skills taught by the user in simulation and the real world. Finally, we showcase the combined system on a challenging food serving scenario in the real world. Videos of all experimental executions, as well as the process of teaching new skills in simulation and the real world, are available on the project’s website1.",
                "authors": "Nils Ingelhag, Jesper Munkeby, Jonne van Haastregt, Anastasia Varava, Michael C. Welle, Danica Kragic",
                "citations": 3
            },
            {
                "title": "Toward Foundation Models for Inclusive Object Detection: Geometry- and Category-Aware Feature Extraction Across Road User Categories",
                "abstract": "The safety of different categories of road users comprising motorized vehicles and vulnerable road users (VRUs) such as pedestrians and cyclists is one of the priorities of automated driving and smart infrastructure services. Three-dimensional (3-D) LiDAR-based object detection has been a promising approach to perceiving road users. Despite accurate 3-D geometry information, the point cloud from LiDAR is usually nonuniform, and learning the effective point cloud abstract representations for diverse road users remains challenging for 3-D object detection, particularly for small objects such as VRUs. For inclusive object detection (IDetect), we propose a general foundation convolution component, called geometry-aware convolution (GA Conv) toward a foundation feature extraction model, to serve as basic convolution operations of the neutral network for inclusive 3-D object detection. Further, the GA Conv operations are then utilized as the elementary feature extraction layers to build a novel elegant and pyramid network for IDetect. It learns the effective geometric-related features from the unstructured point cloud data by implicitly learning the distribution property and geometry-related features from different categories of road users in particular for VRUs. The proposed IDetect is comprehensively evaluated on the large-scale benchmark Waymo open datasets with all categories of road users. The qualitative and quantitative experiment results demonstrate that IDetect can effectively consider the nonuniform distributed point clouds and learn the geometric features to assist the different categories of road user detection. In addition, the GA Conv has been integrated with other state-of-the-art neural networks and a performance boost for VRU detection has been demonstrated, showing the foundation functionality of the GA Conv and making it a general component in the future inclusive 3-D object detection foundation model.",
                "authors": "Zonglin Meng, Xin Xia, Jiaqi Ma",
                "citations": 3
            },
            {
                "title": "ProtoS-ViT: Visual foundation models for sparse self-explainable classifications",
                "abstract": "Prototypical networks aim to build intrinsically explainable models based on the linear summation of concepts. Concepts are coherent entities that we, as humans, can recognize and associate with a certain object or entity. However, important challenges remain in the fair evaluation of explanation quality provided by these models. This work first proposes an extensive set of quantitative and qualitative metrics which allow to identify drawbacks in current prototypical networks. It then introduces a novel architecture which provides compact explanations, outperforming current prototypical models in terms of explanation quality. Overall, the proposed architecture demonstrates how frozen pre-trained ViT backbones can be effectively turned into prototypical models for both general and domain-specific tasks, in our case biomedical image classifiers. Code is available at \\url{https://github.com/hturbe/protosvit}.",
                "authors": "Hugues Turb'e, Mina Bjelogrlic, G. Mengaldo, Christian Lovis",
                "citations": 3
            },
            {
                "title": "Lossless and Near-Lossless Compression for Foundation Models",
                "abstract": "With the growth of model sizes and scale of their deployment, their sheer size burdens the infrastructure requiring more network and more storage to accommodate these. While there is a vast literature about reducing model sizes, we investigate a more traditional type of compression -- one that compresses the model to a smaller form and is coupled with a decompression algorithm that returns it to its original size -- namely lossless compression. Somewhat surprisingly, we show that such lossless compression can gain significant network and storage reduction on popular models, at times reducing over $50\\%$ of the model size. We investigate the source of model compressibility, introduce compression variants tailored for models and categorize models to compressibility groups. We also introduce a tunable lossy compression technique that can further reduce size even on the less compressible models with little to no effect on the model accuracy. We estimate that these methods could save over an ExaByte per month of network traffic downloaded from a large model hub like HuggingFace.",
                "authors": "Moshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, S. Sundararaman, Danny Harnik",
                "citations": 3
            },
            {
                "title": "General-purpose foundation models for increased autonomy in robot-assisted surgery",
                "abstract": "The dominant paradigm for end-to-end robot learning focuses on optimizing task-specific objectives that solve a single robotic problem such as picking up an object or reaching a target position. However, recent work on high-capacity models in robotics has shown promise toward being trained on large collections of diverse and task-agnostic datasets of video demonstrations. These models have shown impressive levels of generalization to unseen circumstances, especially as the amount of data and the model complexity scale. Surgical robot systems that learn from data have struggled to advance as quickly as other fields of robot learning for a few reasons: (1) there is a lack of existing large-scale open-source data to train models, (2) it is challenging to model the soft-body deformations that these robots work with during surgery because simulation cannot match the physical and visual complexity of biological tissue, and (3) surgical robots risk harming patients when tested in clinical trials and require more extensive safety measures. This perspective article aims to provide a path toward increasing robot autonomy in robot-assisted surgery through the development of a multi-modal, multi-task, vision-language-action model for surgical robots. Ultimately, we argue that surgical robots are uniquely positioned to benefit from general-purpose models and provide three guiding actions toward increased autonomy in robot-assisted surgery.",
                "authors": "Samuel Schmidgall, Ji Woong Kim, Alan Kuntz, A. Ghazi, Axel Krieger",
                "citations": 3
            },
            {
                "title": "B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory",
                "abstract": "We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.",
                "authors": "L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto",
                "citations": 3
            },
            {
                "title": "Harnessing the deep learning power of foundation models in single-cell omics.",
                "abstract": null,
                "authors": "Qin Ma, Yi Jiang, Hao Cheng, Dong Xu",
                "citations": 3
            },
            {
                "title": "Interpretable foundation models as decryptors peering into the Earth system",
                "abstract": null,
                "authors": "Chenyu Li, Danfeng Hong, Bing Zhang, Tianjun Liao, N. Yokoya, Pedram Ghamisi, Min Chen, Lizhe Wang, J. Benediktsson, J. Chanussot",
                "citations": 3
            },
            {
                "title": "BrainSegFounder: Towards 3D foundation models for neuroimage segmentation",
                "abstract": null,
                "authors": "Joseph Cox, Peng Liu, Skylar E. Stolte, Yunchao Yang, Kang Liu, Kyle B. See, Huiwen Ju, Ruogu Fang",
                "citations": 3
            },
            {
                "title": "Navi2Gaze: Leveraging Foundation Models for Navigation and Target Gazing",
                "abstract": "Task-aware navigation continues to be a challenging area of research, especially in scenarios involving open vocabulary. Previous studies primarily focus on finding suitable locations for task completion, often overlooking the importance of the robot's pose. However, the robot's orientation is crucial for successfully completing tasks because of how objects are arranged (e.g., to open a refrigerator door). Humans intuitively navigate to objects with the right orientation using semantics and common sense. For instance, when opening a refrigerator, we naturally stand in front of it rather than to the side. Recent advances suggest that Vision-Language Models (VLMs) can provide robots with similar common sense. Therefore, we develop a VLM-driven method called Navigation-to-Gaze (Navi2Gaze) for efficient navigation and object gazing based on task descriptions. This method uses the VLM to score and select the best pose from numerous candidates automatically. In evaluations on multiple photorealistic simulation benchmarks, Navi2Gaze significantly outperforms existing approaches by precisely determining the optimal orientation relative to target objects, resulting in a 68.8% reduction in Distance to Goal (DTG). Real-world video demonstrations can be found on the supplementary website",
                "authors": "Jun Zhu, Zihao Du, Haotian Xu, Fengbo Lan, Zilong Zheng, Bo Ma, Shengjie Wang, Tao Zhang",
                "citations": 2
            },
            {
                "title": "Foundation models for physiological data, how to develop them, and what to expect of them",
                "abstract": "The advance of a new generation arti ﬁ cial intelligence ( AI ) models is accelerating. Their appearance in the public spotlight started with the release of OpenAI ’ s ChatGPT web application driven by a large language model — generative pretrained transformer ( GPT 3.0 ) in November 2022. Since then, even more advanced and diverse large language models have been released such as GPT 4 from OpenAI, LlaMA from Meta, and Claude from Anthropic, just to name a few. In parallel to these large models for languages, models of similar architectures for different data modalities including images, audio, and video have been developed, and have demonstrated impressive performance in various domains at a performance level that could not have been imagined before the release of ChatGPT in 2022. The most recent achievement in this regard was Google ’ s Gemini released in December 2023. The speed at which these models are released and the new levels of capacities they demonstrate are astonishing and have triggered society-wide discussions of issues such as AI safety. An executive order on AI was issued by the US government in November symbolizing the importance and urgency in addressing these issues.",
                "authors": "Xiao Hu",
                "citations": 2
            },
            {
                "title": "23 Security Risks in Black-Box Large Language Model Foundation Models",
                "abstract": "We applied our previous generic machine learning risk analysis to the more specific case of large language models (LLMs), identifying an architectural black box with 23 associated risks—a reasonable starting point for the regulation of LLMs.",
                "authors": "Gary McGraw, Richie Bonett, Harold Figueroa, Katie McMahon, James Bret Michael",
                "citations": 2
            },
            {
                "title": "Multi-task learning for medical foundation models",
                "abstract": null,
                "authors": "Jiancheng Yang",
                "citations": 2
            },
            {
                "title": "Visual–language foundation models in medicine",
                "abstract": null,
                "authors": "Chunyu Liu, Yixiao Jin, Zhouyu Guan, Tingyao Li, Yiming Qin, Bo Qian, Zehua Jiang, Yilan Wu, Xiangning Wang, Ying Feng Zheng, Dian Zeng",
                "citations": 2
            },
            {
                "title": "Foundation models: the future of surgical artificial intelligence?",
                "abstract": null,
                "authors": "K. Lam, Jianing Qiu",
                "citations": 2
            },
            {
                "title": "Managing Misuse Risk for Dual-Use Foundation Models",
                "abstract": "<jats:p/>",
                "authors": "G. Nist",
                "citations": 2
            },
            {
                "title": "Improving Foundation Models",
                "abstract": null,
                "authors": "Aran Komatsuzaki",
                "citations": 0
            },
            {
                "title": "A Legislative Foundation for Foundation Models",
                "abstract": null,
                "authors": "Steven Arango",
                "citations": 0
            },
            {
                "title": "Pathology Foundation Models",
                "abstract": null,
                "authors": "",
                "citations": 0
            },
            {
                "title": "Me-LLaMA: Foundation Large Language Models for Medical Applications",
                "abstract": "Recent advancements in large language models (LLMs) such as ChatGPT and LLaMA have hinted at their potential to revolutionize medical applications, yet their application in clinical settings often reveals limitations due to a lack of specialized training on medical-specific data. In response to this challenge, this study introduces Me-LLaMA, a novel medical LLM family that includes foundation models – Me-LLaMA 13/70B, along with their chat-enhanced versions – Me-LLaMA 13/70B-chat, developed through continual pre-training and instruction tuning of LLaMA2 using large medical datasets. Our methodology leverages a comprehensive domain-specific data suite, including a large-scale, continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a new medical evaluation benchmark (MIBE) across six critical medical tasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me-LLaMA models achieve overall better performance than existing open-source medical LLMs in zero-shot, few-shot and supervised learning abilities. With task-specific instruction tuning, Me-LLaMA models outperform ChatGPT on 7 out of 8 datasets and GPT-4 on 5 out of 8 datasets. In addition, we investigated the catastrophic forgetting problem, and our results show that Me-LLaMA models outperform other open-source medical LLMs in mitigating this issue. Me-LLaMA is one of the largest open-source medical foundation LLMs that use both biomedical and clinical data. It exhibits superior performance across both general and medical tasks compared to other open-source medical LLMs, rendering it an attractive choice for medical AI applications. We release our models, datasets, and evaluation scripts at: https://github.com/BIDS-Xu-Lab/Me-LLaMA.",
                "authors": "Qianqian Xie, Qingyu Chen, Aokun Chen, C.A.I. Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, V. Keloth, Xingyu Zhou, Huan He, Lucila Ohno-Machido, Yonghui Wu, Hua Xu, Jiang Bian",
                "citations": 31
            },
            {
                "title": "The Llama 3 Herd of Models",
                "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
                "authors": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony S. Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, C. Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Cantón Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, J. Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, J. V. D. Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, K. Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, L. Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, M. Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, M. Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasić, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, R. Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, S. Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, S. Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit-ney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, A. Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leonhardi, Po-Yao (Bernie) Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Shang-Wen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzm'an, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, G. Thattai, Grant Herman, G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, U. KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, K. Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, A. Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, M. Tsimpoukelli, Martynas Mankus, Matan Hasson, M. Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, M. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollár, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, S. Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, S. Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, V. Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang",
                "citations": 1674
            },
            {
                "title": "FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model",
                "abstract": "The fast simulation of dynamical systems is a key challenge in many scientific and engineering applications, such as weather forecasting, disease control, and drug discovery. With the recent success of deep learning, there is increasing interest in using neural networks to solve differential equations in a data-driven manner. However, existing methods are either limited to specific types of differential equations or require large amounts of data for training. This restricts their practicality in many real-world applications, where data is often scarce or expensive to obtain. To address this, we propose a novel multi-modal foundation model, named \\textbf{FMint} (\\textbf{F}oundation \\textbf{M}odel based on \\textbf{In}i\\textbf{t}ialization), to bridge the gap between human-designed and data-driven models for the fast simulation of dynamical systems. Built on a decoder-only transformer architecture with in-context learning, FMint utilizes both numerical and textual data to learn a universal error correction scheme for dynamical systems, using prompted sequences of coarse solutions from traditional solvers. The model is pre-trained on a corpus of 40K ODEs, and we perform extensive experiments on challenging ODEs that exhibit chaotic behavior and of high dimensionality. Our results demonstrate the effectiveness of the proposed model in terms of both accuracy and efficiency compared to classical numerical solvers, highlighting FMint's potential as a general-purpose solver for dynamical systems. Our approach achieves an accuracy improvement of 1 to 2 orders of magnitude over state-of-the-art dynamical system simulators, and delivers a 5X speedup compared to traditional numerical algorithms. The code for FMint is available at \\url{https://github.com/margotyjx/FMint}.",
                "authors": "Zezheng Song, Jiaxin Yuan, Haizhao Yang",
                "citations": 15
            },
            {
                "title": "Apple Intelligence Foundation Language Models",
                "abstract": "We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",
                "authors": "Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Kewei Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, A. Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fang Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey P. Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng, Y. Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhaoping Luo, Ouyang Zhi, Alp Aygar, Alvin Wan, Andrew D. Walkingshaw, Tzu-Hsiang Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, J. Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, R. Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",
                "citations": 16
            },
            {
                "title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents",
                "abstract": "Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence. However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments. To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities. Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models. Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning. Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents. Code, train \\&test data, and part of fine-tuned open LMMs are available at \\url{https://github.com/THUDM/VisualAgentBench}.",
                "authors": "Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, Jiadai Sun, Xinyue Yang, Yu Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qi Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, Jie Tang",
                "citations": 14
            },
            {
                "title": "The Essential Role of Causality in Foundation World Models for Embodied AI",
                "abstract": "Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents will require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions and are therefore insufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitating meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook for future research.",
                "authors": "Tarun Gupta, Wenbo Gong, Chao Ma, Nick Pawlowski, Agrin Hilmkil, Meyer Scetbon, Ade Famoti, A. Llorens, Jianfeng Gao, Stefan Bauer, Danica Kragic, Bernhard Schölkopf, Cheng Zhang",
                "citations": 10
            },
            {
                "title": "scGPT: toward building a foundation model for single-cell multi-omics using generative AI.",
                "abstract": null,
                "authors": "Haotian Cui, Chloe X. Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, Bo Wang",
                "citations": 196
            },
            {
                "title": "Prediction of surface settlement around subway foundation pits based on spatiotemporal characteristics and deep learning models",
                "abstract": null,
                "authors": "Wensong Zhang, Ying Yuan, Meng Long, Rong-Han Yao, Lei Jia, Min Liu",
                "citations": 12
            },
            {
                "title": "Sora for foundation robots with parallel intelligence: three world models, three robotic systems",
                "abstract": null,
                "authors": "Lili Fan, Chao Guo, Yonglin Tian, Hui Zhang, Jun Zhang, Fei-Yue Wang",
                "citations": 10
            },
            {
                "title": "Lossy Image Compression with Foundation Diffusion Models",
                "abstract": "Incorporating diffusion models in the image compression domain has the potential to produce realistic and detailed reconstructions, especially at extremely low bitrates. Previous methods focus on using diffusion models as expressive decoders robust to quantization errors in the conditioning signals, yet achieving competitive results in this manner requires costly training of the diffusion model and long inference times due to the iterative generative process. In this work we formulate the removal of quantization error as a denoising task, using diffusion to recover lost information in the transmitted image latent. Our approach allows us to perform less than 10% of the full diffusion generative process and requires no architectural changes to the diffusion model, enabling the use of foundation models as a strong prior without additional fine tuning of the backbone. Our proposed codec outperforms previous methods in quantitative realism metrics, and we verify that our reconstructions are qualitatively preferred by end users, even when other methods use twice the bitrate.",
                "authors": "Lucas Relic, Roberto Azevedo, Markus H. Gross, Christopher Schroers",
                "citations": 5
            },
            {
                "title": "A visual-language foundation model for computational pathology.",
                "abstract": null,
                "authors": "Ming Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, L. Le, Georg K. Gerber, Anil V. Parwani, Andrew Zhang, Faisal Mahmood",
                "citations": 114
            },
            {
                "title": "Thermal Conductivity Predictions with Foundation Atomistic Models",
                "abstract": "Advances in machine learning have led to the development of foundation models for atomistic materials chemistry, enabling quantum-accurate descriptions of interatomic forces across diverse compounds at reduced computational cost. Hitherto, these models have been benchmarked relying on descriptors based on atoms' interaction energies or harmonic vibrations; their accuracy and efficiency in predicting observable and technologically relevant heat-conduction properties remains unknown. Here, we introduce a framework that leverages foundation models and the Wigner formulation of heat transport to overcome the major bottlenecks of current methods for designing heat-management materials: high cost, limited transferability, or lack of physics awareness. We present the standards needed to achieve first-principles accuracy in conductivity predictions through model's fine-tuning, discussing benchmark metrics and precision/cost trade-offs. We apply our framework to a database of solids with diverse compositions and structures, demonstrating its potential to discover materials for next-gen technologies ranging from thermal insulation to neuromorphic computing.",
                "authors": "Bal'azs P'ota, P. Ahlawat, G'abor Cs'anyi, Michele Simoncelli",
                "citations": 2
            },
            {
                "title": "Sapiens: Foundation for Human Vision Models",
                "abstract": "We present Sapiens, a family of models for four fundamental human-centric vision tasks -- 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability -- model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error. Project page: https://about.meta.com/realitylabs/codecavatars/sapiens.",
                "authors": "Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Zhaoen Su, Austin James, Peter Selednik, Stuart Anderson, Shunsuke Saito",
                "citations": 8
            },
            {
                "title": "A comparative analysis of the vibrational behavior of various beam models with different foundation designs",
                "abstract": null,
                "authors": "G. Kanwal, Naveed Ahmed, Rab Nawaz",
                "citations": 7
            },
            {
                "title": "Recent Advances of Foundation Language Models-based Continual Learning: A Survey",
                "abstract": "Recently, foundation language models (LMs) have marked significant achievements in the domains of natural language processing (NLP) and computer vision (CV). Unlike traditional neural network models, foundation LMs obtain a great ability for transfer learning by acquiring rich commonsense knowledge through pre-training on extensive unsupervised datasets with a vast number of parameters. Despite these capabilities, LMs still struggle with catastrophic forgetting, hindering their ability to learn continuously like humans. To address this, continual learning (CL) methodologies have been introduced, allowing LMs to adapt to new tasks while retaining learned knowledge. However, a systematic taxonomy of existing approaches and a comparison of their performance are still lacking. In this paper, we delve into a comprehensive review, summarization, and classification of the existing literature on CL-based approaches applied to foundation language models, such as pre-trained language models (PLMs), large language models (LLMs) and vision-language models (VLMs). We divide these studies into offline and online CL, which consist of traditional methods, parameter-efficient-based methods, instruction tuning-based methods and continual pre-training methods. Additionally, we outline the typical datasets and metrics employed in CL research and provide a detailed analysis of the challenges and future work for LMs-based continual learning.",
                "authors": "Yutao Yang, Jie Zhou, Xuanwen Ding, Tianyu Huai, Shunyu Liu, Qin Chen, Liang He, Yuan Xie",
                "citations": 5
            },
            {
                "title": "Parallel Driving with Big Models and Foundation Intelligence in Cyber–Physical–Social Spaces",
                "abstract": "Recent years have witnessed numerous technical breakthroughs in connected and autonomous vehicles (CAVs). On the one hand, these breakthroughs have significantly advanced the development of intelligent transportation systems (ITSs); on the other hand, these new traffic participants introduce more complex and uncertain elements to ITSs from the social space. Digital twins (DTs) provide real-time, data-driven, precise modeling for constructing the digital mapping of physical-world ITSs. Meanwhile, the metaverse integrates emerging technologies such as virtual reality/mixed reality, artificial intelligence, and DTs to model and explore how to realize improved sustainability, increased efficiency, and enhanced safety. More recently, as a leading effort toward general artificial intelligence, the concept of foundation model was proposed and has achieved significant success, showing great potential to lay the cornerstone for diverse artificial intelligence applications across different domains. In this article, we explore the big models embodied foundation intelligence for parallel driving in cyber-physical-social spaces, which integrate metaverse and DTs to construct a parallel training space for CAVs, and present a comprehensive elucidation of the crucial characteristics and operational mechanisms. Beyond providing the infrastructure and foundation intelligence of big models for parallel driving, this article also discusses future trends and potential research directions, and the “6S” goals of parallel driving.",
                "authors": "Xiao Wang, Jun Huang, Yonglin Tian, Chen Sun, Lie Yang, Shanhe Lou, Chengqi Lv, Changyin Sun, Fei-Yue Wang",
                "citations": 4
            },
            {
                "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites",
                "abstract": "In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448$\\times$448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.",
                "authors": "Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiao-wen Dong, Hang Yan, Hewei Guo, Conghui He, Zhenjiang Jin, Chaochao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao",
                "citations": 268
            },
            {
                "title": "A whole-slide foundation model for digital pathology from real-world data",
                "abstract": null,
                "authors": "Hanwen Xu, Naoto Usuyama, Jaspreet Bagga, Sheng Zhang, Rajesh Rao, Tristan Naumann, Cliff Wong, Zelalem Gero, Javier González, Yu Gu, Yanbo Xu, Mu-Hsin Wei, Wenhui Wang, Shuming Ma, Furu Wei, Jianwei Yang, Chun-yue Li, Jianfeng Gao, Jaylen Rosemon, Tucker Bower, Soohee Lee, R. Weerasinghe, Bill J Wright, Ari Robicsek, B. Piening, Carlo Bifulco, Sheng Wang, Hoifung Poon",
                "citations": 98
            },
            {
                "title": "Variational Language Concepts for Interpreting Foundation Language Models",
                "abstract": "Foundation Language Models (FLMs) such as BERT and its variants have achieved remarkable success in natural language processing. To date, the interpretability of FLMs has primarily relied on the attention weights in their self-attention layers. However, these attention weights only provide word-level interpretations, failing to capture higher-level structures, and are therefore lacking in readability and intuitiveness. To address this challenge, we first provide a formal definition of conceptual interpretation and then propose a variational Bayesian framework, dubbed VAriational Language Concept (VALC), to go beyond word-level interpretations and provide concept-level interpretations. Our theoretical analysis shows that our VALC finds the optimal language concepts to interpret FLM predictions. Empirical results on several real-world datasets show that our method can successfully provide conceptual interpretation for FLMs.",
                "authors": "Hengyi Wang, Shiwei Tan, Zhiqing Hong, Desheng Zhang, Hao Wang",
                "citations": 2
            },
            {
                "title": "A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray Interpretation",
                "abstract": "Over 1.4 billion chest X-rays (CXRs) are performed annually due to their cost-effectiveness as an initial diagnostic test. This scale of radiological studies provides a significant opportunity to streamline CXR interpretation and documentation. While foundation models are a promising solution, the lack of publicly available large-scale datasets and benchmarks inhibits their iterative development and real-world evaluation. To overcome these challenges, we constructed a large-scale dataset (CheXinstruct), which we utilized to train a vision-language foundation model (CheXagent). We systematically demonstrated competitive performance across eight distinct task types on our novel evaluation benchmark (CheXbench). Beyond technical validation, we assessed the real-world utility of CheXagent in directly drafting radiology reports. Our clinical assessment with eight radiologists revealed a 36% time saving for residents using CheXagent-drafted reports, while attending radiologists showed no significant time difference editing resident-drafted or CheXagent-drafted reports. The CheXagent-drafted reports improved the writing efficiency of both radiology residents and attending radiologists in 81% and 61% of cases, respectively, without loss of quality. Overall, we demonstrate that CheXagent can effectively perform a variety of CXR interpretation tasks and holds potential to assist radiologists in routine clinical workflows.",
                "authors": "Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, E. Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, S. Gatidis, Akshay S. Chaudhari, Curtis P. Langlotz",
                "citations": 41
            },
            {
                "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
                "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.",
                "authors": "Yanis Labrak, Adrien Bazoge, Emmanuel Morin, P. Gourraud, Mickael Rouvier, Richard Dufour",
                "citations": 131
            },
            {
                "title": "A foundation model for clinical-grade computational pathology and rare cancers detection",
                "abstract": null,
                "authors": "Eugene Vorontsov, A. Bozkurt, Adam Casson, George Shaikovski, Michal Zelechowski, Kristen Severson, Eric Zimmermann, James Hall, Neil Tenenholtz, Nicolò Fusi, Ellen Yang, Philippe Mathieu, A. van Eck, Donghun Lee, Julian Viret, Eric Robert, Yi Kan Wang, J. Kunz, Matthew C H Lee, Jan H Bernhard, R. Godrich, Gerard Oakley, Ewan Millar, Matthew G Hanna, Hannah Y Wen, Juan A Retamero, William A. Moye, Razik Yousfi, C. Kanan, D.S. Klimstra, B. Rothrock, Siqi Liu, Thomas J Fuchs",
                "citations": 51
            },
            {
                "title": "Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-Shot Metric Depth and Surface Normal Estimation",
                "abstract": "We introduce Metric3D v2, a geometric foundation model designed for zero-shot metric depth and surface normal estimation from single images, critical for accurate 3D recovery. Depth and normal estimation, though complementary, present distinct challenges. State-of-the-art monocular depth methods achieve zero-shot generalization through affine-invariant depths, but fail to recover real-world metric scale. Conversely, current normal estimation techniques struggle with zero-shot performance due to insufficient labeled data. We propose targeted solutions for both metric depth and normal estimation. For metric depth, we present a canonical camera space transformation module that resolves metric ambiguity across various camera models and large-scale datasets, which can be easily integrated into existing monocular models. For surface normal estimation, we introduce a joint depth-normal optimization module that leverages diverse data from metric depth, allowing normal estimators to improve beyond traditional labels. Our model, trained on over 16 million images from thousands of camera models with varied annotations, excels in zero-shot generalization to new camera settings. As shown in Fig. 1, It ranks the 1st in multiple zero-shot and standard benchmarks for metric depth and surface normal prediction. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. Our model also relieves the scale drift issues of monocular-SLAM (Fig. 3), leading to high-quality metric scale dense mapping. Such applications highlight the versatility of Metric3D v2 models as geometric foundation models.",
                "authors": "Mu Hu, Wei Yin, C. Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, Shaojie Shen",
                "citations": 41
            },
            {
                "title": "RudolfV: A Foundation Model by Pathologists for Pathologists",
                "abstract": "Artificial intelligence has started to transform histopathology impacting clinical diagnostics and biomedical research. However, while many computational pathology approaches have been proposed, most current AI models are limited with respect to generalization, application variety, and handling rare diseases. Recent efforts introduced self-supervised foundation models to address these challenges, yet existing approaches do not leverage pathologist knowledge by design. In this study, we present a novel approach to designing foundation models for computational pathology, incorporating pathologist expertise, semi-automated data curation, and a diverse dataset from over 15 laboratories, including 58 tissue types, and encompassing 129 different histochemical and immunohistochemical staining modalities. We demonstrate that our model\"RudolfV\"surpasses existing state-of-the-art foundation models across different benchmarks focused on tumor microenvironment profiling, biomarker evaluation, and reference case search while exhibiting favorable robustness properties. Our study shows how domain-specific knowledge can increase the efficiency and performance of pathology foundation models and enable novel application areas.",
                "authors": "Jonas Dippel, Barbara Feulner, Tobias Winterhoff, S. Schallenberg, Gabriel Dernbach, Andreas Kunft, Stephan Tietz, Philipp Jurmeister, David Horst, Lukas Ruff, Klaus-Robert Müller, Frederick Klauschen, Maximilian Alber",
                "citations": 17
            },
            {
                "title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding",
                "abstract": "Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present Foundation Model Embedded Gaussian Splatting (FMGS), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of the same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by 10.2 percent on open-vocabulary language-based object detection, despite that we are 851X faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code on the project page.",
                "authors": "Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li",
                "citations": 25
            },
            {
                "title": "ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning",
                "abstract": "Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously benchmark the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM to offer a new perspective on handling multi-modal tasks that strongly depend on interpretable patterns, such as reasoning tasks in the field of charts or geometric images. We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set. Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related large models, achieving results comparable to GPT-4V. We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable multi-modal models. Both ChartX and ChartVLM are available at: https://github.com/Alpha-Innovator/ChartVLM",
                "authors": "Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, Yu Qiao",
                "citations": 30
            },
            {
                "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
                "abstract": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/",
                "authors": "Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David Held, Zackory Erickson",
                "citations": 24
            },
            {
                "title": "VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation",
                "abstract": "VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework.",
                "authors": "Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, Yao Lu",
                "citations": 20
            },
            {
                "title": "Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities",
                "abstract": "The development of foundation models has revolutionized our ability to interpret the Earth’s surface using satellite observational data. Traditional models have been siloed, tailored to specific sensors or data types like optical, radar, and hyper-spectral, each with its own unique characteristics. This specialization hinders the potential for a holistic analysis that could benefit from the combined strengths of these diverse data sources. Our novel approach introduces the Dynamic One-For-All (DOFA) model, leveraging the concept of neural plasticity in brain science to integrate various data modalities into a single framework adaptively. This dynamic hypernetwork, adjusting to different wavelengths, enables a single versatile Transformer jointly trained on data from five sensors to excel across 12 distinct Earth observation tasks, including sensors never seen during pretraining. DOFA’s innovative design offers a promising leap towards more accurate, efficient, and unified Earth observation analysis, showcasing remarkable adaptability and performance in harnessing the potential of multimodal Earth observation data.",
                "authors": "Zhitong Xiong, Yi Wang, Fahong Zhang, Adam J. Stewart, Joelle Hanna, Damian Borth, Ioannis Papoutsis, B. L. Saux, G. Camps-Valls, Xiao Xiang Zhu",
                "citations": 16
            },
            {
                "title": "Foundation model for cancer imaging biomarkers",
                "abstract": null,
                "authors": "Suraj Pai, Dennis Bontempi, Ibrahim Hadzic, Vasco Prudente, M. Sokač, T. Chaunzwa, S. Bernatz, A. Hosny, Raymond H. Mak, Nicolai J. Birkbak, Hugo J W L Aerts",
                "citations": 22
            },
            {
                "title": "MTP: Advancing Remote Sensing Foundation Model via Multitask Pretraining",
                "abstract": "Foundation models have reshaped the landscape of remote sensing (RS) by enhancing various image interpretation tasks. Pretraining is an active research topic, encompassing supervised and self-supervised learning methods to initialize model weights effectively. However, transferring the pretrained models to downstream tasks may encounter task discrepancy due to their formulation of pretraining as image classification or object discrimination tasks. In this study, we explore the multitask pretraining (MTP) paradigm for RS foundation models to address this issue. Using a shared encoder and task-specific decoder architecture, we conduct multitask supervised pretraining on the segment anything model annotated remote sensing segmentation dataset, encompassing semantic segmentation, instance segmentation, and rotated object detection. MTP supports both convolutional neural networks and vision transformer foundation models with over 300 million parameters. The pretrained models are finetuned on various RS downstream tasks, such as scene classification, horizontal, and rotated object detection, semantic segmentation, and change detection. Extensive experiments across 14 datasets demonstrate the superiority of our models over existing ones of similar size and their competitive performance compared to larger state-of-the-art models, thus validating the effectiveness of MTP.",
                "authors": "Di Wang, Jing Zhang, Minqiang Xu, Lin Liu, Dongsheng Wang, Erzhong Gao, Chengxi Han, Haonan Guo, Bo Du, Dacheng Tao, L. Zhang",
                "citations": 18
            },
            {
                "title": "Vision–language foundation model for echocardiogram interpretation",
                "abstract": null,
                "authors": "M. Christensen, Milos Vukadinovic, Neal Yuan, D. Ouyang",
                "citations": 32
            },
            {
                "title": "A pathology foundation model for cancer diagnosis and prognosis prediction.",
                "abstract": null,
                "authors": "Xiyue Wang, Junhan Zhao, Eliana Marostica, Wei Yuan, Jietian Jin, Jiayu Zhang, Ruijiang Li, Hongping Tang, Kanran Wang, Yu Li, Fang Wang, Yulong Peng, Junyou Zhu, Jing Zhang, Christopher R Jackson, Jun Zhang, Deborah Dillon, Nancy U Lin, L. Sholl, Thomas Denize, David Meredith, Keith L. Ligon, S. Signoretti, S. Ogino, Jeffrey A Golden, MacLean P Nasrallah, Xiao Han, Sen Yang, Kun-Hsing Yu",
                "citations": 31
            },
            {
                "title": "Rethinking Machine Unlearning for Large Language Models",
                "abstract": "We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.",
                "authors": "Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Chris Liu, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu",
                "citations": 49
            },
            {
                "title": "Foundation Intelligence for Smart Infrastructure Services in Transportation 5.0",
                "abstract": "This perspective paper delves into the concept of foundation intelligence that shapes the future of smart infrastructure services as the transportation sector transitions into the era of Transportation 5.0. First, the discussion focuses on a suite of emerging technologies essential for foundation intelligence. These technologies encompass digital twinning, parallel intelligence, large vision-language models, traffic simulation and transportation systems modeling, vehicle-to-everything (V2X) connectivity, and decentralized/distributed systems. Next, the paper introduces the present landscape of Transportation 5.0 applications as illuminated by the foundational intelligence, and casts a vision towards the future including cooperative driving automation, smart intersection/infrastructure, parallel traffic management, virtual drivers, and mobility systems planning and operations, laying out prospects that are poised to redefine the mobility ecosystem. Last, through a comprehensive outlook, this paper aspires to offer a guiding framework for the intelligent evolution in data generation and model calibration, digital twinning and simulation, scenario development and experimentation, feedback loop for management and control, and continuous learning and adaptation, fostering safety, efficiency, reliability, and sustainability in the future smart transportation infrastructure.",
                "authors": "Xu Han, Zonglin Meng, Xin Xia, Xishun Liao, Yueshuai He, Zhaoliang Zheng, Yutong Wang, Hao Xiang, Zewei Zhou, Letian Gao, Lili Fan, Yuke Li, Jiaqi Ma",
                "citations": 16
            },
            {
                "title": "Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions",
                "abstract": "Foundation model, trained on a diverse range of data and adaptable to a myriad of tasks, is advancing healthcare. It fosters the development of healthcare artificial intelligence (AI) models tailored to the intricacies of the medical field, bridging the gap between limited AI models and the varied nature of healthcare practices. The advancement of a healthcare foundation model (HFM) brings forth tremendous potential to augment intelligent healthcare services across a broad spectrum of scenarios. However, despite the imminent widespread deployment of HFMs, there is currently a lack of clear understanding regarding their operation in the healthcare field, their existing challenges, and their future trajectory. To answer these critical inquiries, we present a comprehensive and in-depth examination that delves into the landscape of HFMs. It begins with a comprehensive overview of HFMs, encompassing their methods, data, and applications, to provide a quick understanding of the current progress. Subsequently, it delves into a thorough exploration of the challenges associated with data, algorithms, and computing infrastructures in constructing and widely applying foundation models in healthcare. Furthermore, this survey identifies promising directions for future development in this field. We believe that this survey will enhance the community's understanding of the current progress of HFMs and serve as a valuable source of guidance for future advancements in this domain. For the latest HFM papers and related resources, please refer to our website.",
                "authors": "Yuting He, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, Hao Chen",
                "citations": 16
            },
            {
                "title": "Transparent medical image AI via an image-text foundation model grounded in medical literature.",
                "abstract": null,
                "authors": "Chanwoo Kim, S. U. Gadgil, A. DeGrave, J. Omiye, Zhuo Ran Cai, Roxana Daneshjou, Su-In Lee",
                "citations": 22
            },
            {
                "title": "OmniJet-α: The first cross-task foundation model for particle physics",
                "abstract": "\n Foundation models are multi-dataset and multi-task machine learning methods that once pre-trained can be fine-tuned for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data. We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with transformer architectures (the common backbone of foundation models). These measures motivate the choice of a higher-fidelity tokenization compared to previous works. Finally, we demonstrate transfer learning between an unsupervised problem (jet generation) and a classic supervised task (jet tagging) with our new OmniJet-α model. This is the first successful transfer between two different and actively studied classes of tasks and constitutes a major step in the building of foundation models for particle physics.",
                "authors": "Joschka Birk, Anna Hallin, G. Kasieczka",
                "citations": 13
            },
            {
                "title": "SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation",
                "abstract": "The rapid evolution of multimodal foundation model has demonstrated significant progresses in vision-language understanding and generation, e.g., our previous work SEED-LLaMA. However, there remains a gap between its capability and the real-world applicability, primarily due to the model's limited capacity to effectively respond to various user instructions and interact with diverse visual data. In this work, we focus on bridging this gap through integrating two enhanced features: (1) comprehending images of arbitrary sizes and ratios, and (2) enabling multi-granularity image generation. We present a unified and versatile foundation model, namely, SEED-X, which is able to model multi-granularity visual semantics for comprehension and generation tasks. Besides the competitive results on public benchmarks, SEED-X demonstrates its effectiveness in handling real-world applications across various domains after instruction tuning. We hope that our work will inspire future research into what can be achieved by versatile multimodal foundation models in real-world applications. The models, codes, and datasets will be released in https://github.com/AILab-CVC/SEED-X.",
                "authors": "Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan",
                "citations": 46
            },
            {
                "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text",
                "abstract": "Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications. Our code and data are available at https://github.com/AI-secure/aug-pe.",
                "authors": "Chulin Xie, Zinan Lin, A. Backurs, Sivakanth Gopi, Da Yu, Huseyin A. Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, Sergey Yekhanin",
                "citations": 16
            },
            {
                "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models",
                "abstract": "We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}.",
                "authors": "Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, Mingqing Gong, Peisong Huang, Qingqing Huang, Zhiying Huang, Yuanyuan Huo, Dongya Jia, Chumin Li, Feiya Li, Hui Li, Jiaxin Li, Xiaoyang Li, Xingxing Li, Lin Liu, Shouda Liu, Sichao Liu, Xudong Liu, Yuchen Liu, Zhengxi Liu, Lu Lu, Junjie Pan, Xin Wang, Yuping Wang, Yuxuan Wang, Zhengnan Wei, Jian Wu, Chao Yao, Yifeng Yang, Yuan-Qiu-Qiang Yi, Junteng Zhang, Qidi Zhang, Shuo Zhang, Wenjie Zhang, Yang Zhang, Zilin Zhao, Dejian Zhong, Xiaobin Zhuang",
                "citations": 35
            },
            {
                "title": "Foundation Policies with Hilbert Representations",
                "abstract": "Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key insight is to learn a structured representation that preserves the temporal structure of the underlying environment, and then to span this learned latent space with directional movements, which enables various zero-shot policy\"prompting\"schemes for downstream tasks. Through our experiments on simulated robotic locomotion and manipulation benchmarks, we show that our unsupervised policies can solve goal-conditioned and general RL tasks in a zero-shot fashion, even often outperforming prior methods designed specifically for each setting. Our code and videos are available at https://seohong.me/projects/hilp/.",
                "authors": "Seohong Park, Tobias Kreiman, Sergey Levine",
                "citations": 13
            },
            {
                "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
                "abstract": "Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long video understanding by incorporating two additional stages, i.e., long context extension and long video supervised fine-tuning. However, training on long video is computationally and memory intensive. We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing. LongVILA efficiently extends the number of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in 6,000-frame (more than 1 million tokens) video needle-in-a-haystack. LongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g. 65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and tensor parallelism. Moreover, it seamlessly integrates with Hugging Face Transformers.",
                "authors": "Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han",
                "citations": 22
            },
            {
                "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
                "abstract": "Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long video understanding by incorporating two additional stages, i.e., long context extension and long video supervised fine-tuning. However, training on long video is computationally and memory intensive. We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing. LongVILA efficiently extends the number of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in 6,000-frame (more than 1 million tokens) video needle-in-a-haystack. LongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g. 65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and tensor parallelism. Moreover, it seamlessly integrates with Hugging Face Transformers.",
                "authors": "Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han",
                "citations": 22
            },
            {
                "title": "Large Language Models are Geographically Biased",
                "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
                "authors": "Rohin Manvi, Samar Khanna, Marshall Burke, David B. Lobell, Stefano Ermon",
                "citations": 32
            },
            {
                "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models",
                "abstract": "This report introduces xGen-MM (also known as BLIP-3), a framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen initiative on foundation AI models. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our pre-trained base model exhibits strong in-context learning capabilities and the instruction-tuned model demonstrates competitive performance among open-source LMMs with similar model sizes. In addition, we introduce a safety-tuned model with DPO, aiming to mitigate harmful behaviors such as hallucinations and improve safety. We open-source our models, curated large-scale datasets, and our fine-tuning codebase to facilitate further advancements in LMM research. Associated resources will be available on our project page above.",
                "authors": "Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S. Ryoo, Shrikant B. Kendre, Jieyu Zhang, Can Qin, Shu Zhen Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, Ran Xu",
                "citations": 33
            },
            {
                "title": "OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer",
                "abstract": "Recent studies have highlighted the importance of fully open foundation models. The Open Whisper-style Speech Model (OWSM) is an initial step towards reproducing OpenAI Whisper using public data and open-source toolkits. However, previous versions of OWSM (v1 to v3) are still based on standard Transformer, which might lead to inferior performance compared to state-of-the-art speech encoder architectures. This work aims to improve the performance and efficiency of OWSM without additional data. We present a series of E-Branchformer-based models named OWSM v3.1, ranging from 100M to 1B parameters. OWSM v3.1 outperforms its predecessor, OWSM v3, in most evaluation benchmarks, while showing an improved inference speed of up to 25%. We further reveal the emergent ability of OWSM v3.1 in zero-shot contextual biasing speech recognition. We also provide a model trained on a subset of data with low license restrictions. We will publicly release the code, pre-trained models, and training logs.",
                "authors": "Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, Jee-weon Jung, Shinji Watanabe",
                "citations": 30
            },
            {
                "title": "EndoDAC: Efficient Adapting Foundation Model for Self-Supervised Depth Estimation from Any Endoscopic Camera",
                "abstract": "Depth estimation plays a crucial role in various tasks within endoscopic surgery, including navigation, surface reconstruction, and augmented reality visualization. Despite the significant achievements of foundation models in vision tasks, including depth estimation, their direct application to the medical domain often results in suboptimal performance. This highlights the need for efficient adaptation methods to adapt these models to endoscopic depth estimation. We propose Endoscopic Depth Any Camera (EndoDAC) which is an efficient self-supervised depth estimation framework that adapts foundation models to endoscopic scenes. Specifically, we develop the Dynamic Vector-Based Low-Rank Adaptation (DV-LoRA) and employ Convolutional Neck blocks to tailor the foundational model to the surgical domain, utilizing remarkably few trainable parameters. Given that camera information is not always accessible, we also introduce a self-supervised adaptation strategy that estimates camera intrinsics using the pose encoder. Our framework is capable of being trained solely on monocular surgical videos from any camera, ensuring minimal training costs. Experiments demonstrate that our approach obtains superior performance even with fewer training epochs and unaware of the ground truth camera intrinsics. Code is available at https://github.com/BeileiCui/EndoDAC.",
                "authors": "Beilei Cui, Mobarakol Islam, Long Bai, An-Chi Wang, Hongliang Ren",
                "citations": 9
            },
            {
                "title": "PRISM: A Multi-Modal Generative Foundation Model for Slide-Level Histopathology",
                "abstract": "Foundation models in computational pathology promise to unlock the development of new clinical decision support systems and models for precision medicine. However, there is a mismatch between most clinical analysis, which is defined at the level of one or more whole slide images, and foundation models to date, which process the thousands of image tiles contained in a whole slide image separately. The requirement to train a network to aggregate information across a large number of tiles in multiple whole slide images limits these models' impact. In this work, we present a slide-level foundation model for H&E-stained histopathology, PRISM, that builds on Virchow tile embeddings and leverages clinical report text for pre-training. Using the tile embeddings, PRISM produces slide-level embeddings with the ability to generate clinical reports, resulting in several modes of use. Using text prompts, PRISM achieves zero-shot cancer detection and sub-typing performance approaching and surpassing that of a supervised aggregator model. Using the slide embeddings with linear classifiers, PRISM surpasses supervised aggregator models. Furthermore, we demonstrate that fine-tuning of the PRISM slide encoder yields label-efficient training for biomarker prediction, a task that typically suffers from low availability of training data; an aggregator initialized with PRISM and trained on as little as 10% of the training data can outperform a supervised baseline that uses all of the data.",
                "authors": "George Shaikovski, Adam Casson, Kristen Severson, Eric Zimmermann, Yi Kan Wang, J. Kunz, J. Retamero, Gerard Oakley, D. Klimstra, C. Kanan, Matthew G Hanna, Michal Zelechowski, Julian Viret, Neil Tenenholtz, James Hall, Nicolò Fusi, Razik Yousfi, Peter Hamilton, William A. Moye, Eugene Vorontsov, Siqi Liu, Thomas J Fuchs",
                "citations": 12
            },
            {
                "title": "LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models",
                "abstract": "The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMS-EVAL offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that emphasizes both coverage and efficiency. Additionally, we present Multimodal LIVEBENCH that utilizes continuously updating news and online forums to assess models' generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs. We opensource our codebase and maintain leaderboard of LIVEBENCH at https://github.com/EvolvingLMMs-Lab/lmms-eval and https://huggingface.co/spaces/lmms-lab/LiveBench.",
                "authors": "Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, Ziwei Liu",
                "citations": 24
            },
            {
                "title": "A Survey on Efficient Federated Learning Methods for Foundation Model Training",
                "abstract": "Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training across a multitude of clients. However, new approaches to FL often discuss their contributions involving small deep-learning models only and focus on training full models on clients. In the wake of Foundation Models (FM), the reality is different for many deep learning applications. Typically, FMs have already been pre-trained across a wide variety of tasks and can be fine-tuned to specific downstream tasks over significantly smaller datasets than required for full model training. However, access to such datasets is often challenging. By its design, FL can help to open data silos. With this survey, we introduce a novel taxonomy focused on computational and communication efficiency, the vital elements to make use of FMs in FL systems. We discuss the benefits and drawbacks of parameter-efficient fine-tuning (PEFT) for FL applications, elaborate on the readiness of FL frameworks to work with FMs and provide future research opportunities on how to evaluate generative models in FL as well as the interplay of privacy and PEFT.",
                "authors": "Herbert Woisetschläger, Alexander Isenko, Shiqiang Wang, R. Mayer, Hans-Arno Jacobsen",
                "citations": 12
            },
            {
                "title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                "abstract": "Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements.",
                "authors": "Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, Huan Sun",
                "citations": 24
            },
            {
                "title": "A Survey on Vision Mamba: Models, Applications and Challenges",
                "abstract": "—Mamba, a recent selective structured state space model, performs excellently on long sequence modeling tasks. Mamba mitigates the modeling constraints of convolutional neural networks and offers advanced modeling capabilities similar to those of Transformers, through global receptive fields and dynamic weighting. Crucially, it achieves this without incurring the quadratic computational complexity typically associated with Transformers. Due to its advantages over the former two mainstream foundation models, Mamba exhibits great potential to be a visual foundation model. Researchers are actively applying Mamba to various computer vision tasks, leading to numerous emerging works. To help keep pace with the rapid advancements in computer vision, this paper aims to provide a comprehensive review of visual Mamba approaches. This paper begins by delineating the formulation of the original Mamba model. Subsequently, our review of visual Mamba delves into several representative backbone networks to elucidate the core insights of the visual Mamba. We then categorize related works using different modalities, including image, video, point cloud, multi-modal, and others. Specifically, for image applications, we further organize them into distinct tasks to facilitate a more structured discussion. Finally, we discuss the challenges and future research directions for visual Mamba, providing insights for future research in this quickly evolving area. A comprehensive list of visual Mamba models reviewed in this work is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models.",
                "authors": "Rui Xu, Shu Yang, Yihui Wang, Bo Du, Hao Chen",
                "citations": 23
            },
            {
                "title": "Generative ConvNet Foundation Model With Sparse Modeling and Low-Frequency Reconstruction for Remote Sensing Image Interpretation",
                "abstract": "Foundation models offer a highly versatile and precise solution for intelligent interpretation of remote sensing images, thus greatly facilitating various remote sensing applications. Nevertheless, conventional remote sensing foundational models based on generative transformers neglect the consideration of multiscale features and frequency information, limiting their potential for dense prediction tasks in remote sensing scenarios. In this article, we make the first attempt to propose a generative convolutional neural network (ConvNet) foundation model tailored for remote sensing scenarios, which comprises two key components: First, a large dataset named GeoSense, containing approximately nine million diverse remote sensing images, is constructed to enhance the robustness and generalization of the foundation model during the pretraining phase. Second, a sparse modeling and low-frequency reconstruction (SMLFR) framework is designed for self-supervised representation learning of the ConvNet foundation model. Specifically, a sparse modeling strategy is proposed in masked image modeling (MIM), which allows ConvNet to process variable-length sequences by treating unmasked patches as voxels and sparsifying the encoder. In addition, a low-frequency reconstruction target is designed to guide the model’s attention toward essential ground object features in remote sensing images, while mitigating unnecessary detail interference. To evaluate the general performance of our proposed foundation model, comprehensive experiments have been carried out on five datasets across three downstream tasks. Experimental results demonstrate that our method consistently achieves state-of-the-art performance across all the benchmark datasets and downstream tasks. The code and pretrained models will be available at https://github.com/HIT-SIRS/SMLFR.",
                "authors": "Zhe Dong, Yanfeng Gu, Tianzhu Liu",
                "citations": 10
            },
            {
                "title": "Foundation Model-Based Multimodal Remote Sensing Data Classification",
                "abstract": "With the increasing availability and openness of remote sensing (RS) data collected from diverse sensors, there has been a growing interest in multimodal RS data classification. Nowadays, in the area of deep learning, there is a paradigm shift with the rise of foundation models, which are trained on large-scale datasets and are adaptable to a wide range of downstream tasks. In this study, the potential and effectiveness of foundation models for multimodal RS data classification is investigated. The training datasets of foundation models and multimodal RS datasets are quite different, and therefore, it is difficult to use a pretrained foundation model for multimodal RS data classification directly. To mitigate this difficulty, this article proposes a foundation model adaptation (FMA) framework for multimodal RS data classification without fine-tuning the parameters. Specifically, two learnable modules, i.e., cross-spatial interaction module and cross-channel interaction module, are proposed to add to the foundation model for extracting multimodal-specific representations. The cross-spatial and cross-channel interaction modules extract the characteristics of unimodal features along the spatial dimension and channel dimension, respectively. To effectively tackle the disparities among various RS modalities, an alignment approach (FMA2) is further explored based on the FMA. The FMA2 describes dependencies between different modalities by establishing a coupling score function, which can further enhance classification performance. To demonstrate the effectiveness and superiority of the FMA framework, comprehensive experiments are conducted on three multimodal RS datasets, showing improvement over the advanced multimodal RS data classification image methods.",
                "authors": "Xin He, Yushi Chen, Lin Huang, Danfeng Hong, Q. Du",
                "citations": 11
            },
            {
                "title": "Towards a Foundation Model for Partial Differential Equations: Multi-Operator Learning and Extrapolation",
                "abstract": "Foundation models, such as large language models, have demonstrated success in addressing various language and image processing tasks. In this work, we introduce a multi-modal foundation model for scientific problems, named PROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is a multi-operator learning approach which can predict future states of spatiotemporal systems while concurrently learning the underlying governing equations of the physical system. Specifically, we focus on multi-operator learning by training distinct one-dimensional time-dependent nonlinear constant coefficient partial differential equations, with potential applications to many physical applications including physics, geology, and biology. More importantly, we provide three extrapolation studies to demonstrate that PROSE-PDE can generalize physical features through the robust training of multiple operators and that the proposed model can extrapolate to predict PDE solutions whose models or data were unseen during the training. Furthermore, we show through systematic numerical experiments that the utilization of the symbolic modality in our model effectively resolves the well-posedness problems with training multiple operators and thus enhances our model's predictive capabilities.",
                "authors": "Jingmin Sun, Yuxuan Liu, Zecheng Zhang, Hayden Schaeffer",
                "citations": 10
            },
            {
                "title": "The Foundation Model Transparency Index v1.1: May 2024",
                "abstract": "Foundation models are increasingly consequential yet extremely opaque. To characterize the status quo, the Foundation Model Transparency Index was launched in October 2023 to measure the transparency of leading foundation model developers. The October 2023 Index (v1.0) assessed 10 major foundation model developers (e.g. OpenAI, Google) on 100 transparency indicators (e.g. does the developer disclose the wages it pays for data labor?). At the time, developers publicly disclosed very limited information with the average score being 37 out of 100. To understand how the status quo has changed, we conduct a follow-up study (v1.1) after 6 months: we score 14 developers against the same 100 indicators. While in v1.0 we searched for publicly available information, in v1.1 developers submit reports on the 100 transparency indicators, potentially including information that was not previously public. We find that developers now score 58 out of 100 on average, a 21 point improvement over v1.0. Much of this increase is driven by developers disclosing information during the v1.1 process: on average, developers disclosed information related to 16.6 indicators that was not previously public. We observe regions of sustained (i.e. across v1.0 and v1.1) and systemic (i.e. across most or all developers) opacity such as on copyright status, data access, data labor, and downstream impact. We publish transparency reports for each developer that consolidate information disclosures: these reports are based on the information disclosed to us via developers. Our findings demonstrate that transparency can be improved in this nascent ecosystem, the Foundation Model Transparency Index likely contributes to these improvements, and policymakers should consider interventions in areas where transparency has not improved.",
                "authors": "Rishi Bommasani, Kevin Klyman, Sayash Kapoor, Shayne Longpre, Betty Xiong, Nestor Maslej, Percy Liang",
                "citations": 10
            },
            {
                "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
                "abstract": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. The code and robust models are available at https://github.com/chs20/RobustVLM",
                "authors": "Christian Schlarmann, Naman D. Singh, Francesco Croce, Matthias Hein",
                "citations": 15
            },
            {
                "title": "Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models",
                "abstract": "Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem (AIW problem) formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models show strong fluctuations across even slight problem variations that should not affect problem solving, also expressing strong overconfidence in the wrong solutions, often backed up by plausible sounding explanation-like confabulations. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs. Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/AIW",
                "authors": "Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, J. Jitsev",
                "citations": 18
            },
            {
                "title": "Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory",
                "abstract": "Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning. In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties. Despite the empirical success, theory of conditional diffusion models is largely missing. This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models. Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound. The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique. Moreover, we demonstrate the utility of our statistical theory in elucidating the performance of conditional diffusion models across diverse applications, including model-based transition kernel estimation in reinforcement learning, solving inverse problems, and reward conditioned sample generation.",
                "authors": "Hengyu Fu, Zhuoran Yang, Mengdi Wang, Minshuo Chen",
                "citations": 15
            },
            {
                "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
                "abstract": "Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\\textbf{A}udio \\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.",
                "authors": "Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, Jingren Zhou",
                "citations": 27
            },
            {
                "title": "SEED-Bench: Benchmarking Multimodal Large Language Models",
                "abstract": "Multimodal large language models (MLLMs), building upon the foundation of powerful large language models (LLMs), have recently demonstrated exceptional capabilities in generating not only texts but also images given in-terleaved multimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However, existing MLLM benchmarks remain limited to assessing only models' comprehension ability of single image-text inputs, failing to keep up with the strides made in MLLMs. A comprehensive benchmark is imperative for investigating the progress and uncovering the limitations of current MLLMs. In this work, we categorize the capabilities of MLLMs into hierarchical levels from L0 to L4 based on the modalities they can ac-cept and generate, and propose SEED-Bench, a comprehensive benchmark that evaluates the hierarchical capa-bilities of MLLMs. Specifically, SEED-Bench comprises 24K multiple-choice questions with accurate human annotations, which span 27 dimensions, including the evaluation of both text and image generation. Multiple-choice questions with ground truth options derived from human annotation enable an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 22 prominent open-source MLLMs and summarize valuable observations. By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench to provide insights that will mo-tivate future research toward the goal of General Artificial Intelligence. Dataset and evaluation code are available at https://github.com/AILab-CVC/SEED-Bench.",
                "authors": "Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan",
                "citations": 28
            },
            {
                "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models",
                "abstract": "This paper presents a novel method for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compared to current SOTA feed-forward 3D generative models, with users preferring our results over 90% of the time.",
                "authors": "Junlin Han, Filippos Kokkinos, Philip Torr",
                "citations": 27
            },
            {
                "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping",
                "abstract": "This paper assesses trending AI foundation models, especially emerging computer vision foundation models and their performance in natural landscape feature segmentation. While the term foundation model has quickly garnered interest from the geospatial domain, its definition remains vague. Hence, this paper will first introduce AI foundation models and their defining characteristics. Built upon the tremendous success achieved by Large Language Models (LLMs) as the foundation models for language tasks, this paper discusses the challenges of building foundation models for geospatial artificial intelligence (GeoAI) vision tasks. To evaluate the performance of large AI vision models, especially Meta’s Segment Anything Model (SAM), we implemented different instance segmentation pipelines that minimize the changes to SAM to leverage its power as a foundation model. A series of prompt strategies were developed to test SAM’s performance regarding its theoretical upper bound of predictive accuracy, zero-shot performance, and domain adaptability through fine-tuning. The analysis used two permafrost feature datasets, ice-wedge polygons and retrogressive thaw slumps because (1) these landform features are more challenging to segment than man-made features due to their complicated formation mechanisms, diverse forms, and vague boundaries; (2) their presence and changes are important indicators for Arctic warming and climate change. The results show that although promising, SAM still has room for improvement to support AI-augmented terrain mapping. The spatial and domain generalizability of this finding is further validated using a more general dataset EuroCrops for agricultural field mapping. Finally, we discuss future research directions that strengthen SAM’s applicability in challenging geospatial domains.",
                "authors": "Wenwen Li, Chia-Yu Hsu, Sizhe Wang, Yezhou Yang, Hyunho Lee, A. Liljedahl, C. Witharana, Yili Yang, B. Rogers, S. Arundel, Matthew B. Jones, Kenton McHenry, Patricia Solis",
                "citations": 9
            },
            {
                "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
                "abstract": "Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc. Project page: https://lotus3d.github.io/.",
                "authors": "Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Liu, Bingbing Liu, Ying-Cong Chen",
                "citations": 12
            },
            {
                "title": "OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification",
                "abstract": "There has been an increasing interest in large speech models that can perform multiple tasks in a single model. Such models usually adopt an encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). Compared to encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up to 24% relative improvement on ST, while it is more robust and 3 to 4 times faster for inference. OWSM-CTC also improves the long-form ASR result with 20x speed-up. We will publicly release our code, pre-trained model, and training logs to promote open science in speech foundation models.",
                "authors": "Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe",
                "citations": 12
            },
            {
                "title": "Unleashing the Power of Multi-Task Learning: A Comprehensive Survey Spanning Traditional, Deep, and Pretrained Foundation Model Eras",
                "abstract": "MTL is a learning paradigm that effectively leverages both task-specific and shared information to address multiple related tasks simultaneously. In contrast to STL, MTL offers a suite of benefits that enhance both the training process and the inference efficiency. MTL's key advantages encompass streamlined model architecture, performance enhancement, and cross-domain generalizability. Over the past twenty years, MTL has become widely recognized as a flexible and effective approach in various fields, including CV, NLP, recommendation systems, disease prognosis and diagnosis, and robotics. This survey provides a comprehensive overview of the evolution of MTL, encompassing the technical aspects of cutting-edge methods from traditional approaches to deep learning and the latest trend of pretrained foundation models. Our survey methodically categorizes MTL techniques into five key areas: regularization, relationship learning, feature propagation, optimization, and pre-training. This categorization not only chronologically outlines the development of MTL but also dives into various specialized strategies within each category. Furthermore, the survey reveals how the MTL evolves from handling a fixed set of tasks to embracing a more flexible approach free from task or modality constraints. It explores the concepts of task-promptable and -agnostic training, along with the capacity for ZSL, which unleashes the untapped potential of this historically coveted learning paradigm. Overall, we hope this survey provides the research community with a comprehensive overview of the advancements in MTL from its inception in 1997 to the present in 2023. We address present challenges and look ahead to future possibilities, shedding light on the opportunities and potential avenues for MTL research in a broad manner. This project is publicly available at https://github.com/junfish/Awesome-Multitask-Learning.",
                "authors": "Jun Yu, Yutong Dai, Xiaokang Liu, Jing Huang, Yishan Shen, Ke Zhang, Rong Zhou, Eashan Adhikarla, Wenxuan Ye, Yixin Liu, Zhaoming Kong, Kai Zhang, Yilong Yin, V. Namboodiri, Brian D. Davison, Jason H. Moore, Yong Chen",
                "citations": 9
            },
            {
                "title": "AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception",
                "abstract": "The highly abstract nature of image aesthetics perception (IAP) poses significant challenge for current multimodal large language models (MLLMs). The lack of human-annotated multi-modality aesthetic data further exacerbates this dilemma, resulting in MLLMs falling short of aesthetics perception capabilities. To address the above challenge, we first introduce a comprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT) dataset, which serves as the footstone for building multi-modality aesthetics foundation models. Specifically, to align MLLMs with human aesthetics perception, we construct a corpus-rich aesthetic critique database with 21,904 diverse-sourced images and 88K human natural language feedbacks, which are collected via progressive questions, ranging from coarse-grained aesthetic grades to fine-grained aesthetic descriptions. To ensure that MLLMs can handle diverse queries, we further prompt GPT to refine the aesthetic critiques and assemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT, which consists of 409K multi-typed instructions to activate stronger aesthetic capabilities. Based on the AesMMIT database, we fine-tune the open-sourced general foundation models, achieving multi-modality Aesthetic Expert models, dubbed AesExpert. Extensive experiments demonstrate that the proposed AesExpert models deliver significantly better aesthetic perception performances than the state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision. Project homepage: https://yipoh.github.io/aes-expert/.",
                "authors": "Yipo Huang, Xiangfei Sheng, Zhichao Yang, Quan Yuan, Zhichao Duan, Pengfei Chen, Leida Li, Weisi Lin, Guangming Shi",
                "citations": 9
            },
            {
                "title": "Endora: Video Generation Models as Endoscopy Simulators",
                "abstract": "Generative models hold promise for revolutionizing medical education, robot-assisted surgery, and data augmentation for machine learning. Despite progress in generating 2D medical images, the complex domain of clinical video generation has largely remained untapped.This paper introduces \\model, an innovative approach to generate medical videos that simulate clinical endoscopy scenes. We present a novel generative model design that integrates a meticulously crafted spatial-temporal video transformer with advanced 2D vision foundation model priors, explicitly modeling spatial-temporal dynamics during video generation. We also pioneer the first public benchmark for endoscopy simulation with video generation models, adapting existing state-of-the-art methods for this endeavor.Endora demonstrates exceptional visual quality in generating endoscopy videos, surpassing state-of-the-art methods in extensive testing. Moreover, we explore how this endoscopy simulator can empower downstream video analysis tasks and even generate 3D medical scenes with multi-view consistency. In a nutshell, Endora marks a notable breakthrough in the deployment of generative AI for clinical endoscopy research, setting a substantial stage for further advances in medical content generation. For more details, please visit our project page: https://endora-medvidgen.github.io/.",
                "authors": "Chenxin Li, Hengyu Liu, Yifan Liu, Brandon Y. Feng, Wuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, Yixuan Yuan",
                "citations": 23
            },
            {
                "title": "Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology",
                "abstract": "Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the ex-isting MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foun-dation model. This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional Transformer (R2T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R2T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R2T can introduce more signifi-cant performance improvements to various MIL models; 3) R2T-MIL, as an R2T-enhanced AB-MIL, outperforms other latest methods by a large margin. The code is available at: https://github.com/DearCaat/RRT-MIL.",
                "authors": "Wenhao Tang, Fengtao Zhou, Shengyue Huang, Xiang Zhu, Yi Zhang, Bo Liu",
                "citations": 12
            },
            {
                "title": "EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging",
                "abstract": "Artificial intelligence (AI) is vital in ophthalmology, tackling tasks like diagnosis, classification, and visual question answering (VQA). However, existing AI models in this domain often require extensive annotation and are task-specific, limiting their clinical utility. While recent developments have brought about foundation models for ophthalmology, they are limited by the need to train separate weights for each imaging modality, preventing a comprehensive representation of multi-modal features. This highlights the need for versatile foundation models capable of handling various tasks and modalities in ophthalmology. To address this gap, we present EyeFound, a multimodal foundation model for ophthalmic images. Unlike existing models, EyeFound learns generalizable representations from unlabeled multimodal retinal images, enabling efficient model adaptation across multiple applications. Trained on 2.78 million images from 227 hospitals across 11 ophthalmic modalities, EyeFound facilitates generalist representations and diverse multimodal downstream tasks, even for detecting challenging rare diseases. It outperforms previous work RETFound in diagnosing eye diseases, predicting systemic disease incidents, and zero-shot multimodal VQA. EyeFound provides a generalizable solution to improve model performance and lessen the annotation burden on experts, facilitating widespread clinical AI applications for retinal imaging.",
                "authors": "Danli Shi, Weiyi Zhang, Xiaolan Chen, Yexin Liu, Jianchen Yang, Siyu Huang, Y. Tham, Yingfeng Zheng, M. He",
                "citations": 8
            },
            {
                "title": "RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation",
                "abstract": "Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1~5 demonstrations, and effectively handles complex, dexterous tasks. We refer to https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.",
                "authors": "Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu",
                "citations": 8
            },
            {
                "title": "Foundation Model Transparency Reports",
                "abstract": "Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g. the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI). Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions. We encourage foundation model developers to regularly publish transparency reports, building upon recommendations from the G7 and the White House.",
                "authors": "Rishi Bommasani, Kevin Klyman, Shayne Longpre, Betty Xiong, Sayash Kapoor, Nestor Maslej, Arvind Narayanan, Percy Liang",
                "citations": 8
            },
            {
                "title": "Rethinking Software Engineering in the Foundation Model Era: From Task-Driven AI Copilots to Goal-Driven AI Pair Programmers",
                "abstract": "The advent of Foundation Models (FMs) and AI-powered copilots has transformed the landscape of software development, offering unprecedented code completion capabilities and enhancing developer productivity. However, the current task-driven nature of these copilots falls short in addressing the broader goals and complexities inherent in software engineering (SE). In this paper, we propose a paradigm shift towards goal-driven AI-powered pair programmers that collaborate with human developers in a more holistic and context-aware manner. We envision AI pair programmers that are goal-driven, human partners, SE-aware, and self-learning. These AI partners engage in iterative, conversation-driven development processes, aligning closely with human goals and facilitating informed decision-making. We discuss the desired attributes of such AI pair programmers and outline key challenges that must be addressed to realize this vision. Ultimately, our work represents a shift from AI-augmented SE to AI-transformed SE by replacing code completion with a collaborative partnership between humans and AI that enhances both productivity and software quality.",
                "authors": "Ahmed E. Hassan, G. Oliva, Dayi Lin, Boyuan Chen, Zhen Ming Jiang",
                "citations": 8
            },
            {
                "title": "ECG-FM: An Open Electrocardiogram Foundation Model",
                "abstract": "The electrocardiogram (ECG) is a ubiquitous diagnostic test. Conventional task-specific ECG analysis models require large numbers of expensive ECG annotations or associated labels to train. Transfer learning techniques have been shown to improve generalization and reduce reliance on labeled data. We present ECG-FM, an open foundation model for ECG analysis, and conduct a comprehensive study performed on a dataset of 1.66 million ECGs sourced from both publicly available and private institutional sources. ECG-FM adopts a transformer-based architecture and is pretrained on 2.5 million samples using ECG-specific augmentations and contrastive learning, as well as a continuous signal masking objective. Our transparent evaluation includes a diverse range of downstream tasks, where we predict ECG interpretation labels, reduced left ventricular ejection fraction, and abnormal cardiac troponin. Affirming ECG-FM's effectiveness as a foundation model, we demonstrate how its command of contextual information results in strong performance, rich pretrained embeddings, and reliable interpretability. Due to a lack of open-weight practices, we highlight how ECG analysis is lagging behind other medical machine learning subfields in terms of foundation model adoption. Our code is available at https://github.com/bowang-lab/ECG-FM/.",
                "authors": "Kaden McKeen, Laura Oliva, Sameer Masood, Augustin Toma, Barry Rubin, Bo Wang",
                "citations": 10
            },
            {
                "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                "abstract": null,
                "authors": "Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, Hanwen Gu",
                "citations": 21
            },
            {
                "title": "Augmented non-hallucinating large language models as medical information curators",
                "abstract": null,
                "authors": "S. Gilbert, J. N. Kather, Aidan Hogan",
                "citations": 19
            },
            {
                "title": "Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models",
                "abstract": "Seeing clearly with high resolution is a foundation of Large Multimodal Models (LMMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the most important tokens accounting for the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a `less is more' pattern, where \\textit{utilizing fewer but more informative local image tokens leads to improved performance}. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed LMM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.",
                "authors": "Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin",
                "citations": 21
            },
            {
                "title": "Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video Diffusion Models",
                "abstract": "The availability of large-scale multimodal datasets and advancements in diffusion models have significantly accelerated progress in 4D content generation. Most prior approaches rely on multiple image or video diffusion models, utilizing score distillation sampling for optimization or generating pseudo novel views for direct supervision. However, these methods are hindered by slow optimization speeds and multi-view inconsistency issues. Spatial and temporal consistency in 4D geometry has been extensively explored respectively in 3D-aware diffusion models and traditional monocular video diffusion models. Building on this foundation, we propose a strategy to migrate the temporal consistency in video diffusion models to the spatial-temporal consistency required for 4D generation. Specifically, we present a novel framework, \\textbf{Diffusion4D}, for efficient and scalable 4D content generation. Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets. To control the dynamic strength of these assets, we introduce a 3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel motion magnitude reconstruction loss and 3D-aware classifier-free guidance to refine the learning and generation of motion dynamics. After obtaining orbital views of the 4D asset, we perform explicit 4D construction with Gaussian splatting in a coarse-to-fine manner. The synthesized multi-view consistent 4D image set enables us to swiftly generate high-fidelity and diverse 4D assets within just several minutes. Extensive experiments demonstrate that our method surpasses prior state-of-the-art techniques in terms of generation efficiency and 4D geometry consistency across various prompt modalities.",
                "authors": "Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos N. Plataniotis, Yao Zhao, Yunchao Wei",
                "citations": 19
            },
            {
                "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction",
                "abstract": "In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and closed-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 91.1% attack success rate on OpenAI GPT-4 chatbot.",
                "authors": "Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen",
                "citations": 22
            },
            {
                "title": "Towards Vision-Language Geo-Foundation Model: A Survey",
                "abstract": "Vision-Language Foundation Models (VLFMs) have made remarkable progress on various multimodal tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding. However, most methods rely on training with general image datasets, and the lack of geospatial data leads to poor performance on earth observation. Numerous geospatial image-text pair datasets and VLFMs fine-tuned on them have been proposed recently. These new approaches aim to leverage large-scale, multimodal geospatial data to build versatile intelligent models with diverse geo-perceptive capabilities, which we refer to as Vision-Language Geo-Foundation Models (VLGFMs). This paper thoroughly reviews VLGFMs, summarizing and analyzing recent developments in the field. In particular, we introduce the background and motivation behind the rise of VLGFMs, highlighting their unique research significance. Then, we systematically summarize the core technologies employed in VLGFMs, including data construction, model architectures, and applications of various multimodal geospatial tasks. Finally, we conclude with insights, issues, and discussions regarding future research directions. To the best of our knowledge, this is the first comprehensive literature review of VLGFMs. We keep tracing related works at https://github.com/zytx121/Awesome-VLGFM.",
                "authors": "Yue Zhou, Litong Feng, Yiping Ke, Xue Jiang, Junchi Yan, Xue Yang, Wayne Zhang",
                "citations": 7
            },
            {
                "title": "Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation",
                "abstract": "The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world clinics. Frontier general-domain models such as GPT-4V still have significant performance gaps in multimodal biomedical applications. More importantly, less-acknowledged pragmatic issues, including accessibility, model cost, and tedious manual evaluation make it hard for clinicians to use state-of-the-art large models directly on private patient data. Here, we explore training open-source small multimodal models (SMMs) to bridge competency gaps for unmet clinical needs in radiology. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space, as exemplified by LLaVA-Med. For training, we assemble a large dataset of over 697 thousand radiology image-text pairs. For evaluation, we propose CheXprompt, a GPT-4-based metric for factuality evaluation, and demonstrate its parity with expert evaluation. For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training. The resulting LlaVA-Rad (7B) model attains state-of-the-art results on standard radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B). The inference of LlaVA-Rad is fast and can be performed on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications.",
                "authors": "Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, H. Awadalla, Julia Gong, Houdong Hu, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Yu Gu, Cliff Wong, Mu-Hsin Wei, Tristan Naumann, Muhao Chen, M. Lungren, S. Yeung-Levy, Curtis P. Langlotz, Sheng Wang, Hoifung Poon",
                "citations": 7
            },
            {
                "title": "Nicheformer: a foundation model for single-cell and spatial omics",
                "abstract": "Tissue makeup relies fundamentally on the cellular microenvironment. Spatial single-cell genomics allows probing the underlying cellular interactions in an unbiased, scalable fashion. To learn a unified cell representation that accounts for local dependencies in the cellular microenvironment, we propose Nicheformer, a transformer-based foundation model that combines human and mouse dissociated single-cell and targeted spatial transcriptomics data. Pretrained on over 57 million dissociated and 53 million spatially resolved cells across 73 tissues on cellular reconstruction, the model is fine-tuned on spatial tasks for spatial omics data to decode spatially resolved cellular information. Nicheformer excels in linear-probing and fine-tuning scenarios for a novel set of downstream tasks, in particular spatial composition prediction and spatial label prediction. We further show that existing foundation models trained on dissociated single-cell data alone are not capable of recapitulating the spatial complexity of cells in their microenvironments, indicating that multiscale models are required to understand complex local dependencies at scale. Nicheformer enables the prediction of the spatial context of dissociated cells, allowing the transfer of rich spatial information to scRNA-seq datasets. Overall, Nicheformer sets the stage for the next generation of machine-learning models in spatial single-cell analysis. Extended Abstract Tissue makeup and the corresponding orchestration of vital biological activities, ranging from development and differentiation to immune response and regeneration, rely fundamentally on the cellular microenvironment and the interactions between cells. Spatial single-cell genomics allows probing such interactions in an unbiased and, increasingly, scalable fashion. To learn a unified cell representation that accounts for local dependencies in the cellular microenvironment and the underlying cell interactions, we propose to generalize recent foundation modeling approaches for disassociated single-cell transcriptomics to the spatial omics setting. Our model, Nicheformer, is a transformer-based foundation model that combines human and mouse dissociated single-cell and targeted spatial transcriptomics data to learn a cellular representation useful for a large variety of downstream tasks. Nicheformer is pretrained on over 57 million dissociated and 53 million spatially resolved cells across 73 tissues from both human and mouse. Subsequently, the model is fine-tuned on spatial tasks for spatial omics data to decode spatially resolved cellular information. We demonstrate the usefulness of Nicheformer in both linear-probing as well as fine-tuning scenarios on a novel set of spatially-relevant downstream tasks such as spatial density prediction or niche and region label prediction. In particular, we show that Nicheformer enables the prediction of the spatial context of dissociated cells, allowing the transfer of rich spatial information to scRNA-seq datasets. We define a series of novel spatial prediction problems and observe consistent top performance of Nicheformer, demonstrating the advantage of the improved model capacity of the underlying transformer. Additionally, we benchmarked Nicheformer in these tasks against scGPT1, Geneformer2, scVI3 and PCA and show that the Nicheformer architecture excels in these tasks. Altogether, our large-scale resource of more than 110 million cells in a partial spatial context, together with the set of novel spatial learning tasks and the Nicheformer model itself, will pave the way for the next generation of machine-learning models for spatial single-cell analysis.",
                "authors": "A. C. Schaar, Alejandro Tejada-Lapuerta, G. Palla, Robert M Gutgesell, Lennard Halle, M. Minaeva, L. Vornholz, L. Dony, Francesca Drummer, Mojtaba Bahrami, F. Theis",
                "citations": 7
            },
            {
                "title": "Playing to Vision Foundation Model's Strengths in Stereo Matching",
                "abstract": "Stereo matching has become a key technique for 3D environment perception in intelligent vehicles. For a considerable time, convolutional neural networks (CNNs) have remained the mainstream choice for feature extraction in this domain. Nonetheless, there is a growing consensus that the existing paradigm should evolve towards vision foundation models (VFM), particularly those developed based on vision Transformers (ViTs) and pre-trained through self-supervision on extensive, unlabeled datasets. While VFMs are adept at extracting informative, general-purpose visual features, specifically for dense prediction tasks, their performance often lacks in geometric vision tasks. This study serves as the first exploration of a viable approach for adapting VFMs to stereo matching. Our ViT adapter, referred to as ViTAS, is constructed upon three types of modules: spatial differentiation, patch attention fusion, and cross-attention. The first module initializes feature pyramids, while the latter two aggregate stereo and multi-scale contextual information into fine-grained features, respectively. ViTAStereo, which combines ViTAS with cost volume-based stereo matching back-end processes, achieves the top rank on the KITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by approximately 7.9% in terms of the percentage of error pixels, with a tolerance of 3 pixels. Additional experiments across diverse scenarios further demonstrate its superior generalizability compared to all other state-of-the-art approaches. We believe this new paradigm will pave the way for the next generation of stereo matching networks.",
                "authors": "Chuangwei Liu, Qijun Chen, Rui Fan",
                "citations": 7
            },
            {
                "title": "Federated Adaptation for Foundation Model-based Recommendations",
                "abstract": "With the recent success of large language models, particularly foundation models with generalization abilities, applying foundation models for recommendations becomes a new paradigm to improve existing recommendation systems. It becomes a new open challenge to enable the foundation model to capture user preference changes in a timely manner with reasonable communication and computation costs while preserving privacy. This paper proposes a novel federated adaptation mechanism to enhance the foundation model-based recommendation system in a privacy-preserving manner. Specifically, each client will learn a lightweight personalized adapter using its private data. The adapter then collaborates with pre-trained foundation models to provide recommendation service efficiently with fine-grained manners. Importantly, users' private behavioral data remains secure as it is not shared with the server. This data localization-based privacy preservation is embodied via the federated learning framework. The model can ensure that shared knowledge is incorporated into all adapters while simultaneously preserving each user's personal preferences. Experimental results on four benchmark datasets demonstrate our method's superior performance. The code is available.",
                "authors": "Chunxu Zhang, Guodong Long, Hongkuan Guo, Xiao Fang, Yang Song, Zhaojie Liu, Guorui Zhou, Zijian Zhang, Yang Liu, Bo Yang",
                "citations": 6
            },
            {
                "title": "UPetu: A Unified Parameter-Efficient Fine-Tuning Framework for Remote Sensing Foundation Model",
                "abstract": "Recent advancements in remote sensing foundation models have unveiled their tremendous potential in addressing Earth observation tasks. Presently, when large-scale foundation models are transferred to downstream tasks, the prevalent approach is to adopt the full-tuning strategy, resulting in significant increases in storage demands and computational costs. Although the introduction of parameter-efficient fine-tuning (PEFT) has mitigated this issue to some extent, mainstream PEFT methods are primarily designed for classification tasks and often prove insufficient to meet the demands of dense prediction tasks. To overcome the aforementioned limitations, we propose a unified PEFT framework UPetu, encompassing two essential and complementary modules: the efficient quantization adapter module (EQAM) and the context-aware prompt module (CAPM). EQAM is specifically designed to enhance the correlation between fine-grained feature information and task-specific knowledge through the introduction of quantization linear (Q-Linear) layers and nonlinear activation functions. In addition, CAPM is introduced to acquire rich contextual features by incorporating trainable prompts into multiscale features. The synergistic integration of both the modules enhances the representation learning capability and generalization transferability of the foundation model. Extensive experiments on three remote sensing scene classification datasets demonstrate the superiority of UPetu over other fine-tuning methods. With the update of only 0.73% of ConvNeXt-B parameters, our UPetu achieves superior performance compared with full-tuning on the UCM-55, AID-28, and AID-55 datasets. Furthermore, experiments conducted on semantic segmentation and change detection tasks provide additional evidence of the effectiveness and generalization capabilities of the proposed UPetu.",
                "authors": "Zhe Dong, Yanfeng Gu, Tianzhu Liu",
                "citations": 7
            },
            {
                "title": "CosmicMan: A Text-to-Image Foundation Model for Humans",
                "abstract": "We present CosmicMan, a text-to-image foundation model specialized for generating high-fidelity human images. Unlike current general-purpose foundation models that are stuck in the dilemma of inferior quality and text-image misalignment for humans, CosmicMan enables generating photo-realistic human images with meticulous appearance, reasonable structure, and precise text-image alignment with detailed dense descriptions. At the heart of CosmicMan's success are the new reflections and perspectives on data and models: (1) We found that data quality and a scalable data production flow are essential for the final results from trained models. Hence, we propose a new data production paradigm, Annotate Anyone, which serves as a perpetual data flywheel to produce high-quality data with accurate yet cost-effective annotations over time. Based on this, we constructed a large-scale dataset, CosmicMan-HQ 1.0, with 6 Million high-quality real-world human images in a mean resolution of $1488 \\times 1255$, and attached with precise text annotations deriving from 115 Million attributes in diverse granularities. (2) We argue that a text-to-image foundation model specialized for humans must be pragmatic – easy to integrate into down-streaming tasks while effective in producing high-quality human images. Hence, we propose to model the relationship between dense text descriptions and image pixels in a decomposed manner, and present Decomposed-Attention-Refocusing (Daring) training framework. It seamlessly decomposes the cross-attention features in existing text-to-image diffusion model, and enforces attention refocusing without adding extra modules. Through Daring, we show that explicitly discretizing continuous text space into several basic groups that align with human body structure is the key to tackling the misalignment problem in a breeze. Project page: https://cosmicman-cvpr2024.github.io/.",
                "authors": "Shikai Li, Jianglin Fu, Kaiyuan Liu, Wentao Wang, Kwan-Yee Lin, Wayne Wu",
                "citations": 7
            },
            {
                "title": "UniGraph: Learning a Unified Cross-Domain Foundation Model for Text-Attributed Graphs",
                "abstract": "Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we recognize text as an effective unifying medium and employ Text-Attributed Graphs (TAGs) to leverage this potential. We present our UniGraph framework, designed to learn a foundation model for TAGs, which is capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages textual features for unifying node representations, even for graphs such as molecular graphs that do not naturally have textual features. We propose a novel cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks. Additionally, we propose the first pre-training algorithm specifically designed for large-scale self-supervised learning on TAGs, based on Masked Graph Modeling. We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets.",
                "authors": "Yufei He, Yuan Sui, Xiaoxin He, Bryan Hooi",
                "citations": 6
            },
            {
                "title": "Agent Design Pattern Catalogue: A Collection of Architectural Patterns for Foundation Model based Agents",
                "abstract": "Foundation model-enabled generative artificial intelligence facilitates the development and implementation of agents, which can leverage distinguished reasoning and language processing capabilities to takes a proactive, autonomous role to pursue users' goals. Nevertheless, there is a lack of systematic knowledge to guide practitioners in designing the agents considering challenges of goal-seeking (including generating instrumental goals and plans), such as hallucinations inherent in foundation models, explainability of reasoning process, complex accountability, etc. To address this issue, we have performed a systematic literature review to understand the state-of-the-art foundation model-based agents and the broader ecosystem. In this paper, we present a pattern catalogue consisting of 18 architectural patterns with analyses of the context, forces, and trade-offs as the outcomes from the previous literature review. We propose a decision model for selecting the patterns. The proposed catalogue can provide holistic guidance for the effective use of patterns, and support the architecture design of foundation model-based agents by facilitating goal-seeking and plan generation.",
                "authors": "Yue Liu, Sin Kit Lo, Qinghua Lu, Liming Zhu, Dehai Zhao, Xiwei Xu, Stefan Harrer, Jon Whittle",
                "citations": 7
            },
            {
                "title": "OpenMedLM: prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models",
                "abstract": null,
                "authors": "Jenish Maharjan, A. Garikipati, N. Singh, Leo Cyrus, Mayank Sharma, M. Ciobanu, G. Barnes, R. Thapa, Q. Mao, R. Das",
                "citations": 13
            },
            {
                "title": "Theoretical Foundations of Deep Selective State-Space Models",
                "abstract": "Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants.",
                "authors": "Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, C. Salvi, Terry Lyons",
                "citations": 13
            },
            {
                "title": "ChemDFM: A Large Language Foundation Model for Chemistry",
                "abstract": "Artificial intelligence (AI) has played an increasingly important role in chemical research. However, most models currently used in chemistry are specialist models that require training and tuning for specific tasks. A more generic and efficient solution would be an AI model that could address many tasks and support free-form dialogue in the broad field of chemistry. In its utmost form, such a generalist AI chemist could be referred to as Chemical General Intelligence. Large language models (LLMs) have recently logged tremendous success in the general domain of natural language processing, showing emerging task generalization and free-form dialogue capabilities. However, domain knowledge of chemistry is largely missing when training general-domain LLMs. The lack of such knowledge greatly hinders the performance of generalist LLMs in the field of chemistry. To this end, we develop ChemDFM, a pioneering LLM for chemistry trained on 34B tokens from chemical literature and textbooks, and fine-tuned using 2.7M instructions. As a result, it can understand and reason with chemical knowledge in free-form dialogue. Quantitative evaluations show that ChemDFM significantly surpasses most representative open-source LLMs. It outperforms GPT-4 on a great portion of chemical tasks, despite the substantial size difference. We have open-sourced the inference codes, evaluation datasets, and model weights of ChemDFM on Huggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B).",
                "authors": "Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, Xin Chen, Kai Yu",
                "citations": 11
            },
            {
                "title": "A vision-language foundation model for the generation of realistic chest X-ray images.",
                "abstract": null,
                "authors": "Christian Bluethgen, Pierre J. Chambon, Jean-Benoit Delbrouck, Rogier van der Sluijs, Malgorzata Polacin, J. M. Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis P. Langlotz, Akshay S. Chaudhari",
                "citations": 10
            },
            {
                "title": "Merlin: A Vision Language Foundation Model for 3D Computed Tomography",
                "abstract": "Over 85 million computed tomography (CT) scans are performed annually in the US, of which approximately one quarter focus on the abdomen. Given the current shortage of both general and specialized radiologists, there is a large impetus to use artificial intelligence to alleviate the burden of interpreting these complex imaging studies while simultaneously using the images to extract novel physiological insights. Prior state-of-the-art approaches for automated medical image interpretation leverage vision language models (VLMs) that utilize both the image and the corresponding textual radiology reports. However, current medical VLMs are generally limited to 2D images and short reports. To overcome these shortcomings for abdominal CT interpretation, we introduce Merlin - a 3D VLM that leverages both structured electronic health records (EHR) and unstructured radiology reports for pretraining without requiring additional manual annotations. We train Merlin using a high-quality clinical dataset of paired CT scans (6+ million images from 15,331 CTs), EHR diagnosis codes (1.8+ million codes), and radiology reports (6+ million tokens) for training. We comprehensively evaluate Merlin on 6 task types and 752 individual tasks. The non-adapted (off-the-shelf) tasks include zero-shot findings classification (31 findings), phenotype classification (692 phenotypes), and zero-shot cross-modal retrieval (image to findings and image to impressions), while model adapted tasks include 5-year chronic disease prediction (6 diseases), radiology report generation, and 3D semantic segmentation (20 organs). We perform internal validation on a test set of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public CT datasets (VerSe, TotalSegmentator). Beyond these clinically-relevant evaluations, we assess the efficacy of various network architectures and training strategies to depict that Merlin has favorable performance to existing task-specific baselines. We derive data scaling laws to empirically assess training data needs for requisite downstream task performance. Furthermore, unlike conventional VLMs that require hundreds of GPUs for training, we perform all training on a single GPU. This computationally efficient design can help democratize foundation model training, especially for health systems with compute constraints. We plan to release our trained models, code, and dataset, pending manual removal of all protected health information.",
                "authors": "Louis Blankemeier, Joseph Paul Cohen, Ashwin Kumar, Dave Van Veen, Syed Jamal Safdar Gardezi, Magdalini Paschali, Zhihong Chen, Jean-Benoit Delbrouck, E. Reis, C. Truyts, Christian Bluethgen, Malte E. K. Jensen, Sophie Ostmeier, Maya Varma, Jeya Maria Jose Valanarasu, Zhongnan Fang, Zepeng Huo, Zaid Nabulsi, Diego Ardila, Wei-Hung Weng, Edson Amaro Junior, Neera Ahuja, J. Fries, Nigam H. Shah, Andrew Johnston, Robert D. Boutin, Andrew Wentland, C. Langlotz, Jason Hom, S. Gatidis, Akshay S. Chaudhari",
                "citations": 11
            },
            {
                "title": "PDEformer: Towards a Foundation Model for One-Dimensional Partial Differential Equations",
                "abstract": "This paper introduces PDEformer, a neural solver for partial differential equations (PDEs) capable of simultaneously addressing various types of PDEs. We propose to represent the PDE in the form of a computational graph, facilitating the seamless integration of both symbolic and numerical information inherent in a PDE. A graph Transformer and an implicit neural representation (INR) are employed to generate mesh-free predicted solutions. Following pretraining on data exhibiting a certain level of diversity, our model achieves zero-shot accuracies on benchmark datasets that is comparable to those of specifically trained expert models. Additionally, PDEformer demonstrates promising results in the inverse problem of PDE coefficient recovery.",
                "authors": "Zhanhong Ye, Xiang Huang, Leheng Chen, Hongsheng Liu, Zidong Wang, Bin Dong",
                "citations": 11
            },
            {
                "title": "The New Agronomists: Language Models are Experts in Crop Management",
                "abstract": "Crop management plays a crucial role in determining crop yield, economic profitability, and environmental sustainability. Despite the availability of management guidelines, optimizing these practices remains a complex and multifaceted challenge. In response, previous studies have explored using reinforcement learning with crop simulators, typically employing simple neural-network-based reinforcement learning (RL) agents. Building on this foundation, this paper introduces a more advanced intelligent crop management system. This system uniquely combines RL, a language model (LM), and crop simulations facilitated by the Decision Support System for Agrotechnology Transfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train management policies that process numerous state variables from the simulator as observations. A novel aspect of our approach is the conversion of these state variables into more informative language, facilitating the language model’s capacity to understand states and explore optimal management practices. The empirical results reveal that the LM exhibits superior learning capabilities. Through simulation experiments with maize crops in Florida (US) and Zaragoza (Spain), the LM not only achieves state-of-the-art performance under various evaluation metrics but also demonstrates a remarkable improvement of over 49% in economic profit, coupled with reduced environmental impact when compared to baseline methods. Our code is available at https://github.com/jingwu6/LM_AG.",
                "authors": "Jing Wu, Zhixin Lai, Suiyao Chen, Ran Tao, Pan Zhao, N. Hovakimyan",
                "citations": 16
            },
            {
                "title": "DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features",
                "abstract": "We propose DistillNeRF, a self-supervised learning framework addressing the challenge of understanding 3D environments from limited 2D observations in outdoor autonomous driving scenes. Our method is a generalizable feedforward model that predicts a rich neural scene representation from sparse, single-frame multi-view camera inputs with limited view overlap, and is trained self-supervised with differentiable rendering to reconstruct RGB, depth, or feature images. Our first insight is to exploit per-scene optimized Neural Radiance Fields (NeRFs) by generating dense depth and virtual camera targets from them, which helps our model to learn enhanced 3D geometry from sparse non-overlapping image inputs. Second, to learn a semantically rich 3D representation, we propose distilling features from pre-trained 2D foundation models, such as CLIP or DINOv2, thereby enabling various downstream tasks without the need for costly 3D human annotations. To leverage these two insights, we introduce a novel model architecture with a two-stage lift-splat-shoot encoder and a parameterized sparse hierarchical voxel representation. Experimental results on the NuScenes and Waymo NOTR datasets demonstrate that DistillNeRF significantly outperforms existing comparable state-of-the-art self-supervised methods for scene reconstruction, novel view synthesis, and depth estimation; and it allows for competitive zero-shot 3D semantic occupancy prediction, as well as open-world scene understanding through distilled foundation model features. Demos and code will be available at https://distillnerf.github.io/.",
                "authors": "Letian Wang, Seung Wook Kim, Jiawei Yang, Cunjun Yu, B. Ivanovic, Steven L. Waslander, Yue Wang, Sanja Fidler, Marco Pavone, Peter Karkus",
                "citations": 4
            },
            {
                "title": "AnyGraph: Graph Foundation Model in the Wild",
                "abstract": "The growing ubiquity of relational data structured as graphs has underscored the need for graph learning models with exceptional generalization capabilities. However, current approaches often struggle to effectively extract generalizable insights, frequently requiring extensive fine-tuning and limiting their versatility. Graph foundation models offer a transformative solution, with the potential to learn robust, generalizable representations from graph data. This enables more effective and adaptable applications across a wide spectrum of tasks and domains. In this work, we investigate a unified graph model, AnyGraph, designed to handle key challenges: i) Structure Heterogenity. Addressing distribution shift in graph structural information; ii) Feature Heterogenity. Handling diverse feature representation spaces across graph datasets; iii) Fast Adaptation. Efficiently adapting the model to new graph domains; iv) Scaling Law Emergence. Enabling the model to exhibit scaling law behavior, where its performance scales favorably with the amount of data and parameter sizes. To tackle these critical challenges, we build the AnyGraph upon a Graph Mixture-of-Experts (MoE) architecture. This approach empowers the model to effectively manage both the in-domain and cross-domain distribution shift concerning structure-level and feature-level heterogeneity. Furthermore, a lightweight graph expert routing mechanism is proposed to facilitate AnyGraph's fast adaptability to new data and domains. Our extensive experiments on diverse 38 graph datasets have demonstrated the strong zero-shot learning performance of AnyGraph across diverse graph domains with significant distribution shift. Furthermore, we have validated the model's fast adaptation ability and scaling law emergence, showcasing its versatility.",
                "authors": "Lianghao Xia, Chao Huang",
                "citations": 4
            },
            {
                "title": "Arc2Face: A Foundation Model for ID-Consistent Human Faces",
                "abstract": null,
                "authors": "Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, Jiankang Deng, Bernhard Kainz, S. Zafeiriou",
                "citations": 9
            },
            {
                "title": "General surgery vision transformer: A video pre-trained foundation model for general surgery",
                "abstract": "The absence of openly accessible data and specialized foundation models is a major barrier for computational research in surgery. Toward this, (i) we open-source the largest dataset of general surgery videos to-date, consisting of 680 hours of surgical videos, including data from robotic and laparoscopic techniques across 28 procedures; (ii) we propose a technique for video pre-training a general surgery vision transformer (GSViT) on surgical videos based on forward video prediction that can run in real-time for surgical applications, toward which we open-source the code and weights of GSViT; (iii) we also release code and weights for procedure-specific fine-tuned versions of GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the Cholec80 phase annotation task, displaying improved performance over state-of-the-art single frame predictors.",
                "authors": "Samuel Schmidgall, Ji Woong Kim, Jeffery Jopling, Axel Krieger",
                "citations": 4
            },
            {
                "title": "Neural Plasticity-Inspired Multimodal Foundation Model for Earth Observation",
                "abstract": "The development of foundation models has revolutionized our ability to interpret the Earth's surface using satellite observational data. Traditional models have been siloed, tailored to specific sensors or data types like optical, radar, and hyperspectral, each with its own unique characteristics. This specialization hinders the potential for a holistic analysis that could benefit from the combined strengths of these diverse data sources. Our novel approach introduces the Dynamic One-For-All (DOFA) model, leveraging the concept of neural plasticity in brain science to integrate various data modalities into a single framework adaptively. This dynamic hypernetwork, adjusting to different wavelengths, enables a single versatile Transformer jointly trained on data from five sensors to excel across 12 distinct Earth observation tasks, including sensors never seen during pretraining. DOFA's innovative design offers a promising leap towards more accurate, efficient, and unified Earth observation analysis, showcasing remarkable adaptability and performance in harnessing the potential of multimodal Earth observation data.",
                "authors": "Zhitong Xiong, Yi Wang, Fahong Zhang, Adam J. Stewart, Joelle Hanna, Damian Borth, Ioannis Papoutsis, B. L. Saux, G. Camps-Valls, Xiao Xiang Zhu",
                "citations": 5
            },
            {
                "title": "Speech Foundation Model Ensembles for the Controlled Singing Voice Deepfake Detection (CTRSVDD) Challenge 2024",
                "abstract": "This work details our approach to achieving a leading system with a 1.79% pooled equal error rate (EER) on the evaluation set of the Controlled Singing Voice Deepfake Detection (CtrSVDD). The rapid advancement of generative AI models presents significant challenges for detecting AI-generated deepfake singing voices, attracting increased research attention. The Singing Voice Deepfake Detection (SVDD) Challenge 2024 aims to address this complex task. In this work, we explore the ensemble methods, utilizing speech foundation models to develop robust singing voice anti-spoofing systems. We also introduce a novel Squeeze-and-Excitation Aggregation (SEA) method, which efficiently and effectively integrates representation features from the speech foundation models, surpassing the performance of our other individual systems. Evaluation results confirm the efficacy of our approach in detecting deepfake singing voices. The codes can be accessed at https://github.com/Anmol2059/SVDD2024.",
                "authors": "Anmol Guragain, Tianchi Liu, Zihan Pan, Hardik B. Sailor, Qiongqiong Wang",
                "citations": 4
            },
            {
                "title": "ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability",
                "abstract": "Earth system predictability is challenged by the complexity of environmental dynamics and the multitude of variables involved. Current AI foundation models, although advanced by leveraging large and heterogeneous data, are often constrained by their size and data integration, limiting their effectiveness in addressing the full range of Earth system prediction challenges. To overcome these limitations, we introduce the Oak Ridge Base Foundation Model for Earth System Predictability (ORBIT), an advanced vision transformer model that scales up to 113 billion parameters using a novel hybrid tensor-data orthogonal parallelism technique. As the largest model of its kind, ORBIT surpasses the current climate AI foundation model size by a thousandfold. Performance scaling tests conducted on the Frontier supercomputer have demonstrated that ORBIT achieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scaling efficiency maintained at 41% to 85% across 49,152 AMD GPUs. These breakthroughs establish new advances in AIdriven climate modeling and demonstrate promise to significantly improve the Earth system predictability.",
                "authors": "Xiao Wang, A. Tsaris, Siyan Liu, Jong-Youl Choi, Ming Fan, Wei Zhang, Ju Yin, M. Ashfaq, Dan Lu, Prasanna Balaprakash",
                "citations": 4
            },
            {
                "title": "Online Foundation Model Selection in Robotics",
                "abstract": "Foundation models have recently expanded into robotics after excelling in computer vision and natural language processing. The models are accessible in two ways: open-source or paid, closed-source options. Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives. We call it the model selection problem. Existing supervised-learning methods are impractical due to the high cost of collecting extensive training data from closed-source models. Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets. We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context. The encoder distills vast data distributions into low-dimensional features, i.e., the context, without additional training. The online learning algorithm aims to maximize a composite reward that includes model performance, execution time, and costs based on the context extracted from the data. It results in an improved trade-off between selecting open-source and closed-source models compared to non-contextual methods, as validated by our theoretical analysis. Experiments across language-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open X-Embodiment demonstrate real-world applications of the solution. The results show that the solution significantly improves the task success rate by up to 14%.",
                "authors": "Po-han Li, Öykü Selin Toprak, Aditya Narayanan, U. Topcu, Sandeep P. Chinchali",
                "citations": 4
            },
            {
                "title": "InfMAE: A Foundation Model in the Infrared Modality",
                "abstract": null,
                "authors": "Fangcen Liu, Chenqiang Gao, Yaming Zhang, Junjie Guo, Jinhao Wang, Deyu Meng",
                "citations": 4
            },
            {
                "title": "MiniMol: A Parameter-Efficient Foundation Model for Molecular Learning",
                "abstract": "In biological tasks, data is rarely plentiful as it is generated from hard-to-gather measurements. Therefore, pre-training foundation models on large quantities of available data and then transfer to low-data downstream tasks is a promising direction. However, how to design effective foundation models for molecular learning remains an open question, with existing approaches typically focusing on models with large parameter capacities. In this work, we propose $\\texttt{MiniMol}$, a foundational model for molecular learning with 10 million parameters. $\\texttt{MiniMol}$ is pre-trained on a mix of roughly 3300 sparsely defined graph- and node-level tasks of both quantum and biological nature. The pre-training dataset includes approximately 6 million molecules and 500 million labels. To demonstrate the generalizability of $\\texttt{MiniMol}$ across tasks, we evaluate it on downstream tasks from the Therapeutic Data Commons (TDC) ADMET group showing significant improvements over the prior state-of-the-art foundation model across 17 tasks. $\\texttt{MiniMol}$ will be a public and open-sourced model for future research.",
                "authors": "Kerstin Kläser, Bla.zej Banaszewski, S. Maddrell-Mander, Callum McLean, Luis Müller, Alipanah Parviz, Shenyang Huang, Andrew W. Fitzgibbon",
                "citations": 4
            },
            {
                "title": "GraphAny: A Foundation Model for Node Classification on Any Graph",
                "abstract": "Foundation models that can perform inference on any new task without requiring specific training have revolutionized machine learning in vision and language applications. However, applications involving graph-structured data remain a tough nut for foundation models, due to challenges in the unique feature- and label spaces associated with each graph. Traditional graph ML models such as graph neural networks (GNNs) trained on graphs cannot perform inference on a new graph with feature and label spaces different from the training ones. Furthermore, existing models learn functions specific to the training graph and cannot generalize to new graphs. In this work, we tackle these two challenges with a new foundational architecture for inductive node classification named GraphAny. GraphAny models inference on a new graph as an analytical solution to a LinearGNN, thereby solving the first challenge. To solve the second challenge, we learn attention scores for each node to fuse the predictions of multiple LinearGNNs. Specifically, the attention module is carefully parameterized as a function of the entropy-normalized distance-features between multiple LinearGNNs predictions to ensure generalization to new graphs. Empirically, GraphAny trained on the Wisconsin dataset with only 120 labeled nodes can effectively generalize to 30 new graphs with an average accuracy of 67.26\\% in an inductive manner, surpassing GCN and GAT trained in the supervised regime, as well as other inductive baselines.",
                "authors": "Jianan Zhao, Hesham Mostafa, Mikhail Galkin, Michael M. Bronstein, Zhaocheng Zhu, Jian Tang",
                "citations": 5
            },
            {
                "title": "Prithvi WxC: Foundation Model for Weather and Climate",
                "abstract": "Triggered by the realization that AI emulators can rival the performance of traditional numerical weather prediction models running on HPC systems, there is now an increasing number of large AI models that address use cases such as forecasting, downscaling, or nowcasting. While the parallel developments in the AI literature focus on foundation models -- models that can be effectively tuned to address multiple, different use cases -- the developments on the weather and climate side largely focus on single-use cases with particular emphasis on mid-range forecasting. We close this gap by introducing Prithvi WxC, a 2.3 billion parameter foundation model developed using 160 variables from the Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2). Prithvi WxC employs an encoder-decoder-based architecture, incorporating concepts from various recent transformer models to effectively capture both regional and global dependencies in the input data. The model has been designed to accommodate large token counts to model weather phenomena in different topologies at fine resolutions. Furthermore, it is trained with a mixed objective that combines the paradigms of masked reconstruction with forecasting. We test the model on a set of challenging downstream tasks namely: Autoregressive rollout forecasting, Downscaling, Gravity wave flux parameterization, and Extreme events estimation. The pretrained model with 2.3 billion parameters, along with the associated fine-tuning workflows, has been publicly released as an open-source contribution via Hugging Face.",
                "authors": "J. Schmude, Sujit Roy, Will Trojak, Johannes Jakubik, D. S. Civitarese, Shraddha Singh, Julian Kuehnert, Kumar Ankur, Aman Gupta, C. Phillips, Romeo Kienzler, Daniela Szwarcman, Vishal Gaur, Rajat Shinde, Rohit Lal, Arlindo Da Silva, Jorge Luis Guevara Diaz, Anne Jones, S. Pfreundschuh, Amy Lin, A. Sheshadri, U. Nair, Valentine Anantharaj, Hendrik F. Hamann, Campbell Watson, M. Maskey, Tsengdar J. Lee, Juan Bernabé-Moreno, Rahul Ramachandran",
                "citations": 4
            },
            {
                "title": "HyperSIGMA: Hyperspectral Intelligence Comprehension Foundation Model",
                "abstract": "Foundation models (FMs) are revolutionizing the analysis and understanding of remote sensing (RS) scenes, including aerial RGB, multispectral, and SAR images. However, hyperspectral images (HSIs), which are rich in spectral information, have not seen much application of FMs, with existing methods often restricted to specific tasks and lacking generality. To fill this gap, we introduce HyperSIGMA, a vision transformer-based foundation model for HSI interpretation, scalable to over a billion parameters. To tackle the spectral and spatial redundancy challenges in HSIs, we introduce a novel sparse sampling attention (SSA) mechanism, which effectively promotes the learning of diverse contextual features and serves as the basic block of HyperSIGMA. HyperSIGMA integrates spatial and spectral features using a specially designed spectral enhancement module. In addition, we construct a large-scale hyperspectral dataset, HyperGlobal-450K, for pre-training, which contains about 450K hyperspectral images, significantly surpassing existing datasets in scale. Extensive experiments on various high-level and low-level HSI tasks demonstrate HyperSIGMA's versatility and superior representational capability compared to current state-of-the-art methods. Moreover, HyperSIGMA shows significant advantages in scalability, robustness, cross-modal transferring capability, and real-world applicability.",
                "authors": "Di Wang, Meiqi Hu, Yao Jin, Yuchun Miao, Jiaqi Yang, Yichu Xu, Xiaolei Qin, Jiaqi Ma, Lingyu Sun, Chenxing Li, C. Fu, Hongruixuan Chen, Chengxi Han, Naoto Yokoya, Jing Zhang, Minqiang Xu, Lin Liu, Lefei Zhang, Chen Wu, Bo Du, Dacheng Tao, Liang-Ying Zhang",
                "citations": 5
            },
            {
                "title": "An Interactive Agent Foundation Model",
                "abstract": "The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. Our approach provides a promising avenue for developing generalist, action-taking, multimodal systems.",
                "authors": "Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori, Yusuke Noda, Paul Tang, Ehsan Adeli, S. Lakshmikanth, Kevin Schulman, Arnold Milstein, D. Terzopoulos, Ade Famoti, N. Kuno, A. Llorens, Hoi Vo, Katsushi Ikeuchi, Fei-Fei Li, Jianfeng Gao, Naoki Wake, Qiuyuan Huang",
                "citations": 9
            },
            {
                "title": "Towards A Generalizable Pathology Foundation Model via Unified Knowledge Distillation",
                "abstract": "Foundation models pretrained on large-scale datasets are revolutionizing the field of computational pathology (CPath). The generalization ability of foundation models is crucial for the success in various downstream clinical tasks. However, current foundation models have only been evaluated on a limited type and number of tasks, leaving their generalization ability and overall performance unclear. To address this gap, we established a most comprehensive benchmark to evaluate the performance of off-the-shelf foundation models across six distinct clinical task types, encompassing a total of 39 specific tasks. Our findings reveal that existing foundation models excel at certain task types but struggle to effectively handle the full breadth of clinical tasks. To improve the generalization of pathology foundation models, we propose a unified knowledge distillation framework consisting of both expert and self knowledge distillation, where the former allows the model to learn from the knowledge of multiple expert models, while the latter leverages self-distillation to enable image representation learning via local-global alignment. Based on this framework, a Generalizable Pathology Foundation Model (GPFM) is pretrained on a large-scale dataset consisting of 190 million images from around 86,000 public H&E whole slides across 34 major tissue types. Evaluated on the established benchmark, GPFM achieves an impressive average rank of 1.36, with 29 tasks ranked 1st, while the the second-best model, UNI, attains an average rank of 2.96, with only 4 tasks ranked 1st. The superior generalization of GPFM demonstrates its exceptional modeling capabilities across a wide range of clinical tasks, positioning it as a new cornerstone for feature representation in CPath.",
                "authors": "Jiabo Ma, Zhengrui Guo, Fengtao Zhou, Yihui Wang, Yingxue Xu, Yu Cai, Zhengjie Zhu, Cheng Jin, Yi-Mou Lin, Xinrui Jiang, Anjia Han, Li Liang, R. Chan, Jiguang Wang, Kwang-Ting Cheng, Hao Chen",
                "citations": 3
            },
            {
                "title": "Semantic Satellite Communications Based on Generative Foundation Model",
                "abstract": "Satellite communications can provide massive connections and seamless coverage, but they also face several challenges, such as rain attenuation, long propagation delays, and co-channel interference. To improve transmission efficiency and address severe scenarios, semantic communication has become a popular choice, particularly when equipped with foundation models (FMs). In this study, we introduce an FM-based semantic satellite communication framework, termed FMSAT. This framework leverages FM-based segmentation and reconstruction to significantly reduce bandwidth requirements and accurately recover semantic features under high noise and interference. Considering the high speed of satellites, an adaptive encoder-decoder is proposed to protect important features and avoid frequent retransmissions. Meanwhile, a well-received image can provide a reference for repairing damaged images under sudden attenuation. Since acknowledgment feedback is subject to long propagation delays when retransmission is unavoidable, a novel error detection method is proposed to roughly detect semantic errors at the regenerative satellite. With the proposed detectors at both the satellite and the gateway, the quality of the received images can be ensured. The simulation results demonstrate that the proposed method can significantly reduce bandwidth requirements, adapt to complex satellite scenarios, and protect semantic information with an acceptable transmission delay.",
                "authors": "Peiwen Jiang, Chao-Kai Wen, Xiao Li, Shi Jin, Geoffrey Ye Li",
                "citations": 3
            },
            {
                "title": "NNsight and NDIF: Democratizing Access to Foundation Model Internals",
                "abstract": "The enormous scale of state-of-the-art foundation models has limited their accessibility to scientists, because customized experiments on large models require costly hardware and complex engineering that is impractical for most researchers. To alleviate these problems, we introduce NNsight, an open-source Python package with a simple, flexible API that can express interventions on any PyTorch model by building computation graphs. We also introduce NDIF, a collaborative research platform providing researchers access to foundation-scale LLMs via the NNsight API. Code, documentation, and tutorials are available at https://nnsight.net/ .",
                "authors": "Jaden Fiotto-Kaufman, Alexander R. Loftus, Eric Todd, Jannik Brinkmann, Caden Juang, Koyena Pal, Can Rager, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Michael Ripa, Adam Belfki, Nikhil Prakash, Sumeet Multani, Carla Brodley, Arjun Guha, Jonathan Bell, Byron C. Wallace, David Bau",
                "citations": 3
            },
            {
                "title": "Learning to Adapt Foundation Model DINOv2 for Capsule Endoscopy Diagnosis",
                "abstract": "Foundation models have become prominent in computer vision, achieving notable success in various tasks. However, their effectiveness largely depends on pre-training with extensive datasets. Applying foundation models directly to small datasets of capsule endoscopy images from scratch is challenging. Pre-training on broad, general vision datasets is crucial for successfully fine-tuning our model for specific tasks. In this work, we introduce a simplified approach called Adapt foundation models with a low-rank adaptation (LoRA) technique for easier customization. Our method, inspired by the DINOv2 foundation model, applies low-rank adaptation learning to tailor foundation models for capsule endoscopy diagnosis effectively. Unlike traditional fine-tuning methods, our strategy includes LoRA layers designed to absorb specific surgical domain knowledge. During the training process, we keep the main model (the backbone encoder) fixed and focus on optimizing the LoRA layers and the disease classification component. We tested our method on two publicly available datasets for capsule endoscopy disease classification. The results were impressive, with our model achieving 97.75% accuracy on the Kvasir-Capsule dataset and 98.81% on the Kvasirv2 dataset. Our solution demonstrates that foundation models can be adeptly adapted for capsule endoscopy diagnosis, highlighting that mere reliance on straightforward fine-tuning or pre-trained models from general computer vision tasks is inadequate for such specific applications.",
                "authors": "Bowen Zhang, Ying Chen, Long Bai, Yan Zhao, Yuxiang Sun, Yixuan Yuan, Jian-hua Zhang, Hongliang Ren",
                "citations": 3
            },
            {
                "title": "The Human Cell Atlas from a cell census to a unified foundation model.",
                "abstract": null,
                "authors": "Jennifer E. Rood, Samantha Wynne, Lucia Robson, A. Hupalowska, John Randell, Sarah A. Teichmann, Aviv Regev",
                "citations": 3
            },
            {
                "title": "Foundation Model-Based Spectral–Spatial Transformer for Hyperspectral Image Classification",
                "abstract": "Recently, deep learning models have dominated hyperspectral image (HSI) classification. Nowadays, deep learning is undergoing a paradigm shift with the rise of transformer-based foundation models. In this study, the potential of transformer-based foundation models, including the vision foundation model (VFM) and language foundation model (LFM), for HSI classification are investigated. First, to improve the performance of traditional HSI classification tasks, a spectral-spatial VFM-based transformer (SS-VFMT) is proposed, which inserts spectral-spatial information into the pretrained foundation transformer. Specifically, a given pretrained transformer receives HSI patch tokens for long-range feature extraction benefiting from the prelearned weights. Meanwhile, two enhancement modules, i.e., spatial and spectral enhancement modules (SpaEMs $\\backslash $ SpeEMs), utilize spectral and spatial information for steering the behavior of the transformer. Besides, an additional patch relationship distillation strategy is designed for SS-VFMT to exploit the pretrained knowledge better, leading to the proposed SS-VFMT-D. Second, based on SS-VFMT, to address a new HSI classification task, i.e., generalized zero-shot classification, a spectral-spatial vision-language-based transformer (SS-VLFMT) is proposed. This task is to recognize novel classes not seen during training, which is more meaningful as the real world is usually open. The SS-VLFMT leverages SS-VFMT to extract spectral-spatial features and corresponding hash codes while integrating a pretrained language model to extract text features from class names. Experimental results on HSI datasets reveal that the proposed methods are competitive compared to the state-of-the-art methods. Moreover, the foundation model-based methods open a new window for HSI classification tasks, especially for HSI zero-shot classification.",
                "authors": "Lin Huang, Yushi Chen, Xin He",
                "citations": 3
            },
            {
                "title": "VISTA3D: A Unified Segmentation Foundation Model For 3D Medical Imaging",
                "abstract": "Foundation models for interactive segmentation in 2D natural images and videos have sparked significant interest in building 3D foundation models for medical imaging. However, the domain gaps and clinical use cases for 3D medical imaging require a dedicated model that diverges from existing 2D solutions. Specifically, such foundation models should support a full workflow that can actually reduce human effort. Treating 3D medical images as sequences of 2D slices and reusing interactive 2D foundation models seems straightforward, but 2D annotation is too time-consuming for 3D tasks. Moreover, for large cohort analysis, it's the highly accurate automatic segmentation models that reduce the most human effort. However, these models lack support for interactive corrections and lack zero-shot ability for novel structures, which is a key feature of\"foundation\". While reusing pre-trained 2D backbones in 3D enhances zero-shot potential, their performance on complex 3D structures still lags behind leading 3D models. To address these issues, we present VISTA3D, Versatile Imaging SegmenTation and Annotation model, that targets to solve all these challenges and requirements with one unified foundation model. VISTA3D is built on top of the well-established 3D segmentation pipeline, and it is the first model to achieve state-of-the-art performance in both 3D automatic (supporting 127 classes) and 3D interactive segmentation, even when compared with top 3D expert models on large and diverse benchmarks. Additionally, VISTA3D's 3D interactive design allows efficient human correction, and a novel 3D supervoxel method that distills 2D pretrained backbones grants VISTA3D top 3D zero-shot performance. We believe the model, recipe, and insights represent a promising step towards a clinically useful 3D foundation model. Code and weights are publicly available at https://github.com/Project-MONAI/VISTA.",
                "authors": "Yufan He, Pengfei Guo, Yucheng Tang, A. Myronenko, V. Nath, Ziyue Xu, Dong Yang, Can Zhao, Benjamin Simon, Mason J. Belue, Stephanie Harmon, B. Turkbey, Daguang Xu, Wenqi Li",
                "citations": 3
            },
            {
                "title": "PathoTune: Adapting Visual Foundation Model to Pathological Specialists",
                "abstract": "As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning. The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality prompt tuning approaches. Significantly, PathoTune facilitates the direct adaptation of natural visual foundation models to pathological tasks, drastically outperforming pathological foundation models with simple linear probing. The code is available at https://github.com/openmedlab/PathoDuet.",
                "authors": "Jiaxuan Lu, Fang Yan, Xiaofan Zhang, Yue Gao, Shaoting Zhang",
                "citations": 3
            },
            {
                "title": "VideoEval: Comprehensive Benchmark Suite for Low-Cost Evaluation of Video Foundation Model",
                "abstract": "With the growth of high-quality data and advancement in visual pre-training paradigms, Video Foundation Models (VFMs) have made significant progress recently, demonstrating their remarkable performance on traditional video understanding benchmarks. However, the existing benchmarks (e.g. Kinetics) and their evaluation protocols are often limited by relatively poor diversity, high evaluation costs, and saturated performance metrics. In this paper, we build a comprehensive benchmark suite to address these issues, namely VideoEval. Specifically, we establish the Video Task Adaption Benchmark (VidTAB) and the Video Embedding Benchmark (VidEB) from two perspectives: evaluating the task adaptability of VFMs under few-shot conditions and assessing their representation power by directly applying to downstream tasks. With VideoEval, we conduct a large-scale study on 20 popular open-source vision foundation models. Our study reveals some insightful findings on VFMs: 1) overall, current VFMs exhibit weak generalization across diverse tasks, 2) increasing video data, whether labeled or weakly-labeled video-text pairs, does not necessarily improve task performance, 3) the effectiveness of some pre-training paradigms may not be fully validated in previous benchmarks, and 4) combining different pre-training paradigms can help improve the generalization capabilities. We believe this study serves as an important complement to the current evaluation for VFMs and offers valuable insights for the future research.",
                "authors": "Xinhao Li, Zhenpeng Huang, Jing Wang, Kunchang Li, Limin Wang",
                "citations": 3
            },
            {
                "title": "Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model",
                "abstract": "Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foundation model teachers and complementary teachers. Complementary teachers possess model characteristics akin to the student's, aiming to bridge the gap between the foundation model and specialized application models for a smoother knowledge transfer. Further, to accommodate the dissimilarity among the teachers in the committee, we introduce DiverseDistill, which allows the student to understand the expertise of each teacher and extract task knowledge. Our evaluations demonstrate that adding complementary teachers enhances student performance. Finally, DiverseDistill consistently outperforms baseline distillation methods, regardless of the teacher choices, resulting in significantly improved student performance.",
                "authors": "Zichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, E. Chi, Zhe Zhao",
                "citations": 3
            },
            {
                "title": "Enrich, Distill and Fuse: Generalized Few-Shot Semantic Segmentation in Remote Sensing Leveraging Foundation Model’s Assistance",
                "abstract": "Generalized few-shot semantic segmentation (GFSS) unifies semantic segmentation with few-shot learning, showing great potential for Earth observation tasks under data scarcity conditions, such as disaster response, urban planning, and natural resource management. GFSS requires simultaneous prediction for both base and novel classes, with the challenge lying in balancing the segmentation performance of both. Therefore, this paper introduces a novel framework named FoMA, Foundation Model Assisted GFSS framework for remote sensing images. We aim to leverage the generic semantic knowledge inherited in foundation models. Specifically, we employ three strategies named Support Label Enrichment (SLE), Distillation of General Knowledge (DGK) and Voting Fusion of Experts (VFE). For the support images, SLE explores credible unlabeled novel categories, ensuring that each support label contains multiple novel classes. For the query images, DGK technique allows an effective transfer of generalizable knowledge of foundation models on certain categories to the GFSS learner. Additionally, VFE strategy integrates the zero-shot prediction of foundation models with the few-shot prediction of GFSS learners, achieving improved segmentation performance. Extensive experiments and ablation studies conducted on the OpenEarthMap few-shot challenge dataset demonstrate that our proposed method achieves state-of-the-art performance.",
                "authors": "Tianyi Gao, Wei Ao, Xing-ao Wang, Yuanhao Zhao, Ping Ma, Mengjie Xie, Hang Fu, Jinchang Ren, Zhi Gao",
                "citations": 3
            },
            {
                "title": "Multimodal Whole Slide Foundation Model for Pathology",
                "abstract": "The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning (SSL). However, translating these advancements to address complex clinical challenges at the patient and slide level remains constrained by limited clinical data in disease-specific cohorts, especially for rare clinical conditions. We propose TITAN, a multimodal whole slide foundation model pretrained using 335,645 WSIs via visual self-supervised learning and vision-language alignment with corresponding pathology reports and 423,122 synthetic captions generated from a multimodal generative AI copilot for pathology. Without any finetuning or requiring clinical labels, TITAN can extract general-purpose slide representations and generate pathology reports that generalize to resource-limited clinical scenarios such as rare disease retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and find that TITAN outperforms both ROI and slide foundation models across machine learning settings such as linear probing, few-shot and zero-shot classification, rare cancer retrieval and cross-modal retrieval, and pathology report generation.",
                "authors": "Tong Ding, Sophia J. Wagner, Andrew H. Song, Richard J. Chen, Ming Y. Lu, Andrew Zhang, Anurag Vaidya, Guillaume Jaume, Muhammad Shaban, Ahrong Kim, Drew F. K. Williamson, Bowen Chen, Cristina Almagro-Pérez, Paul Doucet, S. Sahai, Chengkuan Chen, D. Komura, Akihiro Kawabe, Shumpei Ishikawa, Georg K. Gerber, Tingying Peng, L. Le, Faisal Mahmood",
                "citations": 3
            },
            {
                "title": "Position: Foundation Agents as the Paradigm Shift for Decision Making",
                "abstract": "Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision have showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with their fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future.",
                "authors": "Xiaoqian Liu, Xingzhou Lou, Jianbin Jiao, Junge Zhang",
                "citations": 3
            },
            {
                "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
                "abstract": "GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging in the open-source community. Existing vision-language models rely on external tools for the speech processing, while speech-language models still suffer from limited or even without vision-understanding abilities. To address this gap, we propose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech capabilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we notice surprisingly that omni-modal alignment can further enhance vision-language and speech abilities compared with the corresponding bi-modal aligned counterparts. Moreover, a lightweight style module is proposed for flexible speech style controls (e.g., emotions and pitches). For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.",
                "authors": "Kai Chen, Yunhao Gou, Runhu Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, X. Li, W. Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Lanqing Hong, Lu Hou, Hang Xu",
                "citations": 12
            },
            {
                "title": "GFETM: Genome Foundation-based Embedded Topic Model for scATAC-seq Modeling",
                "abstract": "Single-cell Assay for Transposase-Accessible Chromatin with sequencing (scATAC-seq) has emerged as a powerful technique for investigating open chromatin landscapes at single-cell resolution. However, analyzing scATAC-seq data remain challenging due to its sparsity and noise. Genome Foundation Models (GFMs), pre-trained on massive DNA sequences, have proven effective at genome analysis. Given that open chromatin regions (OCRs) harbour salient sequence features, we hypothesize that leveraging GFMs’ sequence embeddings can improve the accuracy and generalizability of scATAC-seq modeling. Here, we introduce the Genome Foundation Embedded Topic Model (GFETM), an interpretable deep learning framework that combines GFMs with the Embedded Topic Model (ETM) for scATAC-seq data analysis. By integrating the DNA sequence embeddings extracted by a GFM from OCRs, GFETM demonstrates superior accuracy and generalizability and captures cell-state specific TF activity both with zero-shot inference and attention mechanism analysis. Finally, the topic mixtures inferred by GFETM reveal biologically meaningful epigenomic signatures of kidney diabetes.",
                "authors": "Yimin Fan, Adrien Osakwe, Shi Han, Yu Li, Jun Ding, Yue Li",
                "citations": 2
            },
            {
                "title": "GFT: Graph Foundation Model with Transferable Tree Vocabulary",
                "abstract": "Inspired by the success of foundation models in applications such as ChatGPT, as graph data has been ubiquitous, one can envision the far-reaching impacts that can be brought by Graph Foundation Models (GFMs) with broader applications in the areas such as scientific research, social network analysis, drug discovery, and e-commerce. Despite the significant progress of pre-trained graph neural networks, there haven't been GFMs that can achieve desired performance on various graph-learning-related tasks. Building GFMs may rely on a vocabulary that encodes transferable patterns shared among different tasks and domains. Unlike image and text, defining such transferable patterns for graphs remains an open question. In this paper, we aim to bridge this gap by rethinking the transferable patterns on graphs as computation trees -- i.e., tree structures derived from the message-passing process. Based on this insight, we propose a cross-task, cross-domain graph foundation model named GFT, short for Graph Foundation model with transferable Tree vocabulary. By treating computation trees as tokens within the transferable vocabulary, GFT improves model generalization and reduces the risk of negative transfer. The theoretical analyses and extensive experimental studies have demonstrated the transferability of computation trees and shown the effectiveness of GFT across diverse tasks and domains in graph learning. The open source code and data are available at https://github.com/Zehong-Wang/GFT.",
                "authors": "Zehong Wang, Zheyuan Zhang, Nitesh V. Chawla, Chuxu Zhang, Yanfang Ye",
                "citations": 2
            },
            {
                "title": "An overview of domain-specific foundation model: key technologies, applications and challenges",
                "abstract": "The impressive performance of ChatGPT and other foundation-model-based products in human language understanding has prompted both academia and industry to explore how these models can be tailored for specific industries and application scenarios. This process, known as the customization of domain-specific foundation models, addresses the limitations of general-purpose models, which may not fully capture the unique patterns and requirements of domain-specific data. Despite its importance, there is a notable lack of comprehensive overview papers on building domain-specific foundation models, while numerous resources exist for general-purpose models. To bridge this gap, this article provides a timely and thorough overview of the methodology for customizing domain-specific foundation models. It introduces basic concepts, outlines the general architecture, and surveys key methods for constructing domain-specific models. Furthermore, the article discusses various domains that can benefit from these specialized models and highlights the challenges ahead. Through this overview, we aim to offer valuable guidance and reference for researchers and practitioners from diverse fields to develop their own customized foundation models.",
                "authors": "Haolong Chen, Hanzhi Chen, Zijian Zhao, Kaifeng Han, Guangxu Zhu, Yichen Zhao, Ying Du, Wei Xu, Qingjiang Shi",
                "citations": 2
            },
            {
                "title": "A foundation model for generalizable cancer diagnosis and survival prediction from histopathological images",
                "abstract": "Computational pathology, utilizing whole slide image (WSI) for pathological diagnosis, has advanced the development of intelligent healthcare. However, the scarcity of annotated data and histological differences hinder the general application of existing methods. Extensive histopathological data and the robustness of self-supervised models in small-scale data demonstrate promising prospects for developing foundation pathology models. Due to the need for deployment, lightweight foundation models also need to be developed. In this work, we propose the BEPH (BEiT-based model Pre-training on Histopathological images), a general lightweight foundation model that leverages self-supervised learning to learn meaningful representations from 11 million unlabeled histopathological images. These representations are then efficiently adapted to various tasks, including 2 cancer patch-level recognition tasks, 3 cancer WSI-level classification tasks, and 6 cancer subtypes survival prediction tasks. Experimental results demonstrate that our model consistently outperforms several comparative models with similar parameters, even with limited training data reduced to 50%. Especially when the downstream structure is the same, the model can improve ResNet and DINO by up to a maximum increase of 8.8% and 7.2% (WSI level classification), and 6.44% and 3.28% on average (survival prediction), respectively. Therefore, BEPH offers a universal solution to enhance model performance, reduce the burden of expert annotations, and enable widespread clinical applications of artificial intelligence. The code and models can be obtained at https://github.com/Zhcyoung/BEPH. And currently, online fine-tuning of WSI classification tasks is available for use on http://yulab-sjtu.natapp1.cc/BEPH.",
                "authors": "Zhaochang Yang, Ting Wei, Ying Liang, Xin Yuan, Ruitian Gao, Yujia Xia, Jie Zhou, Yue Zhang, Zhangsheng Yu",
                "citations": 2
            },
            {
                "title": "TimeDiT: General-purpose Diffusion Transformers for Time Series Foundation Model",
                "abstract": "With recent advances in building foundation models for texts and video data, there is a surge of interest in foundation models for time series. A family of models have been developed, utilizing a temporal auto-regressive generative Transformer architecture, whose effectiveness has been proven in Large Language Models. While the empirical results are promising, almost all existing time series foundation models have only been tested on well-curated ``benchmark'' datasets very similar to texts. However, real-world time series exhibit unique challenges, such as variable channel sizes across domains, missing values, and varying signal sampling intervals due to the multi-resolution nature of real-world data. Additionally, the uni-directional nature of temporally auto-regressive decoding limits the incorporation of domain knowledge, such as physical laws expressed as partial differential equations (PDEs). To address these challenges, we introduce the Time Diffusion Transformer (TimeDiT), a general foundation model for time series that employs a denoising diffusion paradigm instead of temporal auto-regressive generation. TimeDiT leverages the Transformer architecture to capture temporal dependencies and employs diffusion processes to generate high-quality candidate samples without imposing stringent assumptions on the target distribution via novel masking schemes and a channel alignment strategy. Furthermore, we propose a finetuning-free model editing strategy that allows the seamless integration of external knowledge during the sampling process without updating any model parameters. Extensive experiments conducted on a varity of tasks such as forecasting, imputation, and anomaly detection, demonstrate the effectiveness of TimeDiT.",
                "authors": "Defu Cao, Wen Ye, Yizhou Zhang, Yan Liu",
                "citations": 2
            },
            {
                "title": "VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge",
                "abstract": "The need for improved diagnostic methods in ophthalmology is acute, especially in the less developed regions with limited access to specialists and advanced equipment. Therefore, we introduce VisionUnite, a novel vision-language foundation model for ophthalmology enhanced with clinical knowledge. VisionUnite has been pretrained on an extensive dataset comprising 1.24 million image-text pairs, and further refined using our proposed MMFundus dataset, which includes 296,379 high-quality fundus image-text pairs and 889,137 simulated doctor-patient dialogue instances. Our experiments indicate that VisionUnite outperforms existing generative foundation models such as GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable to junior ophthalmologists. VisionUnite performs well in various clinical scenarios including open-ended multi-disease diagnosis, clinical explanation, and patient interaction, making it a highly versatile tool for initial ophthalmic disease screening. VisionUnite can also serve as an educational aid for junior ophthalmologists, accelerating their acquisition of knowledge regarding both common and rare ophthalmic conditions. VisionUnite represents a significant advancement in ophthalmology, with broad implications for diagnostics, medical education, and understanding of disease mechanisms.",
                "authors": "Zihan Li, Diping Song, Zefeng Yang, Deming Wang, Fei Li, Xiulan Zhang, P. E. Kinahan, Yu Qiao",
                "citations": 2
            },
            {
                "title": "Image Segmentation in Foundation Model Era: A Survey",
                "abstract": "Image segmentation is a long-standing challenge in computer vision, studied continuously over several decades, as evidenced by seminal algorithms such as N-Cut, FCN, and MaskFormer. With the advent of foundation models (FMs), contemporary segmentation methodologies have embarked on a new epoch by either adapting FMs (e.g., CLIP, Stable Diffusion, DINO) for image segmentation or developing dedicated segmentation foundation models (e.g., SAM). These approaches not only deliver superior segmentation performance, but also herald newfound segmentation capabilities previously unseen in deep learning context. However, current research in image segmentation lacks a detailed analysis of distinct characteristics, challenges, and solutions associated with these advancements. This survey seeks to fill this gap by providing a thorough review of cutting-edge research centered around FM-driven image segmentation. We investigate two basic lines of research -- generic image segmentation (i.e., semantic segmentation, instance segmentation, panoptic segmentation), and promptable image segmentation (i.e., interactive segmentation, referring segmentation, few-shot segmentation) -- by delineating their respective task settings, background concepts, and key challenges. Furthermore, we provide insights into the emergence of segmentation knowledge from FMs like CLIP, Stable Diffusion, and DINO. An exhaustive overview of over 300 segmentation approaches is provided to encapsulate the breadth of current research efforts. Subsequently, we engage in a discussion of open issues and potential avenues for future research. We envisage that this fresh, comprehensive, and systematic survey catalyzes the evolution of advanced image segmentation systems. A public website is created to continuously track developments in this fast advancing field: \\url{https://github.com/stanley-313/ImageSegFM-Survey}.",
                "authors": "Tianfei Zhou, Fei Zhang, Boyu Chang, Wenguan Wang, Ye Yuan, E. Konukoglu, Daniel Cremers",
                "citations": 2
            },
            {
                "title": "DeepGene: An Efficient Foundation Model for Genomics based on Pan-genome Graph Transformer",
                "abstract": "Decoding the language of DNA sequences is a fundamental problem in genome research. Mainstream pre-trained models like DNABERT-2 and Nucleotide Transformer have demonstrated remarkable achievements across a spectrum of DNA analysis tasks. Yet, these models still face the pivotal challenge of (1) genetic language diversity, or the capability to capture genetic variations across individuals or populations in the foundation models; (2) model efficiency, specifically how to enhance performance at scalable costs for large-scale genetic foundational models; (3) length extrapolation, or the ability to accurately interpret sequences ranging from short to long within a unified model framework. In response, we introduce DeepGene, a model leveraging Pan-genome and Minigraph representations to encompass the broad diversity of genetic language. DeepGene employs the rotary position embedding to improve the length extrapolation in various genetic analysis tasks. On the 28 tasks in Genome Understanding Evaluation, DeepGene reaches the top position in 9 tasks, second in 5, and achieves the overall best score. DeepGene outperforms other cutting-edge models for its compact model size and superior efficiency in processing sequences of varying lengths. The datasets and source code of DeepGene are available at GitHub (https://github.com/wds-seu/DeepGene).",
                "authors": "Xiang Zhang, Mingjie Yang, Xunhang Yin, Yining Qian, Fei Sun",
                "citations": 2
            },
            {
                "title": "Copyright Law and the Lifecycle of Machine Learning Models",
                "abstract": "Machine learning, a subfield of artificial intelligence (AI), relies on large corpora of data as input for learning algorithms, resulting in trained models that can perform a variety of tasks. While data or information are not subject matter within copyright law, almost all materials used to construct corpora for machine learning are protected by copyright law: texts, images, videos, and so on. There are global policy moves to address the copyright implications of machine learning, in particular in the context of so-called “foundation models” that underpin generative AI. This paper takes a step back, exploring empirically three technological settings through detailed case studies. We set out the established industry methodology of a lifecycle of AI (collecting data, organising data, model training, model operation) to arrive at descriptions suitable for legal analysis. This will allow an assessment of the challenges for a harmonisation of rights, exceptions and disclosure under EU copyright law. The three case studies are: 1. Machine learning for scientific purposes, in the context of a study of regional short-term letting markets; 2. Natural Language Processing (NLP), in the context of large language models; 3. Computer vision, in the context of content moderation of images. We find that the nature and quality of data corpora at the input stage is central to the lifecycle of machine learning. Because of the uncertain legal status of data collection and processing, combined with the competitive advantage gained by firms not disclosing technological advances, the inputs of the models deployed are often unknown. Moreover, the “lawful access” requirement of the EU exception for text and data mining may turn the exception into a decision by rightholders to allow machine learning in the context of their decision to allow access. We assess policy interventions at EU level, seeking to clarify the legal status of input data via copyright exceptions, opt-outs or the forced disclosure of copyright materials. We find that the likely result is a fully copyright-licensed environment of machine learning that may have problematic effects for the structure of industry, innovation and scientific research.",
                "authors": "Martin Kretschmer, Thomas Margoni, Pinar Oruc",
                "citations": 12
            },
            {
                "title": "GET: a foundation model of transcription across human cell types",
                "abstract": "Transcriptional regulation, involving the complex interplay between regulatory sequences and proteins, directs all biological processes. Computational models of transcription lack generalizability to accurately extrapolate in unseen cell types and conditions. Here, we introduce GET, an interpretable foundation model designed to uncover regulatory grammars across 213 human fetal and adult cell types. Relying exclusively on chromatin accessibility data and sequence information, GET achieves experimental-level accuracy in predicting gene expression even in previously unseen cell types. GET showcases remarkable adaptability across new sequencing platforms and assays, enabling regulatory inference across a broad range of cell types and conditions, and uncovering universal and cell type specific transcription factor interaction networks. We evaluated its performance on prediction of regulatory activity, inference of regulatory elements and regulators, and identification of physical interactions between transcription factors. Specifically, we show GET outperforms current models in predicting lentivirus-based massive parallel reporter assay readout with reduced input data. In fetal erythroblasts, we identify distal (>1Mbp) regulatory regions that were missed by previous models. In B cells, we identified a lymphocyte-specific transcription factor-transcription factor interaction that explains the functional significance of a leukemia-risk predisposing germline mutation. In sum, we provide a generalizable and accurate model for transcription together with catalogs of gene regulation and transcription factor interactions, all with cell type specificity.",
                "authors": "Xi Fu, Shentong Mo, Anqi Shao, Anouchka P. Laurent, Alejandro Buendia, Adolfo A. Ferrando, Alberto Ciccia, Yanyan Lan, Teresa Palomero, David M. Owens, Eric P. Xing, Raúl Rabadán",
                "citations": 6
            },
            {
                "title": "LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting",
                "abstract": "Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.",
                "authors": "Haoxin Liu, Zhiyuan Zhao, Jindong Wang, Harshavardhan Kamarthi, B. A. Prakash",
                "citations": 14
            },
            {
                "title": "Diffusion Models in Low-Level Vision: A Survey",
                "abstract": "Deep generative models have garnered significant attention in low-level vision tasks due to their generative capabilities. Among them, diffusion model-based solutions, characterized by a forward diffusion process and a reverse denoising process, have emerged as widely acclaimed for their ability to produce samples of superior quality and diversity. This ensures the generation of visually compelling results with intricate texture information. Despite their remarkable success, a noticeable gap exists in a comprehensive survey that amalgamates these pioneering diffusion model-based works and organizes the corresponding threads. This paper proposes the comprehensive review of diffusion model-based techniques. We present three generic diffusion modeling frameworks and explore their correlations with other deep generative models, establishing the theoretical foundation. Following this, we introduce a multi-perspective categorization of diffusion models, considering both the underlying framework and the target task. Additionally, we summarize extended diffusion models applied in other tasks, including medical, remote sensing, and video scenarios. Moreover, we provide an overview of commonly used benchmarks and evaluation metrics. We conduct a thorough evaluation, encompassing both performance and efficiency, of diffusion model-based techniques in three prominent tasks. Finally, we elucidate the limitations of current diffusion models and propose seven intriguing directions for future research. This comprehensive examination aims to facilitate a profound understanding of the landscape surrounding denoising diffusion models in the context of low-level vision tasks. A curated list of diffusion model-based techniques in over 20 low-level vision tasks can be found at https://github.com/ChunmingHe/awesome-diffusion-models-in-low-level-vision.",
                "authors": "Chunming He, Yuqi Shen, Chengyu Fang, Fengyang Xiao, Longxiang Tang, Yulun Zhang, Wangmeng Zuo, Z. Guo, Xiu Li",
                "citations": 13
            },
            {
                "title": "Latency-Aware Generative Semantic Communications With Pre-Trained Diffusion Models",
                "abstract": "Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this letter, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.",
                "authors": "Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, C. Foh, Pei Xiao, Mehdi Bennis",
                "citations": 13
            },
            {
                "title": "Large Foundation Model for Cancer Segmentation",
                "abstract": "Recently, large language models such as ChatGPT have made huge strides in understanding and generating human-like text and have demonstrated considerable success in natural language processing. These foundation models also perform well in computer vision. However, there is a growing need to use these technologies for specific medical tasks, especially for identifying cancer in images. This paper looks at how these foundation models, such as the segment anything model, could be used for cancer segmentation, discussing the potential benefits and challenges of applying large foundation models to help with cancer diagnoses.",
                "authors": "Zeyu Ren, Yu-dong Zhang, Shuihua Wang",
                "citations": 1
            },
            {
                "title": "Changen2: Multi-Temporal Remote Sensing Generative Change Foundation Model",
                "abstract": "Our understanding of the temporal dynamics of the Earth's surface has been significantly advanced by deep vision models, which often require a massive amount of labeled multi-temporal images for training. However, collecting, preprocessing, and annotating multi-temporal remote sensing images at scale is non-trivial since it is expensive and knowledge-intensive. In this paper, we present scalable multi-temporal change data generators based on generative models, which are cheap and automatic, alleviating these data problems. Our main idea is to simulate a stochastic change process over time. We describe the stochastic change process as a probabilistic graphical model, namely the generative probabilistic change model (GPCM), which factorizes the complex simulation problem into two more tractable sub-problems, i.e., condition-level change event simulation and image-level semantic change synthesis. To solve these two problems, we present Changen2, a GPCM implemented with a resolution-scalable diffusion transformer which can generate time series of remote sensing images and corresponding semantic and change labels from labeled and even unlabeled single-temporal images. Changen2 is a “generative change foundation model” that can be trained at scale via self-supervision, and is capable of producing change supervisory signals from unlabeled single-temporal images. Unlike existing “foundation models”, our generative change foundation model synthesizes change data to train task-specific foundation models for change detection. The resulting model possesses inherent zero-shot change detection capabilities and excellent transferability. Comprehensive experiments suggest Changen2 has superior spatiotemporal scalability in data generation, e.g., Changen2 model trained on 256<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zheng-ieq1-3475824.gif\"/></alternatives></inline-formula> pixel single-temporal images can yield time series of any length and resolutions of 1,024<inline-formula><tex-math notation=\"LaTeX\">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"zheng-ieq2-3475824.gif\"/></alternatives></inline-formula> pixels. Changen2 pre-trained models exhibit superior zero-shot performance (narrowing the performance gap to 3% on LEVIR-CD and approximately 10% on both S2Looking and SECOND, compared to fully supervised counterpart) and transferability across multiple types of change tasks, including ordinary and off-nadir building change, land-use/land-cover change, and disaster assessment.",
                "authors": "Zhuo Zheng, Stefano Ermon, Dongjun Kim, Liangpei Zhang, Yanfei Zhong",
                "citations": 8
            },
            {
                "title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
                "abstract": "Foundation models, e.g., large language models (LLMs), trained on internet-scale data possess zero-shot generalization capabilities that make them a promising technology towards detecting and mitigating out-of-distribution failure modes of robotic systems. Fully realizing this promise, however, poses two challenges: (i) mitigating the considerable computational expense of these models such that they may be applied online, and (ii) incorporating their judgement regarding potential anomalies into a safe control framework. In this work, we present a two-stage reasoning framework: First is a fast binary anomaly classifier that analyzes observations in an LLM embedding space, which may then trigger a slower fallback selection stage that utilizes the reasoning capabilities of generative LLMs. These stages correspond to branch points in a model predictive control strategy that maintains the joint feasibility of continuing along various fallback plans to account for the slow reasoner's latency as soon as an anomaly is detected, thus ensuring safety. We show that our fast anomaly classifier outperforms autoregressive reasoning with state-of-the-art GPT models, even when instantiated with relatively small language models. This enables our runtime monitor to improve the trustworthiness of dynamic robotic systems, such as quadrotors or autonomous vehicles, under resource and time constraints. Videos illustrating our approach in both simulation and real-world experiments are available on this project page: https://sites.google.com/view/aesop-llm.",
                "authors": "Rohan Sinha, Amine Elhafsi, Christopher Agia, Matthew Foutter, E. Schmerling, Marco Pavone",
                "citations": 10
            },
            {
                "title": "PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models",
                "abstract": "Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories. We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective. We further develop an online algorithm with proximal updates to stably optimize the RDP objective. In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training. Furthermore, through large-scale training on text prompts from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen prompts whereas RL-based methods completely fail.",
                "authors": "Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, Tingbo Hou",
                "citations": 11
            },
            {
                "title": "Private Fine-tuning of Large Language Models with Zeroth-order Optimization",
                "abstract": "Differentially private stochastic gradient descent (DP-SGD) allows models to be trained in a privacy-preserving manner, but has proven difficult to scale to the era of foundation models. We introduce DP-ZO, a private fine-tuning framework for large language models by privatizing zeroth order optimization methods. A key insight into the design of our method is that the direction of the gradient in the zeroth-order optimization we use is random and the only information from training data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO provides a strong privacy-utility trade-off across different tasks, and model sizes that are comparable to DP-SGD in $(\\varepsilon,\\delta)$-DP. Notably, DP-ZO possesses significant advantages over DP-SGD in memory efficiency, and obtains higher utility in $\\varepsilon$-DP when using the Laplace mechanism.",
                "authors": "Xinyu Tang, Ashwinee Panda, Milad Nasr, Saeed Mahloujifar, Prateek Mittal",
                "citations": 11
            },
            {
                "title": "Large Language Models in Plant Biology",
                "abstract": "Large language models (LLMs), such as ChatGPT, have taken the world by storm. However, LLMs are not limited to human language and can be used to analyze sequential data, such as DNA, protein, and gene expression. The resulting foundation models can be repurposed to identify the complex patterns within the data, resulting in powerful, multipurpose prediction tools able to predict the state of cellular systems. This review outlines the different types of LLMs and showcases their recent uses in biology. Since LLMs have not yet been embraced by the plant community, we also cover how these models can be deployed for the plant kingdom.",
                "authors": "H. Lam, Xing Er Ong, Marek Mutwil",
                "citations": 10
            },
            {
                "title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
                "abstract": "Foundation models, e.g., large language models (LLMs), trained on internet-scale data possess zero-shot generalization capabilities that make them a promising technology towards detecting and mitigating out-of-distribution failure modes of robotic systems. Fully realizing this promise, however, poses two challenges: (i) mitigating the considerable computational expense of these models such that they may be applied online, and (ii) incorporating their judgement regarding potential anomalies into a safe control framework. In this work, we present a two-stage reasoning framework: First is a fast binary anomaly classifier that analyzes observations in an LLM embedding space, which may then trigger a slower fallback selection stage that utilizes the reasoning capabilities of generative LLMs. These stages correspond to branch points in a model predictive control strategy that maintains the joint feasibility of continuing along various fallback plans to account for the slow reasoner's latency as soon as an anomaly is detected, thus ensuring safety. We show that our fast anomaly classifier outperforms autoregressive reasoning with state-of-the-art GPT models, even when instantiated with relatively small language models. This enables our runtime monitor to improve the trustworthiness of dynamic robotic systems, such as quadrotors or autonomous vehicles, under resource and time constraints. Videos illustrating our approach in both simulation and real-world experiments are available on this project page: https://sites.google.com/view/aesop-llm.",
                "authors": "Rohan Sinha, Amine Elhafsi, Christopher Agia, Matthew Foutter, E. Schmerling, Marco Pavone",
                "citations": 10
            },
            {
                "title": "PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models",
                "abstract": "Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories. We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective. We further develop an online algorithm with proximal updates to stably optimize the RDP objective. In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training. Furthermore, through large-scale training on text prompts from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen prompts whereas RL-based methods completely fail.",
                "authors": "Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, Tingbo Hou",
                "citations": 11
            },
            {
                "title": "Large Language Models in Plant Biology",
                "abstract": "Large language models (LLMs), such as ChatGPT, have taken the world by storm. However, LLMs are not limited to human language and can be used to analyze sequential data, such as DNA, protein, and gene expression. The resulting foundation models can be repurposed to identify the complex patterns within the data, resulting in powerful, multipurpose prediction tools able to predict the state of cellular systems. This review outlines the different types of LLMs and showcases their recent uses in biology. Since LLMs have not yet been embraced by the plant community, we also cover how these models can be deployed for the plant kingdom.",
                "authors": "H. Lam, Xing Er Ong, Marek Mutwil",
                "citations": 10
            },
            {
                "title": "An Improved Traditional Chinese Evaluation Suite for Foundation Model",
                "abstract": "We present TMMLU+, a new benchmark designed for Traditional Chinese language understanding. TMMLU+ is a multi-choice question-answering dataset with 66 subjects from elementary to professional level. It is six times larger and boasts a more balanced subject distribution than its predecessor, Taiwan Massive Multitask Language Understanding (TMMLU). We also benchmark closed-source models and 26 open-weight Chinese large language models (LLMs) of parameters ranging from 1.8B to 72B on the proposed TMMLU+. Our findings reveal that (1.) Traditional Chinese models still trail behind their Simplified Chinese counterparts, highlighting a need for more focused advancements in LLMs catering to Traditional Chinese. (2.) Current LLMs still fall short of human performance in average scores, indicating a potential need for future research to delve deeper into social science and humanities subjects. (3.) Among all the tokenization compression metrics examined, we identify that only the fertility score uniquely demonstrates strong correlations with our benchmark results. We foresee that TMMLU+ will pinpoint areas for future model improvement, thereby narrowing the gap between machine and human linguistic capabilities and supporting researchers in developing Traditional Chinese LLMs. Our dataset, along with the benchmark source code, is accessible at huggingface.co/datasets/ikala/tmmluplus.",
                "authors": "Zhi Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Sega Cheng, Hong-Han Shuai",
                "citations": 7
            },
            {
                "title": "S2MAE: A Spatial-Spectral Pretraining Foundation Model for Spectral Remote Sensing Data",
                "abstract": "In the expansive domain of computer vision, a myr-iad of pretrained models are at our disposal. However, most of these models are designed for natural RGB images and prove inadequate for spectral remote sensing (RS) images. Spectral RS images have two main traits: (1) multiple bands capturing diverse feature information, (2) spatial alignment and consistent spectral sequencing within the spatial-spectral dimension. In this paper, we introduce Spatial-SpectralMAE (S2MAE), a specialized pretrained architecture for spectral RS imagery. S2MAE employs a 3D transformer for masked autoencoder modeling, inte-grating learnable spectral-spatial embeddings with a 90% masking ratio. The model efficiently captures local spec-tral consistency and spatial invariance using compact cube tokens, demonstrating versatility to diverse input characteristics. This adaptability facilitates progressive pretraining on extensive spectral datasets. The effectiveness of S2MAE is validated through continuous pretraining on two sizable datasets, totaling over a million training images. The pretrained model is subsequently applied to three dis-tinct downstream tasks, with in-depth ablation studies conducted to emphasize its efficacy.",
                "authors": "Xuyang Li, Danfeng Hong, J. Chanussot",
                "citations": 7
            },
            {
                "title": "ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with LLM-Enhanced Cardiological Text",
                "abstract": "The utilization of deep learning on electrocardiogram (ECG) analysis has brought the advanced accuracy and efficiency of cardiac healthcare diagnostics. By leveraging the capabilities of deep learning in semantic understanding, especially in feature extraction and representation learning, this study introduces a new multimodal contrastive pretaining framework that aims to improve the quality and robustness of learned representations of 12-lead ECG signals. Our framework comprises two key components, including Cardio Query Assistant (CQA) and ECG Semantics Integrator(ESI). CQA integrates a retrieval-augmented generation (RAG) pipeline to leverage large language models (LLMs) and external medical knowledge to generate detailed textual descriptions of ECGs. The generated text is enriched with information about demographics and waveform patterns. ESI integrates both contrastive and captioning loss to pretrain ECG encoders for enhanced representations. We validate our approach through various downstream tasks, including arrhythmia detection and ECG-based subject identification. Our experimental results demonstrate substantial improvements over strong baselines in these tasks. These baselines encompass supervised and self-supervised learning methods, as well as prior multimodal pretraining approaches.",
                "authors": "Han Yu, Peikun Guo, Akane Sano",
                "citations": 6
            },
            {
                "title": "UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation Model for Urban Indicator Prediction",
                "abstract": "Urban indicator prediction aims to infer socio-economic metrics in diverse urban landscapes using data-driven methods. However, prevalent pre-trained models, particularly those reliant on satellite imagery, face dual challenges. Firstly, concentrating solely on macro-level patterns from satellite data may introduce bias, lacking nuanced details at micro levels, such as architectural details at a place. Secondly, the lack of interpretability in pre-trained models limits their utility in providing transparent evidence for urban planning. In response to these issues, we devise a novel Vision-Language Pre-Trained Model (UrbanVLP) in this paper. Our UrbanVLP seamlessly integrates multi-granularity information from both macro (satellite) and micro (street-view) levels, overcoming the limitations of prior pre-trained models. Moreover, it introduces automatic text generation and calibration, elevating interpretability in down-stream applications by producing high-quality text descriptions of urban imagery. Rigorous experiments conducted across six socio-economic tasks underscore UrbanVLP’s superior performance. We also deploy a web platform to verify its practicality.",
                "authors": "Xixuan Hao, Wei Chen, Yibo Yan, Siru Zhong, Kun Wang, Qingsong Wen, Yuxuan Liang",
                "citations": 6
            },
            {
                "title": "Panacea: A foundation model for clinical trial search, summarization, design, and recruitment",
                "abstract": "Clinical trials are fundamental in developing new drugs, medical devices, and treatments. However, they are often time-consuming and have low success rates. Although there have been initial attempts to create large language models (LLMs) for clinical trial design and patient-trial matching, these models remain task-specific and not adaptable to diverse clinical trial tasks. To address this challenge, we propose a clinical trial foundation model named Panacea, designed to handle multiple tasks, including trial search, trial summarization, trial design, and patient-trial matching. We also assemble a large-scale dataset, named TrialAlign, of 793,279 trial documents and 1,113,207 trial-related scientific papers, to infuse clinical knowledge into the model by pre-training. We further curate TrialInstruct, which has 200,866 of instruction data for fine-tuning. These resources enable Panacea to be widely applicable for a range of clinical trial tasks based on user requirements. We evaluated Panacea on a new benchmark, named TrialPanorama, which covers eight clinical trial tasks. Our method performed the best on seven of the eight tasks compared to six cutting-edge generic or medicine-specific LLMs. Specifically, Panacea showed great potential to collaborate with human experts in crafting the design of eligibility criteria, study arms, and outcome measures, in multi-round conversations. In addition, Panacea achieved 14.42% improvement in patient-trial matching, 41.78% to 52.02% improvement in trial search, and consistently ranked at the top for five aspects of trial summarization. Our approach demonstrates the effectiveness of Panacea in clinical trials and establishes a comprehensive resource, including training data, model, and benchmark, for developing clinical trial foundation models, paving the path for AI-based clinical trial development.",
                "authors": "J. Lin, H. Xu, Z. Wang, S. Wang, J. Sun",
                "citations": 4
            },
            {
                "title": "Multi-Stage and Multi-Parameter Influence Analysis of Deep Foundation Pit Excavation on Surrounding Environment",
                "abstract": "As urbanization accelerates, deep excavation projects have become increasingly vital in the construction of high-rise buildings and underground facilities. However, the potential risks to the surrounding environment and the inherent complexities involved necessitate thorough research to ensure the safety of those engineering projects with deep foundation pit excavation and to minimize their impact on adjacent structures. This study introduces a multi-stage and multi-parameter numerical simulation method to scrutinize the construction process of deep foundation pits. This approach not only investigates the influence of excavation activities on nearby buildings and roads but also enhances the fidelity of simulation models by establishing a three-dimensional finite element model integrated with on-site investigated geological information. Therefore, the proposed method can provide a more holistic and accurate analysis of the overall impacts of the pit excavation process. To examine the feasibility and effectiveness of the proposed method, this study adopts the multi-stage and multi-parameter influence analysis approach for a real practical engineering case to explore the impact of excavation on the foundation pit support structure, nearby buildings, and surrounding roads. The foundation pit support’s maximum displacement was 8.64 mm, well under the 25 mm standard limit. Anchor rod forces were about 10% below the standard limit. Building and road settlements were also minimal, at 10.33 mm and 16.44 mm, respectively, far below their respective limits of 200 mm and 300 mm. This study not only validates the feasibility of design and construction stability of deep foundation pits but also contributes theoretical and practical insights, serving as a valuable reference for future engineering projects of a similar scope.",
                "authors": "Dunqing Li, Feng Liao, Lixin Wang, Jianfu Lin, Junfang Wang",
                "citations": 5
            },
            {
                "title": "A Numerical Model for the Scour Effect on the Bearing Capacity of an Offshore Wind Turbine with a Five-Bucket Jacket Foundation",
                "abstract": "As offshore wind farms move into deeper waters and the capacity of offshore wind turbines (OWTs) increases, a new type of OWT foundation needs to be developed. In this study, a new type of five-bucket jacket foundation (FBJF) was proposed based on the broad application of a multi-bucket jacket foundation (MBJF) in offshore wind farms. The soil around the OWT foundation is subject to scour due to the complex marine environment. To investigate the effects of scouring on the FBJF, a series of local-scour simplified finite-element models of the FBJF were established using ABAQUS, and the effects of scouring depth and the extent on the bearing capacity of the FBJF with the monotonic load were analyzed. Then, the failure envelopes of the FBJF under combined loading were obtained using the fixed-displacement ratio method, and the effects of various scour conditions on the failure envelopes were compared. The results indicate that the failure envelope profile contracts inward, and the bearing capacity decreases with the increasing scouring depth and extent. Furthermore, the failure envelopes of the FBJF under different vertical loads were calculated, and the FV-FH-FM failure envelopes of the FBJF were obtained through interpolation. Finally, the effects of different scour conditions on the FV-FH-FM failure envelopes of the FBJF were analyzed. The results show that the FV-FH-FM failure envelopes of the FBJF have similar profiles and follow the same trend under different scour conditions.",
                "authors": "Hang Zhu, Jijian Lian, Yaohua Guo, Haijun Wang",
                "citations": 4
            },
            {
                "title": "MIO: A Foundation Model on Multimodal Tokens",
                "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.",
                "authors": "Z. Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang",
                "citations": 4
            },
            {
                "title": "HSIGene: A Foundation Model For Hyperspectral Image Generation",
                "abstract": "Hyperspectral image (HSI) plays a vital role in various fields such as agriculture and environmental monitoring. However, due to the expensive acquisition cost, the number of hyperspectral images is limited, degenerating the performance of downstream tasks. Although some recent studies have attempted to employ diffusion models to synthesize HSIs, they still struggle with the scarcity of HSIs, affecting the reliability and diversity of the generated images. Some studies propose to incorporate multi-modal data to enhance spatial diversity, but the spectral fidelity cannot be ensured. In addition, existing HSI synthesis models are typically uncontrollable or only support single-condition control, limiting their ability to generate accurate and reliable HSIs. To alleviate these issues, we propose HSIGene, a novel HSI generation foundation model which is based on latent diffusion and supports multi-condition control, allowing for more precise and reliable HSI generation. To enhance the spatial diversity of the training data while preserving spectral fidelity, we propose a new data augmentation method based on spatial super-resolution, in which HSIs are upscaled first, and thus abundant training patches could be obtained by cropping the high-resolution HSIs. In addition, to improve the perceptual quality of the augmented data, we introduce a novel two-stage HSI super-resolution framework, which first applies RGB bands super-resolution and then utilizes our proposed Rectangular Guided Attention Network (RGAN) for guided HSI super-resolution. Experiments demonstrate that the proposed model is capable of generating a vast quantity of realistic HSIs for downstream tasks such as denoising and super-resolution. The code and models are available at https://github.com/LiPang/HSIGene.",
                "authors": "Li Pang, Datao Tang, Shuang Xu, Deyu Meng, Xiangyong Cao",
                "citations": 3
            },
            {
                "title": "CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications",
                "abstract": "Autonomous robot operation in unstructured environments is often underpinned by spatial understanding through vision. Systems composed of multiple concurrently operating robots additionally require access to frequent, accurate and reliable pose estimates. In this work, we propose CoViS-Net, a decentralized visual spatial foundation model that learns spatial priors from data, enabling pose estimation as well as spatial comprehension. Our model is fully decentralized, platform-agnostic, executable in real-time using onboard compute, and does not require existing networking infrastructure. CoViS-Net provides relative pose estimates and a local bird's-eye-view (BEV) representation, even without camera overlap between robots (in contrast to classical methods). We demonstrate its use in a multi-robot formation control task across various real-world settings. We provide code, models and supplementary material online. https://proroklab.github.io/CoViS-Net/",
                "authors": "J. Blumenkamp, Steven D. Morad, Jennifer Gielis, Amanda Prorok",
                "citations": 3
            },
            {
                "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
                "abstract": "Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.",
                "authors": "Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, Yu Qiao",
                "citations": 3
            },
            {
                "title": "Integration of Industry Foundation Classes and Ontology: Data, Applications, Modes, Challenges, and Opportunities",
                "abstract": "Industry Foundation Classes (IFCs), as the most recognized data schema for Building Information Modeling (BIM), are increasingly combined with ontology to facilitate data interoperability across the whole lifecycle in the Architecture, Engineering, Construction, and Facility Management (AEC/FM). This paper conducts a bibliometric analysis of 122 papers from the perspective of data, model, and application to summarize the modes of IFC and ontology integration (IFCOI). This paper first analyzes the data and models of the integration from IFC data formats and ontology development models to the IfcOWL data model. Next, the application status is summed up from objective and phase dimensions, and four frequent applications with maturity are identified. Based on the aforementioned multi-dimensional analysis, three integration modes are summarized, taking into account various data interoperability requirements. Accordingly, ontology behaves as the representation of domain knowledge, an enrichment tool for IFC model semantics, and a linkage between IFC data and other heterogeneous data. Finally, this paper points out the challenges and opportunities for IFCOI in the data, domain ontology, and integration process and proposes a building lifecycle management model based on IFCOI.",
                "authors": "Jing Jia, Hongxin Ma, Zijing Zhang",
                "citations": 3
            },
            {
                "title": "BrainWave: A Brain Signal Foundation Model for Clinical Applications",
                "abstract": "Neural electrical activity is fundamental to brain function, underlying a range of cognitive and behavioral processes, including movement, perception, decision-making, and consciousness. Abnormal patterns of neural signaling often indicate the presence of underlying brain diseases. The variability among individuals, the diverse array of clinical symptoms from various brain disorders, and the limited availability of diagnostic classifications, have posed significant barriers to formulating reliable model of neural signals for diverse application contexts. Here, we present BrainWave, the first foundation model for both invasive and non-invasive neural recordings, pretrained on more than 40,000 hours of electrical brain recordings (13.79 TB of data) from approximately 16,000 individuals. Our analysis show that BrainWave outperforms all other competing models and consistently achieves state-of-the-art performance in the diagnosis and identification of neurological disorders. We also demonstrate robust capabilities of BrainWave in enabling zero-shot transfer learning across varying recording conditions and brain diseases, as well as few-shot classification without fine-tuning, suggesting that BrainWave learns highly generalizable representations of neural signals. We hence believe that open-sourcing BrainWave will facilitate a wide range of clinical applications in medicine, paving the way for AI-driven approaches to investigate brain disorders and advance neuroscience research.",
                "authors": "Zhizhang Yuan, Fanqi Shen, Meng Li, Yuguo Yu, Chenhao Tan, Yang Yang",
                "citations": 3
            },
            {
                "title": "PROSE-FD: A Multimodal PDE Foundation Model for Learning Multiple Operators for Forecasting Fluid Dynamics",
                "abstract": "We propose PROSE-FD, a zero-shot multimodal PDE foundational model for simultaneous prediction of heterogeneous two-dimensional physical systems related to distinct fluid dynamics settings. These systems include shallow water equations and the Navier-Stokes equations with incompressible and compressible flow, regular and complex geometries, and different buoyancy settings. This work presents a new transformer-based multi-operator learning approach that fuses symbolic information to perform operator-based data prediction, i.e. non-autoregressive. By incorporating multiple modalities in the inputs, the PDE foundation model builds in a pathway for including mathematical descriptions of the physical behavior. We pre-train our foundation model on 6 parametric families of equations collected from 13 datasets, including over 60K trajectories. Our model outperforms popular operator learning, computer vision, and multi-physics models, in benchmark forward prediction tasks. We test our architecture choices with ablation studies.",
                "authors": "Yuxuan Liu, Jingmin Sun, Xinjie He, Griffin Pinney, Zecheng Zhang, Hayden Schaeffer",
                "citations": 3
            },
            {
                "title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters",
                "abstract": "We propose a novel framework for filtering image-text data by leveraging fine-tuned Multimodal Language Models (MLMs). Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in MLMs. We design four distinct yet complementary metrics to holistically measure the quality of image-text data. A new pipeline is established to construct high-quality instruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore, our MLM filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design choices for the MLM filter.",
                "authors": "Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng Yan, Heng Wang",
                "citations": 9
            },
            {
                "title": "Do GPT Language Models Suffer From Split Personality Disorder? The Advent Of Substrate-Free Psychometrics",
                "abstract": "Previous research on emergence in large language models shows these display apparent human-like abilities and psychological latent traits. However, results are partly contradicting in expression and magnitude of these latent traits, yet agree on the worrisome tendencies to score high on the Dark Triad of narcissism, psychopathy, and Machiavellianism, which, together with a track record of derailments, demands more rigorous research on safety of these models. We provided a state of the art language model with the same personality questionnaire in nine languages, and performed Bayesian analysis of Gaussian Mixture Model, finding evidence for a deeper-rooted issue. Our results suggest both interlingual and intralingual instabilities, which indicate that current language models do not develop a consistent core personality. This can lead to unsafe behaviour of artificial intelligence systems that are based on these foundation models, and are increasingly integrated in human life. We subsequently discuss the shortcomings of modern psychometrics, abstract it, and provide a framework for its species-neutral, substrate-free formulation.",
                "authors": "P. Romero, Stephen Fitz, Teruo Nakatsuma",
                "citations": 9
            },
            {
                "title": "Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology",
                "abstract": "Foundation models are rapidly being developed for computational pathology applications. However, it remains an open question which factors are most important for downstream performance with data scale and diversity, model size, and training algorithm all playing a role. In this work, we propose algorithmic modifications, tailored for pathology, and we present the result of scaling both data and model size, surpassing previous studies in both dimensions. We introduce three new models: Virchow2, a 632 million parameter vision transformer, Virchow2G, a 1.9 billion parameter vision transformer, and Virchow2G Mini, a 22 million parameter distillation of Virchow2G, each trained with 3.1 million histopathology whole slide images, with diverse tissues, originating institutions, and stains. We achieve state of the art performance on 12 tile-level tasks, as compared to the top performing competing models. Our results suggest that data diversity and domain-specific methods can outperform models that only scale in the number of parameters, but, on average, performance benefits from the combination of domain-specific methods, data scale, and model scale.",
                "authors": "Eric Zimmermann, Eugene Vorontsov, Julian Viret, Adam Casson, Michal Zelechowski, George Shaikovski, Neil Tenenholtz, James Hall, Thomas J Fuchs, Nicolò Fusi, Siqi Liu, Kristen Severson",
                "citations": 9
            },
            {
                "title": "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models",
                "abstract": "Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To fully revitalize the general-purpose token transition and multi-step generation capability of large language models, we propose AutoTimes to repurpose LLMs as autoregressive time series forecasters, which projects time series into the embedding space of language tokens and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the lookback length and scalability with larger LLMs. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed in-context forecasting. By introducing LLM-embedded textual timestamps, AutoTimes can utilize chronological information to align multivariate time series. Empirically, AutoTimes achieves state-of-the-art with 0.1% trainable parameters and over $5\\times$ training/inference speedup compared to advanced LLM-based forecasters. Code is available at this repository: https://github.com/thuml/AutoTimes.",
                "authors": "Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long",
                "citations": 9
            },
            {
                "title": "Birds, bats and beyond: evaluating generalization in bioacoustics models",
                "abstract": "In the context of passive acoustic monitoring (PAM) better models are needed to reliably gain insights from large amounts of raw, unlabeled data. Bioacoustics foundation models, which are general-purpose, adaptable models that can be used for a wide range of downstream tasks, are an effective way to meet this need. Measuring the capabilities of such models is essential for their development, but the design of robust evaluation procedures is a complex process. In this review we discuss a variety of fields that are relevant for the evaluation of bioacoustics models, such as sound event detection, machine learning metrics, and transfer learning (including topics such as few-shot learning and domain generalization). We contextualize these topics using the particularities of bioacoustics data, which is characterized by large amounts of noise, strong class imbalance, and distribution shifts (differences in the data between training and deployment stages). Our hope is that these insights will help to inform the design of evaluation protocols that can more accurately predict the ability of bioacoustics models to be deployed reliably in a wide variety of settings.",
                "authors": "Bart van Merriënboer, Jenny Hamer, Vincent Dumoulin, Eleni Triantafillou, Tom Denton",
                "citations": 9
            },
            {
                "title": "In-context learning enables multimodal large language models to classify cancer pathology images",
                "abstract": null,
                "authors": "Dyke Ferber, Georg Wolflein, I. Wiest, M. Ligero, Srividhya Sainath, Narmin Ghaffari Laleh, O. S. E. Nahhas, Gustav Muller-Franzes, Dirk Jager, Daniel Truhn, J. N. Kather",
                "citations": 9
            },
            {
                "title": "Decision Transformer as a Foundation Model for Partially Observable Continuous Control",
                "abstract": "Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools. Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior. To establish a general controller synthesis framework, we explore the Decision Transformer (DT) architecture. Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design. Then, we leverage the pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT) series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA). Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations (PDEs), demonstrate DT's capability to capture the parameter-agnostic structures intrinsic to control tasks. DT exhibits remarkable zero-shot generalization abilities for completely new tasks and rapidly surpasses expert performance levels with a minimal amount of demonstration data. These findings highlight the potential of DT as a foundational controller for general control applications.",
                "authors": "Xiangyuan Zhang, Weichao Mao, Haoran Qiu, Tamer Başar",
                "citations": 4
            },
            {
                "title": "LucaOne: Generalized Biological Foundation Model with Unified Nucleic Acid and Protein Language",
                "abstract": "In recent years, significant advancements have been observed in the domain of Natural Language Processing(NLP) with the introduction of pre-trained foundational models, paving the way for utilizing similar AI technologies to interpret the language of biology. In this research, we introduce “LucaOne”, a novel pre-trained foundational model designed to integratively learn from the genetic and proteomic languages, encapsulating data from 169,861 species en-compassing DNA, RNA, and proteins. This work illuminates the potential for creating a biological language model aimed at universal bioinformatics appli-cation. Remarkably, through few-shot learning, this model efficiently learns the central dogma of molecular biology and demonstrably outperforms com-peting models. Furthermore, in tasks requiring inputs of DNA, RNA, proteins, or a combination thereof, LucaOne exceeds the state-of-the-art performance using a streamlined downstream architecture, thereby providing empirical ev-idence and innovative perspectives on the potential of foundational models to comprehend complex biological systems.",
                "authors": "Yong He, Pan Fang, Yong-tao Shan, Yuanfei Pan, Yanhong Wei, Yichang Chen, Yihao Chen, Yi Liu, Zhenyu Zeng, Zhan Zhou, Feng Zhu, Edward C Holmes, Jieping Ye, Jun Li, Yuelong Shu, Mang Shi, Zhaorong Li",
                "citations": 5
            },
            {
                "title": "A cell atlas foundation model for scalable search of similar human cells.",
                "abstract": null,
                "authors": "Graham Heimberg, Tony Kuo, D. DePianto, Omar Salem, Tobias Heigl, Nathaniel Diamant, Gabriele Scalia, Tommasso Biancalani, Shannon J. Turley, Jason R. Rock, Héctor Corrada Bravo, Josh Kaminker, J. V. Vander Heiden, Aviv Regev",
                "citations": 4
            },
            {
                "title": "GP-MoLFormer: A Foundation Model For Molecular Generation",
                "abstract": "Transformer-based models trained on large and general purpose datasets consisting of molecular strings have recently emerged as a powerful tool for successfully modeling various structure-property relations. Inspired by this success, we extend the paradigm of training chemical language transformers on large-scale chemical datasets to generative tasks in this work. Specifically, we propose GP-MoLFormer, an autoregressive molecular string generator that is trained on more than 1.1B chemical SMILES. GP-MoLFormer uses a 46.8M parameter transformer decoder model with linear attention and rotary positional encodings as the base architecture. We explore the utility of GP-MoLFormer in generating novel, valid, and unique SMILES. Impressively, we find GP-MoLFormer is able to generate a significant fraction of novel, valid, and unique SMILES even when the number of generated molecules is in the 10 billion range and the reference set is over a billion. We also find strong memorization of training data in GP-MoLFormer generations, which has so far remained unexplored for chemical language models. Our analyses reveal that training data memorization and novelty in generations are impacted by the quality of the training data; duplication bias in training data can enhance memorization at the cost of lowering novelty. We evaluate GP-MoLFormer's utility and compare it with that of existing baselines on three different tasks: de novo generation, scaffold-constrained molecular decoration, and unconstrained property-guided optimization. While the first two are handled with no additional training, we propose a parameter-efficient fine-tuning method for the last task, which uses property-ordered molecular pairs as input. We call this new approach pair-tuning. Our results show GP-MoLFormer performs better or comparable with baselines across all three tasks, demonstrating its general utility.",
                "authors": "Jerret Ross, Brian M. Belgodere, Samuel C. Hoffman, V. Chenthamarakshan, Youssef Mroueh, Payel Das",
                "citations": 4
            },
            {
                "title": "Towards a Formal Foundation for Blockchain Rollups",
                "abstract": "Blockchains like Bitcoin and Ethereum have revolutionized digital transactions, yet scalability issues persist. Layer 2 solutions, such as validity proof Rollups (ZK-Rollups), aim to address these challenges by processing transactions off-chain and validating them on the main chain. However, concerns remain about security and censorship resistance, particularly regarding centralized control in Layer 2 and inadequate mechanisms for enforcing these properties through Layer 1 contracts. This work presents a formal analysis using the Alloy specification language to examine and design key Layer 2 functionalities, including forced transaction queues, safe blacklisting, and upgradeability. Through this analysis, we identify potential vulnerabilities in current mechanisms and propose enhanced models to strengthen security and censorship resistance, setting new standards for the security of rollups.",
                "authors": "Stefanos Chaliasos, Denis Firsov, Benjamin Livshits",
                "citations": 4
            },
            {
                "title": "PowerPM: Foundation Model for Power Systems",
                "abstract": "The emergence of abundant electricity time series (ETS) data provides ample opportunities for various applications in the power systems, including demand-side management, grid stability, and consumer behavior analysis. Deep learning models have advanced ETS modeling by effectively capturing sequence dependence. Nevertheless, learning a generic representation of ETS data for various applications remains challenging due to the inherently complex hierarchical structure of ETS data. Moreover, ETS data exhibits intricate temporal dependencies and is suscepti ble to the influence of exogenous variables. Furthermore, different instances exhibit diverse electricity consumption behavior. In this paper, we propose a foundation model PowerPM to model ETS data, providing a large-scale, off-the-shelf model for power systems. PowerPM consists of a temporal encoder and a hierarchical encoder. The temporal encoder captures both temporal dependencies in ETS data, considering exogenous variables. The hierarchical encoder models the correlation between hierarchy. Furthermore, PowerPM leverages a novel self-supervised pretraining framework consisting of masked ETS modeling and dual-view contrastive learning, which enable PowerPM to capture temporal dependency within ETS windows and aware the discrepancy across ETS windows, providing two different perspectives to learn generic representation. Our experiments involve five real world scenario datasets, comprising private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Impressively, when transferred to the public datasets, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Moreover, ablation studies, few-shot experiments provide additional evidence of the effectiveness of our model.",
                "authors": "Shihao Tu, Yupeng Zhang, Jing Zhang, Yang Yang",
                "citations": 2
            },
            {
                "title": "Towards a Wireless Physical-Layer Foundation Model: Challenges and Strategies",
                "abstract": "Artificial intelligence (AI) plays an important role in the dynamic landscape of wireless communications, solving challenges unattainable by traditional approaches. This paper discusses the evolution of wireless AI, emphasizing the transition from isolated task-specific models to more generalizable and adaptable AI models inspired by recent successes in large language models (LLMs) and computer vision. To overcome task-specific AI strategies in wireless networks, we propose a unified wireless physical-layer foundation model (WPFM). Challenges include the design of effective pre-training tasks, support for embedding heterogeneous time series and human-understandable interaction. The paper presents a strategic framework, focusing on embedding wireless time series, self-supervised pre-training, and semantic representation learning. The proposed WPFM aims to understand and describe diverse wireless signals, allowing human interactivity with wireless networks. The paper concludes by outlining next research steps for WPFMs, including the integration with LLMs.",
                "authors": "Jaron Fontaine, Adnan Shahid, E. D. Poorter",
                "citations": 2
            },
            {
                "title": "DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology",
                "abstract": "In hematology, computational models offer significant potential to improve diagnostic accuracy, streamline workflows, and reduce the tedious work of analyzing single cells in peripheral blood or bone marrow smears. However, clinical adoption of computational models has been hampered by the lack of generalization due to large batch effects, small dataset sizes, and poor performance in transfer learning from natural images. To address these challenges, we introduce DinoBloom, the first foundation model for single cell images in hematology, utilizing a tailored DINOv2 pipeline. Our model is built upon an extensive collection of 13 diverse, publicly available datasets of peripheral blood and bone marrow smears, the most substantial open-source cohort in hematology so far, comprising over 380,000 white blood cell images. To assess its generalization capability, we evaluate it on an external dataset with a challenging domain shift. We show that our model outperforms existing medical and non-medical vision models in (i) linear probing and k-nearest neighbor evaluations for cell-type classification on blood and bone marrow smears and (ii) weakly supervised multiple instance learning for acute myeloid leukemia subtyping by a large margin. A family of four DinoBloom models (small, base, large, and giant) can be adapted for a wide range of downstream applications, be a strong baseline for classification problems, and facilitate the assessment of batch effects in new datasets. All models are available at github.com/marrlab/DinoBloom.",
                "authors": "Valentin Koch, S. Wagner, Salome Kazeminia, Ece Sancar, Matthias Hehr, Julia A. Schnabel, Tingying Peng, Carsten Marr",
                "citations": 2
            },
            {
                "title": "Centaur: a foundation model of human cognition",
                "abstract": "Establishing a unified theory of cognition has been a major goal of psychology. While there have been previous attempts to instantiate such theories by building computational models, we currently do not have one model that captures the human mind in its entirety. Here we introduce Centaur, a computational model that can predict and simulate human behavior in any experiment expressible in natural language. We derived Centaur by finetuning a state-of-the-art language model on a novel, large-scale data set called Psych-101. Psych-101 reaches an unprecedented scale, covering trial-by-trial data from over 60,000 participants performing over 10,000,000 choices in 160 experiments. Centaur not only captures the behavior of held-out participants better than existing cognitive models, but also generalizes to new cover stories, structural task modifications, and entirely new domains. Furthermore, we find that the model's internal representations become more aligned with human neural activity after finetuning. Taken together, Centaur is the first real candidate for a unified model of human cognition. We anticipate that it will have a disruptive impact on the cognitive sciences, challenging the existing paradigm for developing computational models.",
                "authors": "Marcel Binz, Elif Akata, Matthias Bethge, Franziska Brandle, Frederick Callaway, Julian Coda-Forno, Peter Dayan, Can Demircan, Maria K. Eckstein, No'emi 'EltetHo, Thomas L. Griffiths, Susanne Haridi, Akshay Jagadish, Ji-An Li, Alex Kipnis, Sreejan Kumar, Tobias Ludwig, Marvin Mathony, Marcelo Mattar, Alireza Modirshanechi, Surabhi S. Nath, Joshua C. Peterson, Milena Rmuš, Evan M Russek, Tankred Saanum, Natalia Scharfenberg, Johannes A. Schubert, Luca M. Schulze Buschoff, Nishad Singhi, Xin Sui, Mirko Thalmann, Fabian Theis, Vuong Truong, Vishaal Udandarao, Konstantinos Voudouris, Robert Wilson, Kristin Witte, Shuchen Wu, Dirk Wulff, Huadong Xiong, Eric Schulz",
                "citations": 3
            },
            {
                "title": "Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost",
                "abstract": "Retriever Augmented Generation (RAG) systems have become pivotal in enhancing the capabilities of language models by incorporating external knowledge retrieval mechanisms. However, a significant challenge in deploying these systems in industry applications is the detection and mitigation of hallucinations: instances where the model generates information that is not grounded in the retrieved context. Addressing this issue is crucial for ensuring the reliability and accuracy of responses generated by large language models (LLMs) in diverse industry settings. Current hallucination detection techniques fail to deliver accuracy, low latency, and low cost simultaneously. We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and commercial evaluation frameworks on the hallucination detection task, with 97% and 91% reduction in cost and latency, respectively. Luna is lightweight and generalizes across multiple industry verticals and out-of-domain data, making it an ideal candidate for industry LLM applications.",
                "authors": "Masha Belyi, Robert Friel, Shuai Shao, Atindriyo Sanyal",
                "citations": 3
            },
            {
                "title": "MMA: Multi-Modal Adapter for Vision-Language Models",
                "abstract": "Pretrained Vision-Language Models (VLMs) have served as excellent foundation models for transfer learning in diverse downstream tasks. However, tuning VLMs for few-shot generalization tasks faces a discrimination - generalization dilemma, i.e., general knowledge should be preserved and task-specific knowledge should be fine-tuned. How to precisely identify these two types of representations remains a challenge. In this paper, we propose a Multi-Modal Adapter (MMA) for VLMs to improve the alignment between representations from text and vision branches. MMA aggregates features from different branches into a shared feature space so that gradients can be communicated across branches. To determine how to incorporate MMA, we systematically analyze the discriminability and generalizability of features across diverse datasets in both the vision and language branches, and find that (1) higher lay-ers contain discriminable dataset-specific knowledge, while lower layers contain more generalizable knowledge, and (2) language features are more discriminable than visual features, and there are large semantic gaps between the features of the two modalities, especially in the lower layers. Therefore, we only incorporate MMA to a few higher lay-ers of transformers to achieve an optimal balance between discrimination and generalization. We evaluate the effectiveness of our approach on three tasks: generalization to novel classes, novel target datasets, and domain generalization. Compared to many state-of-the-art methods, our MMA achieves leading performance in all evaluations. Code is at https://github.com/ZjjConan/Multi-Modal-Adapter",
                "authors": "Lingxiao Yang, Ru-Yuan Zhang, Yanchen Wang, Xiaohua Xie",
                "citations": 8
            },
            {
                "title": "Towards a foundation large events model for soccer",
                "abstract": null,
                "authors": "Tiago Mendes-Neves, Lu'is Meireles, João Mendes-Moreira",
                "citations": 2
            },
            {
                "title": "Exploring financing models for clean energy adoption: Lessons from the United States and Nigeria",
                "abstract": "This paper examines financing models for clean energy adoption, drawing insights from experiences in the United States and Nigeria. It underscores the significance of clean energy and the need to explore effective financing mechanisms to facilitate its widespread adoption. Through an analysis of the current state of clean energy adoption in both countries, existing initiatives and challenges are outlined. Lessons gleaned from successful financing models in the United States, including government incentives, public-private partnerships, and renewable energy credits, are then explored. These lessons serve as a foundation for assessing how similar approaches can be adapted to the Nigerian context. Specific attention is given to the unique challenges facing Nigeria, such as infrastructure limitations and socioeconomic disparities, and how financing models can be tailored to address these obstacles. A comparative analysis between the two countries identifies similarities, differences, and the potential transferability of financing models. Based on these insights, recommendations are proposed for Nigeria, emphasizing the importance of context-specific approaches to accelerate clean energy adoption. Overall, this paper provides a comprehensive exploration of financing models for clean energy adoption, offering valuable insights for policymakers, stakeholders, and practitioners in both the United States and Nigeria.",
                "authors": "Portia Oduro, Peter Simpa, Darlington Eze Ekechukwu",
                "citations": 11
            },
            {
                "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models",
                "abstract": "Big models have achieved revolutionary breakthroughs in the field of AI, but they also pose potential ethical and societal risks to humans. Addressing such problems, alignment technologies were introduced to make these models conform to human preferences and values. Despite the considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question. In this survey paper, we comprehensively investigate value alignment approaches. We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges. Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: RL-based Alignment, SFT-based Alignment, and Inference-Time Alignment, and demonstrate their intrinsic connections, strengths, and limitations, helping readers better understand this research area. In addition, two emerging topics, alignment goal and multimodal alignment, are also discussed as novel frontiers in the field. Looking forward, we discuss potential alignment paradigms and how they could handle remaining challenges, prospecting where future alignment will go.",
                "authors": "Xinpeng Wang, Shitong Duan, Xiaoyuan Yi, Jing Yao, Shanlin Zhou, Zhihua Wei, Peng Zhang, Dongkuan Xu, Maosong Sun, Xing Xie",
                "citations": 10
            },
            {
                "title": "NetLLM: Adapting Large Language Models for Networking",
                "abstract": "Many networking tasks now employ deep learning (DL) to solve complex prediction and optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments. Motivated by the recent success of large language models (LLMs), this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the powerful pre-trained knowledge, the LLM is promising to serve as the foundation model to achieve\"one model for all tasks\"with even better performance and stronger generalization. In pursuit of this vision, we present NetLLM, the first framework that provides a coherent design to harness the powerful capabilities of LLMs with low efforts to solve networking problems. Specifically, NetLLM empowers the LLM to effectively process multimodal data in networking and efficiently generate task-specific answers. Besides, NetLLM drastically reduces the costs of fine-tuning the LLM to acquire domain knowledge for networking. Across three networking-related use cases - viewport prediction, adaptive bitrate streaming and cluster job scheduling, we showcase that the NetLLM-adapted LLM significantly outperforms state-of-the-art algorithms.",
                "authors": "Duo Wu, Xianda Wang, Yaqi Qiao, Zhi Wang, Junchen Jiang, Shuguang Cui, Fangxin Wang",
                "citations": 10
            },
            {
                "title": "SymbolicAI: A framework for logic-based approaches combining generative models and solvers",
                "abstract": "We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for multi-modal data that connects multi-step generative processes and aligns their outputs with user objectives in complex workflows. As a result, we can transition between the capabilities of various foundation models with in-context learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. Through these operations based on in-context learning our framework enables the creation and evaluation of explainable computational graphs. Finally, we introduce a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the\"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short. The framework codebase and benchmark are linked below.",
                "authors": "Marius-Constantin Dinu, Claudiu Leoveanu-Condrei, Markus Holzleitner, Werner Zellinger, Sepp Hochreiter",
                "citations": 6
            },
            {
                "title": "Medical education with large language models in ophthalmology: custom instructions and enhanced retrieval capabilities",
                "abstract": "Foundation models are the next generation of artificial intelligence that has the potential to provide novel use cases for healthcare. Large language models (LLMs), a type of foundation model, are capable of language comprehension and the ability to generate human-like text. Researchers and developers have been tuning LLMs to optimise their performance in specific tasks, such as medical challenge problems. Until recently, tuning required technical programming expertise, but the release of custom generative pre-trained transformers (GPTs) by OpenAI has allowed users to tune their own GPTs with natural language. This has the potential to democratise access to high-quality bespoke LLMs globally. In this review, we provide an overview of LLMs, how they are tuned and how custom GPTs work. We provide three use cases of custom GPTs in ophthalmology to demonstrate the versatility and effectiveness of these tools. First, we present ‘EyeTeacher’, an educational aid that generates questions from clinical guidelines to facilitate learning. Second, we built ‘EyeAssistant’, a clinical support tool that is tuned with clinical guidelines to respond to various physician queries. Lastly, we design ‘The GPT for GA’, which offers clinicians a comprehensive summary of emerging management strategies for geographic atrophy by analysing peer-reviewed documents. The review underscores the significance of custom instructions and information retrieval in tuning GPTs for specific tasks in ophthalmology. We also discuss the evaluation of LLM responses and address critical aspects such as privacy and accountability in their clinical application. Finally, we discuss their potential in ophthalmic education and clinical practice.",
                "authors": "Mertcan Sevgi, F. Antaki, P. Keane",
                "citations": 7
            },
            {
                "title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
                "abstract": "This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models.",
                "authors": "Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo Su, Bo Zheng",
                "citations": 6
            },
            {
                "title": "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models",
                "abstract": "While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments reveal the challenging nature of such problems for MLLMs while showcasing the immense gap between open-source and closed-source models. We also uncover critical shortcomings of visual and textual perceptions, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experiment with different methods, such as Chain-of-Thought prompting, leading to a significant (up to 100%) boost in performance. Our code and datasets are available at https://github.com/usc-isi-i2/isi-mmlm-rpm.",
                "authors": "Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, Jay Pujara",
                "citations": 7
            },
            {
                "title": "A Lost Opportunity for Vision-Language Models: A Comparative Study of Online Test-time Adaptation for Vision-Language Models",
                "abstract": "In deep learning, maintaining model robustness against distribution shifts is critical. This work explores a broad range of possibilities to adapt vision-language foundation models at test-time, with a particular emphasis on CLIP and its variants. The study systematically examines prompt-based techniques and existing test-time adaptation methods, aiming to improve the robustness under distribution shift in diverse real-world scenarios. Specifically, the investigation covers various prompt engineering strategies, including handcrafted prompts, prompt ensembles, and prompt learning techniques. Additionally, we introduce a vision-text-space ensemble that substantially enhances average performance compared to text-space-only ensembles. Since online test-time adaptation has shown to be effective to mitigate performance drops under distribution shift, the study extends its scope to evaluate the effectiveness of existing test-time adaptation methods that were originally designed for vision-only classification models. Through extensive experimental evaluations conducted across multiple datasets and diverse model architectures, the research demonstrates the effectiveness of these adaptation strategies. Code is available at: https://github.com/mariodoebler/test-time-adaptation",
                "authors": "Mario Döbler, Robert A. Marsden, Tobias Raichle, Bin Yang",
                "citations": 4
            },
            {
                "title": "SpatialBot: Precise Spatial Understanding with Vision Language Models",
                "abstract": "Vision Language Models (VLMs) have achieved impressive performance in 2D image understanding, however they are still struggling with spatial understanding which is the foundation of Embodied AI. In this paper, we propose SpatialBot for better spatial understanding by feeding both RGB and depth images. Additionally, we have constructed the SpatialQA dataset, which involves multi-level depth-related questions to train VLMs for depth understanding. Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilities in spatial understanding at different levels. Extensive experiments on our spatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks, demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The model, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.",
                "authors": "Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao",
                "citations": 9
            },
            {
                "title": "Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models",
                "abstract": "It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly analyze this new threat. Our findings highlight a critical privacy concern within the machine learning community and call for a reevaluation of safety protocols in the use of open-source pre-trained models.",
                "authors": "Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, Nicholas Carlini",
                "citations": 9
            },
            {
                "title": "BEACON: Benchmark for Comprehensive RNA Tasks and Language Models",
                "abstract": "RNA plays a pivotal role in translating genetic instructions into functional outcomes, underscoring its importance in biological processes and disease mechanisms. Despite the emergence of numerous deep learning approaches for RNA, particularly universal RNA language models, there remains a significant lack of standardized benchmarks to assess the effectiveness of these methods. In this study, we introduce the first comprehensive RNA benchmark BEACON (BEnchmArk for COmprehensive RNA Task and Language Models). First, BEACON comprises 13 distinct tasks derived from extensive previous work covering structural analysis, functional studies, and engineering applications, enabling a comprehensive assessment of the performance of methods on various RNA understanding tasks. Second, we examine a range of models, including traditional approaches like CNNs, as well as advanced RNA foundation models based on language models, offering valuable insights into the task-specific performances of these models. Third, we investigate the vital RNA language model components from the tokenizer and positional encoding aspects. Notably, our findings emphasize the superiority of single nucleotide tokenization and the effectiveness of Attention with Linear Biases (ALiBi) over traditional positional encoding methods. Based on these insights, a simple yet strong baseline called BEACON-B is proposed, which can achieve outstanding performance with limited data and computational resources. The datasets and source code of our benchmark are available at https://github.com/terry-r123/RNABenchmark.",
                "authors": "Yuchen Ren, Zhiyuan Chen, Lifeng Qiao, Hongtai Jing, Yuchen Cai, Sheng Xu, Peng Ye, Xinzhu Ma, Siqi Sun, Hongliang Yan, Dong Yuan, Wanli Ouyang, Xihui Liu",
                "citations": 5
            },
            {
                "title": "HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology Datasets with Foundational Embedding Models",
                "abstract": "Developing accurate machine learning models for oncology requires large-scale, high-quality multimodal datasets. However, creating such datasets remains challenging due to the complexity and heterogeneity of medical data. To address this challenge, we introduce HoneyBee, a scalable modular framework for building multimodal oncology datasets that leverages foundation models to generate representative embeddings. HoneyBee integrates various data modalities, including clinical diagnostic and pathology imaging data, medical notes, reports, records, and molecular data. It employs data preprocessing techniques and foundation models to generate embeddings that capture the essential features and relationships within the raw medical data. The generated embeddings are stored in a structured format using Hugging Face datasets and PyTorch dataloaders for accessibility. Vector databases enable efficient querying and retrieval for machine learning applications. We demonstrate the effectiveness of HoneyBee through experiments assessing the quality and representativeness of these embeddings. The framework is designed to be extensible to other medical domains and aims to accelerate oncology research by providing high-quality, machine learning-ready datasets. HoneyBee is an ongoing open-source effort, and the code, datasets, and models are available at the project repository.",
                "authors": "Aakash Tripathi, Asim Waqas, Yasin Yilmaz, Ghulam Rasool",
                "citations": 4
            },
            {
                "title": "Explore In-Context Segmentation via Latent Diffusion Models",
                "abstract": "In-context segmentation has drawn more attention with the introduction of vision foundation models. Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual prompts and input image queries. In this work, we explore this problem from a new perspective, using one representative generation model, the latent diffusion model (LDM). We observe a task gap between generation and segmentation in diffusion models, but LDM is still an effective minimalist for in-context segmentation. In particular, we propose two meta-architectures and correspondingly design several output alignment and optimization strategies. We have conducted comprehensive ablation studies and empirically found that the segmentation quality counts on output alignment and in-context instructions. Moreover, we build a new and fair in-context segmentation benchmark that includes both image and video datasets. Experiments validate the efficiency of our approach, demonstrating comparable or even stronger results than previous specialist models or visual foundation models. Our study shows that LDMs can also achieve good enough results for challenging in-context segmentation tasks.",
                "authors": "Chaoyang Wang, Xiangtai Li, Henghui Ding, Lu Qi, Jiangning Zhang, Yunhai Tong, Chen Change Loy, Shuicheng Yan",
                "citations": 5
            },
            {
                "title": "ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models",
                "abstract": "In robot manipulation tasks with large observation and action spaces, reinforcement learning (RL) often suffers from low sample efficiency and uncertain convergence. As an alternative, foundation models have shown promise in zero-shot and few-shot applications. However, these models can be unreliable due to their limited reasoning and challenges in understanding physical and spatial contexts. This paper introduces ExploRLLM, a method that combines the commonsense reasoning of foundation models with the experiential learning capabilities of RL. We leverage the strengths of both paradigms by using foundation models to obtain a base policy, an efficient representation, and an exploration policy. A residual RL agent learns when and how to deviate from the base policy while its exploration is guided by the exploration policy. In table-top manipulation experiments, we demonstrate that ExploRLLM outperforms both baseline foundation model policies and baseline RL policies. Additionally, we show that this policy can be transferred to the real world without further training. Supplementary material is available at https://explorllm.github.io.",
                "authors": "Runyu Ma, Jelle Luijkx, Zlatan Ajanović, Jens Kober",
                "citations": 4
            },
            {
                "title": "Using Large Language Models to Understand Telecom Standards",
                "abstract": "The Third Generation Partnership Project (3GPP) has successfully introduced standards for global mobility. However, the volume and complexity of these standards has increased over time, thus complicating access to relevant information for vendors and service providers. Use of Generative AI and in particular Large Language Models, may provide faster access to relevant information. In this paper, we evaluate the capability of state-of-art models to be used as Question-Answering assistants for technical standards document reference. Our contribution is threefold. First, we provide a benchmark and measuring method for evaluating performance of Large Language Models. Second, we do data preprocessing and finetuning for one of these models and provide guidelines to increase model performance. Third, we provide a model of our own, TeleRoBERTa, that performs on-par with foundation models but with an order of magnitude less number of parameters. Results show that Large Language Models can be used as a credible reference tool on telecom technical documents, and thus have potential for a number of different applications from troubleshooting and maintenance, to network operations and software product development.",
                "authors": "Athanasios Karapantelakis, Mukesh Shakur, Alexandros Nikou, Farnaz Moradi, Christian Orlog, Fitsum Gaim, Henrik Holm, Doumitrou Daniil Nimara, Vincent Huang",
                "citations": 5
            },
            {
                "title": "PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models",
                "abstract": "We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a given observation). Our study demonstrates the potential for using generalist foundation models rather than task-specific models for interacting with astronomical data by leveraging text as an interface.",
                "authors": "S. Mishra-Sharma, Yiding Song, Jesse Thaler",
                "citations": 5
            },
            {
                "title": "Measuring Vision-Language STEM Skills of Neural Models",
                "abstract": "We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance. To understand and increase the performance on our dataset, we teach the models on a training split of our dataset. Even though we observe improved performance, the model performance remains relatively low compared to average elementary students. To solve STEM problems, we will need novel algorithmic innovations from the community.",
                "authors": "Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, Chenguang Wang",
                "citations": 5
            },
            {
                "title": "Augmenting Large Language Models with Rules for Enhanced Domain-Specific Interactions: The Case of Medical Diagnosis",
                "abstract": "In this paper, we present a novel Artificial Intelligence (AI) -empowered system that enhances large language models and other machine learning tools with rules to provide primary care diagnostic advice to patients. Specifically, we introduce a novel methodology, represented through a process diagram, which allows the definition of generative AI processes and functions with a focus on the rule-augmented approach. Our methodology separates various components of the generative AI process as blocks that can be used to generate an implementation data flow diagram. Building upon this framework, we utilize the concept of a dialogue process as a theoretical foundation. This is specifically applied to the interactions between a user and an AI-empowered software program, which is called “Med|Primary AI assistant” (Alpha Version at the time of writing), and provides symptom analysis and medical advice in the form of suggested diagnostics. By leveraging current advancements in natural language processing, a novel approach is proposed to define a blueprint of domain-specific knowledge and a context for instantiated advice generation. Our approach not only encompasses the interaction domain, but it also delves into specific content that is relevant to the user, offering a tailored and effective AI–user interaction experience within a medical context. Lastly, using an evaluation process based on rules, defined by context and dialogue theory, we outline an algorithmic approach to measure content and responses.",
                "authors": "Dimitrios P. Panagoulias, M. Virvou, G. Tsihrintzis",
                "citations": 9
            },
            {
                "title": "iVideoGPT: Interactive VideoGPTs are Scalable World Models",
                "abstract": "World models empower model-based agents to interactively explore, reason, and plan within imagined environments for real-world decision-making. However, the high demand for interactivity poses challenges in harnessing recent advancements in video generative models for developing world models at scale. This work introduces Interactive VideoGPT (iVideoGPT), a scalable autoregressive transformer framework that integrates multimodal signals--visual observations, actions, and rewards--into a sequence of tokens, facilitating an interactive experience of agents via next-token prediction. iVideoGPT features a novel compressive tokenization technique that efficiently discretizes high-dimensional visual observations. Leveraging its scalable architecture, we are able to pre-train iVideoGPT on millions of human and robotic manipulation trajectories, establishing a versatile foundation that is adaptable to serve as interactive world models for a wide range of downstream tasks. These include action-conditioned video prediction, visual planning, and model-based reinforcement learning, where iVideoGPT achieves competitive performance compared with state-of-the-art methods. Our work advances the development of interactive general world models, bridging the gap between generative video models and practical model-based reinforcement learning applications. Code and pre-trained models are available at https://thuml.github.io/iVideoGPT.",
                "authors": "Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, Mingsheng Long",
                "citations": 9
            },
            {
                "title": "Augmenting Sentiment Analysis Prediction in Binary Text Classification through Advanced Natural Language Processing Models and Classifiers",
                "abstract": "Sentiment analysis is a critical component in natural language processing applications, particularly for text classification. By employing state-of-the-art techniques such as ensemble methods, transfer learning and deep learning architectures, our methodology significantly enhances the robustness and precision of sentiment predictions. We systematically investigate the impact of various NLP models, including recurrent neural networks and transformer-based architectures, on sentiment classification tasks. Furthermore, we introduce a novel ensemble method that combines the strengths of multiple classifiers to improve the predictive ability of the system. The results demonstrate the potential of integrating state-of-the-art Natural Language Processing (NLP) models with ensemble classifiers to advance sentiment analysis. This lays the foundation for a more advanced comprehension of textual sentiments in diverse applications.",
                "authors": "Zhengbing Hu, I. Dychka, Kateryna Potapova, Vasyl Meliukh",
                "citations": 9
            },
            {
                "title": "Position: Leverage Foundational Models for Black-Box Optimization",
                "abstract": "Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision. Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research. However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration. In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature. We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces.",
                "authors": "Xingyou Song, Yingtao Tian, Robert Tjarko Lange, Chansoo Lee, Yujin Tang, Yutian Chen",
                "citations": 3
            },
            {
                "title": "A Comparison of Parameter-Efficient ASR Domain Adaptation Methods for Universal Speech and Language Models",
                "abstract": "A recent paradigm shift in artificial intelligence has seen the rise of foundation models, such as the large language models and the universal speech models. With billions of model parameters and trained with a wide range of data, these foundation models are expected to have a better generalization to different downstream tasks. Efficient adaptation is the key to leveraging these foundation models in a new task or domain. In this paper, we compare several popular parameter-efficient tuning methods, such as vector adaptation, residual adapters, low-rank adapter (LoRA) and prompt-tuning, for automatic speech recognition (ASR) domain adaptation. We use the connectionist temporal classification (CTC) model with Conformer encoder and fused it with a universal language model. We study the effect of adapting either or both of the Conformer encoder and the universal language model. We carry out extensive experiments to study these methods under different hyper-parameter settings and the effect of combining some of these methods. We find that combining vector adaptation and residual adapters with increasing bottleneck dimension achieved the best performance.",
                "authors": "K. Sim, Zhouyuan Huo, Tsendsuren Munkhdalai, Nikhil Siddhartha, Adam Stooke, Zhong Meng, Bo Li, Tara N. Sainath",
                "citations": 3
            },
            {
                "title": "Pretraining Billion-Scale Geospatial Foundational Models on Frontier",
                "abstract": "As AI workloads increase in scope, generalization capability becomes challenging for small task-specific models and their demand for large amounts of labeled training samples increases. On the contrary, Foundation Models (FMs) are trained with internet-scale unlabeled data via self-supervised learning and have been shown to adapt to various tasks with minimal fine-tuning. Although large FMs have demonstrated significant impact in natural language processing and computer vision, efforts toward FMs for geospatial applications have been restricted to smaller size models, as pretraining larger models requires very large computing resources equipped with state-of-the-art hardware accelerators. Current satellite constellations collect 100+TBs of data a day, resulting in images that are billions of pixels and multimodal in nature. Such geospatial data poses unique challenges opening up new opportunities to develop FMs. We investigate billion scale FMs and HPC training profiles for geospatial applications by pretraining on publicly available data. We studied from end-to-end the performance and impact in the solution by scaling the model size. Our larger 3B parameter size model achieves up to 30% improvement in top1 scene classification accuracy when comparing a 100M parameter model. Moreover, we detail performance experiments on the Frontier supercomputer, America's first exascale system, where we study different model and data parallel approaches using PyTorch's Fully Sharded Data Parallel library. Specifically, we study variants of the Vision Transformer architecture (ViT), conducting performance analysis for ViT models with size up to 15B parameters. By discussing throughput and performance bottlenecks under different parallelism configurations, we offer insights on how to leverage such leadership-class HPC resources when developing large models for geospatial imagery applications.",
                "authors": "A. Tsaris, P. Dias, Abhishek Potnis, Junqi Yin, Feiyi Wang, D. Lunga",
                "citations": 3
            },
            {
                "title": "Sensor2Scene: Foundation Model-Driven Interactive Realities",
                "abstract": "Augmented Reality (AR) is acclaimed for its potential to bridge the physical and virtual worlds. Yet, current integration between these realms often lacks a deep under-standing of the physical environment and the subsequent scene generation that reflects this understanding. This research introduces Sensor2Scene, a novel system framework designed to enhance user interactions with sensor data through AR. At its core, an AI agent leverages large language models (LLMs) to decode subtle information from sensor data, constructing detailed scene descriptions for visualization. To enable these scenes to be rendered in AR, we decompose the scene creation process into tasks of text-to-3D model generation and spatial composition, allowing new AR scenes to be sketched from the descriptions. We evaluated our framework using an LLM evaluator based on five metrics on various datasets to examine the correlation between sensor readings and corresponding visualizations, and demonstrated the system's effectiveness with scenes generated from end-to-end. The results highlight the potential of LLMs to understand IoT sensor data. Furthermore, generative models can aid in transforming these interpretations into visual formats, thereby enhancing user interaction. This work not only displays the capabilities of Sensor2Scene but also lays a foundation for advancing AR with the goal of creating more immersive and contextually rich experiences.",
                "authors": "Yunqi Guo, Kaiyuan Hou, Zhenyu Yan, Hongkai Chen, Guoliang Xing, Xiaofan Jiang",
                "citations": 0
            },
            {
                "title": "Large Language Models and Video Games: A Preliminary Scoping Review",
                "abstract": "Large language models (LLMs) hold interesting potential for the design, development, and research of video games. Building on the decades of prior research on generative AI in games, many researchers have sped to investigate the power and potential of LLMs for games. Given the recent spike in LLM-related research in games, there is already a wealth of relevant research to survey. In order to capture a snapshot of the state of LLM research in games, and to help lay the foundation for future work, we carried out an initial scoping review of relevant papers published so far. In this paper, we review 76 papers published between 2022 to early 2024 on LLMs and video games, with key focus areas in game AI, game development, narrative, and game research and reviews. Our paper provides an early state of the field and lays the groundwork for future research and reviews on this topic.",
                "authors": "Penny Sweetser",
                "citations": 8
            },
            {
                "title": "Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models",
                "abstract": "Text-to-image diffusion models have shown remarkable success in generating personalized subjects based on a few reference images. However, current methods often fail when generating multiple subjects simultaneously, resulting in mixed identities with combined attributes from different subjects. In this work, we present MuDI, a novel framework that enables multi-subject personalization by effectively decoupling identities from multiple subjects. Our main idea is to utilize segmented subjects generated by a foundation model for segmentation (Segment Anything) for both training and inference, as a form of data augmentation for training and initialization for the generation process. Moreover, we further introduce a new metric to better evaluate the performance of our method on multi-subject personalization. Experimental results show that our MuDI can produce high-quality personalized images without identity mixing, even for highly similar subjects as shown in Figure 1. Specifically, in human evaluation, MuDI obtains twice the success rate for personalizing multiple subjects without identity mixing over existing baselines and is preferred over 70% against the strongest baseline.",
                "authors": "Sang-Sub Jang, Jaehyeong Jo, Kimin Lee, Sung Ju Hwang",
                "citations": 6
            },
            {
                "title": "Timo: Towards Better Temporal Reasoning for Language Models",
                "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot generalize to a wider spectrum of temporal reasoning tasks. Therefore, we propose a crucial question: Can we build a universal framework to handle a variety of temporal reasoning tasks? To that end, we systematically study 38 temporal reasoning tasks. Based on the observation that 19 tasks are directly related to mathematics, we first leverage the available mathematical dataset to set a solid foundation for temporal reasoning. However, the in-depth study indicates that focusing solely on mathematical enhancement falls short of addressing pure temporal reasoning tasks. To mitigate this limitation, we propose a simple but effective self-critic temporal optimization method to enhance the model's temporal reasoning capabilities without sacrificing general task abilities. Finally, we develop Timo, a model designed to excel in temporal reasoning at the 7B and 13B scales. Notably, Timo outperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and achieves the new state-of-the-art (SOTA) performance of comparable size. Extensive experiments further validate our framework's effectiveness and its generalization across diverse temporal tasks. The code is available at https://github.com/zhaochen0110/Timo.",
                "authors": "Zhao-yu Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, Yu Cheng",
                "citations": 7
            },
            {
                "title": "Time-SSM: Simplifying and Unifying State Space Models for Time Series Forecasting",
                "abstract": "State Space Models (SSMs) have emerged as a potent tool in sequence modeling tasks in recent years. These models approximate continuous systems using a set of basis functions and discretize them to handle input data, making them well-suited for modeling time series data collected at specific frequencies from continuous systems. Despite its potential, the application of SSMs in time series forecasting remains underexplored, with most existing models treating SSMs as a black box for capturing temporal or channel dependencies. To address this gap, this paper proposes a novel theoretical framework termed Dynamic Spectral Operator, offering more intuitive and general guidance on applying SSMs to time series data. Building upon our theory, we introduce Time-SSM, a novel SSM-based foundation model with only one-seventh of the parameters compared to Mamba. Various experiments validate both our theoretical framework and the superior performance of Time-SSM.",
                "authors": "Jiaxi Hu, Disen Lan, Ziyu Zhou, Qingsong Wen, Yuxuan Liang",
                "citations": 6
            },
            {
                "title": "Unusual shocks in our usual models",
                "abstract": "We propose an event-study research design to identify the nature and propagation of large unusual shocks in DSGE models and apply it to study the macroeconomic effects of the Covid shock. The initial outbreak is represented as the onset of a new shock process where the shock loads on wedges associated with the model’s usual shocks. Realizations of the Covid shock come with news about its propagation, allowing us to disentangle the role of beliefs about the future of the pandemic. The model attributes a crucial role to the novel Covid shock in explaining the large contraction in output in the second quarter of 2020 and the rebound in growth expected at the same time. The Covid shock loads significantly on wedges that generate both demand and supply effects but, on net, supply forces dominate. The effects of Covid on hours worked are quite persistent, although the successive pandemic waves (e.g., the Delta wave) have a progressively smaller impact on the macroeconomy. Our methods provide a foundation to estimate structural models with data that include the pandemic without having to specify a micro-founded epidemiological block. JEL Classification Numbers: C51, E10, E31, E32, E52",
                "authors": "Filippo Ferroni, Jonas D. M. Fisher, Leonardo Melosi",
                "citations": 6
            },
            {
                "title": "Do Large Language Models Show Human-like Biases? Exploring Confidence - Competence Gap in AI",
                "abstract": "This study investigates self-assessment tendencies in Large Language Models (LLMs), examining if patterns resemble human cognitive biases like the Dunning–Kruger effect. LLMs, including GPT, BARD, Claude, and LLaMA, are evaluated using confidence scores on reasoning tasks. The models provide self-assessed confidence levels before and after responding to different questions. The results show cases where high confidence does not correlate with correctness, suggesting overconfidence. Conversely, low confidence despite accurate responses indicates potential underestimation. The confidence scores vary across problem categories and difficulties, reducing confidence for complex queries. GPT-4 displays consistent confidence, while LLaMA and Claude demonstrate more variations. Some of these patterns resemble the Dunning–Kruger effect, where incompetence leads to inflated self-evaluations. While not conclusively evident, these observations parallel this phenomenon and provide a foundation to further explore the alignment of competence and confidence in LLMs. As LLMs continue to expand their societal roles, further research into their self-assessment mechanisms is warranted to fully understand their capabilities and limitations.",
                "authors": "Aniket Kumar Singh, Bishal Lamichhane, Suman Devkota, Uttam Dhakal, Chandra Dhakal",
                "citations": 7
            },
            {
                "title": "Relational Programming with Foundational Models",
                "abstract": "Foundation models have vast potential to enable diverse AI applications. The powerful yet incomplete nature of these models has spurred a wide range of mechanisms to augment them with capabilities such as in-context learning, information retrieval, and code interpreting. We propose Vieira, a declarative framework that unifies these mechanisms in a general solution for programming with foundation models. Vieira follows a probabilistic relational paradigm and treats foundation models as stateless functions with relational inputs and outputs. It supports neuro-symbolic applications by enabling the seamless combination of such models with logic programs, as well as complex, multi-modal applications by streamlining the composition of diverse sub-models. We implement Vieira by extending the Scallop compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in Vieira are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines.",
                "authors": "Ziyang Li, Jiani Huang, Jason Liu, Felix Zhu, Eric Zhao, William Dodds, Neelay Velingker, R. Alur, Mayur Naik",
                "citations": 1
            },
            {
                "title": "FINITE ELEMENT ANALYSIS OF SLABS-ON-GRADE USING A VARIETY OF SUPPORT MODELS",
                "abstract": "The Finite Element Method has been a very powerful tool for the analysis of slab-on­ grade type pavements in the last two decades. A number of programs have been developed which treat the slab as an elastic plate and characterize foundation support either as a dense liquid or as an elastic solid. This paper describes the development of an expanded and revised version of ILLI-SLAB, which now incorporates four subgrade idealizations: the Winkler dense liquid, the Boussinesq elastic solid, the stress dependent resilient subgrade and the Vlasov two-parameter foundation. Comparative studies are greatly facilitated and results from numerous runs are presented to illustrate the scope of the program's applicability. The efficient utilization of ILLI­ SLAB is ensured by adherence to guidelines established during several convergence studies described.",
                "authors": "A. Ioannides, M. Thompson, E. Barenberg",
                "citations": 34
            },
            {
                "title": "Cosmos World Foundation Model Platform for Physical AI",
                "abstract": "Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via https://github.com/NVIDIA/Cosmos.",
                "authors": "Nvidia Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, J. Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Kl'ar, Grace Lam, Shiyi Lan, L. Leal-Taixé, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Mingkun Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, A. Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, F. Reda, Xiao-Shuai Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne P. Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Tingwei Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yuan Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski",
                "citations": 2
            },
            {
                "title": "The Emergence of Large Language Models in Static Analysis: A First Look Through Micro-Benchmarks",
                "abstract": "The application of Large Language Models (LLMs) in software engineering, particularly in static analysis tasks, represents a paradigm shift in the field. In this paper, we investigate the role that current LLMs can play in improving callgraph analysis and type inference for Python programs. Using the PyCG, HeaderGen, and TypeEvalPy micro-benchmarks, we evaluate 26 LLMs, including OpenAI's GPT series and open-source models such as LLaMA. Our study reveals that LLMs show promising results in type inference, demonstrating higher accuracy than traditional methods, yet they exhibit limitations in callgraph analysis. This contrast emphasizes the need for specialized fine-tuning of LLMs to better suit specific static analysis tasks. Our findings provide a foundation for further research towards integrating LLMs for static analysis tasks.",
                "authors": "Ashwini Venkatesh, Samkutty Sabu, Amir M. Mir, Sofia Reis, Eric Bodden",
                "citations": 5
            },
            {
                "title": "RakutenAI-7B: Extending Large Language Models for Japanese",
                "abstract": "We introduce RakutenAI-7B, a suite of Japanese-oriented large language models that achieve the best performance on the Japanese LM Harness benchmarks among the open 7B models. Along with the foundation model, we release instruction- and chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat respectively, under the Apache 2.0 license.",
                "authors": "Aaron Levine, Connie Huang, Chenguang Wang, Eduardo Batista, Ewa Szymanska, Hongyi Ding, Houwei Chou, Jean-François Pessiot, Johanes Effendi, Justin Chiu, Kai Torben Ohlhus, Karan Chopra, Keiji Shinzato, Koji Murakami, Lee Xiong, Lei Chen, Maki Kubota, Maksim Tkatchenko, Miroku Lee, Naoki Takahashi, Prathyusha Jwalapuram, Ryutaro Tatsushima, Saurabh Jain, Sunil Kumar Yadav, Ting Cai, Wei-Te Chen, Yandi Xia, Yuki Nakayama, Yutaka Higashiyama",
                "citations": 5
            },
            {
                "title": "Traditional Chinese Medicine Knowledge Graph Construction Based on Large Language Models",
                "abstract": "This study explores the use of large language models in constructing a knowledge graph for Traditional Chinese Medicine (TCM) to improve the representation, storage, and application of TCM knowledge. The knowledge graph, based on a graph structure, effectively organizes entities, attributes, and relationships within the TCM domain. By leveraging large language models, we collected and embedded substantial TCM–related data, generating precise representations transformed into a knowledge graph format. Experimental evaluations confirmed the accuracy and effectiveness of the constructed graph, extracting various entities and their relationships, providing a solid foundation for TCM learning, research, and application. The knowledge graph has significant potential in TCM, aiding in teaching, disease diagnosis, treatment decisions, and contributing to TCM modernization. In conclusion, this paper utilizes large language models to construct a knowledge graph for TCM, offering a vital foundation for knowledge representation and application in the field, with potential for future expansion and refinement.",
                "authors": "Yichong Zhang, Yongtao Hao",
                "citations": 5
            },
            {
                "title": "Epistemology of Language Models: Do Language Models Have Holistic Knowledge?",
                "abstract": "This paper investigates the inherent knowledge in language models from the perspective of epistemological holism. The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism. These characteristics suggest that core knowledge, such as general scientific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise. To assess these traits related to holism, we created a scientific reasoning dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation. In the abduction task, the language models explained situations while avoiding revising the core knowledge. However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles.",
                "authors": "Minsu Kim, James Thorne",
                "citations": 4
            },
            {
                "title": "From Generalization Analysis to Optimization Designs for State Space Models",
                "abstract": "A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.",
                "authors": "Fusheng Liu, Qianxiao Li",
                "citations": 5
            },
            {
                "title": "scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation",
                "abstract": "Despite the inherent limitations of existing Large Language Models in directly reading and interpreting single-cell omics data, they demonstrate significant potential and flexibility as the Foundation Model. This research focuses on how to train and adapt the Large Language Model with the capability to interpret and distinguish cell types in single-cell RNA sequencing data. Our preliminary research results indicate that these foundational models excel in accurately categorizing known cell types, demonstrating the potential of the Large Language Models as effective tools for uncovering new biological insights.",
                "authors": "Cong Li, Meng Xiao, Pengfei Wang, Guihai Feng, Xin Li, Yuanchun Zhou",
                "citations": 2
            },
            {
                "title": "ProtoSAM: One-Shot Medical Image Segmentation With Foundational Models",
                "abstract": "This work introduces a new framework, ProtoSAM, for one-shot medical image segmentation. It combines the use of prototypical networks, known for few-shot segmentation, with SAM - a natural image foundation model. The method proposed creates an initial coarse segmentation mask using the ALPnet prototypical network, augmented with a DINOv2 encoder. Following the extraction of an initial mask, prompts are extracted, such as points and bounding boxes, which are then input into the Segment Anything Model (SAM). State-of-the-art results are shown on several medical image datasets and demonstrate automated segmentation capabilities using a single image example (one shot) with no need for fine-tuning of the foundation model. Our code is available at: https://github.com/levayz/ProtoSAM",
                "authors": "Lev Ayzenberg, Raja Giryes, H. Greenspan",
                "citations": 2
            },
            {
                "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",
                "abstract": "Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.",
                "authors": "Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zi-Liang Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Yaqiong Zhang, Yunxin Liu",
                "citations": 93
            },
            {
                "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{https://chat.lmsys.org}.",
                "authors": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, Ion Stoica",
                "citations": 272
            },
            {
                "title": "Segment Anything Model for Medical Image Segmentation: Current Applications and Future Directions",
                "abstract": "Due to the inherent flexibility of prompting, foundation models have emerged as the predominant force in the fields of natural language processing and computer vision. The recent introduction of the Segment Anything Model (SAM) signifies a noteworthy expansion of the prompt-driven paradigm into the domain of image segmentation, thereby introducing a plethora of previously unexplored capabilities. However, the viability of its application to medical image segmentation remains uncertain, given the substantial distinctions between natural and medical images. In this work, we provide a comprehensive overview of recent endeavors aimed at extending the efficacy of SAM to medical image segmentation tasks, encompassing both empirical benchmarking and methodological adaptations. Additionally, we explore potential avenues for future research directions in SAM's role within medical image segmentation. While direct application of SAM to medical image segmentation does not yield satisfactory performance on multi-modal and multi-target medical datasets so far, numerous insights gleaned from these efforts serve as valuable guidance for shaping the trajectory of foundational models in the realm of medical image analysis. To support ongoing research endeavors, we maintain an active repository that contains an up-to-date paper list and a succinct summary of open-source projects at https://github.com/YichiZhang98/SAM4MIS.",
                "authors": "Yichi Zhang, Zhenrong Shen, Rushi Jiao",
                "citations": 70
            },
            {
                "title": "Evolutionary Optimization of Model Merging Recipes",
                "abstract": "We present a novel application of evolutionary algorithms to automate the creation of powerful foundation models. While model merging has emerged as a promising approach for LLM development due to its cost-effectiveness, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.",
                "authors": "Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha",
                "citations": 61
            },
            {
                "title": "Machine learning for functional protein design",
                "abstract": null,
                "authors": "Pascal Notin, Nathan J. Rollins, Yarin Gal, Chris Sander, Debora Marks",
                "citations": 63
            },
            {
                "title": "VM-UNet: Vision Mamba UNet for Medical Image Segmentation",
                "abstract": "In the realm of medical image segmentation, both CNN-based and Transformer-based models have been extensively explored. However, CNNs exhibit limitations in long-range modeling capabilities, whereas Transformers are hampered by their quadratic computational complexity. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They not only excel in modeling long-range interactions but also maintain a linear computational complexity. In this paper, leveraging state space models, we propose a U-shape architecture model for medical image segmentation, named Vision Mamba UNet (VM-UNet). Specifically, the Visual State Space (VSS) block is introduced as the foundation block to capture extensive contextual information, and an asymmetrical encoder-decoder structure is constructed with fewer convolution layers to save calculation cost. We conduct comprehensive experiments on the ISIC17, ISIC18, and Synapse datasets, and the results indicate that VM-UNet performs competitively in medical image segmentation tasks. To our best knowledge, this is the first medical image segmentation model constructed based on the pure SSM-based model. We aim to establish a baseline and provide valuable insights for the future development of more efficient and effective SSM-based segmentation systems. Our code is available at https://github.com/JCruan519/VM-UNet.",
                "authors": "Jiacheng Ruan, Suncheng Xiang",
                "citations": 150
            },
            {
                "title": "ORPO: Monolithic Preference Optimization without Reference Model",
                "abstract": "While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we revisit SFT in the context of preference alignment, emphasizing that a minor penalty for the disfavored style is sufficient for preference alignment. Building on this foundation, we introduce a straightforward reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the need for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models including Llama-2 Chat and Zephyr with more than 7B and 13B parameters: achieving up to 12.20% on AlpacaEval 2.0 (Figure 1), and 7.32 in MT-Bench (Table 2). We release code and model checkpoints for Mistral-ORPO-\\alpha (7B) and Mistral-ORPO-\\beta (7B).",
                "authors": "Jiwoo Hong, Noah Lee, James Thorne",
                "citations": 117
            },
            {
                "title": "Plasma Fibrinogen as a Biomarker for Mortality and Hospitalized Exacerbations in People with COPD.",
                "abstract": "BACKGROUND\nIn 2010 the COPD Foundation established the COPD Biomarkers Qualification Consortium (CBQC) as a partnership between the Foundation, the Food and Drug Administration (FDA), and the pharmaceutical industry to pool publicly-funded and industry data to develop innovative tools to facilitate the development and approval of new therapies for COPD. We present data from the initial project seeking regulatory qualification of fibrinogen as a biomarker for the stratification of COPD patients into clinical trials.\n\n\nMETHODS\nThis analysis pooled data from 4 publicly-funded studies and 1 industry study into a common database resulting in 6376 individuals with spirometric evidence of COPD. We used a threshold of 350 mg/dL to determine high vs. low fibrinogen, and determined the subsequent risk of hospitalizations from exacerbations and death using Cox proportional hazards models.\n\n\nRESULTS\nHigh fibrinogen levels at baseline were present in 2853 (44.7%) of individuals with COPD. High fibrinogen was associated with an increased risk of hospitalized COPD exacerbations within 12 months (hazard ratio [HR]: 1.64; 95% confidence interval [CI]: 1.39-1.93) among participants in the Atherosclerosis Risk in Communities Study (ARIC), the Cardiovascular Health Study (CHS), and the Evaluation of COPD Longitudinally to Identify Predictive Surrogate Endpoints (ECLIPSE) study. High fibrinogen was associated with an increased risk of death within 36 months (HR: 1.94; 95% CI: 1.62-2.31) among all participants.\n\n\nCONCLUSIONS\nFibrinogen levels ≥ 350 mg/dL identify COPD individuals at an increased risk of exacerbations and death and could be a useful biomarker for enriching clinical trials in the COPD population.",
                "authors": "jason. simeone, jason. simeone",
                "citations": 99
            },
            {
                "title": "Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling",
                "abstract": "Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance.",
                "authors": "Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, Volodymyr Kuleshov",
                "citations": 42
            },
            {
                "title": "Three Epochs of Artificial Intelligence in Health Care.",
                "abstract": "Importance\nInterest in artificial intelligence (AI) has reached an all-time high, and health care leaders across the ecosystem are faced with questions about where, when, and how to deploy AI and how to understand its risks, problems, and possibilities.\n\n\nObservations\nWhile AI as a concept has existed since the 1950s, all AI is not the same. Capabilities and risks of various kinds of AI differ markedly, and on examination 3 epochs of AI emerge. AI 1.0 includes symbolic AI, which attempts to encode human knowledge into computational rules, as well as probabilistic models. The era of AI 2.0 began with deep learning, in which models learn from examples labeled with ground truth. This era brought about many advances both in people's daily lives and in health care. Deep learning models are task-specific, meaning they do one thing at a time, and they primarily focus on classification and prediction. AI 3.0 is the era of foundation models and generative AI. Models in AI 3.0 have fundamentally new (and potentially transformative) capabilities, as well as new kinds of risks, such as hallucinations. These models can do many different kinds of tasks without being retrained on a new dataset. For example, a simple text instruction will change the model's behavior. Prompts such as \"Write this note for a specialist consultant\" and \"Write this note for the patient's mother\" will produce markedly different content.\n\n\nConclusions and Relevance\nFoundation models and generative AI represent a major revolution in AI's capabilities, ffering tremendous potential to improve care. Health care leaders are making decisions about AI today. While any heuristic omits details and loses nuance, the framework of AI 1.0, 2.0, and 3.0 may be helpful to decision-makers because each epoch has fundamentally different capabilities and risks.",
                "authors": "Michael D Howell, G. Corrado, Karen B. DeSalvo",
                "citations": 43
            },
            {
                "title": "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding",
                "abstract": "With the success of large language models (LLMs), integrating the vision model into LLMs to build vision-language foundation models has gained much more interest recently. However, existing LLM-based large multimodal models (e.g., Video-LLaMA, VideoChat) can only take in a limited number of frames for short video understanding. In this study, we mainly focus on designing an efficient and effective model for long-term video understanding. Instead of trying to process more frames simultaneously like most existing work, we propose to process videos in an online manner and store past video information in a memory bank. This allows our model to reference historical video content for long-term analysis without exceeding LLMs' context length constraints or GPU memory limits. Our memory bank can be seamlessly integrated into current multimodal LLMs in an off-the-shelf manner. We conduct extensive experiments on various video understanding tasks, such as long-video understanding, video question answering, and video captioning, and our model can achieve state-of-the-art performances across multiple datasets.",
                "authors": "Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim",
                "citations": 38
            },
            {
                "title": "Agent AI: Surveying the Horizons of Multimodal Interaction",
                "abstract": "Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments. At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents. Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems. For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. To accelerate research on agent-based multimodal intelligence, we define\"Agent AI\"as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, and can produce meaningful embodied actions. In particular, we explore systems that aim to improve agents based on next-embodied action prediction by incorporating external knowledge, multi-sensory inputs, and human feedback. We argue that by developing agentic AI systems in grounded environments, one can also mitigate the hallucinations of large foundation models and their tendency to generate environmentally incorrect outputs. The emerging field of Agent AI subsumes the broader embodied and agentic aspects of multimodal interactions. Beyond agents acting and interacting in the physical world, we envision a future where people can easily create any virtual reality or simulated scene and interact with agents embodied within the virtual environment.",
                "authors": "Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, J. Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, D. Terzopoulos, Yejin Choi, Katsushi Ikeuchi, Hoi Vo, Fei-Fei Li, Jianfeng Gao",
                "citations": 37
            },
            {
                "title": "An Image Grid Can Be Worth a Video: Zero-Shot Video Question Answering Using a VLM",
                "abstract": "Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging multiple frames in our proposed grid layout. The resulting single image is termed as an image grid. This format, while maintaining the appearance of a solitary image, effectively retains temporal information within the grid structure at the pixel level. Therefore, the image grid approach enables direct application of a single high-performance VLM without necessitating any video-data training. Our extensive experimental analysis across ten zero-shot VQA benchmarks, including five open-ended and five multiple-choice benchmarks, reveals that the proposed Image Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out of ten benchmarks. We also discuss how IG-VLM can be extended for long videos and provide an extension method that consistently and reliably improves the performance. Our code is are available at: https://github.com/imagegridworth/IG-VLM",
                "authors": "Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee",
                "citations": 35
            },
            {
                "title": "VideoAgent: Long-form Video Understanding with Large Language Model as Agent",
                "abstract": "Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.",
                "authors": "Xiaohan Wang, Yuhui Zhang, Orr Zohar, S. Yeung-Levy",
                "citations": 35
            },
            {
                "title": "Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively",
                "abstract": null,
                "authors": "Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, Chen Change Loy",
                "citations": 36
            },
            {
                "title": "Bilingual language model for protein sequence and structure",
                "abstract": "Adapting large language models (LLMs) to protein sequences spawned the development of powerful protein language models (pLMs). Concurrently, AlphaFold2 broke through in protein structure prediction. Now we can systematically and comprehensively explore the dual nature of proteins that act and exist as three-dimensional (3D) machines and evolve as linear strings of one-dimensional (1D) sequences. Here, we leverage pLMs to simultaneously model both modalities by combining 1D sequences with 3D structure in a single model. We encode protein structures as token sequences using the 3Di-alphabet introduced by the 3D-alignment method Foldseek. This new foundation pLM extracts the features and patterns of the resulting “structure-sequence” representation. Toward this end, we built a non-redundant dataset from AlphaFoldDB and fine-tuned an existing pLM (ProtT5) to translate between 3Di and amino acid sequences. As a proof-of-concept for our novel approach, dubbed Protein structure-sequence T5 (ProstT5), we showed improved performance for subsequent prediction tasks, and for “inverse folding”, namely the generation of novel protein sequences adopting a given structural scaffold (“fold”). Our work showcased the potential of pLMs to tap into the information-rich protein structure revolution fueled by AlphaFold2. ProstT5 paves the way to develop new tools integrating the vast resource of 3D predictions, and opens new research avenues in the post-AlphaFold2 era. Our model is freely available for all at https://github.com/mheinzinger/ProstT5.",
                "authors": "M. Heinzinger, Konstantin Weissenow, Joaquin Gomez Sanchez, Adrian Henkel, Martin Steinegger, B. Rost",
                "citations": 61
            },
            {
                "title": "Sequence modeling and design from molecular to genome scale with Evo",
                "abstract": "The genome is a sequence that completely encodes the DNA, RNA, and proteins that orchestrate the function of a whole organism. Advances in machine learning combined with massive datasets of whole genomes could enable a biological foundation model that accelerates the mechanistic understanding and generative design of complex molecular interactions. We report Evo, a genomic foundation model that enables prediction and generation tasks from the molecular to genome scale. Using an architecture based on advances in deep signal processing, we scale Evo to 7 billion parameters with a context length of 131 kilobases (kb) at single-nucleotide, byte resolution. Trained on 2.7M prokaryotic and phage genomes, Evo can generalize across the three fundamental modalities of the central dogma of molecular biology to perform zero-shot function prediction that is competitive with, or outperforms, leading domain-specific language models. Evo also excels at multi-element generation tasks, which we demonstrate by generating synthetic CRISPR-Cas molecular complexes and entire transposable systems for the first time. Using information learned over whole genomes, Evo can also predict gene essentiality at nucleotide resolution and can generate coding-rich sequences up to 650 kb in length, orders of magnitude longer than previous methods. Advances in multi-modal and multiscale learning with Evo provides a promising path toward improving our understanding and control of biology across multiple levels of complexity.",
                "authors": "Eric Nguyen, Michael Poli, Matthew G. Durrant, Armin W. Thomas, Brian Kang, Jeremy Sullivan, Madelena Y Ng, Ashley Lewis, Aman Patel, Aaron Lou, Stefano Ermon, S. Baccus, Tina Hernandez-Boussard, Christopher Ré, Patrick D. Hsu, Brian L. Hie",
                "citations": 53
            },
            {
                "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
                "abstract": "We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.",
                "authors": "Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou",
                "citations": 52
            },
            {
                "title": "VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding",
                "abstract": "We explore how reconciling several foundation models (large language models and vision-language models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos. In particular, the proposed multimodal agent VideoAgent: 1) constructs a structured memory to store both the generic temporal event descriptions and object-centric tracking states of the video; 2) given an input task query, it employs tools including video segment localization and object memory querying along with other visual foundation models to interactively solve the task, utilizing the zero-shot tool-use ability of LLMs. VideoAgent demonstrates impressive performances on several long-horizon video understanding benchmarks, an average increase of 6.6% on NExT-QA and 26.0% on EgoSchema over baselines, closing the gap between open-sourced models and private counterparts including Gemini 1.5 Pro.",
                "authors": "Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, Qing Li",
                "citations": 29
            },
            {
                "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
                "abstract": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.",
                "authors": "Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, Chuang Gan",
                "citations": 32
            },
            {
                "title": "Is Mamba Capable of In-Context Learning?",
                "abstract": "State of the art foundation models such as GPT-4 perform surprisingly well at in-context learning (ICL), a variant of meta-learning concerning the learned ability to solve tasks during a neural network forward pass, exploiting contextual information provided as input to the model. This useful ability emerges as a side product of the foundation model's massive pretraining. While transformer models are currently the state of the art in ICL, this work provides empirical evidence that Mamba, a newly proposed state space model which scales better than transformers w.r.t. the input sequence length, has similar ICL capabilities. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that, across both categories of tasks, Mamba closely matches the performance of transformer models for ICL. Further analysis reveals that, like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving long input sequences. This is an exciting finding in meta-learning and may enable generalizations of in-context learned AutoML algorithms (like TabPFN or Optformer) to long input sequences.",
                "authors": "Riccardo Grazzi, Julien N. Siems, Simon Schrodi, Thomas Brox, Frank Hutter",
                "citations": 32
            },
            {
                "title": "HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing",
                "abstract": "This study introduces HQ-Edit, a high-quality instruction-based image editing dataset with around 200,000 edits. Unlike prior approaches relying on attribute guidance or human feedback on building datasets, we devise a scalable data collection pipeline leveraging advanced foundation models, namely GPT-4V and DALL-E 3. To ensure its high quality, diverse examples are first collected online, expanded, and then used to create high-quality diptychs featuring input and output images with detailed text prompts, followed by precise alignment ensured through post-processing. In addition, we propose two evaluation metrics, Alignment and Coherence, to quantitatively assess the quality of image edit pairs using GPT-4V. HQ-Edits high-resolution images, rich in detail and accompanied by comprehensive editing prompts, substantially enhance the capabilities of existing image editing models. For example, an HQ-Edit finetuned InstructPix2Pix can attain state-of-the-art image editing performance, even surpassing those models fine-tuned with human-annotated data. The project page is https://thefllood.github.io/HQEdit_web.",
                "authors": "Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, Cihang Xie",
                "citations": 25
            },
            {
                "title": "Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation",
                "abstract": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps. In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-storage environments. We provide code and trial video data at http://hovsg.github.io/.",
                "authors": "Abdelrhman Werby, Chen Huang, M. Büchner, A. Valada, Wolfram Burgard",
                "citations": 27
            },
            {
                "title": "GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image",
                "abstract": "We introduce GeoWizard, a new generative foundation model designed for estimating geometric attributes, e.g., depth and normals, from single images. While significant research has already been conducted in this area, the progress has been substantially limited by the low diversity and poor quality of publicly available datasets. As a result, the prior works either are constrained to limited scenarios or suffer from the inability to capture geometric details. In this paper, we demonstrate that generative models, as opposed to traditional discriminative models (e.g., CNNs and Transformers), can effectively address the inherently ill-posed problem. We further show that leveraging diffusion priors can markedly improve generalization, detail preservation, and efficiency in resource usage. Specifically, we extend the original stable diffusion model to jointly predict depth and normal, allowing mutual information exchange and high consistency between the two representations. More importantly, we propose a simple yet effective strategy to segregate the complex data distribution of various scenes into distinct sub-distributions. This strategy enables our model to recognize different scene layouts, capturing 3D geometry with remarkable fidelity. GeoWizard sets new benchmarks for zero-shot depth and normal prediction, significantly enhancing many downstream applications such as 3D reconstruction, 2D content creation, and novel viewpoint synthesis.",
                "authors": "Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, Xiaoxiao Long",
                "citations": 44
            },
            {
                "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference",
                "abstract": "In recent years, the application of multimodal large language models (MLLM) in various fields has achieved remarkable success. However, as the foundation model for many downstream tasks, current MLLMs are composed of the well-known Transformer network, which has a less efficient quadratic computation complexity. To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM. Specifically, Cobra integrates the efficient Mamba language model into the visual modality. Moreover, we explore and study various modal fusion schemes to create an effective multi-modal Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra's linear sequential modeling. (2) Interestingly, the results of closed-set challenging prediction benchmarks show that Cobra performs well in overcoming visual illusions and spatial relationship judgments. (3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters. We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM. Our project page is available at: https://sites.google.com/view/cobravlm.",
                "authors": "Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, Donglin Wang",
                "citations": 46
            },
            {
                "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference",
                "abstract": "In recent years, the application of multimodal large language models (MLLM) in various fields has achieved remarkable success. However, as the foundation model for many downstream tasks, current MLLMs are composed of the well-known Transformer network, which has a less efficient quadratic computation complexity. To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM. Specifically, Cobra integrates the efficient Mamba language model into the visual modality. Moreover, we explore and study various modal fusion schemes to create an effective multi-modal Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra's linear sequential modeling. (2) Interestingly, the results of closed-set challenging prediction benchmarks show that Cobra performs well in overcoming visual illusions and spatial relationship judgments. (3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters. We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM. Our project page is available at: https://sites.google.com/view/cobravlm.",
                "authors": "Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, Donglin Wang",
                "citations": 46
            },
            {
                "title": "Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation",
                "abstract": "Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains, such as robotic navigation and manipulation, can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling robotic arms, quadcopters, quadrupeds, and mobile bases. We then investigate the extent to which transfer can occur across navigation and manipulation on these embodiments by framing them as a single goal-reaching task. We find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across various embodiments. Further information and robot videos can be found on our project website http://extreme-cross-embodiment.github.io.",
                "authors": "Jonathan Yang, Catherine Glossop, Arjun Bhorkar, Dhruv Shah, Quan Vuong, Chelsea Finn, Dorsa Sadigh, Sergey Levine",
                "citations": 25
            },
            {
                "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
                "abstract": "Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.",
                "authors": "Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, P. Talukdar",
                "citations": 26
            },
            {
                "title": "ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs",
                "abstract": "With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models. Codes and data are available at https://github.com/NineAbyss/ZeroG.",
                "authors": "Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li",
                "citations": 24
            },
            {
                "title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
                "abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.",
                "authors": "Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, Ying Shan",
                "citations": 39
            },
            {
                "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM",
                "abstract": "The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page: https://vita-home.github.io.",
                "authors": "Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yi-Fan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, Xing Sun",
                "citations": 34
            },
            {
                "title": "An Embarrassingly Simple Approach for LLM with Strong ASR Capacity",
                "abstract": "In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech benchmark among LLM-based ASR models and even outperforms the latest LLM-based audio-universal model trained on massive pair data. Finally, we explore the capability emergence of LLM-based ASR in the process of modal alignment. We hope that our study can facilitate the research on extending LLM with cross-modality capacity and shed light on the LLM-based ASR community.",
                "authors": "Ziyang Ma, Yifan Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, Xie Chen",
                "citations": 35
            },
            {
                "title": "Mapping the Increasing Use of LLMs in Scientific Papers",
                "abstract": "Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.",
                "authors": "Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D. Manning, James Y. Zou",
                "citations": 34
            },
            {
                "title": "Advanced Mechanics of Structures",
                "abstract": "Fundamental aspects of engineering mechanics stresses in beams of uniform and variable cross sections large and small deformations of beams of uniform and variable cross section beams and plates on elastic foundation elastic and inelastic analysis of beams and plates stability and instability of structural and mechanical systems experimental investigations and preparation of test models additional popular subjects and methods of analysis. Appendices: basic rules of matrix algebra computer program student critique on history of mechanics in section 7.2 references answers to selected problems. (Part contents)",
                "authors": "D. Fertis",
                "citations": 34
            },
            {
                "title": "SpeechVerse: A Large-scale Generalizable Audio Language Model",
                "abstract": "Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions. Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic speech recognition and translation. We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained speech and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training. The models are instruction finetuned using continuous latent representations extracted from the speech foundation model to achieve optimal zero-shot performance on a diverse range of speech processing tasks using natural language instructions. We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks. Furthermore, we evaluate the model's capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks. Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks.",
                "authors": "Nilaksh Das, Saket Dingliwal, S. Ronanki, Rohit Paturi, David Huang, Prashant Mathur, Jie Yuan, Dhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, Xilai Li, Karel Mundnich, Monica Sunkara, S. Srinivasan, Kyu J Han, Katrin Kirchhoff",
                "citations": 21
            },
            {
                "title": "The political preferences of LLMs",
                "abstract": "I report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, I administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both closed and open source. When probed with questions/statements with political connotations, most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. This does not appear to be the case for five additional base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, the weak performance of the base models at coherently answering the tests’ questions makes this subset of results inconclusive. Finally, I demonstrate that LLMs can be steered towards specific locations in the political spectrum through Supervised Fine-Tuning (SFT) with only modest amounts of politically aligned data, suggesting SFT’s potential to embed political orientation in LLMs. With LLMs beginning to partially displace traditional information sources like search engines and Wikipedia, the societal implications of political biases embedded in LLMs are substantial.",
                "authors": "David Rozado",
                "citations": 22
            },
            {
                "title": "Vlogger: Make Your Dream A Vlog",
                "abstract": "In this work, we present Vlogger, a generic AI systemfor generating a minute-level video blog (i.e., vlog) of user de-scriptions. Different from short videos with a few seconds, vlog often contains a complex storyline with diversified scenes, which is challenging for most existing video generation approaches. To break through this bottleneck, our Vlogger smartly leverages Large Language Model (LLM) as Director and decomposes a long video generation task of vlog into four key stages, where we invoke various foundation models to play the critical roles of vlog profession-als, including (1) Script, (2) Actor, (3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings, our Vlogger can generate vlogs through explainable cooperation of top-down planning and bottom-up shooting. More-over, we introduce a novel video diffusion model, Show-Maker, which serves as a videographer in our Vlogger for generating the video snippet of each shooting scene. By incorporating Script and Actor attentively as textual and visual prompts, it can effectively enhance spatial-temporal coherence in the snippet. Besides, we design a concise mixed training paradigm for ShowMaker, boosting its ca-pacity for both T2V generation and prediction. Finally, the extensive experiments show that our method achieves state-of-the-art performance on zero-shot T2V generation and prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs from open-world descriptions, without loss of video coherence on script and actor.",
                "authors": "Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, Yali Wang",
                "citations": 22
            },
            {
                "title": "Revolutionizing personalized medicine with generative AI: a systematic review",
                "abstract": null,
                "authors": "Isaias Ghebrehiwet, Nazar Zaki, Rafat Damseh, Mohd Saberi Mohamad",
                "citations": 20
            },
            {
                "title": "Social Choice for AI Alignment: Dealing with Diverse Human Feedback",
                "abstract": "Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise problematic behavior, so that, for example, they refuse to comply with requests for help with committing crimes or with producing racist text. One approach to fine-tuning, called reinforcement learning from human feed-back , learns from humans’ expressed preferences over multiple outputs. Another approach is constitutional AI , in which the input from humans is a list of high-level principles. But how do we deal with potentially diverging input from humans? How can we aggregate the input into consistent data about “collective” preferences or otherwise use it to make collective choices about model behavior? In this paper, we argue that the field of social choice is well positioned to address these questions, and we discuss ways forward for this agenda, drawing on discussions in a recent work-shop on Social Choice for AI Ethics and Safety held in Berkeley, CA, USA in December 2023.",
                "authors": "Vincent Conitzer, Rachel Freedman, J. Heitzig, Wesley H. Holliday, Bob M. Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, William S. Zwicker",
                "citations": 18
            },
            {
                "title": "Artificial intelligence in surgery.",
                "abstract": null,
                "authors": "Chris Varghese, Ewen M. Harrison, Greg O’Grady, E. Topol",
                "citations": 19
            },
            {
                "title": "New regulatory thinking is needed for AI-based personalised drug and cell therapies in precision oncology",
                "abstract": null,
                "authors": "Bouchra Derraz, Gabriele Breda, Christoph Kaempf, Franziska Baenke, Fabienne Cotte, Kristin Reiche, Ulrike Köhl, J. N. Kather, Deborah Eskenazy, S. Gilbert",
                "citations": 18
            },
            {
                "title": "FairCLIP: Harnessing Fairness in Vision-Language Learning",
                "abstract": "Fairness is a critical concern in deep learning, especially in healthcare, where these models influence diagnoses and treatment decisions. Although fairness has been investigated in the vision-only domain, the fairness of medical vision-language (VL) models remains unexplored due to the scarcity of medical VL datasets for studying fairness. To bridge this research gap, we introduce the first fair vision-language medical dataset (Harvard-FairVLMed) that provides detailed demographic attributes, ground-truth labels, and clinical notes to facilitate an in-depth examination of fairness within VL foundation models. Using Harvard-FairVLMed, we conduct a comprehensive fairness analysis of two widely-used VL models (CLIP and BLIP2), pre-trained on both natural and medical domains, across four different protected attributes. Our results highlight significant biases in all VL models, with Asian, Male, Non-Hispanic, and Spanish being the preferred subgroups across the protected attributes of race, gender, ethnicity, and language, respectively. In order to alleviate these biases, we propose FairCLIP an optimal-transport-based approach that achieves a favorable trade-off between performance and fairness by reducing the Sinkhorn distance between the overall sample distribution and the distributions corresponding to each demographic group. As the first VL dataset of its kind, Harvard-FairVLMed holds the potential to catalyze advancements in the development of machine learning models that are both ethically aware and clinically effective. Our dataset and code are available at https://ophai.hms.harvard.edu/datasets/harvard-fairvlmed10k.",
                "authors": "Yan Luo, Minfei Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, T. Elze, Yi Fang, Mengyu Wang",
                "citations": 15
            },
            {
                "title": "Automated Design of Agentic Systems",
                "abstract": "Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. We formulate a new research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. We further demonstrate that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, control flows, and combinations thereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, we show that our algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided we develop it safely, our work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.",
                "authors": "Shengran Hu, Cong Lu, Jeff Clune",
                "citations": 16
            },
            {
                "title": "Pandora: Towards General World Model with Natural Language Actions and Video States",
                "abstract": "World models simulate future states of the world in response to different actions. They facilitate interactive content creation and provides a foundation for grounded, long-horizon reasoning. Current foundation models do not fully meet the capabilities of general world models: large language models (LLMs) are constrained by their reliance on language modality and their limited understanding of the physical world, while video models lack interactive action control over the world simulations. This paper makes a step towards building a general world model by introducing Pandora, a hybrid autoregressive-diffusion model that simulates world states by generating videos and allows real-time control with free-text actions. Pandora achieves domain generality, video consistency, and controllability through large-scale pretraining and instruction tuning. Crucially, Pandora bypasses the cost of training-from-scratch by integrating a pretrained LLM (7B) and a pretrained video model, requiring only additional lightweight finetuning. We illustrate extensive outputs by Pandora across diverse domains (indoor/outdoor, natural/urban, human/robot, 2D/3D, etc.). The results indicate great potential of building stronger general world models with larger-scale training.",
                "authors": "Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, Zhengzhong Liu, Eric P. Xing, Zhiting Hu",
                "citations": 17
            },
            {
                "title": "MovieChat+: Question-aware Sparse Memory for Long Video Question Answering",
                "abstract": "Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific pre-defined vision tasks. Yet, existing methods either employ complex spatial-temporal modules or rely heavily on additional perception models to extract temporal features for video understanding, and they only perform well on short videos. For long videos, the computational complexity and memory costs associated with long-term temporal connections are significantly increased, posing additional challenges.Taking advantage of the Atkinson-Shiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination with our specially designed memory mechanism, we propose MovieChat to overcome these challenges. We lift pre-trained multi-modal large language models for understanding long videos without incorporating additional trainable temporal modules, employing a zero-shot approach. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video, 2K temporal grounding labels, and 14K manual annotations for validation of the effectiveness of our method. The code along with the dataset can be accessed via the following https://github.com/rese1f/MovieChat.",
                "authors": "Enxin Song, Wenhao Chai, Tianbo Ye, Jenq-Neng Hwang, Xi Li, Gaoang Wang",
                "citations": 17
            },
            {
                "title": "Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback",
                "abstract": "Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise problematic behavior, such as helping to commit crimes or producing racist text. One approach to fine-tuning, called reinforcement learning from human feedback, learns from humans' expressed preferences over multiple outputs. Another approach is constitutional AI, in which the input from humans is a list of high-level principles. But how do we deal with potentially diverging input from humans? How can we aggregate the input into consistent data about\"collective\"preferences or otherwise use it to make collective choices about model behavior? In this paper, we argue that the field of social choice is well positioned to address these questions, and we discuss ways forward for this agenda, drawing on discussions in a recent workshop on Social Choice for AI Ethics and Safety held in Berkeley, CA, USA in December 2023.",
                "authors": "Vincent Conitzer, Rachel Freedman, J. Heitzig, Wesley H. Holliday, Bob M. Jacobs, Nathan Lambert, Milan Moss'e, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, W. Zwicker",
                "citations": 17
            },
            {
                "title": "Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships",
                "abstract": "Current approaches for 3D scene graph prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories. We present Open3DSG, an alternative approach to learn 3D scene graph prediction in an open world without requiring labeled scene graph data. We co-embed the features from a 3D scene graph prediction backbone with the feature space of pow-erful open world 2D vision language foundation models. This enables us to predict 3D scene graphs from 3D point clouds in a zero-shot manner by querying object classes from an open vocabulary and predicting the inter-object relationships from a grounded LLM with scene graph features and queried object classes as context. Open3DSG is the first 3D point cloud method to predict not only explicit open-vocabulary object classes, but also open-set relation-ships that are not limited to a predefined label set, making it possible to express rare as well as specific objects and relationships in the predicted 3D scene graph. Our exper-iments show that Open3DSG is effective at predicting arbitrary object classes as well as their complex inter-object relationships describing spatial, supportive, semantic and comparative relationships.",
                "authors": "Sebastian Koch, N. Vaskevicius, Mirco Colosi, P. Hermosilla, Timo Ropinski",
                "citations": 15
            },
            {
                "title": "Segment Anything in Medical Images and Videos: Benchmark and Deployment",
                "abstract": "Recent advances in segmentation foundation models have enabled accurate and efficient segmentation across a wide range of natural images and videos, but their utility to medical data remains unclear. In this work, we first present a comprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11 medical image modalities and videos and point out its strengths and weaknesses by comparing it to SAM1 and MedSAM. Then, we develop a transfer learning pipeline and demonstrate SAM2 can be quickly adapted to medical domain by fine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio API for efficient 3D image and video segmentation. The code has been made publicly available at \\url{https://github.com/bowang-lab/MedSAM}.",
                "authors": "Jun Ma, Sumin Kim, Feifei Li, Mohammed Baharoon, Reza Asakereh, Hongwei Lyu, Bo Wang",
                "citations": 16
            },
            {
                "title": "The Real Dangers of Generative AI",
                "abstract": "Abstract:As perhaps the most consequential technology of our time, Generative Foundation Models (GFMs) present unprecedented challenges for democratic institutions. By allowing deception and de-contextualized information sharing at a previously unimaginable scale and pace, GFMs could undermine the foundations of democracy. At the same time, the investment scale required to develop the models and the race dynamics around that development threaten to enable concentrations of democratically unaccountable power (both public and private). This essay examines the twin threats of collapse and singularity occasioned by the rise of GFMs.",
                "authors": "Danielle Allen, E. G. Weyl, James Bryant",
                "citations": 15
            },
            {
                "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
                "abstract": "The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.",
                "authors": "Wei-Bang Jiang, Li-Ming Zhao, Bao-Liang Lu",
                "citations": 30
            },
            {
                "title": "LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model",
                "abstract": "The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Additionally, we introduce LHRS-Bench, a benchmark for thoroughly evaluating MLLMs' abilities in RS image understanding. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.",
                "authors": "Dilxat Muhtar, Zhenshi Li, Feng Gu, Xue-liang Zhang, P. Xiao",
                "citations": 25
            },
            {
                "title": "ChatQA: Surpassing GPT-4 on Conversational QA and RAG",
                "abstract": "In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models. Notably, the Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09, achieving a 4.4% improvement. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community: https://chatqa-project.github.io/.",
                "authors": "Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro",
                "citations": 25
            },
            {
                "title": "ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation",
                "abstract": "Open-vocabulary semantic segmentation requires models to effectively integrate visual representations with open-vocabulary semantic labels. While Contrastive Language-Image Pre-training (CLIP) models shine in recognizing visual concepts from text, they often struggle with segment coherence due to their limited localization ability. In contrast, Vision Foundation Models (VFMs) excel at acquiring spatially consistent local visual representations, yet they fall short in semantic understanding. This paper introduces ProxyCLIP, an innovative framework designed to harmonize the strengths of both CLIP and VFMs, facilitating enhanced open-vocabulary semantic segmentation. ProxyCLIP leverages the spatial feature correspondence from VFMs as a form of proxy attention to augment CLIP, thereby inheriting the VFMs' robust local consistency and maintaining CLIP's exceptional zero-shot transfer capacity. We propose an adaptive normalization and masking strategy to get the proxy attention from VFMs, allowing for adaptation across different VFMs. Remarkably, as a training-free approach, ProxyCLIP significantly improves the average mean Intersection over Union (mIoU) across eight benchmarks from 40.3 to 44.4, showcasing its exceptional efficacy in bridging the gap between spatial precision and semantic richness for the open-vocabulary segmentation task.",
                "authors": "Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, Wayne Zhang",
                "citations": 14
            },
            {
                "title": "Parameter-Efficient Fine-Tuning with Discrete Fourier Transform",
                "abstract": "Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats $\\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\\Delta W$. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. Our code is released at \\url{https://github.com/Chaos96/fourierft}.",
                "authors": "Ziqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, Jia Li",
                "citations": 13
            },
            {
                "title": "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data",
                "abstract": "Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs. NuNER and NuNER’s dataset are open-sourced with MIT License.",
                "authors": "Sergei Bogdanov, Alexandre Constantin, Timoth'ee Bernard, Benoit Crabb'e, Etienne Bernard",
                "citations": 12
            },
            {
                "title": "CricaVPR: Cross-Image Correlation-Aware Representation Learning for Visual Place Recognition",
                "abstract": "Over the past decade, most methods in visual place recognition (VPR) have used neural networks to produce feature representations. These networks typically produce a global representation of a place image using only this image itself and neglect the cross-image variations (e.g. viewpoint and illumination), which limits their robustness in challenging scenes. In this paper, we propose a robust global representation method with cross-image correlation awareness for VPR, named CricaVPR. Our method uses the attention mechanism to correlate multiple images within a batch. These images can be taken in the same place with different conditions or viewpoints, or even captured from different places. Therefore, our method can utilize the cross-image variations as a cue to guide the representation learning, which ensures more robust features are produced. To further facilitate the robustness, we propose a multi-scale convolution-enhanced adaptation method to adapt pre-trained visual foundation models to the VPR task, which introduces the multi-scale local information to further enhance the cross-image correlation-aware representation. Experimental results show that our method out-performs state-of-the-art methods by a large margin with significantly less training time. The code is released at https://github.com/Lu-Feng/CricaVPR.",
                "authors": "Feng Lu, Xiangyuan Lan, Lijun Zhang, Dongmei Jiang, Yaowei Wang, Chun Yuan",
                "citations": 13
            },
            {
                "title": "Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI",
                "abstract": "In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.",
                "authors": "Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David B. Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, Jos'e Miguel Hern'andez-Lobato, A. Hubin, Alexander Immer, Theofanis Karaletsos, Mohammad Emtiyaz Khan, Agustinus Kristiadi, Yingzhen Li, Stephan Mandt, Christopher Nemeth, Michael A. Osborne, Tim G. J. Rudner, David Rugamer, Y. W. Teh, M. Welling, Andrew Gordon Wilson, Ruqi Zhang",
                "citations": 14
            },
            {
                "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
                "abstract": "Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent works leverage text instructions to allow users to more freely express their search intents. However, they primarily focus on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions via foundation models. Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relations mined from the web, MagicLens achieves results comparable with or better than prior best on eight benchmarks of various image retrieval tasks, while maintaining high parameter efficiency with a significantly smaller model size. Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens. Code and models are publicly available at https://open-vision-language.github.io/MagicLens/.",
                "authors": "Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, Ming-Wei Chang",
                "citations": 14
            },
            {
                "title": "Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving",
                "abstract": "In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible. To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner. Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 13638 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.",
                "authors": "Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, Junchi Yan",
                "citations": 13
            },
            {
                "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild",
                "abstract": "Given the ubiquity of charts as a data analysis, visualization, and decision-making tool across industries and sciences, there has been a growing interest in developing pre-trained foundation models as well as general purpose instruction-tuned models for chart understanding and reasoning. However, existing methods suffer crucial drawbacks across two critical axes affecting the performance of chart representation models: they are trained on data generated from underlying data tables of the charts, ignoring the visual trends and patterns in chart images, and use weakly aligned vision-language backbone models for domain-specific training, limiting their generalizability when encountering charts in the wild. We address these important drawbacks and introduce ChartGemma, a novel chart understanding and reasoning model developed over PaliGemma. Rather than relying on underlying data tables, ChartGemma is trained on instruction-tuning data generated directly from chart images, thus capturing both high-level trends and low-level visual information from a diverse set of charts. Our simple approach achieves state-of-the-art results across $5$ benchmarks spanning chart summarization, question answering, and fact-checking, and our elaborate qualitative studies on real-world charts show that ChartGemma generates more realistic and factually correct summaries compared to its contemporaries. We release the code, model checkpoints, dataset, and demos at https://github.com/vis-nlp/ChartGemma.",
                "authors": "Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, Shafiq R. Joty",
                "citations": 12
            },
            {
                "title": "Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection",
                "abstract": "The recently developed and publicly available synthetic image generation methods and services make it possible to create extremely realistic imagery on demand, raising great risks for the integrity and safety of online information. State-of-the-art Synthetic Image Detection (SID) research has led to strong evidence on the advantages of feature extraction from foundation models. However, such extracted features mostly encapsulate high-level visual semantics instead of fine-grained details, which are more important for the SID task. On the contrary, shallow layers encode low-level visual information. In this work, we leverage the image representations extracted by intermediate Transformer blocks of CLIP's image-encoder via a lightweight network that maps them to a learnable forgery-aware vector space capable of generalizing exceptionally well. We also employ a trainable module to incorporate the importance of each Transformer block to the final prediction. Our method is compared against the state-of-the-art by evaluating it on 20 test datasets and exhibits an average +10.6% absolute performance improvement. Notably, the best performing models require just a single epoch for training (~8 minutes). Code available at https://github.com/mever-team/rine.",
                "authors": "C. Koutlis, Symeon Papadopoulos",
                "citations": 12
            },
            {
                "title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
                "abstract": "A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.",
                "authors": "Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar",
                "citations": 12
            },
            {
                "title": "Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing",
                "abstract": "Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/",
                "authors": "Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang",
                "citations": 12
            },
            {
                "title": "DNAct: Diffusion Guided Multi-Task 3D Policy Learning",
                "abstract": "This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry. Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing different modalities and thus improving the robustness and the generalizability of the learned representation. DNAct significantly surpasses SOTA NeRF-based multi-task manipulation approaches with over 30% improvement in success rate. Project website: dnact.github.io.",
                "authors": "Ge Yan, Yueh-hua Wu, Xiaolong Wang",
                "citations": 12
            },
            {
                "title": "ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation",
                "abstract": "In the realm of artificial intelligence, the emergence of foundation models, backed by high computing capabilities and extensive data, has been revolutionary. Segment Anything Model (SAM), built on the Vision Transformer (ViT) model with millions of parameters and vast training dataset SA-1B, excels in various segmentation scenarios relying on its significance of semantic information and generalization ability. Such achievement of visual foundation model stimulates continuous researches on specific downstream tasks in computer vision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the high-performing SAM for landcover classification on space-borne Synthetic Aperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's parameters and incorporates lightweight adapters for parameter efficient fine-tuning, and a classwise mask decoder is designed to achieve semantic segmentation task. This adapt-tuning method allows for efficient landcover classification of SAR images, balancing the accuracy with computational demand. In addition, the task specific input module injects low frequency information of SAR images by MLP-based layers to improve the model performance. Compared to conventional state-of-the-art semantic segmentation algorithms by extensive experiments, CWSAM showcases enhanced performance with fewer computing resources, highlighting the potential of leveraging foundational models like SAM for specific downstream tasks in the SAR domain. The source code is available at: https://github.com/xypu98/CWSAM.",
                "authors": "Xinyang Pu, He Jia, Linghao Zheng, Feng Wang, Feng Xu",
                "citations": 12
            },
            {
                "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code",
                "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.",
                "authors": "Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi",
                "citations": 12
            },
            {
                "title": "Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation",
                "abstract": "Despite the significant progress in deep learning for dense visual recognition problems, such as semantic segmentation, traditional methods are constrained by fixed class sets. Meanwhile, vision-language foundation models, such as CLIP, have showcased remarkable effectiveness in numerous zero-shot image-level tasks, owing to their robust generalizability. Recently, a body of work has investigated utilizing these models in open-vocabulary semantic segmentation (OVSS). However, existing approaches often rely on impractical supervised pre-training or access to additional pre-trained networks. In this work, we propose a strong baseline for training-free OVSS, termed Neighbour-Aware CLIP (NACLIP), representing a straightforward adaptation of CLIP tailored for this scenario. Our method enforces localization of patches in the self-attention of CLIP's vision transformer which, despite being crucial for dense prediction tasks, has been overlooked in the OVSS literature. By incorporating design choices favouring segmentation, our approach significantly improves performance without requiring additional data, auxiliary pre-trained networks, or extensive hyperparameter tuning, making it highly practical for real-world applications. Experiments are performed on 8 popular semantic segmentation benchmarks, yielding state-of-the-art performance on most scenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP.",
                "authors": "Sina Hajimiri, Ismail Ben Ayed, J. Dolz",
                "citations": 10
            },
            {
                "title": "Business analytics and decision science: A review of techniques in strategic business decision making",
                "abstract": "Business analytics and decision science have emerged as pivotal domains in enhancing strategic business decision-making processes. This review delves into various techniques that organizations employ to optimize their operations and achieve competitive advantages. At the forefront of strategic decision-making is data analytics, where vast amounts of data are analyzed to extract valuable insights. Descriptive analytics provides a historical perspective by examining past data trends, enabling businesses to understand their performance over time. This retrospective analysis serves as a foundation for predictive analytics, which utilizes statistical models and machine learning algorithms to forecast future trends and outcomes. By leveraging predictive analytics, organizations can anticipate market shifts, customer preferences, and potential risks, thereby making informed decisions. Prescriptive analytics uses predictive models to guide strategic decision-making, utilizing optimization algorithms and simulation tools to identify optimal actions. Decision science integrates analytical techniques with human judgment, focusing on consumer behavior and psychological factors to tailor marketing strategies and product offerings. Additionally, artificial intelligence (AI) and machine learning (ML) technologies are revolutionizing strategic decision-making by automating complex tasks and providing real-time insights. Natural language processing (NLP) algorithms analyze unstructured data sources, such as customer reviews and social media posts, to extract valuable information and sentiment analysis. This enables businesses to gauge customer satisfaction levels and identify areas for improvement promptly. Decision trees, regression analysis, and clustering techniques are widely used in business analytics to segment customers, identify patterns, forecast sales trends, evaluate alternatives, assess risks, and optimize resource allocation. In conclusion, business analytics and decision science offer a plethora of techniques that empower organizations to make informed, data-driven decisions. By leveraging descriptive, predictive, and prescriptive analytics, along with AI and ML technologies, businesses can navigate complex environments, capitalize on opportunities, and mitigate risks effectively. This review underscores the importance of integrating analytical techniques with human expertise to achieve strategic objectives and sustainable growth.",
                "authors": "Andrew Ifesinachi Daraojimba, Chidera Victoria, Ibeh, Onyeka Franca Asuzu, Temidayo Olorunsogo, Oluwafunmi Adijat Elufioye, Ndubuisi Leonard Nduubuisi",
                "citations": 23
            },
            {
                "title": "GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist Collaboration",
                "abstract": "Generalist foundation models (GFMs) are renowned for their exceptional capability and flexibility in effectively generalizing across diverse tasks and modalities. In the field of medicine, while GFMs exhibit superior generalizability based on their extensive intrinsic knowledge as well as proficiency in instruction following and in-context learning, specialist models excel in precision due to their domain knowledge. In this work, for the first time, we explore the synergy between the GFM and specialist models, to enable precise medical image analysis on a broader scope. Specifically, we propose a cooperative framework, Generalist-Specialist Collaboration (GSCo), which consists of two stages, namely the construction of GFM and specialists, and collaborative inference on downstream tasks. In the construction stage, we develop MedDr, the largest open-source GFM tailored for medicine, showcasing exceptional instruction-following and in-context learning capabilities. Meanwhile, a series of lightweight specialists are crafted for downstream tasks with low computational cost. In the collaborative inference stage, we introduce two cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented Diagnosis, to harvest the generalist's in-context learning abilities alongside the specialists' domain expertise. For a comprehensive evaluation, we curate a large-scale benchmark featuring 28 datasets and about 250,000 images. Extensive results demonstrate that MedDr consistently outperforms state-of-the-art GFMs on downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists across all out-of-domain disease diagnosis datasets. These findings indicate a significant paradigm shift in the application of GFMs, transitioning from separate models for specific tasks to a collaborative approach between GFMs and specialists, thereby advancing the frontiers of generalizable AI in medicine.",
                "authors": "Sunan He, Yuxiang Nie, Hongmei Wang, Shu Yang, Yihui Wang, Zhiyuan Cai, Zhixuan Chen, Yingxue Xu, Luyang Luo, Huiling Xiang, Xi Lin, Mingxiang Wu, Yifan Peng, George Shih, Ziyang Xu, Xian Wu, Qiong Wang, R. Chan, V. Vardhanabhuti, Winnie Chiu Wing Chu, Yefeng Zheng, Pranav Rajpurkar, Kang Zhang, Hao Chen",
                "citations": 10
            },
            {
                "title": "Segment Any Cell: A SAM-based Auto-prompting Fine-tuning Framework for Nuclei Segmentation",
                "abstract": "In the rapidly evolving field of AI research, foundational models like BERT and GPT have significantly advanced language and vision tasks. The advent of pretrain-prompting models such as ChatGPT and Segmentation Anything Model (SAM) has further revolutionized image segmentation. However, their applications in specialized areas, particularly in nuclei segmentation within medical imaging, reveal a key challenge: the generation of high-quality, informative prompts is as crucial as applying state-of-the-art (SOTA) fine-tuning techniques on foundation models. To address this, we introduce Segment Any Cell (SAC), an innovative framework that enhances SAM specifically for nuclei segmentation. SAC integrates a Low-Rank Adaptation (LoRA) within the attention layer of the Transformer to improve the fine-tuning process, outperforming existing SOTA methods. It also introduces an innovative auto-prompt generator that produces effective prompts to guide segmentation, a critical factor in handling the complexities of nuclei segmentation in biomedical imaging. Our extensive experiments demonstrate the superiority of SAC in nuclei segmentation tasks, proving its effectiveness as a tool for pathologists and researchers. Our contributions include a novel prompt generation strategy, automated adaptability for diverse segmentation tasks, the innovative application of Low-Rank Attention Adaptation in SAM, and a versatile framework for semantic segmentation challenges.",
                "authors": "Saiyang Na, Yuzhi Guo, Feng Jiang, Hehuan Ma, Junzhou Huang",
                "citations": 10
            },
            {
                "title": "ChatGPT-Based Scenario Engineer: A New Framework on Scenario Generation for Trajectory Prediction",
                "abstract": "The latest developments in parallel driving foreshadow the possibility of delivering intelligence across organizations using foundation models. As is well-known, there are limitations in scenario acquisition in the field of intelligent vehicles (IV), such as efficiency, diversity, and complexity, which hinder in-depth research of vehicle intelligence. To address this issue, this manuscript draws inspiration from scenarios engineering, parallel driving and introduces a pioneering framework for scenario generation, leveraging the ChatGPT, denoted as SeGPT. Within this framework, we define a trajectory scenario and design prompts engineering to generate complex and challenging scenarios. Furthermore, SeGPT, in combination with “Three Modes”, foundation models, vehicle operating system, and other advanced infrastructure, holds the potential to achieve higher levels of autonomous driving. Experimental outcomes substantiate SeGPT's adeptness in producing a spectrum of varied scenarios, underscoring its potential to augment the development of trajectory prediction algorithms. These findings mark significant progress in the domain of scenario generation, also pointing towards new directions in the research of vehicle intelligence and scenarios engineering.",
                "authors": "Xuan Li, Enlu Liu, Tianyu Shen, Jun Huang, Fei-Yue Wang",
                "citations": 11
            },
            {
                "title": "ChatNT: A Multimodal Conversational Agent for DNA, RNA and Protein Tasks",
                "abstract": "Language models are thriving, powering conversational agents that assist and empower humans to solve a number of tasks. Recently, these models were extended to support additional modalities including vision, audio and video, demonstrating impressive capabilities across multiple domains including healthcare. Still, conversational agents remain limited in biology as they cannot yet fully comprehend biological sequences. On the other hand, high-performance foundation models for biological sequences have been built through self-supervision over sequencing data, but these need to be fine-tuned for each specific application, preventing transfer and generalization between tasks. In addition, these models are not conversational which limits their utility to users with coding capabilities. In this paper, we propose to bridge the gap between biology foundation models and conversational agents by introducing ChatNT, the first multimodal conversational agent with an advanced understanding of biological sequences. ChatNT achieves new state-of-the-art results on the Nucleotide Transformer benchmark while being able to solve all tasks at once, in English, and to generalize to unseen questions. In addition, we have curated a new set of more biologically relevant instructions tasks from DNA, RNA and proteins, spanning multiple species, tissues and biological processes. ChatNT reaches performance on par with state-of-the-art specialized methods on those tasks. We also present a novel perplexity-based technique to help calibrate the confidence of our model predictions. Our framework for genomics instruction-tuning can be easily extended to more tasks and biological data modalities (e.g. structure, imaging), making it a widely applicable tool for biology. ChatNT is the first model of its kind and constitutes an initial step towards building generally capable agents that understand biology from first principles while being accessible to users with no coding background.",
                "authors": "Guillaume Richard, B. P. de Almeida, Hugo Dalla-torre, Christopher Blum, Lorenz Hexemer, Priyanka Pandey, Stefan Laurent, Marie Lopez, Alexandre Laterre, Maren Lang, U. Sahin, Karim Beguir, Thomas Pierrot",
                "citations": 11
            },
            {
                "title": "Language-driven Grasp Detection",
                "abstract": "Grasp detection is a persistent and intricate challenge with various industrial applications. Recently, many meth-ods and datasets have been proposed to tackle the grasp detection problem. However, most of them do not consider using natural language as a condition to detect the grasp poses. In this paper, we introduce Grasp-Anything++, a new language-driven grasp detection dataset featuring 1M samples, over 3M objects, and upwards of 10M grasping in-structions. We utilize foundation models to create a large-scale scene corpus with corresponding images and grasp prompts. We approach the language-driven grasp detection task as a conditional generation problem. Drawing on the success of diffusion models in generative tasks and given that language plays a vital role in this task, we propose a new language-driven grasp detection method based on dif-fusion models. Our key contribution is the contrastive training objective, which explicitly contributes to the denoising process to detect the grasp pose given the language instructions. We illustrate that our approach is theoretically sup-portive. The intensive experiments show that our method outperforms state-of-the-art approaches and allows real-world robotic grasping. Finally, we demonstrate our large-scale dataset enables zero-short grasp detection and is a challenging benchmark for future work.",
                "authors": "Vuong Dinh An, M. Vu, Baoru Huang, Nghia Nguyen, Hieu Le, T. Vo, Anh Nguyen",
                "citations": 11
            },
            {
                "title": "Transformers in single-cell omics: a review and new perspectives.",
                "abstract": null,
                "authors": "Artur Szałata, Karin Hrovatin, Sören Becker, Alejandro Tejada-Lapuerta, Haotian Cui, Bo Wang, F. Theis",
                "citations": 10
            },
            {
                "title": "Simulating human mobility with a trajectory generation framework based on diffusion model",
                "abstract": "Abstract Most mobility modeling methods are designed to solve specific tasks, leading to questions regarding their deficiency in generalizability. Inspired by the bloom of foundation models, we proposed a Trajectory Generation framework based on the Diffusion Model (TrajGDM) to capture the universal mobility pattern in a trajectory dataset by learning the trajectory generation process. The process is modeled as a step-by-step uncertainty-reducing process, in which a deep learning network with a novel training method is proposed to learn from the process. We compared the proposed trajectory generation method with six baselines on two public trajectory datasets. The results showed that the similarity between the generated and real trajectory movements measured by the Jensen-Shannon Divergence improved significantly on both datasets. Moreover, we applied zero-shot inferences on two basic trajectory tasks: trajectory prediction and trajectory reconstruction. The accuracy improved by a maximum of 25.6% on two tasks. The universal mobility pattern that is suitable for solving multiple trajectory tasks is verified, inferring the strong generalizability of our model. Finally, the study provides insights into artificial intelligence’s understanding of human mobility by exploring the way the model maps the trajectory in the latent space into reality.",
                "authors": "Chen Chu, Hengcai Zhang, Peixiao Wang, Feng Lu",
                "citations": 10
            },
            {
                "title": "A Survey of Mamba",
                "abstract": "As one of the most representative DL techniques, Transformer architecture has empowered numerous advanced models, especially the large language models (LLMs) that comprise billions of parameters, becoming a cornerstone in deep learning. Despite the impressive achievements, Transformers still face inherent limitations, particularly the time-consuming inference resulting from the quadratic computation complexity of attention calculation. Recently, a novel architecture named Mamba, drawing inspiration from classical state space models (SSMs), has emerged as a promising alternative for building foundation models, delivering comparable modeling abilities to Transformers while preserving near-linear scalability concerning sequence length. This has sparked an increasing number of studies actively exploring Mamba's potential to achieve impressive performance across diverse domains. Given such rapid evolution, there is a critical need for a systematic review that consolidates existing Mamba-empowered models, offering a comprehensive understanding of this emerging model architecture. In this survey, we therefore conduct an in-depth investigation of recent Mamba-associated studies, covering three main aspects: the advancements of Mamba-based models, the techniques of adapting Mamba to diverse data, and the applications where Mamba can excel. Specifically, we first review the foundational knowledge of various representative deep learning models and the details of Mamba-1&2 as preliminaries. Then, to showcase the significance of Mamba for AI, we comprehensively review the related studies focusing on Mamba models' architecture design, data adaptability, and applications. Finally, we present a discussion of current limitations and explore various promising research directions to provide deeper insights for future investigations.",
                "authors": "Haohao Qu, Liang-bo Ning, Rui An, Wenqi Fan, Tyler Derr, Hui Liu, Xin Xu, Qing Li",
                "citations": 10
            },
            {
                "title": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection",
                "abstract": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a curated IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.",
                "authors": "Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang",
                "citations": 10
            },
            {
                "title": "FLoRA: Low-Rank Core Space for N-dimension",
                "abstract": "Adapting pre-trained foundation models for various downstream tasks has been prevalent in artificial intelligence. Due to the vast number of tasks and high costs, adjusting all parameters becomes unfeasible. To mitigate this, several fine-tuning techniques have been developed to update the pre-trained model weights in a more resource-efficient manner, such as through low-rank adjustments. Yet, almost all of these methods focus on linear weights, neglecting the intricacies of parameter spaces in higher dimensions like 4D. Alternatively, some methods can be adapted for high-dimensional parameter space by compressing changes in the original space into two dimensions and then employing low-rank matrix decomposition. However, these approaches destructs the structural integrity of the involved high-dimensional spaces. To tackle the diversity of dimensional spaces across different foundation models and provide a more precise representation of the changes within these spaces, this paper introduces a generalized parameter-efficient fine-tuning framework, FLoRA, designed for various dimensional parameter space. Specifically, utilizing Tucker decomposition, FLoRA asserts that changes in each dimensional parameter space are based on a low-rank core space which maintains the consistent topological structure with the original space. It then models the changes through this core space alongside corresponding weights to reconstruct alterations in the original space. FLoRA effectively preserves the structural integrity of the change of original N-dimensional parameter space, meanwhile decomposes it via low-rank tensor decomposition. Extensive experiments on computer vision, natural language processing and multi-modal tasks validate FLoRA's effectiveness. Codes are available at https://github.com/SJTU-DeepVisionLab/FLoRA.",
                "authors": "Chongjie Si, Xuehui Wang, Xue Yang, Zhengqin Xu, Qingyun Li, Jifeng Dai, Yu Qiao, Xiaokang Yang, Wei Shen",
                "citations": 9
            },
            {
                "title": "RAP-SAM: Towards Real-Time All-Purpose Segment Anything",
                "abstract": "Advanced by transformer architecture, vision foundation models (VFMs) achieve remarkable progress in performance and generalization ability. Segment Anything Model (SAM) is one remarkable model that can achieve generalized segmentation. However, most VFMs cannot run in realtime, which makes it difficult to transfer them into several products. On the other hand, current real-time segmentation mainly has one purpose, such as semantic segmentation on the driving scene. We argue that diverse outputs are needed for real applications. Thus, this work explores a new real-time segmentation setting, named all-purpose segmentation in real-time, to transfer VFMs in real-time deployment. It contains three different tasks, including interactive segmentation, panoptic segmentation, and video segmentation. We aim to use one model to achieve the above tasks in real-time. We first benchmark several strong baselines. Then, we present Real-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an efficient decoupled decoder to perform prompt-driven decoding. Moreover, we further explore different training strategies and tuning methods to boost co-training performance further. Our code and model are available at https://github.com/xushilin1/RAP-SAM/.",
                "authors": "Shilin Xu, Haobo Yuan, Qingyu Shi, Lu Qi, Jingbo Wang, Yibo Yang, Yining Li, Kai Chen, Yunhai Tong, Bernard Ghanem, Xiangtai Li, Ming-Hsuan Yang",
                "citations": 9
            },
            {
                "title": "Recovering Generalization via Pre-Training-Like Knowledge Distillation for Out-of-Distribution Visual Question Answering",
                "abstract": "With the emergence of large-scale multi-modal foundation models, significant improvements have been made towards Visual Question Answering (VQA) in recent years via the “Pre-training and Fine-tuning” paradigm. However, the fine-tuned VQA model, which is more specialized for the downstream training data, may fail to generalize well when there is a distribution shift between the training and test data, which is defined as the Out-of-Distribution (OOD) problem. An intuitive way to solve this problem is to transfer the common knowledge from the foundation model to the fine-tuned VQA model via knowledge distillation for better generalization. However, the generality of distilled knowledge based on the task-specific training data is questionable due to the bias between the training and test data. An ideal way is to adopt the pre-training data to distill the common knowledge shared by the training and OOD test samples, which however is impracticable due to the huge size of pre-training data. Based on the above considerations, in this article, we propose a method, named Pre-training-like Knowledge Distillation (PKD), to imitate the pre-training feature distribution and leverage it to distill the common knowledge, which can improve the generalization performance of the fine-tuned model for OOD VQA. Specifically, we first leverage the in-domain VQA data as guidance and adopt two cross-modal feature prediction networks, which are learned under the supervision of image-text matching loss and feature divergence loss, to estimate pre-training-like vision and text features. Next, we conduct feature-level distillation by explicitly integrating the downstream VQA input features with the predicted pre-training-like features through a memory mechanism. In the meantime, we also conduct model-level distillation by constraining the image-text matching output of the downstream VQA model and the output of the foundation model for the pre-training-like image and text features. Extensive experiments on the VQA-CP v2 and VQA v2 datasets demonstrate the effectiveness of our method.",
                "authors": "Y. Song, Xiaoshan Yang, Yaowei Wang, Changsheng Xu",
                "citations": 9
            },
            {
                "title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet",
                "abstract": "With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis. Based on our results, we discussed how researchers with different backgrounds could help with this problem from different perspectives.",
                "authors": "Weizhe Chen, Sven Koenig, B. Dilkina",
                "citations": 9
            },
            {
                "title": "Large Language Model Supply Chain: A Research Agenda",
                "abstract": "The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, introducing unprecedented capabilities in natural language processing and multimodal content generation. However, the increasing complexity and scale of these models have given rise to a multifaceted supply chain that presents unique challenges across infrastructure, foundation models, and downstream applications. This paper provides the first comprehensive research agenda of the LLM supply chain, offering a structured approach to identify critical challenges and opportunities through the dual lenses of software engineering (SE) and security & privacy (S&P). We begin by establishing a clear definition of the LLM supply chain, encompassing its components and dependencies. We then analyze each layer of the supply chain, presenting a vision for robust and secure LLM development, reviewing the current state of practices and technologies, and identifying key challenges and research opportunities. This work aims to bridge the existing research gap in systematically understanding the multifaceted issues within the LLM supply chain, offering valuable insights to guide future efforts in this rapidly evolving domain.",
                "authors": "Shenao Wang, Yanjie Zhao, Xinyi Hou, Haoyu Wang",
                "citations": 9
            },
            {
                "title": "RingMo-Lite: A Remote Sensing Lightweight Network With CNN-Transformer Hybrid Framework",
                "abstract": "In recent years, remote sensing (RS) vision foundation models, such as RingMo, have emerged and achieved excellent performance in various downstream tasks. However, the high demand for computing resources limits the application of these models on edge devices. It is necessary to design a more lightweight foundation model to support on-orbit RS image interpretation. Existing methods face challenges in achieving lightweight solutions while retaining generalization in RS image interpretation. This is due to the complex high-frequency (H-F) and low-frequency (L-F) spectral components in RS images, which make traditional single convolutional neural network (CNN) or vision Transformer methods unsuitable for the task. Therefore, this article proposes RingMo-lite, an RS lightweight network with a CNN-Transformer hybrid framework, which effectively exploits the frequency-domain properties of RS to optimize the interpretation process on several tasks like classification, object detection, semantic segmentation, and change detection. It is combined by the Transformer module as a low-pass filter to extract global features of RS images through a dual-branch structure and the CNN module as a stacked high-pass filter to extract fine-grained details effectively. Furthermore, a novelty-designed frequency-domain masked image modeling (FD-MIM) is employed during the pretraining stage for self-supervised learning, which combines the H-F and L-F characteristics of each image patch. This approach effectively captures the latent feature representation in RS data. Compared with RingMo, the proposed RingMo-lite reduces the parameters by over 60% in various RS image interpretation tasks, and the average accuracy drops by less than 2% in most of the scenes and achieves state-of-the-art (SOTA) performance compared to models of similar size. In addition, our work will be integrated into the MindSpore computing platform in the near future.",
                "authors": "Yuelei Wang, Ting Zhang, Liangjin Zhao, Lin Hu, Zhechao Wang, Ziqing Niu, Peirui Cheng, Kaiqiang Chen, Xuan Zeng, Zhirui Wang, Hongqi Wang, Xian Sun",
                "citations": 9
            },
            {
                "title": "MegaScenes: Scene-Level View Synthesis at Scale",
                "abstract": "Scene-level novel view synthesis (NVS) is fundamental to many vision and graphics applications. Recently, pose-conditioned diffusion models have led to significant progress by extracting 3D information from 2D foundation models, but these methods are limited by the lack of scene-level training data. Common dataset choices either consist of isolated objects (Objaverse), or of object-centric scenes with limited pose distributions (DTU, CO3D). In this paper, we create a large-scale scene-level dataset from Internet photo collections, called MegaScenes, which contains over 100K structure from motion (SfM) reconstructions from around the world. Internet photos represent a scalable data source but come with challenges such as lighting and transient objects. We address these issues to further create a subset suitable for the task of NVS. Additionally, we analyze failure cases of state-of-the-art NVS methods and significantly improve generation consistency. Through extensive experiments, we validate the effectiveness of both our dataset and method on generating in-the-wild scenes. For details on the dataset and code, see our project page at https://megascenes.github.io.",
                "authors": "Joseph Tung, Gene Chou, Ruojin Cai, Guandao Yang, Kai Zhang, Gordon Wetzstein, Bharath Hariharan, Noah Snavely",
                "citations": 9
            },
            {
                "title": "An empirical study of LLaMA3 quantization: from LLMs to MLLMs",
                "abstract": null,
                "authors": "Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, Michele Magno",
                "citations": 21
            },
            {
                "title": "SpiRit-LM: Interleaved Spoken and Written Language Model",
                "abstract": "Abstract We introduce SpiRit-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code.1,2",
                "authors": "Tu Nguyen, Benjamin Muller, Bokai Yu, M. Costa-jussà, Maha Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Gabriel Synnaeve, Juan Pino, Benoit Sagot, Emmanuel Dupoux",
                "citations": 21
            },
            {
                "title": "Heavy Metal Exposure: Molecular Pathways, Clinical Implications, and Protective Strategies",
                "abstract": "Heavy metals are often found in soil and can contaminate drinking water, posing a serious threat to human health. Molecular pathways and curation therapies for mitigating heavy metal toxicity have been studied for a long time. Recent studies on oxidative stress and aging have shown that the molecular foundation of cellular damage caused by heavy metals, namely, apoptosis, endoplasmic reticulum stress, and mitochondrial stress, share the same pathways as those involved in cellular senescence and aging. In recent aging studies, many types of heavy metal exposures have been used in both cellular and animal aging models. Chelation therapy is a traditional treatment for heavy metal toxicity. However, recently, various antioxidants have been found to be effective in treating heavy metal-induced damage, shifting the research focus to investigating the interplay between antioxidants and heavy metals. In this review, we introduce the molecular basis of heavy metal-induced cellular damage and its relationship with aging, summarize its clinical implications, and discuss antioxidants and other agents with protective effects against heavy metal damage.",
                "authors": "Hajime Koyama, Teru Kamogashira, Tatsuya Yamasoba",
                "citations": 20
            },
            {
                "title": "A Patterned Human Neural Tube Model Using Microfluidic Gradients.",
                "abstract": null,
                "authors": "X. Xue, Yung Su Kim, Alfredo-Isaac Ponce-Arias, Richard O'Laughlin, R. Yan, Norio Kobayashi, R. Y. Tshuva, Yu-Hwai Tsai, Shiyu Sun, Yi Zheng, Yue Liu, Frederick C K Wong, A. Surani, Jason R. Spence, Hongjun Song, G. Ming, O. Reiner, Jianping Fu",
                "citations": 21
            },
            {
                "title": "VLP: Vision Language Planning for Autonomous Driving",
                "abstract": "Autonomous driving is a complex and challenging task that aims at safe motion planning through scene understanding and reasoning. While vision-only autonomous driving methods have recently achieved notable performance, through enhanced scene understanding, several key issues, including lack of reasoning, low generalization performance and long-tail scenarios, still need to be addressed. In this paper, we present VLP, a novel Vision-Language-Planningframework that exploits language models to bridge the gap between linguistic understanding and autonomous driving. VLP enhances autonomous driving systems by strengthening both the source memory foundation and the self-driving car's contextual understanding. VLP achieves state-of-the-art end-to-end planning performance on the challenging NuScenes dataset by achieving 35.9% and 60.5% reduction in terms of average L2 error and collision rates, respectively, compared to the previous best method. Moreover, VLP shows improved performance in challenging long-tail scenarios and strong generalization capabilities when faced with new urban environments.",
                "authors": "Chenbin Pan, Burhaneddin Yaman, T. Nesti, Abhirup Mallik, A. Allievi, Senem Velipasalar, Liu Ren",
                "citations": 21
            },
            {
                "title": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
                "abstract": "Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.",
                "authors": "Yang Liu, Weixing Chen, Yongjie Bai, Jing-Hua Luo, Xinshuai Song, Kaixuan Jiang, Zhida Li, Ganlong Zhao, Junyi Lin, Guanbin Li, Wen Gao, Liang Lin",
                "citations": 21
            },
            {
                "title": "Language-based game theory in the age of artificial intelligence",
                "abstract": "Understanding human behaviour in decision problems and strategic interactions has wide-ranging applications in economics, psychology and artificial intelligence. Game theory offers a robust foundation for this understanding, based on the idea that individuals aim to maximize a utility function. However, the exact factors influencing strategy choices remain elusive. While traditional models try to explain human behaviour as a function of the outcomes of available actions, recent experimental research reveals that linguistic content significantly impacts decision-making, thus prompting a paradigm shift from outcome-based to language-based utility functions. This shift is more urgent than ever, given the advancement of generative AI, which has the potential to support humans in making critical decisions through language-based interactions. We propose sentiment analysis as a fundamental tool for this shift and take an initial step by analysing 61 experimental instructions from the dictator game, an economic game capturing the balance between self-interest and the interest of others, which is at the core of many social interactions. Our meta-analysis shows that sentiment analysis can explain human behaviour beyond economic outcomes. We discuss future research directions. We hope this work sets the stage for a novel game-theoretical approach that emphasizes the importance of language in human decisions.",
                "authors": "V. Capraro, Roberto Di Paolo, M. Perc, Veronica Pizziol",
                "citations": 20
            },
            {
                "title": "Visual Knowledge in the Big Model Era: Retrospect and Prospect",
                "abstract": "Visual knowledge is a new form of knowledge representation that can encapsulate visual concepts and their relations in a succinct, comprehensive, and interpretable manner, with a deep root in cognitive psychology. As the knowledge about the visual world has been identified as an indispensable component of human cognition and intelligence, visual knowledge is poised to have a pivotal role in establishing machine intelligence. With the recent advance of Artificial Intelligence (AI) techniques, large AI models (or foundation models) have emerged as a potent tool capable of extracting versatile patterns from broad data as implicit knowledge, and abstracting them into an outrageous amount of numeric parameters. To pave the way for creating visual knowledge empowered AI machines in this coming wave, we present a timely review that investigates the origins and development of visual knowledge in the pre-big model era, and accentuates the opportunities and unique role of visual knowledge in the big model era.",
                "authors": "Wenguan Wang, Yi Yang, Yunhe Pan",
                "citations": 8
            },
            {
                "title": "Performance and Non-adversarial Robustness of the Segment Anything Model 2 in Surgical Video Segmentation",
                "abstract": "Fully supervised deep learning (DL) models for surgical video segmentation have been shown to struggle with non-adversarial, real-world corruptions of image quality including smoke, bleeding, and low illumination. Foundation models for image segmentation, such as the segment anything model (SAM) that focuses on interactive prompt-based segmentation, move away from semantic classes and thus can be trained on larger and more diverse data, which offers outstanding zero-shot generalization with appropriate user prompts. Recently, building upon this success, SAM-2 has been proposed to further extend the zero-shot interactive segmentation capabilities from independent frame-by-frame to video segmentation. In this paper, we present a first experimental study evaluating SAM-2's performance on surgical video data. Leveraging the SegSTRONG-C MICCAI EndoVIS 2024 sub-challenge dataset, we assess SAM-2's effectiveness on uncorrupted endoscopic sequences and evaluate its non-adversarial robustness on videos with corrupted image quality simulating smoke, bleeding, and low brightness conditions under various prompt strategies. Our experiments demonstrate that SAM-2, in zero-shot manner, can achieve competitive or even superior performance compared to fully-supervised deep learning models on surgical video data, including under non-adversarial corruptions of image quality. Additionally, SAM-2 consistently outperforms the original SAM and its medical variants across all conditions. Finally, frame-sparse prompting can consistently outperform frame-wise prompting for SAM-2, suggesting that allowing SAM-2 to leverage its temporal modeling capabilities leads to more coherent and accurate segmentation compared to frequent prompting.",
                "authors": "Yiqing Shen, Hao Ding, Xinyuan Shao, Mathias Unberath",
                "citations": 8
            },
            {
                "title": "DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences",
                "abstract": "Recent advancements in foundation models (FMs) have unlocked new prospects in autonomous driving, yet the experimental settings of these studies are preliminary, oversimplified, and fail to capture the complexity of real-world driving scenarios in human environments. It remains under-explored whether FM agents can handle long-horizon navigation tasks with free-from dialogue and deal with unexpected situations caused by environmental dynamics or task changes. To explore the capabilities and boundaries of FMs faced with the challenges above, we introduce DriVLMe, a video-language-model-based agent to facilitate natural and effective communication between humans and autonomous vehicles that perceive the environment and navigate. We develop DriVLMe from both embodied experiences in a simulated environment and social experiences from real human dialogue. While DriVLMe demonstrates competitive performance in both open-loop benchmarks and closed-loop human studies, we reveal several limitations and challenges, including unacceptable inference time, imbalanced training data, limited visual understanding, challenges with multi-turn interactions, simplified language generation from robotic experiences, and difficulties in handling on-the-fly unexpected situations like environmental dynamics and task changes. Nevertheless, DriVLMe offers a promising new direction for autonomous driving agents that need to navigate not just complex environments but also complex social interactions.",
                "authors": "Yidong Huang, Jacob Sansom, Ziqiao Ma, Felix Gervits, Joyce Chai",
                "citations": 8
            },
            {
                "title": "TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis",
                "abstract": "This work studies the problem of time series analysis with generalist (or foundation) models, which are models trained across many data domains. Drawing inspiration from the widespread success of large language models, we consider the simple strategy of discretely tokenizing time series data drawn from a myriad of datasets via self-supervision, then using the fixed tokenization to solve a variety of tasks across many data domains. Canonically, time series models are either trained on a single dataset or built in a task-specific manner (e.g., a forecasting-only model), where many use patches of time as inputs to the model. As such, performant generalist, discrete representation time series models explored across many tasks are of value. Our method, TOkenized Time Series EMbeddings (TOTEM), produces such generalist time series models with minimal or no fine-tuning while exhibiting strong zero-shot performance. We evaluate TOTEM extensively over nearly 500 experiments on three commonly-studied time series tasks with real-world data: imputation (17 baselines, 12 datasets), anomaly detection (19 baselines, 25 datasets), and forecasting (14 baselines, 12 datasets). We conclude that TOTEM matches or outperforms existing state-of-the-art models in both the canonical specialist setting (i.e., training one model on one domain) as well as the generalist setting (i.e., training a single model on many domains), which demonstrates the efficacy of tokenization for general time series analysis. The open-source implementation is available here: https://github.com/SaberaTalukder/TOTEM; a video summary is available here: https://www.youtube.com/watch?v=OqrCpdb6MJk.",
                "authors": "Sabera Talukder, Yisong Yue, Georgia Gkioxari",
                "citations": 8
            },
            {
                "title": "Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment",
                "abstract": "Deep Learning (DL) is undergoing a paradigm shift with the emergence of foundation models, aptly named by their crucial, yet incomplete nature. In this work, we focus on Contrastive Language-Image Pre-training (CLIP), an open-vocabulary foundation model, which achieves high accuracy across many image classification tasks and is often competitive with a fully supervised baseline without being explicitly trained. Nevertheless, there are still domains where zero-shot CLIP performance is far from optimal, such as Remote Sensing (RS) and medical imagery. These domains do not only exhibit fundamentally different distributions compared to natural images, but also commonly rely on complementary modalities, beyond RGB, to derive meaningful insights. To this end, we propose a methodology for the purpose of aligning distinct RS imagery modalities with the visual and textual modalities of CLIP. Our two-stage procedure, comprises of robust fine-tuning CLIP in order to deal with the distribution shift, accompanied by the cross-modal alignment of a RS modality encoder, in an effort to extend the zero-shot capabilities of CLIP. We ultimately demonstrate our method on the tasks of RS imagery classification and cross-modal retrieval. We empirically show that both robust fine-tuning and cross-modal alignment translate to significant performance gains, across several RS benchmark datasets. Notably, these enhancements are achieved without the reliance on textual descriptions, without introducing any task-specific parameters, without training from scratch and without catastrophic forgetting.",
                "authors": "Angelos Zavras, Dimitrios Michail, Begum Demir, Ioannis Papoutsis",
                "citations": 8
            },
            {
                "title": "Segment Any Change",
                "abstract": "Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem. In this paper, we propose the segment any change models (AnyChange), a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions. AnyChange is built on the segment anything model (SAM) via our training-free adaptation method, bitemporal latent matching. By revealing and exploiting intra-image and inter-image semantic similarities in SAM's latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way. We also propose a point query mechanism to enable AnyChange's zero-shot object-centric change detection capability. We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection. AnyChange sets a new record on the SECOND benchmark for unsupervised change detection, exceeding the previous SOTA by up to 4.4% F$_1$ score, and achieving comparable accuracy with negligible manual annotations (1 pixel per image) for supervised change detection.",
                "authors": "Zhuo Zheng, Yanfei Zhong, Liangpei Zhang, Stefano Ermon",
                "citations": 8
            },
            {
                "title": "VistaRAG: Toward Safe and Trustworthy Autonomous Driving Through Retrieval-Augmented Generation",
                "abstract": "Autonomous driving based on foundation models has recently garnered widespread attention. However, the risk of hallucinations inherent in foundation models could compromise the safety and reliability of autonomous driving systems. This letter, as part of a series of reports from the Distributed/Decentralized Hybrid Workshop on Foundation/Infrastructure Intelligence (DHW-FII), aims to tackle these issues. We introduce VistaRAG, which integrates retrieval-augmented generation (RAG) technologies into autonomous driving systems based on foundation models, to address the inherent reliability challenges in decision-making. VistaRAG employs a dynamic retrieval mechanism to access highly relevant driving experience, real-time road network status, and other contextual information from external databases. This aids foundation models in informed reasoning and decision-making, thereby enhancing the safety and trustworthiness of foundation-model-based autonomous driving systems under complex traffic scenarios.",
                "authors": "Xingyuan Dai, Chao Guo, Yun Tang, Haichuan Li, Yutong Wang, Jun Huang, Yonglin Tian, Xin Xia, Yisheng Lv, Fei-Yue Wang",
                "citations": 8
            },
            {
                "title": "Performance Assessment of Universal Machine Learning Interatomic Potentials: Challenges and Directions for Materials' Surfaces.",
                "abstract": "Machine learning interatomic potentials (MLIPs) are one of the main techniques in the materials science toolbox, able to bridge ab initio accuracy with the computational efficiency of classical force fields. This allows simulations ranging from atoms, molecules, and biosystems, to solid and bulk materials, surfaces, nanomaterials, and their interfaces and complex interactions. A recent class of advanced MLIPs, which use equivariant representations and deep graph neural networks, is known as universal models. These models are proposed as foundation models suitable for any system, covering most elements from the periodic table. Current universal MLIPs (UIPs) have been trained with the largest consistent data set available nowadays. However, these are composed mostly of bulk materials' DFT calculations. In this article, we assess the universality of all openly available UIPs, namely MACE, CHGNet, and M3GNet, in a representative task of generalization: calculation of surface energies. We find that the out-of-the-box foundation models have significant shortcomings in this task, with errors correlated to the total energy of surface simulations, having an out-of-domain distance from the training data set. Our results show that while UIPs are an efficient starting point for fine-tuning specialized models, we envision the potential of increasing the coverage of the materials space toward universal training data sets for MLIPs.",
                "authors": "B. Focassio, Luis Paulo M Freitas, G. R. Schleder",
                "citations": 8
            },
            {
                "title": "Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study",
                "abstract": "Despite the success in specific tasks and scenarios, existing foundation agents, empowered by large models (LMs) and advanced tools, still cannot generalize to different scenarios, mainly due to dramatic differences in the observations and actions across scenarios. In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction. The main challenges of achieving GCC are: 1) the multimodal observations for decision-making, 2) the requirements of accurate control of keyboard and mouse, 3) the need for long-term memory and reasoning, and 4) the abilities of efficient exploration and self-improvement. To target GCC, we introduce C RADLE , an agent framework with six main modules, including: 1) information gathering to extract multi-modality information, 2) self-reflection to rethink",
                "authors": "Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, Yifei Bi, Pengjie Gu, Xinrun Wang, Börje F. Karlsson, Bo An, Zongqing Lu",
                "citations": 18
            },
            {
                "title": "A Complete Survey on LLM-based AI Chatbots",
                "abstract": "The past few decades have witnessed an upsurge in data, forming the foundation for data-hungry, learning-based AI technology. Conversational agents, often referred to as AI chatbots, rely heavily on such data to train large language models (LLMs) and generate new content (knowledge) in response to user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have set new standards in the AI community. This paper presents a complete survey of the evolution and deployment of LLM-based chatbots in various sectors. We first summarize the development of foundational chatbots, followed by the evolution of LLMs, and then provide an overview of LLM-based chatbots currently in use and those in the development phase. Recognizing AI chatbots as tools for generating new knowledge, we explore their diverse applications across various industries. We then discuss the open challenges, considering how the data used to train the LLMs and the misuse of the generated knowledge can cause several issues. Finally, we explore the future outlook to augment their efficiency and reliability in numerous applications. By addressing key milestones and the present-day context of LLM-based chatbots, our survey invites readers to delve deeper into this realm, reflecting on how their next generation will reshape conversational AI.",
                "authors": "Sumit Kumar Dam, Choong-Seon Hong, Yu Qiao, Chaoning Zhang",
                "citations": 18
            },
            {
                "title": "Memristive Tabu Learning Neuron Generated Multi-Wing Attractor With FPGA Implementation and Application in Encryption",
                "abstract": "Memristors, with their unique nonlinear characteristics, are highly suitable for construction novel neural models with rich dynamic behaviors. In this paper, a memristor with piecewise nonlinear state function is introduced into the tabu learning neuron model, resulting in a novel memristive tabu learning neuron model capable of generating a double-wing chaotic butterfly. By modulating the state function of the memristor, we can effectively and easily alter the number of wings of the chaotic butterfly. Equilibrium points analysis further elucidates the mechanism behind the generation of multi-wing chaos. Various numerical simulation techniques, including phase portraits, bifurcation diagrams, Lyapunov exponent spectra, and local attraction basins, are employed to illustrate the dynamical behaviors of the proposed model. Moreover, the newly constructed neuron model is validated using FPGA hardware, with the results aligning with numerical simulations, thereby offering a dependable foundation for a memristor digital circuit-based brain-like neuron model. Lastly, an image encryption application based on the multi-wing chaotic butterfly is developed to demonstrate the potential application of the model.",
                "authors": "Quanli Deng, Chunhua Wang, Yichuang Sun, Zekun Deng, Gang Yang",
                "citations": 8
            },
            {
                "title": "A Smart Irrigation System Using the IoT and Advanced Machine Learning Model",
                "abstract": "The rapid advancement of IoT (Internet of Things) technologies and sophisticated machine learning models is driving innovation in irrigation systems, laying the foundation for more effective and eco-friendly smart agricultural procedures. This systematic literature review strives to uncover the advancements and challenges in the advancement and implementation of IoT-based smart irrigation systems integrated with advanced machine learning techniques. By analyzing 43 relevant studies published between 2017 and 2024, the research focuses on the ability of these technologies have evolved to meet the challenges of modern agriculture irrigation system. Predictive analytics, anomaly detection, and adaptive control—that enhance irrigation precision and decision-making processes. Employing the PRISMA methodology, this review uncovers the strengths and limitations of current systems, highlighting significant achievements in real-time data utilization and system responsiveness. However, it also brings attention to unresolved issues, including the complexities of data integration, network reliability, and the scalability of IoT-based frameworks. Additionally, the study identifies crucial gaps in standardization and the need for flexible solutions that can adapt to diverse environmental conditions. By offering a comprehensive analysis, this review provides key insights for advancing smart irrigation technologies, emphasizing the importance of continued research in overcoming the existing barriers to wider adoption and effectiveness in various agricultural settings.",
                "authors": "Ponugoti Kalpana, L. Smitha, Dasari Madhavi, S. A. Nabi, G. Kalpana, Sarangam Kodati",
                "citations": 15
            },
            {
                "title": "Predicting financial enterprise stocks and economic data trends using machine learning time series analysis",
                "abstract": "This paper explores the application of machine learning in financial time series analysis, focusing on predicting trends in financial enterprise stocks and economic data. It begins by distinguishing stocks from stocks and elucidates risk management strategies in the stock market. Traditional statistical methods such as ARIMA and exponential smoothing are discussed in terms of their advantages and limitations in economic forecasting. Subsequently, the effectiveness of machine learning techniques, particularly LSTM and CNN-BiLSTM hybrid models, in financial market prediction is detailed, highlighting their capability to capture nonlinear patterns in dynamic markets. Finally, the paper outlines prospects for machine learning in financial forecasting, laying a theoretical foundation and methodological framework for achieving more precise and reliable economic predictions.",
                "authors": "Haotian Zheng, Jiang Wu, Runze Song, Lingfeng Guo, Zeqiu Xu",
                "citations": 16
            },
            {
                "title": "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing",
                "abstract": "Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides LLM-powered agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user’s footage, serving as the foundation for enabling the LLM to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated LAVE’s effectiveness. The results also shed light on user perceptions of the proposed LLM-assisted editing paradigm and its impact on users’ creativity and sense of co-creation. Based on these findings, we propose design implications to inform the future development of agent-assisted content editing.",
                "authors": "Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, Raj Sodhi",
                "citations": 15
            },
            {
                "title": "A Survey of Data-Efficient Graph Learning",
                "abstract": "Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including self-supervised graph learning, semi-supervised graph learning, and few-shot graph learning. Also, we state promising directions for future research, contributing to the evolution of graph machine learning.",
                "authors": "Wei Ju, Siyu Yi, Yifan Wang, Qingqing Long, Junyu Luo, Zhiping Xiao, Ming Zhang",
                "citations": 15
            },
            {
                "title": "LLMScenario: Large Language Model Driven Scenario Generation",
                "abstract": "Scenario engineering plays a vital role in various Industry 5.0 applications. In the field of autonomous driving systems, driving scenario data are important for the training and testing of critical modules. However, the corner scenario cases are usually rare and necessary to be extended. Existing methods cannot handle the interpretation and reasoning of the generation process well, which reduces the reliability and usability of the generated scenarios. With the rapid development of Foundation Models, especially the large language model (LLM), we can conduct scenario generation via more powerful tools. In this article, we propose LLMScenario, a novel LLM-driven scenario generation framework, which is composed of scenario prompt engineering, LLM scenario generation, and evaluation feedback tuning. The minimum scenario description specific to LLM is given by scenario analysis and ablation studies. We also appropriately design the score functions in terms of reality and rarity to evaluate the generated scenarios. The model performance is further enhanced through chain-of-thoughts and experiences. Different LLMs are also compared with our framework. Experimental results on naturalistic datasets demonstrate the effectiveness of LLMScenario, which can provide solid support for scenario engineering in Industry 5.0.",
                "authors": "Chen Chang, Siqi Wang, Jiawei Zhang, Jingwei Ge, Li Li",
                "citations": 6
            },
            {
                "title": "Adapting Segment Anything Model to Aerial Land Cover Classification With Low-Rank Adaptation",
                "abstract": "Recently, vision foundation models have gathered wide attention. Among the many endeavors, the segment anything model (SAM) makes remarkable progress toward a universal model showing unprecedented generalization ability. We propose a novel semantic segmentation model that combines SAM’s image encoder and a low-rank adaptation (LoRA) approach for feature extraction and fine-tuning on aerial images. We also employ an auxiliary CNN encoder to facilitate downstream adaptation and complement the ViT encoder on dense vision tasks. Furthermore, cross-attention is utilized to implement feature interactions between the two encoders. Finally, the UperNet head is employed for multiscale feature fusion and generating segmentation masks. The proposed model was evaluated on the ISPRS Vaihingen and Potsdam datasets and achieved the best mean intersection-over-union (mIoU) of 76.44 and 78.01 for the two datasets. The evaluation results demonstrate the superiority of our model.",
                "authors": "Bowei Xue, Han Cheng, Qingqing Yang, Yi Wang, Xiaoning He",
                "citations": 7
            },
            {
                "title": "From Image to Video, what do we need in multimodal LLMs?",
                "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated profound capabilities in understanding multimodal information, covering from Image LLMs to the more complex Video LLMs. Numerous studies have illustrated their exceptional cross-modal comprehension. Recently, integrating video foundation models with large language models to build a comprehensive video understanding system has been proposed to overcome the limitations of specific pre-defined vision tasks. However, the current advancements in Video LLMs tend to overlook the foundational contributions of Image LLMs, often opting for more complicated structures and a wide variety of multimodal data for pre-training. This approach significantly increases the costs associated with these methods.In response to these challenges, this work introduces an efficient method that strategically leverages the priors of Image LLMs, facilitating a resource-efficient transition from Image to Video LLMs. We propose RED-VILLM, a Resource-Efficient Development pipeline for Video LLMs from Image LLMs, which utilizes a temporal adaptation plug-and-play structure within the image fusion module of Image LLMs. This adaptation extends their understanding capabilities to include temporal information, enabling the development of Video LLMs that not only surpass baseline performances but also do so with minimal instructional data and training resources. Our approach highlights the potential for a more cost-effective and scalable advancement in multimodal models, effectively building upon the foundational work of Image LLMs.",
                "authors": "Suyuan Huang, Haoxin Zhang, Yan Gao, Yao Hu, Zengchang Qin",
                "citations": 7
            },
            {
                "title": "SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and Medical Image Segmentation",
                "abstract": "Image segmentation plays an important role in vision understanding. Recently, the emerging vision foundation models continuously achieved superior performance on various tasks. Following such success, in this paper, we prove that the Segment Anything Model 2 (SAM2) can be a strong encoder for U-shaped segmentation models. We propose a simple but effective framework, termed SAM2-UNet, for versatile image segmentation. Specifically, SAM2-UNet adopts the Hiera backbone of SAM2 as the encoder, while the decoder uses the classic U-shaped design. Additionally, adapters are inserted into the encoder to allow parameter-efficient fine-tuning. Preliminary experiments on various downstream tasks, such as camouflaged object detection, salient object detection, marine animal segmentation, mirror detection, and polyp segmentation, demonstrate that our SAM2-UNet can simply beat existing specialized state-of-the-art methods without bells and whistles. Project page: \\url{https://github.com/WZH0120/SAM2-UNet}.",
                "authors": "Xinyu Xiong, Zihuang Wu, Shuangyi Tan, Wenxue Li, Feilong Tang, Ying Chen, Siying Li, Jie Ma, Guanbin Li",
                "citations": 6
            },
            {
                "title": "The harms of terminology: why we should reject so-called \"frontier AI\"",
                "abstract": null,
                "authors": "Gina Helfrich",
                "citations": 7
            },
            {
                "title": "AiRs: Adapter in Remote Sensing for Parameter-Efficient Transfer Learning",
                "abstract": "Remote sensing is stepping into the era of the foundation model, where the fine-tuning paradigm is widely adopted to transfer the profound knowledge of pretrained foundation models to downstream tasks. However, the full fine-tuning method would become inefficient in terms of training and storage, as the foundation models are getting larger and larger. Recently, a lot of deep learning research has proposed various parameter-efficient fine-tuning (PEFT) methods that perform well with a few trainable parameters. However, most of them focus on fine-tuning general foundation models without considering the special properties of remote sensing. In this article, we propose an adapter in remote sensing (AiRs) to fine-tune large foundation models for remote sensing downstream tasks by introducing the adapter-tuning framework. Specifically, we construct AiRs from two aspects: more expressive adaptation modules and a more efficient integration strategy. Specialized adaptation modules are applied to different functional layers in AiRs, which encode the inductive bias of remote sensing images and enhance the semantic concepts of geography. Moreover, AiRs establishes pathways between trainable modules with residual connections, which reduces training difficulty and improves performance. We conduct extensive experiments on object detection, semantic segmentation, and scene classification tasks. By training only 4.4% parameters of the pretrained backbone, AiRs surpasses the previous state-of-the-art (SOTA) PEFT competitors on all experimental datasets and outperforms the full fine-tuning on six out of ten datasets.",
                "authors": "Leiyi Hu, Hongfeng Yu, Wanxuan Lu, Dongshuo Yin, Xian Sun, Kun Fu",
                "citations": 7
            },
            {
                "title": "Deep learning-based predictions of gene perturbation effects do not yet outperform simple linear methods",
                "abstract": "Advanced deep-learning methods, such as transformer-based foundation models, promise to learn representations of biology that can be employed to predict in silico the outcome of unseen experiments, such as the effect of genetic perturbations on the transcriptomes of human cells. To see whether current models already reach this goal, we benchmarked two state-of-the-art foundation models and one popular graph-based deep learning framework against deliberately simplistic linear models in two important use cases: For combinatorial perturbations of two genes for which only data for the individual single perturbations have been seen, we find that a simple additive model outperformed the deep learning-based approaches. Also, for perturbations of genes that have not yet been seen, but which may be “interpolated” from biological similarity or network context, a simple linear model performed as good as the deep learning-based approaches. While the promise of deep neural networks for the representation of biological systems and prediction of experimental outcomes is plausible, our work highlights the need for critical benchmarking to direct research efforts that aim to bring transfer learning to biology.",
                "authors": "C. Ahlmann-Eltze, W. Huber, Simon Anders",
                "citations": 7
            },
            {
                "title": "Imagining a Future of Designing with AI: Dynamic Grounding, Constructive Negotiation, and Sustainable Motivation",
                "abstract": "We ideate a future design workflow that involves AI technology. Drawing from activity and communication theory, we attempt to isolate the new value that large AI models can provide design compared to past technologies. We arrive at three affordances—dynamic grounding, constructive negotiation, and sustainable motivation—that summarize latent qualities of natural language-enabled foundation models that, if explicitly designed for, can support the process of design. Through design fiction, we then imagine a future interface as a diegetic prototype, the story of Squirrel Game, that demonstrates each of our three affordances in a realistic usage scenario. Our design process, terminology, and diagrams aim to contribute to future discussions about the relative affordances of AI technology with regard to collaborating with human designers.",
                "authors": "Priyan Vaithilingam, Ian Arawjo, Elena L. Glassman",
                "citations": 7
            },
            {
                "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
                "abstract": "Dense linear layers are the dominant computational bottleneck in foundation models. Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain. In this work, we systematically explore structured matrices as replacements for dense matrices. We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale. Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers. Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute. We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models.",
                "authors": "Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, Andrew Gordon Wilson",
                "citations": 7
            },
            {
                "title": "RIGID: A Training-free and Model-Agnostic Framework for Robust AI-Generated Image Detection",
                "abstract": "The rapid advances in generative AI models have empowered the creation of highly realistic images with arbitrary content, raising concerns about potential misuse and harm, such as Deepfakes. Current research focuses on training detectors using large datasets of generated images. However, these training-based solutions are often computationally expensive and show limited generalization to unseen generated images. In this paper, we propose a training-free method to distinguish between real and AI-generated images. We first observe that real images are more robust to tiny noise perturbations than AI-generated images in the representation space of vision foundation models. Based on this observation, we propose RIGID, a training-free and model-agnostic method for robust AI-generated image detection. RIGID is a simple yet effective approach that identifies whether an image is AI-generated by comparing the representation similarity between the original and the noise-perturbed counterpart. Our evaluation on a diverse set of AI-generated images and benchmarks shows that RIGID significantly outperforms existing trainingbased and training-free detectors. In particular, the average performance of RIGID exceeds the current best training-free method by more than 25%. Importantly, RIGID exhibits strong generalization across different image generation methods and robustness to image corruptions.",
                "authors": "Zhiyuan He, Pin-Yu Chen, Tsung-Yi Ho",
                "citations": 6
            },
            {
                "title": "Biomedical SAM 2: Segment Anything in Biomedical Images and Videos",
                "abstract": "Medical image segmentation and video object segmentation are essential for diagnosing and analyzing diseases by identifying and measuring biological structures. Recent advances in natural domain have been driven by foundation models like the Segment Anything Model 2 (SAM-2). To explore the performance of SAM-2 in biomedical applications, we designed three evaluation pipelines for single-frame 2D image segmentation, multi-frame 3D image segmentation and multi-frame video segmentation with varied prompt designs, revealing SAM-2's limitations in medical contexts. Consequently, we developed BioSAM-2, an enhanced foundation model optimized for biomedical data based on SAM-2. Our experiments show that BioSAM-2 not only surpasses the performance of existing state-of-the-art foundation models but also matches or even exceeds specialist models, demonstrating its efficacy and potential in the medical domain.",
                "authors": "Zhiling Yan, Weixiang Sun, Rong Zhou, Zhengqing Yuan, Kai Zhang, Yiwei Li, Tianming Liu, Quanzheng Li, Xiang Li, Lifang He, Lichao Sun",
                "citations": 7
            },
            {
                "title": "MERA: A Comprehensive LLM Evaluation in Russian",
                "abstract": "Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find that they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential societal drawbacks.",
                "authors": "Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, M. Tikhonova, Albina Akhmetgareeva, Anton A. Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid S. Sinev, Ulyana Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova, Denis Dimitrov, Alexander Panchenko, Sergey Markov",
                "citations": 7
            },
            {
                "title": "Segment any medical model extended",
                "abstract": "The Segment Anything Model (SAM) has drawn significant attention from researchers who work on medical image segmentation because of its generalizability. However, researchers have found that SAM may have limited performance on medical images compared to state-of-the-art non-foundation models. Regardless, the community sees potential in extending, fine-tuning, modifying, and evaluating SAM for analysis of medical imaging. An increasing number of works have been published focusing on the mentioned four directions, where variants of SAM are proposed. To this end, a unified platform helps push the boundary of the foundation model for medical images, facilitating the use, modification, and validation of SAM and its variants in medical image segmentation. In this work, we introduce SAMM Extended (SAMME), a platform that integrates new SAM variant models, adopts faster communication protocols, accommodates new interactive modes, and allows for fine-tuning of subcomponents of the models. These features can expand the potential of foundation models like SAM, and the results can be translated to applications such as image-guided therapy, mixed reality interaction, robotic navigation, and data augmentation.",
                "authors": "Yihao Liu, Jiaming Zhang, Andres Diaz-Pinto, Haowei Li, A. Martin-Gomez, Amir Kheradmand, Mehran Armand",
                "citations": 7
            },
            {
                "title": "Adapting to Distribution Shift by Visual Domain Prompt Generation",
                "abstract": "In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.",
                "authors": "Zhixiang Chi, Li Gu, Tao Zhong, Huan Liu, Yuanhao Yu, Konstantinos N. Plataniotis, Yang Wang",
                "citations": 6
            },
            {
                "title": "An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems",
                "abstract": "The increasing prevalence of Cyber- Physical Systems and the Internet of Things (CPS-loT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting real-time feedback on human preferences in such human-in-the-Ioop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-the-Ioop based reinforcement learning algorithm AitL-RL, which employs the LLM as a dynamic simulation of the physical environment to learn how to balance between energy savings and occupant comfort. Our results show that LLMs are capable of simulating complex population movements within large open spaces. Besides, AitL- RLdemonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-loT applications. Through this case study, we demonstrate the potential of integrating advanced Foundation Models like LLMs into CPS-loT to enhance system adaptability and efficiency. The project's code can be found on our GitHub repository.",
                "authors": "Hanqing Yang, Marie Siew, Carlee Joe-Wong",
                "citations": 7
            },
            {
                "title": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies",
                "abstract": "Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-Bench 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-Bench 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.",
                "authors": "Yi Zeng, Yu Yang, Andy Zhou, J. Tan, Yuheng Tu, Yifan Mai, Kevin Klyman, Minzhou Pan, Ruoxi Jia, Dawn Song, Percy Liang, Bo Li",
                "citations": 6
            },
            {
                "title": "Open-Endedness is Essential for Artificial Superhuman Intelligence",
                "abstract": "In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internetscale data. Nevertheless, the creation of openended, ever self-improving AI remains elusive. In this position paper, we argue that the ingredients are now in place to achieve openendedness in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI). We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, humanrelevant discoveries. We conclude by examining the safety implications of generally-capable openended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.",
                "authors": "Edward Hughes, Michael D. Dennis, Jack Parker-Holder, Feryal M. P. Behbahani, Aditi Mavalankar, Yuge Shi, Tom Schaul, Tim Rocktaschel",
                "citations": 7
            },
            {
                "title": "The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?",
                "abstract": "There are pronounced differences in the extent to which industrial and academic AI labs use computing resources. We provide a data-driven survey of the role of the compute divide in shaping machine learning research. We show that a compute divide has coincided with a reduced representation of academic-only research teams in compute intensive research topics, especially foundation models. We argue that, academia will likely play a smaller role in advancing the associated techniques, providing critical evaluation and scrutiny, and in the diffusion of such models. Concurrent with this change in research focus, there is a noticeable shift in academic research towards embracing open source, pre-trained models developed within the industry. To address the challenges arising from this trend, especially reduced scrutiny of influential models, we recommend approaches aimed at thoughtfully expanding academic insights. Nationally-sponsored computing infrastructure coupled with open science initiatives could judiciously boost academic compute access, prioritizing research on interpretability, safety and security. Structured access programs and third-party auditing may also allow measured external evaluation of industry systems.",
                "authors": "T. Besiroglu, S. Bergerson, Amelia Michael, Lennart Heim, Xueyun Luo, Neil Thompson",
                "citations": 6
            },
            {
                "title": "Point-SAM: Promptable 3D Segmentation Model for Point Clouds",
                "abstract": "The development of 2D foundation models for image segmentation has been significantly advanced by the Segment Anything Model (SAM). However, achieving similar success in 3D models remains a challenge due to issues such as non-unified data formats, poor model scalability, and the scarcity of labeled data with diverse masks. To this end, we propose a 3D promptable segmentation model Point-SAM, focusing on point clouds. We employ an efficient transformer-based architecture tailored for point clouds, extending SAM to the 3D domain. We then distill the rich knowledge from 2D SAM for Point-SAM training by introducing a data engine to generate part-level and object-level pseudo-labels at scale from 2D SAM. Our model outperforms state-of-the-art 3D segmentation models on several indoor and outdoor benchmarks and demonstrates a variety of applications, such as interactive 3D annotation and zero-shot 3D instance proposal. Codes and demo can be found at https://github.com/zyc00/Point-SAM.",
                "authors": "Yuchen Zhou, Jiayuan Gu, Tung Yen Chiang, Fanbo Xiang, Hao Su",
                "citations": 6
            },
            {
                "title": "The harms of terminology: why we should reject so-called \"frontier AI\"",
                "abstract": null,
                "authors": "Gina Helfrich",
                "citations": 7
            },
            {
                "title": "Text-centric Alignment for Multi-Modality Learning",
                "abstract": "This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availability is dynamic and uncertain.",
                "authors": "Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, Shou-De Lin",
                "citations": 7
            },
            {
                "title": "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost",
                "abstract": "Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget.",
                "authors": "Oana Ignat, Longju Bai, Joan Nwatu, Rada Mihalcea",
                "citations": 6
            },
            {
                "title": "MERA: A Comprehensive LLM Evaluation in Russian",
                "abstract": "Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zero- and few-shot fixed instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find that they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential societal drawbacks.",
                "authors": "Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, M. Tikhonova, Albina Akhmetgareeva, Anton A. Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid S. Sinev, Ulyana Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova, Denis Dimitrov, Alexander Panchenko, Sergey Markov",
                "citations": 7
            },
            {
                "title": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies",
                "abstract": "Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-Bench 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-Bench 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.",
                "authors": "Yi Zeng, Yu Yang, Andy Zhou, J. Tan, Yuheng Tu, Yifan Mai, Kevin Klyman, Minzhou Pan, Ruoxi Jia, Dawn Song, Percy Liang, Bo Li",
                "citations": 6
            },
            {
                "title": "HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?",
                "abstract": "There is an ongoing debate regarding the potential of Large Language Models (LLMs) as foundational models seamlessly integrated with Cyber- Physical Systems (CPS) for interpreting the physical world. In this paper, we carry out a case study to answer the following question: Are LLMs capable of zero-shot human activity recognition (HAR)? Our study, HARGPT, presents an affirmative answer by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data into LLMs and utilizes the role-play and “think step-by-step“ strategies for prompting. We benchmark HARGPT on GPT4 using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and state-of-the-art deep classification models. Remarkably, LLMs successfully recognize human activities from raw IMU data and consistently outperform all the baselines on both datasets. Our findings indicate that by effective prompting, LLMs can interpret raw IMU data based on their knowledge base, possessing a promising potential to analyze raw sensor data of the physical world effectively.",
                "authors": "Sijie Ji, Xinzhe Zheng, Chenshu Wu",
                "citations": 16
            },
            {
                "title": "CapHuman: Capture Your Moments in Parallel Universes",
                "abstract": "We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, facial expressions, and illuminations in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the “encode then learn to align” paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at https://github.com/VamosC/CapHuman.",
                "authors": "Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang",
                "citations": 13
            },
            {
                "title": "Research on Prediction Recommendation System Based on Improved Markov Model",
                "abstract": ": With the rapid development of the Internet and information technology, recommendation systems are playing an increasingly important role in various applications. Traditional recommendation algorithms, such as content-based recommendations and collaborative filtering, have achieved success to some extent. However, they show limitations when dealing with issues like data sparsity and the complexity of user behavior. This paper proposes a prediction recommendation system based on an improved Markov model to address these issues. By introducing the Hidden Markov Model (HMM) and an improved state transition mechanism, the model's predictive capability in handling user behavior sequences is enhanced. This paper first introduces the background and theoretical foundation of recommendation systems and Markov models, then details the design and implementation of the improved Markov model. Experiments on public datasets demonstrate that the recommendation system based on the improved Markov model outperforms traditional methods in terms of recommendation accuracy and user satisfaction. Finally, the paper summarizes the main contributions and suggests potential directions for future research.",
                "authors": "Zhizhong Wu, Xueshe Wang, Shuaishuai Huang, Haowei Yang, Danqing Ma",
                "citations": 13
            },
            {
                "title": "Few-shot learning based on deep learning: A survey.",
                "abstract": "In recent years, with the development of science and technology, powerful computing devices have been constantly developing. As an important foundation, deep learning (DL) technology has achieved many successes in multiple fields. In addition, the success of deep learning also relies on the support of large-scale datasets, which can provide models with a variety of images. The rich information in these images can help the model learn more about various categories of images, thereby improving the classification performance and generalization ability of the model. However, in real application scenarios, it may be difficult for most tasks to collect a large number of images or enough images for model training, which also restricts the performance of the trained model to a certain extent. Therefore, how to use limited samples to train the model with high performance becomes key. In order to improve this problem, the few-shot learning (FSL) strategy is proposed, which aims to obtain a model with strong performance through a small amount of data. Therefore, FSL can play its advantages in some real scene tasks where a large number of training data cannot be obtained. In this review, we will mainly introduce the FSL methods for image classification based on DL, which are mainly divided into four categories: methods based on data enhancement, metric learning, meta-learning and adding other tasks. First, we introduce some classic and advanced FSL methods in the order of categories. Second, we introduce some datasets that are often used to test the performance of FSL methods and the performance of some classical and advanced FSL methods on two common datasets. Finally, we discuss the current challenges and future prospects in this field.",
                "authors": "Wu Zeng, Zheng-ying Xiao",
                "citations": 13
            },
            {
                "title": "Octopus: On-device language model for function calling of software APIs",
                "abstract": "In the rapidly evolving domain of artificial intelligence, Large Language Models (LLMs) play a crucial role due to their advanced text processing and generation abilities. This study introduces a new strategy aimed at harnessing on-device LLMs in invoking software APIs. We meticulously compile a dataset derived from software API documentation and apply fine-tuning to LLMs with capacities of 2B, 3B and 7B parameters, specifically to enhance their proficiency in software API interactions. Our approach concentrates on refining the models' grasp of API structures and syntax, significantly enhancing the accuracy of API function calls. Additionally, we propose \\textit{conditional masking} techniques to ensure outputs in the desired formats and reduce error rates while maintaining inference speeds. We also propose a novel benchmark designed to evaluate the effectiveness of LLMs in API interactions, establishing a foundation for subsequent research. Octopus, the fine-tuned model, is proved to have better performance than GPT-4 for the software APIs calling. This research aims to advance automated software development and API integration, representing substantial progress in aligning LLM capabilities with the demands of practical software engineering applications.",
                "authors": "Wei Chen, Zhiyuan Li, Mingyuan Ma",
                "citations": 12
            },
            {
                "title": "Enhancing LLM-Based Feedback: Insights from Intelligent Tutoring Systems and the Learning Sciences",
                "abstract": "The field of Artificial Intelligence in Education (AIED) focuses on the intersection of technology, education, and psychology, placing a strong emphasis on supporting learners' needs with compassion and understanding. The growing prominence of Large Language Models (LLMs) has led to the development of scalable solutions within educational settings, including generating different types of feedback in Intelligent Tutoring Systems. However, the approach to utilizing these models often involves directly formulating prompts to solicit specific information, lacking a solid theoretical foundation for prompt construction and empirical assessments of their impact on learning. This work advocates careful and caring AIED research by going through previous research on feedback generation in ITS, with emphasis on the theoretical frameworks they utilized and the efficacy of the corresponding design in empirical evaluations, and then suggesting opportunities to apply these evidence-based principles to the design, experiment, and evaluation phases of LLM-based feedback generation. The main contributions of this paper include: an avocation of applying more cautious, theoretically grounded methods in feedback generation in the era of generative AI; and practical suggestions on theory and evidence-based feedback design for LLM-powered ITS.",
                "authors": "John C. Stamper, Ruiwei Xiao, Xinynig Hou",
                "citations": 12
            },
            {
                "title": "Targeting Cancer Hallmarks with Epigallocatechin Gallate (EGCG): Mechanistic Basis and Therapeutic Targets",
                "abstract": "Epigallocatechin gallate (EGCG) is a catechin, which is a type of flavonoid found in high concentrations in green tea. EGCG has been studied extensively for its potential health benefits, particularly in cancer. EGCG has been found to exhibit anti-proliferative, anti-angiogenic, and pro-apoptotic effects in numerous cancer cell lines and animal models. EGCG has demonstrated the ability to interrupt various signaling pathways associated with cellular proliferation and division in different cancer types. EGCG anticancer activity is mediated by interfering with various cancer hallmarks. This article summarize and highlight the effects of EGCG on cancer hallmarks and focused on the impacts of EGCG on these cancer-related hallmarks. The studies discussed in this review enrich the understanding of EGCG’s potential as a therapeutic tool against cancer, offering a substantial foundation for scientists and medical experts to advance scientific and clinical investigations regarding EGCG’s possibility as a potential anticancer treatment.",
                "authors": "Wamidh H. Talib, Dima Awajan, A. Alqudah, Razan Alsawwaf, Raha Althunibat, Mahmoud Abu AlRoos, Ala’a Al Safadi, Sharif Abu Asab, R. W. Hadi, L. A. Al Kury",
                "citations": 12
            },
            {
                "title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix",
                "abstract": "Large Language Models (LLMs) have shown immense potential in enhancing various aspects of our daily lives, from conversational AI to search and AI assistants. However, their growing capabilities come at the cost of extremely large model sizes, making deployment on edge devices challenging due to memory and computational constraints. This paper introduces a novel approach to LLM weight pruning that directly optimizes for approximating the attention matrix, a core component of transformer architectures. Unlike existing methods that focus on linear approximations, our approach accounts for the non-linear nature of the Softmax attention mechanism. We provide theoretical guarantees for the convergence of our Gradient Descent-based optimization method to a near-optimal pruning mask solution. Our preliminary empirical results demonstrate the effectiveness of this approach in maintaining model performance while significantly reducing computational costs. This work establishes a new theoretical foundation for pruning algorithm design in LLMs, potentially paving the way for more efficient LLM inference on resource-constrained devices.",
                "authors": "Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Yufa Zhou",
                "citations": 12
            },
            {
                "title": "Cross Prompting Consistency with Segment Anything Model for Semi-supervised Medical Image Segmentation",
                "abstract": "Semi-supervised learning (SSL) has achieved notable progress in medical image segmentation. To achieve effective SSL, a model needs to be able to efficiently learn from limited labeled data and effectively exploiting knowledge from abundant unlabeled data. Recent developments in visual foundation models, such as the Segment Anything Model (SAM), have demonstrated remarkable adaptability with improved sample efficiency. To harness the power of foundation models for application in SSL, we propose a cross prompting consistency method with segment anything model (CPC-SAM) for semi-supervised medical image segmentation. Our method employs SAM's unique prompt design and innovates a cross-prompting strategy within a dual-branch framework to automatically generate prompts and supervisions across two decoder branches, enabling effectively learning from both scarce labeled and valuable unlabeled data. We further design a novel prompt consistency regularization, to reduce the prompt position sensitivity and to enhance the output invariance under different prompts. We validate our method on two medical image segmentation tasks. The extensive experiments with different labeled-data ratios and modalities demonstrate the superiority of our proposed method over the state-of-the-art SSL methods, with more than 9% Dice improvement on the breast cancer segmentation task.",
                "authors": "Juzheng Miao, Cheng Chen, Keli Zhang, Jie Chuai, Quanzheng Li, P. Heng",
                "citations": 5
            },
            {
                "title": "An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape",
                "abstract": "Deepfake or synthetic images produced using deep generative models pose serious risks to online platforms. This has triggered several research efforts to accurately detect deepfake images, achieving excellent performance on publicly available deepfake datasets. In this work, we study 8 state-of-the-art detectors and argue that they are far from being ready for deployment due to two recent developments. First, the emergence of lightweight methods to customize large generative models, can enable an attacker to create many customized generators (to create deepfakes), thereby substantially increasing the threat surface. We show that existing defenses fail to generalize well to such user-customized generative models that are publicly available today. We discuss new machine learning approaches based on content-agnostic features, and ensemble modeling to improve generalization performance against user-customized models. Second, the emergence of vision foundation models—machine learning models trained on broad data that can be easily adapted to several downstream tasks—can be misused by attackers to craft adversarial deepfakes that can evade existing defenses. We propose a simple adversarial attack that leverages existing foundation models to craft adversarial samples without adding any adversarial noise, through careful semantic manipulation of the image content. We highlight the vulnerabilities of several defenses against our attack, and explore directions leveraging advanced foundation models and adversarial training to defend against this new threat.",
                "authors": "Sifat Muhammad Abdullah, Aravind Cheruvu, Shravya Kanchi, Taejoong Chung, Peng Gao, Murtuza Jadliwala, Bimal Viswanath, Virginia Tech, UT San",
                "citations": 5
            },
            {
                "title": "Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning",
                "abstract": "AI Foundation models are gaining traction in various applications, including medical fields like radiology. However, medical foundation models are often tested on limited tasks, leaving their generalisability and biases unexplored. We present RayDINO, a large visual encoder trained by self-supervision on 873k chest X-rays. We compare RayDINO to previous state-of-the-art models across nine radiology tasks, from classification and dense segmentation to text generation, and provide an in depth analysis of population, age and sex biases of our model. Our findings suggest that self-supervision allows patient-centric AI proving useful in clinical workflows and interpreting X-rays holistically. With RayDINO and small task-specific adapters, we reach state-of-the-art results and improve generalization to unseen populations while mitigating bias, illustrating the true promise of foundation models: versatility and robustness.",
                "authors": "Théo Moutakanni, Piotr Bojanowski, G. Chassagnon, C'eline Hudelot, Armand Joulin, Yann LeCun, Matthew J. Muckley, Maxime Oquab, M. Revel, M. Vakalopoulou",
                "citations": 5
            },
            {
                "title": "U2++ MoE: Scaling 4.7x parameters with minimal impact on RTF",
                "abstract": "Scale has opened new frontiers in natural language processing, but at a high cost. In response, by learning to only activate a subset of parameters in training and inference, Mixture-of-Experts (MoE) have been proposed as an energy efficient path to even larger and more capable language models and this shift towards a new generation of foundation models is gaining momentum, particularly within the field of Automatic Speech Recognition (ASR). Recent works that incorporating MoE into ASR models have complex designs such as routing frames via supplementary embedding network, improving multilingual ability for the experts, and utilizing dedicated auxiliary losses for either expert load balancing or specific language handling. We found that delicate designs are not necessary, while an embarrassingly simple substitution of MoE layers for all Feed-Forward Network (FFN) layers is competent for the ASR task. To be more specific, we benchmark our proposed model on a large scale inner-source dataset (160k hours), the results show that we can scale our baseline Conformer (Dense-225M) to its MoE counterparts (MoE-1B) and achieve Dense-1B level Word Error Rate (WER) while maintaining a Dense-225M level Real Time Factor (RTF). Furthermore, by applying Unified 2-pass framework with bidirectional attention decoders (U2++), we achieve the streaming and non-streaming decoding modes in a single MoE based model, which we call U2++ MoE. We hope that our study can facilitate the research on scaling speech foundation models without sacrificing deployment efficiency.",
                "authors": "Xingchen Song, Di Wu, Binbin Zhang, Dinghao Zhou, Zhendong Peng, Bo Dang, Fuping Pan, Chao Yang",
                "citations": 5
            },
            {
                "title": "PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences",
                "abstract": "Large foundation models pretrained on raw web-scale data are not readily deployable without additional step of extensive alignment to human preferences. Such alignment is typically done by collecting large amounts of pairwise comparisons from humans (\"Do you prefer output A or B?\") and learning a reward model or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a human's underlying implicit preferences. These methods generally suffer from assuming a universal preference shared by all humans, which lacks the flexibility of adapting to plurality of opinions and preferences. In this work, we propose PAL, a framework to model human preference complementary to existing pretraining strategies, which incorporates plurality from the ground up. We propose using the ideal point model as a lens to view alignment using preference comparisons. Together with our novel reformulation and using mixture modeling, our framework captures the plurality of population preferences while simultaneously learning a common preference latent space across different preferences, which can few-shot generalize to new, unseen users. Our approach enables us to use the penultimate-layer representation of large foundation models and simple MLP layers to learn reward functions that are on-par with the existing large state-of-the-art reward models, thereby enhancing efficiency of reward modeling significantly. We show that PAL achieves competitive reward model accuracy compared to strong baselines on 1) Language models with Summary dataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new semisynthetic heterogeneous dataset generated using Anthropic Personas. Finally, our experiments also highlight the shortcoming of current preference datasets that are created using rigid rubrics which wash away heterogeneity, and call for more nuanced data collection approaches.",
                "authors": "Daiwei Chen, Yi Chen, Aniket Rege, Ramya Korlakai Vinayak",
                "citations": 5
            },
            {
                "title": "Improving 2D Feature Representations by 3D-Aware Fine-Tuning",
                "abstract": "Current visual foundation models are trained purely on unstructured 2D data, limiting their understanding of 3D structure of objects and scenes. In this work, we show that fine-tuning on 3D-aware data improves the quality of emerging semantic features. We design a method to lift semantic 2D features into an efficient 3D Gaussian representation, which allows us to re-render them for arbitrary views. Using the rendered 3D-aware features, we design a fine-tuning strategy to transfer such 3D awareness into a 2D foundation model. We demonstrate that models fine-tuned in that way produce features that readily improve downstream task performance in semantic segmentation and depth estimation through simple linear probing. Notably, though fined-tuned on a single indoor dataset, the improvement is transferable to a variety of indoor datasets and out-of-domain datasets. We hope our study encourages the community to consider injecting 3D awareness when training 2D foundation models. Project page: https://ywyue.github.io/FiT3D.",
                "authors": "Yuanwen Yue, Anurag Das, Francis Engelmann, Siyu Tang, J. E. Lenssen",
                "citations": 5
            },
            {
                "title": "Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning",
                "abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as the predominant technique for fine-tuning in the era of large language models. However, existing PEFT methods still have inadequate training efficiency. Firstly, the utilization of large-scale foundation models during the training process is excessively redundant for certain fine-tuning tasks. Secondly, as the model size increases, the growth in trainable parameters of empirically added PEFT modules becomes non-negligible and redundant, leading to inefficiency. To achieve task-specific efficient fine-tuning, we propose the Light-PEFT framework, which includes two methods: Masked Early Pruning of the Foundation Model and Multi-Granularity Early Pruning of PEFT. The Light-PEFT framework allows for the simultaneous estimation of redundant parameters in both the foundation model and PEFT modules during the early stage of training. These parameters can then be pruned for more efficient fine-tuning. We validate our approach on GLUE, SuperGLUE, QA tasks, and various models. With Light-PEFT, parameters of the foundation model can be pruned by up to over 40%, while still controlling trainable parameters to be only 25% of the original PEFT method. Compared to utilizing the PEFT method directly, Light-PEFT achieves training and inference speedup, reduces memory usage, and maintains comparable performance and the plug-and-play feature of PEFT.",
                "authors": "Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, Weiping Wang",
                "citations": 5
            },
            {
                "title": "Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?",
                "abstract": "New capabilities in foundation models are owed in large part to massive, widely-sourced, and under-documented training data collections. Existing practices in data collection have led to challenges in tracing authenticity, verifying consent, preserving privacy, addressing representation and bias, respecting copyright, and overall developing ethical and trustworthy foundation models. In response, regulation is emphasizing the need for training data transparency to understand foundation models' limitations. Based on a large-scale analysis of the foundation model training data landscape and existing solutions, we identify the missing infrastructure to facilitate responsible foundation model development practices. We examine the current shortcomings of common tools for tracing data authenticity, consent, and documentation, and outline how policymakers, developers, and data creators can facilitate responsible foundation model development by adopting universal data provenance standards.",
                "authors": "Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, Katy Gero, Sandy Pentland, Jad Kabbara",
                "citations": 5
            },
            {
                "title": "TMO-Net: an explainable pretrained multi-omics model for multi-task learning in oncology",
                "abstract": null,
                "authors": "Feng-ao Wang, Zhenfeng Zhuang, Feng Gao, Ruikun He, Shaoting Zhang, Liansheng Wang, Junwei Liu, Yixue Li",
                "citations": 5
            },
            {
                "title": "PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning",
                "abstract": "We present PyTorch Frame, a PyTorch-based framework for deep learning over multi-modal tabular data. PyTorch Frame makes tabular deep learning easy by providing a PyTorch-based data structure to handle complex tabular data, introducing a model abstraction to enable modular implementation of tabular models, and allowing external foundation models to be incorporated to handle complex columns (e.g., LLMs for text columns). We demonstrate the usefulness of PyTorch Frame by implementing diverse tabular models in a modular way, successfully applying these models to complex multi-modal tabular data, and integrating our framework with PyTorch Geometric, a PyTorch library for Graph Neural Networks (GNNs), to perform end-to-end learning over relational databases.",
                "authors": "Weihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan, J. Leskovec, Matthias Fey",
                "citations": 5
            },
            {
                "title": "Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation",
                "abstract": "Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal retriever. Given a radiology image, our retriever can identify high-quality reference reports to augment multimodal foundation models, thus enhancing the factual completeness and correctness of report generation. Experiments on two benchmark datasets show that our multimodal retriever outperforms state-of-the-art retrievers on both language generation and radiology-specific metrics, up to 6.5% and 2% score in F1CheXbert and F1RadGraph. Further analysis indicates that employing our factually-informed training strategy imposes an effective supervision signal, without relying on explicit diagnostic label guidance, and successfully propagates fact-aware capabilities from the multimodal retriever to the multimodal foundation model in radiology report generation.",
                "authors": "Liwen Sun, James Zhao, Megan Han, Chenyan Xiong",
                "citations": 5
            },
            {
                "title": "VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework",
                "abstract": "With the emergence of large language models (LLMs) and vision foundation models, how to combine the intelligence and capacity of these open-sourced or API-available models to achieve open-world visual perception remains an open question. In this paper, we introduce VisionGPT to consolidate and automate the integration of state-of-the-art foundation models, thereby facilitating vision-language understanding and the development of vision-oriented AI. VisionGPT builds upon a generalized multimodal framework that distinguishes itself through three key features: (1) utilizing LLMs (e.g., LLaMA-2) as the pivot to break down users' requests into detailed action proposals to call suitable foundation models; (2) integrating multi-source outputs from foundation models automatically and generating comprehensive responses for users; (3) adaptable to a wide range of applications such as text-conditioned image understanding/generation/editing and visual question answering. This paper outlines the architecture and capabilities of VisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, and generalization, and performance. Our code and models will be made publicly available. Keywords: VisionGPT, Open-world visual perception, Vision-language understanding, Large language model, and Foundation model",
                "authors": "Chris Kelly, Luhui Hu, Bang Yang, Yu Tian, Deshun Yang, Cindy Yang, Zaoshan Huang, Zihao Li, Jiayin Hu, Yuexian Zou",
                "citations": 5
            },
            {
                "title": "Better Call SAL: Towards Learning to Segment Anything in Lidar",
                "abstract": "We propose the SAL (Segment Anything in Lidar) method consisting of a text-promptable zero-shot model for segmenting and classifying any object in Lidar, and a pseudo-labeling engine that facilitates model training without manual supervision. While the established paradigm for Lidar Panoptic Segmentation (LPS) relies on manual supervision for a handful of object classes defined a priori, we utilize 2D vision foundation models to generate 3D supervision ``for free''. Our pseudo-labels consist of instance masks and corresponding CLIP tokens, which we lift to Lidar using calibrated multi-modal data. By training our model on these labels, we distill the 2D foundation models into our Lidar SAL model. Even without manual labels, our model achieves $91\\%$ in terms of class-agnostic segmentation and $54\\%$ in terms of zero-shot Lidar Panoptic Segmentation of the fully supervised state-of-the-art. Furthermore, we outperform several baselines that do not distill but only lift image features to 3D. More importantly, we demonstrate that SAL supports arbitrary class prompts, can be easily extended to new datasets, and shows significant potential to improve with increasing amounts of self-labeled data. Code and models are available at this $\\href{https://github.com/nv-dvl/segment-anything-lidar}{URL}$.",
                "authors": "Aljovsa Ovsep, Tim Meinhardt, Francesco Ferroni, Neehar Peri, Deva Ramanan, Laura Leal-Taix'e",
                "citations": 5
            },
            {
                "title": "UniCorn: A Unified Contrastive Learning Approach for Multi-view Molecular Representation Learning",
                "abstract": "Recently, a noticeable trend has emerged in developing pre-trained foundation models in the domains of CV and NLP. However, for molecular pre-training, there lacks a universal model capable of effectively applying to various categories of molecular tasks, since existing prevalent pre-training methods exhibit effectiveness for specific types of downstream tasks. Furthermore, the lack of profound understanding of existing pre-training methods, including 2D graph masking, 2D-3D contrastive learning, and 3D denoising, hampers the advancement of molecular foundation models. In this work, we provide a unified comprehension of existing pre-training methods through the lens of contrastive learning. Thus their distinctions lie in clustering different views of molecules, which is shown beneficial to specific downstream tasks. To achieve a complete and general-purpose molecular representation, we propose a novel pre-training framework, named UniCorn, that inherits the merits of the three methods, depicting molecular views in three different levels. SOTA performance across quantum, physicochemical, and biological tasks, along with comprehensive ablation study, validate the universality and effectiveness of UniCorn.",
                "authors": "Shikun Feng, Yuyan Ni, Minghao Li, Yanwen Huang, Zhiming Ma, Wei-Ying Ma, Yanyan Lan",
                "citations": 4
            },
            {
                "title": "UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding",
                "abstract": "Vision-language foundation models, represented by Contrastive Language-Image Pre-training (CLIP), have gained increasing attention for jointly understanding both vision and textual tasks. However, existing approaches primarily focus on training models to match global image representations with textual descriptions, thereby overlooking the critical alignment between local regions and corresponding text tokens. This paper extends CLIP with multi-granularity alignment. Notably, we deliberately construct a new dataset comprising pseudo annotations at various levels of granularities, encompassing image-level, region-level as well as pixel-level captions and tags. Accordingly, we develop a Unified Multi-Granularity learning framework, termed UMG-CLIP, which simultaneously empowers the model with versatile perception abilities across different levels of detail. With parameter efficient tuning, UMG-CLIP surpasses current widely used CLIP variants and achieves state-of-the-art performance on diverse image understanding benchmarks, including open-world recognition, retrieval, semantic segmentation, and panoptic segmentation tasks. We believe that UMG-CLIP represents a valuable advancement in vision-language foundation models. The code is available at https://github.com/lygsbw/UMG-CLIP.",
                "authors": "Bowen Shi, Peisen Zhao, Zichen Wang, Yuhang Zhang, Yaoming Wang, Jin Li, Wenrui Dai, Junni Zou, Hongkai Xiong, Qi Tian, Xiaopeng Zhang",
                "citations": 4
            },
            {
                "title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages",
                "abstract": "Large Language Models (LLMs) demonstrate remarkable translation capabilities in high-resource language tasks, yet their performance in low-resource languages is hindered by insufficient multilingual data during pre-training. To address this, we conduct extensive multilingual continual pre-training on the LLaMA series models, enabling translation support across more than 100 languages. Through a comprehensive analysis of training strategies, such as vocabulary expansion and data augmentation, we develop LLaMAX. Remarkably, without sacrificing its generalization ability, LLaMAX achieves significantly higher translation performance compared to existing open-source LLMs (by more than 10 spBLEU points) and performs on-par with specialized translation model (M2M-100-12B) on the Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve as a robust multilingual foundation model. The code \\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and the models \\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available.",
                "authors": "Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, Fei Yuan",
                "citations": 10
            },
            {
                "title": "Qwen2.5 Technical Report",
                "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
                "authors": "Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan",
                "citations": 10
            },
            {
                "title": "MoST: Multi-modality Scene Tokenization for Motion Prediction",
                "abstract": "Many existing motion prediction approaches rely on symbolic perception outputs to generate agent trajectories, such as bounding boxes, road graph information and traffic lights. This symbolic representation is a high-level abstraction of the real world, which may render the motion prediction model vulnerable to perception errors (e.g., failures in detecting open-vocabulary obstacles) while missing salient information from the scene context (e.g., poor road conditions). An alternative paradigm is end-to-end learning from raw sensors. However, this approach suffers from the lack of interpretability and requires significantly more training resources. In this work, we propose tokenizing the visual world into a compact set of scene elements and then leveraging pre-trained image foundation models and LiDAR neural networks to encode all the scene elements in an open-vocabulary manner. The image foundation model enables our scene tokens to encode the general knowledge of the open world while the LiDAR neural network encodes geometry information. Our proposed representation can efficiently encode the multi-frame multi-modality observations with a few hundred tokens and is compatible with most transformer-based architectures. To evaluate our method, we have augmented Waymo Open Motion Dataset with camera embeddings. Experiments over Waymo Open Motion Dataset show that our approach leads to significant performance improvements over the state-of-the-art.",
                "authors": "Norman Mu, Jingwei Ji, Zhenpei Yang, Nate Harada, Haotian Tang, Kan Chen, Charles R. Qi, Runzhou Ge, Kratarth Goel, Zoey Yang, Scott Ettinger, Rami Al-Rfou, Drago Anguelov, Yin Zhou",
                "citations": 4
            },
            {
                "title": "Navigating the Upcoming European Union AI Act",
                "abstract": "The upcoming Artificial Intelligence (AI) Act is the European Union’s attempt to regulate high-risk AI systems and foundation models. We give an up-to-date overview of the act’s key requirements, explain how the high-risk classification works, and highlight what matters for its operationalization.",
                "authors": "Matthias Wagner, Markus Borg, Per Runeson, Matthias Wagner",
                "citations": 4
            },
            {
                "title": "Buffer Overflow in Mixture of Experts",
                "abstract": "Mixture of Experts (MoE) has become a key ingredient for scaling large foundation models while keeping inference costs steady. We show that expert routing strategies that have cross-batch dependencies are vulnerable to attacks. Malicious queries can be sent to a model and can affect a model's output on other benign queries if they are grouped in the same batch. We demonstrate this via a proof-of-concept attack in a toy experimental setting.",
                "authors": "Jamie Hayes, Ilia Shumailov, Itay Yona",
                "citations": 4
            },
            {
                "title": "Re-Mix: Optimizing Data Mixtures for Large Scale Imitation Learning",
                "abstract": "Increasingly large imitation learning datasets are being collected with the goal of training foundation models for robotics. However, despite the fact that data selection has been of utmost importance in vision and natural language processing, little work in robotics has questioned what data such models should actually be trained on. In this work we investigate how to weigh different subsets or ``domains'' of robotics datasets for robot foundation model pre-training. Concrete, we use distributionally robust optimization (DRO) to maximize worst-case performance across all possible downstream domains. Our method, Re-Mix, addresses the wide range of challenges that arise when applying DRO to robotics datasets including variability in action spaces and dynamics across different datasets. Re-Mix employs early stopping, action normalization, and discretization to counteract these issues. Through extensive experimentation on the largest open-source robot manipulation dataset, the Open X-Embodiment dataset, we demonstrate that data curation can have an outsized impact on downstream performance. Specifically, domain weights learned by Re-Mix outperform uniform weights by 38\\% on average and outperform human-selected weights by 32\\% on datasets used to train existing generalist robot policies, specifically the RT-X models.",
                "authors": "Joey Hejna, Chethan Bhateja, Yichen Jian, Karl Pertsch, Dorsa Sadigh",
                "citations": 4
            },
            {
                "title": "Boosting Image Quality Assessment Through Efficient Transformer Adaptation with Local Feature Enhancement",
                "abstract": "Image Quality Assessment (IQA) constitutes a funda-mental task within the field of computer vision, yet it re-mains an unresolved challenge, owing to the intricate dis-tortion conditions, diverse image contents, and limited availability of data. Recently, the community has wit-nessed the emergence of numerous large-scale pretrained foundation models. However, it remains an open problem whether the scaling law in high-level tasks is also appli-cable to IQA tasks which are closely related to low-level clues. In this paper, we demonstrate that with a proper in-jection of local distortion features, a larger pretrained vision transformer (ViT) foundation model performs better in IQA tasks. Specifically, for the lack of local distortion structure and inductive bias of the large-scale pretrained ViT, we use another pretrained convolution neural networks (CNNs), which is well known for capturing the local structure, to extract multi-scale image features. Further, we propose a local distortion extractor to obtain local distortion features from the pretrained CNNs and a local distortion in-jector to inject the local distortion features into ViT. By only training the extractor and injector, our method can benefit from the rich knowledge in the powerful foundation models and achieve state-of-the-art performance on popular IQA datasets, indicating that IQA is not only a low-level problem but also benefits from stronger high-level features drawn from large-scale pretrained models. Codes are publicly available at: https://github.com/NeosXu/LoDa.",
                "authors": "Kangmin Xu, Liang Liao, Jing Xiao, Chaofeng Chen, Haoning Wu, Qiong Yan, Weisi Lin",
                "citations": 4
            },
            {
                "title": "Membership Inference Attacks Cannot Prove that a Model Was Trained On Your Data",
                "abstract": "We consider the problem of a training data proof, where a data creator or owner wants to demonstrate to a third party that some machine learning model was trained on their data. Training data proofs play a key role in recent lawsuits against foundation models trained on web-scale data. Many prior works suggest to instantiate training data proofs using membership inference attacks. We argue that this approach is fundamentally unsound: to provide convincing evidence, the data creator needs to demonstrate that their attack has a low false positive rate, i.e., that the attack's output is unlikely under the null hypothesis that the model was not trained on the target data. Yet, sampling from this null hypothesis is impossible, as we do not know the exact contents of the training set, nor can we (efficiently) retrain a large foundation model. We conclude by offering two paths forward, by showing that data extraction attacks and membership inference on special canary data can be used to create sound training data proofs.",
                "authors": "Jie Zhang, Debeshee Das, Gautam Kamath, F. Tramèr",
                "citations": 4
            },
            {
                "title": "On Prompt Sensitivity of ChatGPT in Affective Computing",
                "abstract": "Recent studies have demonstrated the emerging capabilities of foundation models like ChatGPT in several fields, including affective computing. However, accessing these emerging capabilities is facilitated through prompt engineering. Despite the existence of some prompting techniques, the field is still rapidly evolving and many prompting ideas still require investigation. In this work, we introduce a method to evaluate and investigate the sensitivity of the performance of foundation models based on different prompts or generation parameters. We perform our evaluation on ChatGPT within the scope of affective computing on three major problems, namely sentiment analysis, toxicity detection, and sarcasm detection. First, we carry out a sensitivity analysis on pivotal parameters in auto-regressive text generation, specifically the temperature parameter $T$ and the top-$p$ parameter in Nucleus sampling, dictating how conservative or creative the model should be during generation. Furthermore, we explore the efficacy of several prompting ideas, where we explore how giving different incentives or structures affect the performance. Our evaluation takes into consideration performance measures on the affective computing tasks, and the effectiveness of the model to follow the stated instructions, hence generating easy-to-parse responses to be smoothly used in downstream applications.",
                "authors": "Mostafa M. Amin, Björn W. Schuller",
                "citations": 4
            },
            {
                "title": "A Memristive Hénon Map Based on the State Variable Difference and Its Analog Circuit Implementation",
                "abstract": "In this article, an improved memristive Hénon map is proposed by using the state variable difference instead of the state variable itself as the input of a discrete memristor. As a result, the cumulative module in the discrete memristor can avoid divergence. The bifurcation diagrams, Lyapunov exponent spectrums, and sample entropy complexity analysis results show that the system has rich dynamical behaviors. In addition, analog circuits of the discrete memristor and the memristive chaotic map are designed for the first time. PSIM circuit simulations and breadboard experiments verify the effectiveness of the proposed models. The analog circuits indicate the physical realizability of the discrete memristor. It lays the foundation for the applications of discrete memristor and helps to explain their physics significance.",
                "authors": "Longxiang Fu, Xianming Wu, Shaobo He, Huihai Wang, Ke Sun",
                "citations": 10
            },
            {
                "title": "An Empirical Evaluation of a Generative Artificial Intelligence Technology Adoption Model from Entrepreneurs' Perspectives",
                "abstract": "Technologies, such as Chat Generative Pre-Trained Transformer (ChatGPT, Smart PLS version 4), are prime examples of Generative Artificial Intelligence (AI), which is a constantly evolving area. SMEs, particularly startups, can obtain a competitive edge, innovate their business models, gain business value, and undergo a digital transformation by implementing these technologies. Continuous but gradual experimentation with these technologies is the foundation for their adoption. The experience that comes from trying new technologies can help entrepreneurs adopt new technologies more strategically and experiment more with them. The urgent need for an in-depth investigation is highlighted by the paucity of previous research on ChatGPT uptake in the startup context, particularly from an entrepreneurial perspective. The objective of this research study is to empirically validate the Generative AI technology adoption model to establish the direction and strength of the correlations among the adoption factors from the perspectives of the entrepreneurs. The data are collected from 482 entrepreneurs who exhibit great diversity in their genders, the countries in which their startups are located, the industries their startups serve, their age, their educational levels, their work experience as entrepreneurs, and the length of time the startups have been on the market. Collected data are analyzed using the Partial Least Squares Structural Equation Modeling (PLS-SEM) technique, which results in a statistical examination of the relationships between the adoption model’s factors. The results indicate that social influence, domain experience, technology familiarity, system quality, training and support, interaction convenience, and anthropomorphism are the factors that impact the pre-perception and perception phase of adoption. These factors motivate entrepreneurs to experiment more with the technology, thereby building perceptions of its usefulness, perceived ease of use, and perceived enjoyment, three factors that in turn affect emotions toward the technology and, finally, switching intentions. Control variables like age, gender, and educational attainment have no appreciable effect on switching intentions to alternatives of the Generative AI technology. Rather, the experience factor of running businesses shows itself to be a crucial one. The results have practical implications for entrepreneurs and other innovation ecosystem actors, including, for instance, technology providers, libraries, and policymakers. This research study enriches the Generative AI technology acceptance theory and extends the existing literature by introducing new adoption variables and stages specific to entrepreneurship.",
                "authors": "Varun Gupta",
                "citations": 11
            },
            {
                "title": "Neural Markov Random Field for Stereo Matching",
                "abstract": "Stereo matching is a core task for many computer vision and robotics applications. Despite their dominance in traditional stereo methods, the hand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy compared to end-to-end deep models. While deep learning representations have greatly improved the unary terms of the MRF models, the overall accuracy is still severely limited by the hand-crafted pairwise terms and message passing. To address these issues, we propose a neural MRF model, where both potential functions and message passing are designed using data-driven neural networks. Our fully data-driven model is built on the foundation of variational inference theory, to prevent convergence issues and retain stereo MRF's graph inductive bias. To make the inference tractable and scale well to high-resolution images, we also propose a Disparity Proposal Network (DPN) to adaptively prune the search space of disparity. The proposed approach ranks 1st on both KITTI 2012 and 2015 leaderboards among all published methods while running faster than 100 ms. This approach significantly outperforms prior global methods, e.g., lowering D1 metric by more than 50% on KITTI 2015. In addition, our method exhibits strong cross-domain generalization and can recover sharp edges. The codes at https://github.com/aeolusguan/NMRF.",
                "authors": "Tongfan Guan, Chen Wang, Yunchun Liu",
                "citations": 11
            },
            {
                "title": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation",
                "abstract": "Systematic Literature Reviews (SLRs) have become the foundation of evidence-based studies, enabling researchers to identify, classify, and combine existing studies based on specific research questions. Conducting an SLR is largely a manual process. Over the previous years, researchers have made significant progress in automating certain phases of the SLR process, aiming to reduce the effort and time needed to carry out high-quality SLRs. However, there is still a lack of AI agent-based models that automate the entire SLR process. To this end, we introduce a novel multi-AI agent model designed to fully automate the process of conducting an SLR. By utilizing the capabilities of Large Language Models (LLMs), our proposed model streamlines the review process, enhancing efficiency and accuracy. The model operates through a user-friendly interface where researchers input their topic, and in response, the model generates a search string used to retrieve relevant academic papers. Subsequently, an inclusive and exclusive filtering process is applied, focusing on titles relevant to the specific research area. The model then autonomously summarizes the abstracts of these papers, retaining only those directly related to the field of study. In the final phase, the model conducts a thorough analysis of the selected papers concerning predefined research questions. We also evaluated the proposed model by sharing it with ten competent software engineering researchers for testing and analysis. The researchers expressed strong satisfaction with the proposed model and provided feedback for further improvement. The code for this project can be found on the GitHub repository at https://github.com/GPT-Laboratory/SLR-automation.",
                "authors": "Malik Abdul Sami, Zeeshan Rasheed, Kai-Kristian Kemell, Muhammad Waseem, Terhi Kilamo, Mika Saari, Anh Nguyen-Duc, Kari Systä, Pekka Abrahamsson",
                "citations": 11
            },
            {
                "title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments",
                "abstract": "Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.",
                "authors": "Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Luyao Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang",
                "citations": 11
            },
            {
                "title": "RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT Analysis",
                "abstract": "Developing generalist foundation model has recently attracted tremendous attention among researchers in the field of AI for Medicine (AI4Medicine). A pivotal insight in developing these models is their reliance on dataset scaling, which emphasizes the requirements on developing open-source medical image datasets that incorporate diverse supervision signals across various imaging modalities. In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE. Specifically, we leverage the latest powerful universal segmentation and large language models, to extend the original datasets (over 25,692 non-contrast 3D chest CT volume and reports from 20,000 patients) from the following aspects: (i) organ-level segmentation masks covering 197 categories, which provide intermediate reasoning visual clues for interpretation; (ii) 665 K multi-granularity grounded reports, where each sentence of the report is linked to the corresponding anatomical region of CT volume in the form of a segmentation mask; (iii) 1.3 M grounded VQA pairs, where questions and answers are all linked with reference segmentation masks, enabling models to associate visual evidence with textual explanations. All grounded reports and VQA pairs in the validation set have gone through manual verification to ensure dataset quality. We believe that RadGenome-Chest CT can significantly advance the development of multimodal medical foundation models, by training to generate texts based on given segmentation regions, which is unattainable with previous relevant datasets. We will release all segmentation masks, grounded reports, and VQA pairs to facilitate further research and development in this field.",
                "authors": "Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Jiayu Lei, Ya Zhang, Yanfeng Wang, Weidi Xie",
                "citations": 10
            },
            {
                "title": "BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks (Student Abstract)",
                "abstract": "Image segmentation is foundational to computer vision applications, and the Segment Anything Model (SAM) has become a leading base model for these tasks. However, SAM falters in specialized downstream challenges, leading to various customized SAM models. We introduce BadSAM, a backdoor attack tailored for SAM, revealing that customized models can harbor malicious behaviors. Using the CAMO dataset, we confirm BadSAM's efficacy and identify SAM vulnerabilities. This study paves the way for the development of more secure and customizable vision foundation models.",
                "authors": "Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, Ninghao Liu",
                "citations": 3
            },
            {
                "title": "Zero-shot forecasting of chaotic systems",
                "abstract": "Time-series forecasting is a challenging problem that traditionally requires specialized models custom-trained for the specific task at hand. Recently, inspired by the success of large language models, foundation models pre-trained on vast amounts of time-series data from diverse domains have emerged as a promising candidate for general-purpose time-series forecasting. The defining characteristic of these foundation models is their ability to perform zero-shot learning, that is, forecasting a new system from limited context data without explicit re-training or fine-tuning. Here, we evaluate whether the zero-shot learning paradigm extends to the challenging task of forecasting chaotic systems. Across 135 distinct chaotic dynamical systems and $10^8$ timepoints, we find that foundation models produce competitive forecasts compared to custom-trained models (including NBEATS, TiDE, etc.), particularly when training data is limited. Interestingly, even after point forecasts fail, large foundation models are able to preserve the geometric and statistical properties of the chaotic attractors. We attribute this success to foundation models' ability to perform in-context learning and identify context parroting as a simple mechanism used by these models to capture the long-term behavior of chaotic dynamical systems. Our results highlight the potential of foundation models as a tool for probing nonlinear and complex systems.",
                "authors": "Yuanzhao Zhang, William Gilpin",
                "citations": 3
            },
            {
                "title": "Adapting the Segment Anything Model During Usage in Novel Situations",
                "abstract": "The interactive segmentation task consists in the creation of object segmentation masks based on user interactions. The most common way to guide a model towards producing a correct segmentation consists in clicks on the object and background. The recently published Segment Anything Model (SAM) supports a generalized version of the interactive segmentation problem and has been trained on an object segmentation dataset which contains 1.1B masks. Though being trained extensively and with the explicit purpose of serving as a foundation model, we show significant limitations of SAM when being applied for interactive segmentation on novel domains or object types. On the used datasets, SAM displays a failure rate FR30@90 of up to 72.6%. Since we still want such foundation models to be immediately applicable, we present a framework that can adapt SAM during immediate usage. For this we will leverage the user interactions and masks, which are constructed during the interactive segmentation process. We use this information to generate pseudo-labels, which we use to compute a loss function and optimize a part of the SAM model. The presented method causes a relative reduction of up to 48.1% in the FR20@85 and 46.6% in the FR30@90 metrics.",
                "authors": "Robin Schön, Julian Lorenz, K. Ludwig, Rainer Lienhart",
                "citations": 3
            },
            {
                "title": "PathOCl: Path-Based Prompt Augmentation for OCL Generation with GPT-4",
                "abstract": "The rapid progress of AI-powered programming assistants, such as GitHub Copilot, has facilitated the development of software applications. These assistants rely on large language models (LLMs), which are foundation models (FMs) that support a wide range of tasks related to understanding and generating language. LLMs have demonstrated their ability to express UML model specifications using formal languages like the Object Constraint Language (OCL). However, the context size of the prompt is limited by the number of tokens an LLM can process. This limitation becomes significant as the size of UML class models increases. In this study, we intro-duce PathOCL, a novel path-based prompt augmentation technique designed to facilitate OCL generation. PathOCL addresses the limi-tations of LLMs, specifically their token processing limit and the challenges posed by large UML class models. PathOCL is based on the concept of chunking, which selectively augments the prompts with a subset of UML classes relevant to the English specification. Our findings demonstrate that PathOCL, compared to augmenting the complete UML class model (UML-Augmentation), generates a higher number of valid and correct OCL constraints using the GPT-4 model. Moreover, the average prompt size crafted using PathOCL significantly decreases when scaling the size of the UML class models.",
                "authors": "Seif Abukhalaf, Mohammad Hamdaqa, Foutse Khomh",
                "citations": 3
            },
            {
                "title": "Innovating for Tomorrow: The Convergence of SE and Green AI",
                "abstract": "The latest advancements in machine learning, specifically in foundation models, are revolutionizing the frontiers of existing software engineering (SE) processes. This is a bi-directional phenomona, where 1) software systems are now challenged to provide AI-enabled features to their users, and 2) AI is used to automate tasks within the software development lifecycle. In an era where sustainability is a pressing societal concern, our community needs to adopt a long-term plan enabling a conscious transformation that aligns with environmental sustainability values. In this paper, we reflect on the impact of adopting environmentally friendly practices to create AI-enabled software systems and make considerations on the environmental impact of using foundation models for software development.",
                "authors": "Lu'is Cruz, Xavier Franch Gutierrez, Silverio Mart'inez-Fern'andez",
                "citations": 3
            },
            {
                "title": "Emergent Visual-Semantic Hierarchies in Image-Text Representations",
                "abstract": "While recent vision-and-language models (VLMs) like CLIP are a powerful tool for analyzing text and images in a shared semantic space, they do not explicitly model the hierarchical nature of the set of texts which may describe an image. Conversely, existing multimodal hierarchical representation learning methods require costly training from scratch, failing to leverage the knowledge encoded by state-of-the-art multimodal foundation models. In this work, we study the knowledge of existing foundation models, finding that they exhibit emergent understanding of visual-semantic hierarchies despite not being directly trained for this purpose. We propose the Radial Embedding (RE) framework for probing and optimizing hierarchical understanding, and contribute the HierarCaps dataset, a benchmark facilitating the study of hierarchical knowledge in image--text representations, constructed automatically via large language models. Our results show that foundation VLMs exhibit zero-shot hierarchical understanding, surpassing the performance of prior models explicitly designed for this purpose. Furthermore, we show that foundation models may be better aligned to hierarchical reasoning via a text-only fine-tuning phase, while retaining pretraining knowledge.",
                "authors": "Morris Alper, Hadar Averbuch-Elor",
                "citations": 3
            },
            {
                "title": "ASAM: Boosting Segment Anything Model with Adversarial Tuning",
                "abstract": "In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting ex-ceptional adaptability to a myriad of tasks. Among these, the Segment Anything Model (SAM) by Meta AI has distin-guished itself in image segmentation. However, SAM, like its counterparts, encounters limitations in specific niche ap-plications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities. This pa-per introduces ASAM, a novel methodology that amplifies SAM's performance through adversarial tuning. We har-ness the potential of natural adversarial examples, inspired by their successful implementation in natural language pro-cessing. By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversar-ial instances that are more representative of natural variations rather than conventional imperceptible perturbations. Our approach maintains the photorealism of adversarial ex-amples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task. The fine-tuned ASAM demonstrates significant im-provements across a diverse range of segmentation tasks without necessitating additional data or architectural mod-ifications. The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision. Our project page is in https://asam2024.github.io/.",
                "authors": "Bo Li, Haoke Xiao, Lv Tang",
                "citations": 3
            },
            {
                "title": "SAM-Med3D-MoE: Towards a Non-Forgetting Segment Anything Model via Mixture of Experts for 3D Medical Image Segmentation",
                "abstract": "Volumetric medical image segmentation is pivotal in enhancing disease diagnosis, treatment planning, and advancing medical research. While existing volumetric foundation models for medical image segmentation, such as SAM-Med3D and SegVol, have shown remarkable performance on general organs and tumors, their ability to segment certain categories in clinical downstream tasks remains limited. Supervised Finetuning (SFT) serves as an effective way to adapt such foundation models for task-specific downstream tasks but at the cost of degrading the general knowledge previously stored in the original foundation model.To address this, we propose SAM-Med3D-MoE, a novel framework that seamlessly integrates task-specific finetuned models with the foundational model, creating a unified model at minimal additional training expense for an extra gating network. This gating network, in conjunction with a selection strategy, allows the unified model to achieve comparable performance of the original models in their respective tasks both general and specialized without updating any parameters of them.Our comprehensive experiments demonstrate the efficacy of SAM-Med3D-MoE, with an average Dice performance increase from 53 to 56.4 on 15 specific classes. It especially gets remarkable gains of 29.6, 8.5, 11.2 on the spinal cord, esophagus, and right hip, respectively. Additionally, it achieves 48.9 Dice on the challenging SPPIN2023 Challenge, significantly surpassing the general expert's performance of 32.3. We anticipate that SAM-Med3D-MoE can serve as a new framework for adapting the foundation model to specific areas in medical image analysis. Codes and datasets will be publicly available.",
                "authors": "Guoan Wang, Jin Ye, Junlong Cheng, Tian-Xin Li, Zhaolin Chen, Jianfei Cai, Junjun He, Bohan Zhuang",
                "citations": 2
            },
            {
                "title": "Toto: Time Series Optimized Transformer for Observability",
                "abstract": "This technical report describes the Time Series Optimized Transformer for Observability (Toto), a new state of the art foundation model for time series forecasting developed by Datadog. In addition to advancing the state of the art on generalized time series benchmarks in domains such as electricity and weather, this model is the first general-purpose time series forecasting foundation model to be specifically tuned for observability metrics. Toto was trained on a dataset of one trillion time series data points, the largest among all currently published time series foundation models. Alongside publicly available time series datasets, 75% of the data used to train Toto consists of fully anonymous numerical metric data points from the Datadog platform. In our experiments, Toto outperforms existing time series foundation models on observability data. It does this while also excelling at general-purpose forecasting tasks, achieving state-of-the-art zero-shot performance on multiple open benchmark datasets.",
                "authors": "Ben Cohen, E. Khwaja, Kan Wang, Charles Masson, Elise Ram'e, Youssef Doubli, Othmane Abou-Amal",
                "citations": 2
            },
            {
                "title": "Hermes 3 Technical Report",
                "abstract": "Instruct (or\"chat\") tuned models have become the primary way in which most people interact with large language models. As opposed to\"base\"or\"foundation\"models, instruct-tuned models are optimized to respond to imperative statements. We present Hermes 3, a neutrally-aligned generalist instruct and tool use model with strong reasoning and creative abilities. Its largest version, Hermes 3 405B, achieves state of the art performance among open weight models on several public benchmarks.",
                "authors": "Ryan Teknium, Jeffrey Quesnelle, Chen Guang",
                "citations": 8
            },
            {
                "title": "Unveiling the Potential of LLM-Based ASR on Chinese Open-Source Datasets",
                "abstract": "Large Language Models (LLMs) have demonstrated un-paralleled effectiveness in various NLP tasks, and integrating LLMs with automatic speech recognition (ASR) is becoming a mainstream paradigm. Building upon this momentum, our research delves into an in-depth examination of this paradigm on a large open-source Chinese dataset. Specifically, our research aims to evaluate the impact of various configurations of speech encoders, LLMs, and projector modules in the context of the speech foundation encoder-LLM ASR paradigm. Furthermore, we introduce a three-stage training approach, expressly developed to enhance the model's ability to align auditory and textual information. The implementation of this approach, along-side the strategic integration of ASR components, enabled us to achieve the SOTA performance on the AISHELL-1, Test_Net, and Test_Meeting test sets. Our analysis presents an empirical foundation for future research in LLM-based ASR systems and offers insights into optimizing performance using Chinese datasets. We will publicly release all scripts used for data preparation, training, inference, and scoring, as well as pre-trained models and training logs to promote reproducible research.",
                "authors": "Xuelong Geng, Tianyi Xu, Kun Wei, Bingshen Mu, Hongfei Xue, He Wang, Yangze Li, Pengcheng Guo, Yuhang Dai, Longhao Li, Mingchen Shao, Lei Xie",
                "citations": 9
            },
            {
                "title": "SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning",
                "abstract": "Recent development in Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs) have leverage Attention-based Transformer architectures and achieved superior performance and generalization capabilities. They have since covered extensive areas of traditional learning tasks. For instance, text-based tasks such as text-classification and sequence-labeling, as well as multi-modal tasks like Visual Question Answering (VQA) and Optical Character Recognition (OCR), which were previously addressed using different models, can now be tackled based on one foundation model. Consequently, the training and lightweight fine-tuning of LLMs and MLLMs, especially those based on Transformer architecture, has become particularly important. In recognition of these overwhelming needs, we develop SWIFT, a customizable one-stop infrastructure for large models. With support of over $300+$ LLMs and $50+$ MLLMs, SWIFT stands as the open-source framework that provide the most comprehensive support for fine-tuning large models. In particular, it is the first training framework that provides systematic support for MLLMs. In addition to the core functionalities of fine-tuning, SWIFT also integrates post-training processes such as inference, evaluation, and model quantization, to facilitate fast adoptions of large models in various application scenarios. With a systematic integration of various training techniques, SWIFT offers helpful utilities such as benchmark comparisons among different training techniques for large models. For fine-tuning models specialized in agent framework, we show that notable improvements on the ToolBench leader-board can be achieved by training with customized dataset on SWIFT, with an increase of 5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in hallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.",
                "authors": "Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, Yingda Chen",
                "citations": 9
            },
            {
                "title": "Implementation of seamless assistance with Google Assistant leveraging cloud computing",
                "abstract": "AI and cloud native are mutually reinforcing and inseparable. Due to the huge storage and computing power requirements, most AI applications need cloud support, especially large model applications If cloud native has influenced the software industry to a considerable extent in the past few years, the big model boom means that cloud native has become a standard option for developers.This paper describes the rise of AI model applications and their integration with traditional development workflows, pointing out the challenges that enterprises and developers face when integrating large models. With the rise of cloud-native technologies, the combination of artificial intelligence and cloud computing is becoming increasingly important. Cloud-native technologies provide the infrastructure needed to build and run resilient and scalable applications, while distributed infrastructure supports multi-cloud integration, enabling a unified foundation of \"one cloud, multiple computing.\" As an intelligent voice Assistant, Google Assistant achieves a more intelligent, convenient and efficient user experience through applications in smart home control, enterprise customer service and healthcare. Finally, this paper points out the advantages of combining Google Assistant with cloud computing, providing a more intelligent, convenient, and efficient user experience.",
                "authors": "Jiaxin Huang, Yifan Zhang, Jingyu Xu, Binbin Wu, Bo Liu, Yulu Gong",
                "citations": 9
            },
            {
                "title": "OmniLearn: A Method to Simultaneously Facilitate All Jet Physics Tasks",
                "abstract": "Machine learning has become an essential tool in jet physics. Due to their complex, high-dimensional nature, jets can be explored holistically by neural networks in ways that are not possible manually. However, innovations in all areas of jet physics are proceeding in parallel. We show that specially constructed machine learning models trained for a specific jet classification task can improve the accuracy, precision, or speed of all other jet physics tasks. This is demonstrated by training on a particular multiclass classification task and then using the learned representation for different classification tasks, for datasets with a different (full) detector simulation, for jets from a different collision system ($pp$ versus $ep$), for generative models, for likelihood ratio estimation, and for anomaly detection. Our OmniLearn approach is thus a foundation model and is made publicly available for use in any area where state-of-the-art precision is required for analyses involving jets and their substructure.",
                "authors": "Vinicius Mikuni, Benjamin Nachman",
                "citations": 8
            },
            {
                "title": "Robust Counterfactual Explanations in Machine Learning: A Survey",
                "abstract": "Counterfactual explanations (CEs) are advocated as being ideally suited to providing algorithmic recourse for subjects affected by the predictions of machine learning models. While CEs can be beneficial to affected individuals, recent work has exposed severe issues related to the robustness of state-of-the-art methods for obtaining CEs. Since a lack of robustness may compromise the validity of CEs, techniques to mitigate this risk are in order. In this survey, we review works in the rapidly growing area of robust CEs and perform an in-depth analysis of the forms of robustness they consider. We also discuss existing solutions and their limitations, providing a solid foundation for future developments.",
                "authors": "Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni",
                "citations": 8
            },
            {
                "title": "Revolutionizing Cybersecurity: The GPT-2 Enhanced Attack Detection and Defense (GEADD) Method for Zero-Day Threats",
                "abstract": "The escalating sophistication of cyber threats, particularly zero-day attacks, necessitates advanced detection methodologies in cybersecurity. This study introduces the GPT-2 Enhanced Attack Detection and Defense (GEADD) method, an innovative approach that integrates the GPT-2 model with metaheuristic optimization techniques for enhanced detection of zero-day threats. The GEADD method encompasses data preprocessing, Equilibrium Optimization (EO)-based feature selection, and Salp Swarm Algorithm-Based Optimization (SABO) for hyperparameter tuning, culminating in a robust framework capable of identifying and classifying zero-day attacks with high accuracy. Through a comprehensive evaluation using standard datasets, the GEADD method demonstrates superior performance in detecting zero-day threats compared to existing models, highlighting its potential as a significant contribution to the field of cybersecurity. This study not only presents a novel application of deep learning for cyber threat detection but also sets a foundation for future research in AI-driven cybersecurity solutions",
                "authors": "Rebet Jones, Marwan Omar",
                "citations": 8
            },
            {
                "title": "Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition",
                "abstract": "With the rapid development of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) has become a predominant method in the field of professional knowledge-based question answering. Presently, major foundation model companies have opened up Embedding and Chat API interfaces, and frameworks like LangChain have already integrated the RAG process. It appears that the key models and steps in RAG have been resolved, leading to the question: are professional knowledge QA systems now approaching perfection? This article discovers that current primary methods depend on the premise of accessing high-quality text corpora. However, since professional documents are mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts the effectiveness of professional knowledge-based QA. We conducted an empirical RAG experiment across hundreds of questions from the corresponding real-world professional documents. The results show that, ChatDOC, a RAG system equipped with a panoptic and pinpoint PDF parser, retrieves more accurate and complete segments, and thus better answers. Empirical experiments show that ChatDOC is superior to baseline on nearly 47% of questions, ties for 38% of cases, and falls short on only 15% of cases. It shows that we may revolutionize RAG with enhanced PDF structure recognition.",
                "authors": "Demiao Lin",
                "citations": 8
            },
            {
                "title": "Sequence modeling and design from molecular to genome scale with Evo.",
                "abstract": "The genome is a sequence that encodes the DNA, RNA, and proteins that orchestrate an organism's function. We present Evo, a long-context genomic foundation model with a frontier architecture trained on millions of prokaryotic and phage genomes, and report scaling laws on DNA to complement observations in language and vision. Evo generalizes across DNA, RNA, and proteins, enabling zero-shot function prediction competitive with domain-specific language models and the generation of functional CRISPR-Cas and transposon systems, representing the first examples of protein-RNA and protein-DNA codesign with a language model. Evo also learns how small mutations affect whole-organism fitness and generates megabase-scale sequences with plausible genomic architecture. These prediction and generation capabilities span molecular to genomic scales of complexity, advancing our understanding and control of biology.",
                "authors": "Eric Nguyen, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B Li, Liam J. Bartie, Armin W. Thomas, Samuel King, Garyk Brixi, Jeremy Sullivan, Madelena Y Ng, Ashley Lewis, Aaron Lou, Stefano Ermon, S. Baccus, Tina Hernandez-Boussard, Christopher Ré, Patrick D. Hsu, B. Hie",
                "citations": 8
            },
            {
                "title": "Learning Arabic Language Sciences Based on Technology in Traditional Islamic Boarding Schools in Indonesia",
                "abstract": "This research aims to analyze traditional Islamic boarding school leaders' perceptions of technology, models of learning Arabic sciences in traditional Islamic boarding schools, forms of using technology in learning Arabic language sciences in traditional Islamic boarding schools, and foundation support for using technology in learning sciences. -Arabic language knowledge in traditional Islamic boarding schools. The type of research used is qualitative-descriptive. Research data was obtained using observation methods, in-depth interviews, and documentation. The instruments used are observation lists, interview lists, and documentation lists. Data were analyzed using ATLAS—ti9 software. The research results show that (1). traditional Islamic boarding school leaders' perception of technology is very positive, and there is no rejection of the presence of technology; (2). The model of learning Arabic language sciences in traditional Islamic boarding schools is still preserved, such as using the yellow book with the bandongan, sorogan, and wetonan methods (3). Using technology in learning Arabic language sciences in traditional Islamic boarding schools by operating computers, laptops, focus, and cell phones with the help of the internet (4). The foundation supports the use of technology in learning Arabic language sciences in traditional Islamic boarding schools by disbursing sufficient funds to procure learning technology tools or by building collaborations with other parties. This research aims to increase the use of learning technology in traditional Islamic boarding schools to create effective and efficient learning but remain resistant to Islamic boarding school values and traditions.",
                "authors": "Sahkholid Nasution, Hasan Asari, Harun Al-Rasyid, R. A. Dalimunthe, Aulia Rahman",
                "citations": 8
            },
            {
                "title": "Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness",
                "abstract": "Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence (CTI). In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly, Stanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary classification and Named Entity Recognition (NER) tasks performed using Open Source INTelligence (OSINT). We utilize well-established data collected in previous research from Twitter to assess the competitiveness of these chatbots when compared to specialized models trained for those tasks. In binary classification experiments, Chatbot GPT-4 as a commercial model achieved an acceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1 score of 0.90. However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective. This study demonstrates the capability of chatbots for OSINT binary classification and shows that they require further improvement in NER to effectively replace specially trained models. Our results shed light on the limitations of the LLM chatbots when compared to specialized models, and can help researchers improve chatbots technology with the objective to reduce the required effort to integrate machine learning in OSINT-based CTI tools.",
                "authors": "Samaneh Shafee, A. Bessani, P. M. Ferreira",
                "citations": 8
            },
            {
                "title": "MESA: Matching Everything by Segmenting Anything",
                "abstract": "Feature matching is a crucial task in the field of computer vision, which involves finding correspondences between images. Previous studies achieve remarkable performance using learning-based feature comparison. However, the pervasive presence of matching redundancy between images gives rise to unnecessary and error-prone computations in these methods, imposing limitations on their accuracy. To address this issue, we propose MESA, a novel approach to establish precise area (or region) matches for efficient matching redundancy reduction. MESA first leverages the advanced image understanding capability of SAM, a state-of-the-art foundation model for image segmentation, to obtain image areas with implicit semantic. Then, a multi-relational graph is proposed to model the spatial structure of these areas and construct their scale hierarchy. Based on graphical models derived from the graph, the area matching is reformulated as an energy minimization task and effectively resolved. Extensive experiments demonstrate that MESA yields substantial precision improvement for multiple point matchers in indoor and outdoor downstream tasks, e.g. +13.61% for DKM in indoor pose estimation.",
                "authors": "Yesheng Zhang, Xu Zhao",
                "citations": 7
            },
            {
                "title": "Prompting Multi-Modal Image Segmentation with Semantic Grouping",
                "abstract": "Multi-modal image segmentation is one of the core issues in computer vision. The main challenge lies in integrating common information between modalities while retaining specific patterns for each modality. Existing methods typically perform full fine-tuning on RGB-based pre-trained parameters to inherit the powerful representation of the foundation model. Although effective, such paradigm is not optimal due to weak transferability and scarce downstream data. Inspired by the recent success of prompt learning in language models, we propose the Grouping Prompt Tuning Framework (GoPT), which introduces explicit semantic grouping to learn modal-related prompts, adapting the frozen pre-trained foundation model to various downstream multi-modal segmentation tasks. Specifically, a class-aware uni-modal prompter is designed to balance intra- and inter-modal semantic propagation by grouping modality-specific class tokens, thereby improving the adaptability of spatial information. Furthermore, an alignment-induced cross-modal prompter is introduced to aggregate class-aware representations and share prompt parameters among different modalities to assist in modeling common statistics. Extensive experiments show the superiority of our GoPT, which achieves SOTA performance on various downstream multi-modal image segmentation tasks by training only < 1% model parameters.",
                "authors": "Qibin He",
                "citations": 7
            },
            {
                "title": "What Makes a Business Model Sustainable? Activities, Design Themes, and Value Functions",
                "abstract": "What makes a business model sustainable? To answer this question, we conducted a systematic review of 390 journal articles on business models for sustainability (BMfS). Building on the activity system perspective, we engaged in an active categorization process, in which we identified 26 activity groups focused on 12 design themes. These activities and design themes are associated with organizations’ potential to use their business models to contribute to sustainable value creation. Our analysis also revealed that the identified activities and design themes can be related to three overarching value functions of BMfS: maintaining, unlocking, and sharing value. Our findings indicate that these value functions play a pivotal role in creating sustainable value through business models. The identified design themes can serve as guiding principles for organizations seeking to make their business models sustainable, while the identified value functions can provide a foundation for theorizing on sustainable value creation through business models.",
                "authors": "Florian Lüdeke‐Freund, Tobias Froese, Krzysztof Dembek, Francesco Rosati, Lorenzo Massa",
                "citations": 7
            },
            {
                "title": "RETFound-enhanced community-based fundus disease screening: real-world evidence and decision curve analysis",
                "abstract": null,
                "authors": "Ju-zheng Zhang, Senlin Lin, Tianhao Cheng, Yi Xu, Lina Lu, Jiangnan He, Tao Yu, Yajun Peng, Yuejie Zhang, Haidong Zou, Yingyan Ma",
                "citations": 6
            },
            {
                "title": "DNA language model GROVER learns sequence context in the human genome",
                "abstract": null,
                "authors": "Melissa Sanabria, Jonas Hirsch, Pierre M. Joubert, A. Poetsch",
                "citations": 7
            },
            {
                "title": "Falcon2-11B Technical Report",
                "abstract": "We introduce Falcon2-11B, a foundation model trained on over five trillion tokens, and its multimodal counterpart, Falcon2-11B-vlm, which is a vision-to-text model. We report our findings during the training of the Falcon2-11B which follows a multi-stage approach where the early stages are distinguished by their context length and a final stage where we use a curated, high-quality dataset. Additionally, we report the effect of doubling the batch size mid-training and how training loss spikes are affected by the learning rate. The downstream performance of the foundation model is evaluated on established benchmarks, including multilingual and code datasets. The foundation model shows strong generalization across all the tasks which makes it suitable for downstream finetuning use cases. For the vision language model, we report the performance on several benchmarks and show that our model achieves a higher average score compared to open-source models of similar size. The model weights and code of both Falcon2-11B and Falcon2-11B-vlm are made available under a permissive license.",
                "authors": "Quentin Malartic, Nilabhra Roy Chowdhury, Ruxandra-Aimée Cojocaru, Mugariya Farooq, Giulia Campesan, Y. A. D. Djilali, Sanath Narayan, Ankit Singh, Maksim Velikanov, Basma El Amel Boussaha, Mohammed Al-Yafeai, Hamza Alobeidli, Leen Al Qadi, M. Seddik, Kirill Fedyanin, Réda Alami, Hakim Hacid",
                "citations": 6
            },
            {
                "title": "Orb: A Fast, Scalable Neural Network Potential",
                "abstract": "We introduce Orb, a family of universal interatomic potentials for atomistic modelling of materials. Orb models are 3-6 times faster than existing universal potentials, stable under simulation for a range of out of distribution materials and, upon release, represented a 31% reduction in error over other methods on the Matbench Discovery benchmark. We explore several aspects of foundation model development for materials, with a focus on diffusion pretraining. We evaluate Orb as a model for geometry optimization, Monte Carlo and molecular dynamics simulations.",
                "authors": "Mark Neumann, James Gin, Benjamin Rhodes, Steven Bennett, Zhiyi Li, Hitarth Choubisa, Arthur Hussey, Jonathan Godwin",
                "citations": 6
            },
            {
                "title": "Decisions Under Uncertainty as Bayesian Inference on Choice Options",
                "abstract": "Standard models of decision making under risk and uncertainty are deterministic. Inconsistencies in choices are accommodated by separate error models. The combination of decision model and error model, however, is arbitrary. Here, I derive a model of decision making under uncertainty in which choice options are mentally encoded by noisy signals, which are optimally decoded by Bayesian combination with preexisting information. The model predicts diminishing sensitivity toward both likelihoods and rewards, thus providing cognitive microfoundations for the patterns documented in the prospect theory literature. The model is, however, inherently stochastic, so that choices and noise are determined by the same underlying parameters. This results in several novel predictions, which I test on one existing data set and in two new experiments. This paper was accepted by Manel Baucells, behavioral economics and decision analysis. Funding: The author gratefully acknowledges financial support from the Research Foundation—Flanders (FWO) under the project “Causal Determinants of Preferences” [Grant G008021N] and the special research fund (BOF) at Ghent University under the project “The role of noise in the determination of risk preferences.” Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2023.00265 .",
                "authors": "Ferdinand M. Vieider",
                "citations": 7
            },
            {
                "title": "Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family",
                "abstract": "To foster the neural encoding of Portuguese, this paper contributes foundation encoder models that represent an expansion of the still very scarce ecosystem of large language models specifically developed for this language that are fully open, in the sense that they are open source and openly distributed for free under an open license for any purpose, thus including research and commercial usages. Like most languages other than English, Portuguese is low-resourced in terms of these foundational language resources, there being the inaugural 900 million parameter Albertina and 335 million Bertimbau. Taking this couple of models as an inaugural set, we present the extension of the ecosystem of state-of-the-art open encoders for Portuguese with a larger, top performance-driven model with 1.5 billion parameters, and a smaller, efficiency-driven model with 100 million parameters. While achieving this primary goal, further results that are relevant for this ecosystem were obtained as well, namely new datasets for Portuguese based on the SuperGLUE benchmark, which we also distribute openly.",
                "authors": "Rodrigo Santos, João Rodrigues, Luís Gomes, João Silva, António Branco, Henrique Lopes Cardoso, T. Os'orio, Bernardo Leite",
                "citations": 7
            },
            {
                "title": "Re-Envisioning Numerical Information Field Theory (NIFTy.re): A Library for Gaussian Processes and Variational Inference",
                "abstract": "Imaging is the process of transforming noisy, incomplete data into a space that humans can interpret. NIFTy is a Bayesian framework for imaging and has already successfully been applied to many fields in astrophysics. Previous design decisions held the performance and the development of methods in NIFTy back. We present a rewrite of NIFTy, coined NIFTy.re, which reworks the modeling principle, extends the inference strategies, and outsources much of the heavy lifting to JAX. The rewrite dramatically accelerates models written in NIFTy, lays the foundation for new types of inference machineries, improves maintainability, and enables interoperability between NIFTy and the JAX machine learning ecosystem.",
                "authors": "G. Edenhofer, Philipp Frank, Jakob Roth, R. Leike, Massin Guerdi, L. Scheel-Platz, Matteo Guardiani, Vincent Eberle, Margret Westerkamp, Torsten A. Enßlin",
                "citations": 6
            },
            {
                "title": "Detecting student depression on Weibo based on various multimodal fusion methods",
                "abstract": "This study uses a multimodal fusion model for early student depression detection by analysing student data from Sina Weibo. It compares early and late fusion methods with traditional Natural Language Processing models and achieves a 3% accuracy improvement over 100 cycles. The study shows that standardising only structured data without neural network mapping reduces predictive performance. It was also found that while both fusion methods exhibited similar predictive capabilities, the late fusion model exhibited overfitting, suggesting that there is potential for the late fusion strategy to further improve model performance performance. This study summarises the ability of multimodal fusion models to effectively detect early signs of student depression and lays the foundation for future research on model interpretability for early student depression detection and future research on student behaviour analysis.",
                "authors": "Yiming Luo, Zhanghao Ye, Rui Lyu",
                "citations": 5
            },
            {
                "title": "OpenROAD-Assistant: An Open-Source Large Language Model for Physical Design Tasks",
                "abstract": "Large language models (LLMs) have shown significant potential in serving as domain-specific chatbots. Recently, these models have emerged as powerful tools for chip design, providing both natural language responses and script generation for domain-specific inquiries. Previous work has demonstrated the effectiveness of LLMs in assisting with physical design automation; however, these approaches often rely on proprietary tools, APIs, technologies, and designs. As a result, access to these models is extremely limited, particularly for new chip designers who could greatly benefit from a design assistant. This paper introduces OpenROAD-Assistant, an open-source chatbot for OpenROAD that relies only on public data and responds to queries in either prose or Python script using the OpenROAD APIs. OpenROAD-Assistant leverages the Llama3-8B foundation model and employs retrieval-aware fine-tuning (RAFT) to respond to physical design-specific questions for OpenROAD. Notably, OpenROAD-Assistant outperforms other foundational models such as ChatGPT3.5, ChatGPT4, Code Llama, Claude3, and other ablation study baselines on the measured metrics (pass@ k for scripting and BERTScore/BARTScore for question-answering). OpenROAD-Assistant achieves a 77% pass@1 score, 80% pass@3 score for scripting, and it achieves a 98% BERTScore and 96% BARTScore on question-answering.",
                "authors": "Utsav Sharma, Bing-Yue Wu, Sai Rahul Dhanvi Kankipati, V. A. Chhabria, Austin Rovinski",
                "citations": 4
            },
            {
                "title": "Controllable Text-to-Image Synthesis for Multi-Modality MR Images",
                "abstract": "Generative modeling has seen significant advancements in recent years, especially in the realm of text-to-image synthesis. Despite this progress, the medical field has yet to fully leverage the capabilities of large-scale foundational models for synthetic data generation. This paper introduces a framework for text-conditional magnetic resonance (MR) imaging generation, addressing the complexities associated with multi-modality considerations. The framework comprises a pre-trained large language model, a diffusion-based prompt-conditional image generation architecture, and an additional denoising network for input structural binary masks. Experimental results demonstrate that the proposed framework is capable of generating realistic, high-resolution, and high-fidelity multi-modal MR images that align with medical language text prompts. Further, the study interprets the cross-attention maps of the generated results based on text-conditional statements. The contributions of this research lay a robust foundation for future studies in text-conditional medical image generation and hold significant promise for accelerating advancements in medical imaging research.",
                "authors": "Kyuri Kim, Yoonho Na, Sung-Joon Ye, Jimin Lee, Sungsoo Ahn, Ji Eun Park, Hwiyoung Kim",
                "citations": 4
            },
            {
                "title": "FOXK1 promotes hormonally responsive breast carcinogenesis by suppressing apoptosis.",
                "abstract": "BACKGROUND\nGlobally, breast cancer constitutes the predominant malignancy in women. Abnormal regulation of epigenetic factors plays a key role in the development of tumors. Anti-apoptosis is a characteristic of tumor cells. Therefore, exploring and identifying relevant epigenetic factors that regulate the apoptosis of tumor cells is the foundation for clarifying the pathogenesis of tumors and achieving precision antitumor therapy.\n\n\nMETHOD\nThis study focused on exploring the epigenetic mechanism of FOXK1 in the development of estrogen receptor-positive (ER+ ) breast cancer. We used overexpressing FLAG-FOXK1 MCF-7 cells to perform silver staining mass spectrometry analysis and conducted Co-IP experiments to verify the interactions. ChIP-seq was conducted on MCF-7 cells to examine FOXK1's binding across the genome and its transcriptional target sites. To validate the ChIP-seq results, qChIP, western blotting, and quantitative polymerase chain reaction (qPCR) were performed. Through TUNEL assay, cell counting assay, colony formation assay, and the mouse xenograft models, the effect of FOXK1 on breast cancer progression was detected. Finally, by analyzing online databases, the correlation between FOXK1 and the survival of breast cancer patients was examined.\n\n\nRESULTS\nFOXK1 interacts with the REST/CoREST transcriptional corepression complex to transcriptionally inhibit target genes representing the apoptotic pathway. Abnormally high expression of FOXK1 prevents the apoptosis of ER+ breast cancer cells in vitro and promotes ER+ breast tumor progression in vivo. Furthermore, the expression of FOXK1 is negatively correlated with the survival of ER+ breast cancer patients.\n\n\nCONCLUSION\nFOXK1 promotes ER+ breast carcinogenesis through anti-apoptosis and acts as a potential target for ER+ breast cancer treatment.",
                "authors": "Minghui Zhao, Tingyao Ma, Zhaohan Zhang, Yu Wang, Xilin Wang, Wenjuan Wang, Xiaohong Chen, Ran Gao, Lin Shan",
                "citations": 3
            },
            {
                "title": "Towards a general-purpose foundation model for computational pathology.",
                "abstract": null,
                "authors": "Richard J. Chen, Tong Ding, Ming Y. Lu, Drew F. K. Williamson, Guillaume Jaume, Andrew H. Song, Bowen Chen, Andrew Zhang, Daniel Shao, Muhammad Shaban, Mane Williams, Lukas Oldenburg, Luca L Weishaupt, Judy J. Wang, Anurag Vaidya, L. Le, Georg K. Gerber, S. Sahai, Walt Williams, Faisal Mahmood",
                "citations": 210
            },
            {
                "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
                "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .",
                "authors": "Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chaochao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun",
                "citations": 174
            },
            {
                "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
                "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.",
                "authors": "S. Tonmoy, S. M. M. Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das",
                "citations": 130
            },
            {
                "title": "Leveraging large language models for predictive chemistry",
                "abstract": null,
                "authors": "K. Jablonka, P. Schwaller, Andres Ortega‐Guerrero, Berend Smit",
                "citations": 103
            },
            {
                "title": "PMC-LLaMA: toward building open-source language models for medicine",
                "abstract": "OBJECTIVE\nRecently, large language models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering (QA) situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this article, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA.\n\n\nMATERIALS AND METHODS\nWe adapt a general-purpose LLM toward the medical domain, involving data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive domain-specific instruction fine-tuning, encompassing medical QA, rationale for reasoning, and conversational dialogues with 202M tokens.\n\n\nRESULTS\nWhile evaluating various public medical QA benchmarks and manual rating, our lightweight PMC-LLaMA, which consists of only 13B parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, and datasets for instruction tuning will be released to the research community.\n\n\nDISCUSSION\nOur contributions are 3-fold: (1) we build up an open-source LLM toward the medical domain. We believe the proposed PMC-LLaMA model can promote further development of foundation models in medicine, serving as a medical trainable basic generative language backbone; (2) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component, demonstrating how different training data and model scales affect medical LLMs; (3) we contribute a large-scale, comprehensive dataset for instruction tuning.\n\n\nCONCLUSION\nIn this article, we systematically investigate the process of building up an open-source medical-specific LLM, PMC-LLaMA.",
                "authors": "Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, Yanfeng Wang",
                "citations": 96
            },
            {
                "title": "Universal Cell Embeddings: A Foundation Model for Cell Biology",
                "abstract": "Developing a universal representation of cells which encompasses the tremendous molecular diversity of cell types within the human body and more generally, across species, would be transformative for cell biology. Recent work using single-cell transcriptomic approaches to create molecular definitions of cell types in the form of cell atlases has provided the necessary data for such an endeavor. Here, we present the Universal Cell Embedding (UCE) foundation model. UCE was trained on a corpus of cell atlas data from human and other species in a completely self-supervised way without any data annotations. UCE offers a unified biological latent space that can represent any cell, regardless of tissue or species. This universal cell embedding captures important biological variation despite the presence of experimental noise across diverse datasets. An important aspect of UCE’s universality is that any new cell from any organism can be mapped to this embedding space with no additional data labeling, model training or fine-tuning. We applied UCE to create the Integrated Mega-scale Atlas, embedding 36 million cells, with more than 1,000 uniquely named cell types, from hundreds of experiments, dozens of tissues and eight species. We uncovered new insights about the organization of cell types and tissues within this universal cell embedding space, and leveraged it to infer function of newly discovered cell types. UCE’s embedding space exhibits emergent behavior, uncovering new biology that it was never explicitly trained for, such as identifying developmental lineages and embedding data from novel species not included in the training set. Overall, by enabling a universal representation for every cell state and type, UCE provides a valuable tool for analysis, annotation and hypothesis generation as the scale and diversity of single cell datasets continues to grow.",
                "authors": "Yanay Rosen, Yusuf Roohani, Ayush Agrawal, Leon Samotorčan, S. Quake, J. Leskovec",
                "citations": 27
            },
            {
                "title": "What matters when building vision-language models?",
                "abstract": "The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.",
                "authors": "Hugo Laurençon, Léo Tronchon, Matthieu Cord, Victor Sanh",
                "citations": 94
            },
            {
                "title": "Moshi: a speech-text foundation model for real-time dialogue",
                "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this\"Inner Monologue\"method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
                "authors": "Alexandre D'efossez, Laurent Mazar'e, Manu Orsini, Am'elie Royer, Patrick P'erez, Herv'e J'egou, Edouard Grave, Neil Zeghidour",
                "citations": 22
            },
            {
                "title": "A Foundation Model for the Earth System",
                "abstract": "Reliable forecasts of the Earth system are crucial for human progress and safety from natural disasters. Artificial intelligence offers substantial potential to improve prediction accuracy and computational efficiency in this field, however this remains underexplored in many domains. Here we introduce Aurora, a large-scale foundation model for the Earth system trained on over a million hours of diverse data. Aurora outperforms operational forecasts for air quality, ocean waves, tropical cyclone tracks, and high-resolution weather forecasting at orders of magnitude smaller computational expense than dedicated existing systems. With the ability to fine-tune Aurora to diverse application domains at only modest computational cost, Aurora represents significant progress in making actionable Earth system predictions accessible to anyone.",
                "authors": "Cristian Bodnar, W. Bruinsma, Ana Lucic, Megan Stanley, Anna Vaughan, Johannes Brandstetter, P. Garvan, Maik Riechert, Jonathan A. Weyn, Haiyu Dong, Jayesh K. Gupta, Kit Thambiratnam, Alexander T. Archibald, Chun-Chieh Wu, E. Heider, Max Welling, Richard E. Turner, P. Perdikaris",
                "citations": 39
            },
            {
                "title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Models",
                "abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.",
                "authors": "Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Gunther, Anton Korinek, J. Hernández-Orallo, Lewis Hammond, Eric J. Bigelow, Alexander Pan, L. Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Se'an 'O h'Eigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Y. Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob N. Foerster, Florian Tramèr, He He, Atoosa Kasirzadeh, Yejin Choi, David Krueger",
                "citations": 72
            },
            {
                "title": "BrainLM: A foundation model for brain activity recordings",
                "abstract": "We introduce the Brain Language Model (BrainLM), a foundation model for brain activity dynamics trained on 6,700 hours of fMRI recordings. Utilizing self-supervised masked-prediction training, BrainLM demonstrates proficiency in both fine-tuning and zero-shot inference tasks. Fine-tuning allows for the accurate prediction of clinical variables like age, anxiety, and PTSD as well as forecasting of future brain states. Critically, the model generalizes well to entirely new external cohorts not seen during training. In zero-shot inference mode, BrainLM can identify intrinsic functional networks directly from raw fMRI data without any network-based supervision during training. The model also generates interpretable latent representations that reveal relationships between brain activity patterns and cognitive states. Overall, BrainLM offers a versatile and interpretable framework for elucidating the complex spatiotemporal dynamics of human brain activity. It serves as a powerful “lens” through which massive repositories of fMRI data can be analyzed in new ways, enabling more effective interpretation and utilization at scale. The work demonstrates the potential of foundation models to advance computational neuroscience research.",
                "authors": "J. O. Caro, Antonio H. O. Fonseca, Christopher Averill, S. Rizvi, Matteo Rosati, James L. Cross, Prateek Mittal, E. Zappala, Daniel Levine, Rahul M. Dhodapkar, Insu Han, Amin Karbasi, C. Abdallah, David van Dijk",
                "citations": 25
            },
            {
                "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
                "abstract": "Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.",
                "authors": "Kaining Ying, Fanqing Meng, Jin Wang, Zhiqiang Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yuning Qiao, Ping Luo, Kaipeng Zhang, Wenqi Shao",
                "citations": 51
            },
            {
                "title": "Research progress of 3D-bioprinted functional pancreas and in vitro tumor models",
                "abstract": "With the rapid development of three-dimensional (3D) bioprinting technology, the research revolving around in vitro functional pancreas and tumor models has become the focus of attention in the field of life sciences. This review aims to summarize and deeply discuss the research progress and prospects of 3D-bioprinted functional pancreas and in vitro tumor models. The efforts in improving 3D printing technology to increase its accuracy and reliability in the biomedical applications have been ramped up over the past few years. Researchers are now able to create highly complex 3D structures through precise layering of biological materials at the micron scale. For instance, a functional pancreas can be printed in vitro by combining cells, biomaterials, and growth factors. The introduction of new technologies allows researchers to more accurately simulate the growth and spread of tumors, providing a more realistic platform for cancer treatment research. This not only helps accelerate the process of drug screening, but also lays the foundation for personalized medicine. As multiple disciplines, such as materials science, cell biology, and engineering, continue to converge with 3D bioprinting, emergence of more innovative applications is anticipated. However, despite significant progress, many technical and ethical challenges still need to be overcome before practical clinical applications can be implemented. In summary, the application of bioprinting technology is of great significance to the study of functional pancreas and in vitro tumor models, which could lead to new breakthroughs in the development of clinical treatment and personalized medicine.",
                "authors": "Liusheng Wu, Huansong Li, Yangsui Liu, Zhengyang Fan, Jingyi Xu, Ning Li, Xinye Qian, Zewei Lin, Xiaoqiang Li, Jun Yan",
                "citations": 44
            },
            {
                "title": "General Flow as Foundation Affordance for Scalable Robot Learning",
                "abstract": "We address the challenge of acquiring real-world manipulation skills with a scalable framework. We hold the belief that identifying an appropriate prediction target capable of leveraging large-scale datasets is crucial for achieving efficient and universal learning. Therefore, we propose to utilize 3D flow, which represents the future trajectories of 3D points on objects of interest, as an ideal prediction target. To exploit scalable data resources, we turn our attention to human videos. We develop, for the first time, a language-conditioned 3D flow prediction model directly from large-scale RGBD human video datasets. Our predicted flow offers actionable guidance, thus facilitating zero-shot skill transfer in real-world scenarios. We deploy our method with a policy based on closed-loop flow prediction. Remarkably, without any in-domain finetuning, our method achieves an impressive 81\\% success rate in zero-shot human-to-robot skill transfer, covering 18 tasks in 6 scenes. Our framework features the following benefits: (1) scalability: leveraging cross-embodiment data resources; (2) wide application: multiple object categories, including rigid, articulated, and soft bodies; (3) stable skill transfer: providing actionable guidance with a small inference domain-gap. Code, data, and supplementary materials are available https://general-flow.github.io",
                "authors": "Chengbo Yuan, Chuan Wen, Tong Zhang, Yang Gao",
                "citations": 15
            },
            {
                "title": "A Survey on Knowledge Distillation of Large Language Models",
                "abstract": "In the era of Large Language Models (LLMs), Knowledge Distillation (KD) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary LLMs, such as GPT-4, to their open-source counterparts like LLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a crucial role in both compressing these models, and facilitating their self-improvement by employing themselves as teachers. This paper presents a comprehensive survey of KD's role within the realm of LLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self-improvement. Our survey is meticulously structured around three foundational pillars: \\textit{algorithm}, \\textit{skill}, and \\textit{verticalization} -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in KD and proposing future research directions. Importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.",
                "authors": "Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou",
                "citations": 48
            },
            {
                "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
                "abstract": "Today’s most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like",
                "authors": "Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Christopher Callison-Burch, Andrew Head, Rose Hendrix, F. Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Christopher Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jennifer Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Marie Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hanna Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi",
                "citations": 27
            },
            {
                "title": "Pretraining a foundation model for generalizable fluorescence microscopy-based image restoration.",
                "abstract": null,
                "authors": "Chenxi Ma, Weimin Tan, Ruian He, Bo Yan",
                "citations": 13
            },
            {
                "title": "Datasets for Large Language Models: A Comprehensive Survey",
                "abstract": "This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.",
                "authors": "Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin",
                "citations": 35
            },
            {
                "title": "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation",
                "abstract": "As large language models (LLMs) evolve, evaluating their output reliably becomes increasingly difficult due to the high cost of human evaluation. To address this, we introduce FLAMe, a family of Foundational Large Autorater Models. FLAMe is trained on a diverse set of over 100 quality assessment tasks, incorporating 5M+ human judgments curated from publicly released human evaluations. FLAMe outperforms models like GPT-4 and Claude-3 on various held-out tasks, and serves as a powerful starting point for fine-tuning, as shown in our reward model evaluation case study (FLAMe-RM). On Reward-Bench, FLAMe-RM-24B achieves 87.8% accuracy, surpassing GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we introduce FLAMe-Opt-RM, an efficient tail-patch fine-tuning approach that offers competitive RewardBench performance using 25×fewer training datapoints. Our FLAMe variants outperform popular proprietary LLM-as-a-Judge models on 8 of 12 autorater benchmarks, covering 53 quality assessment tasks, including RewardBench and LLM-AggreFact. Finally, our analysis shows that FLAMe is significantly less biased than other LLM-as-a-Judge models on the CoBBLEr autorater bias benchmark.",
                "authors": "Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung",
                "citations": 18
            },
            {
                "title": "Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models",
                "abstract": "The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.",
                "authors": "Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, Carl Yang, Yue Cheng, Liang Zhao",
                "citations": 35
            },
            {
                "title": "Interaction model for horizontal dynamic response of monopile-friction wheel composite foundation in marine area",
                "abstract": null,
                "authors": "Zijian Yang, X. Zou, Shun Chen",
                "citations": 13
            },
            {
                "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey",
                "abstract": "Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of\"jailbreaking\", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.",
                "authors": "Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, Qi Li",
                "citations": 31
            },
            {
                "title": "Adapting Large Language Models for Document-Level Machine Translation",
                "abstract": "Large language models (LLMs) have significantly advanced various natural language processing (NLP) tasks. Recent research indicates that moderately-sized LLMs often outperform larger ones after task-specific fine-tuning. This study focuses on adapting LLMs for document-level machine translation (DocMT) for specific language pairs. We first investigate the impact of prompt strategies on translation performance and then conduct extensive experiments using two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our results show that specialized models can sometimes surpass GPT-4 in translation performance but still face issues like off-target translation due to error propagation in decoding. We provide an in-depth analysis of these LLMs tailored for DocMT, examining translation errors, discourse phenomena, strategies for training and inference, the data efficiency of parallel documents, recent test set evaluations, and zero-shot crosslingual transfer. Our findings highlight the strengths and limitations of LLM-based DocMT models and provide a foundation for future research.",
                "authors": "Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, Gholamreza Haffari",
                "citations": 31
            },
            {
                "title": "Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges",
                "abstract": "Sequence modeling is a crucial area across various domains, including Natural Language Processing (NLP), speech recognition, time series forecasting, music generation, and bioinformatics. Recurrent Neural Networks (RNNs) and Long Short Term Memory Networks (LSTMs) have historically dominated sequence modeling tasks like Machine Translation, Named Entity Recognition (NER), etc. However, the advancement of transformers has led to a shift in this paradigm, given their superior performance. Yet, transformers suffer from $O(N^2)$ attention complexity and challenges in handling inductive bias. Several variations have been proposed to address these issues which use spectral networks or convolutions and have performed well on a range of tasks. However, they still have difficulty in dealing with long sequences. State Space Models(SSMs) have emerged as promising alternatives for sequence modeling paradigms in this context, especially with the advent of S4 and its variants, such as S4nd, Hippo, Hyena, Diagnol State Spaces (DSS), Gated State Spaces (GSS), Linear Recurrent Unit (LRU), Liquid-S4, Mamba, etc. In this survey, we categorize the foundational SSMs based on three paradigms namely, Gating architectures, Structural architectures, and Recurrent architectures. This survey also highlights diverse applications of SSMs across domains such as vision, video, audio, speech, language (especially long sequence modeling), medical (including genomics), chemical (like drug design), recommendation systems, and time series analysis, including tabular data. Moreover, we consolidate the performance of SSMs on benchmark datasets like Long Range Arena (LRA), WikiText, Glue, Pile, ImageNet, Kinetics-400, sstv2, as well as video datasets such as Breakfast, COIN, LVU, and various time series datasets. The project page for Mamba-360 work is available on this webpage.\\url{https://github.com/badripatro/mamba360}.",
                "authors": "B. N. Patro, Vijay S. Agneeswaran",
                "citations": 26
            },
            {
                "title": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
                "abstract": "General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey.",
                "authors": "Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, Yang You, Zhaoxiang Zhang, Dawei Zhao, Liang Xiao, Jian Zhao, Jiwen Lu, Guan Huang",
                "citations": 25
            },
            {
                "title": "A Survey on Diffusion Models for Time Series and Spatio-Temporal Data",
                "abstract": "The study of time series is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.",
                "authors": "Yiyuan Yang, Ming Jin, Haomin Wen, Chaoli Zhang, Yuxuan Liang, Lintao Ma, Yi Wang, Cheng-Ming Liu, Bin Yang, Zenglin Xu, Jiang Bian, Shirui Pan, Qingsong Wen",
                "citations": 22
            },
            {
                "title": "Advancing entity recognition in biomedicine via instruction tuning of large language models",
                "abstract": "Abstract Motivation Large Language Models (LLMs) have the potential to revolutionize the field of Natural Language Processing, excelling not only in text generation and reasoning tasks but also in their ability for zero/few-shot learning, swiftly adapting to new tasks with minimal fine-tuning. LLMs have also demonstrated great promise in biomedical and healthcare applications. However, when it comes to Named Entity Recognition (NER), particularly within the biomedical domain, LLMs fall short of the effectiveness exhibited by fine-tuned domain-specific models. One key reason is that NER is typically conceptualized as a sequence labeling task, whereas LLMs are optimized for text generation and reasoning tasks. Results We developed an instruction-based learning paradigm that transforms biomedical NER from a sequence labeling task into a generation task. This paradigm is end-to-end and streamlines the training and evaluation process by automatically repurposing pre-existing biomedical NER datasets. We further developed BioNER-LLaMA using the proposed paradigm with LLaMA-7B as the foundational LLM. We conducted extensive testing on BioNER-LLaMA across three widely recognized biomedical NER datasets, consisting of entities related to diseases, chemicals, and genes. The results revealed that BioNER-LLaMA consistently achieved higher F1-scores ranging from 5% to 30% compared to the few-shot learning capabilities of GPT-4 on datasets with different biomedical entities. We show that a general-domain LLM can match the performance of rigorously fine-tuned PubMedBERT models and PMC-LLaMA, biomedical-specific language model. Our findings underscore the potential of our proposed paradigm in developing general-domain LLMs that can rival SOTA performances in multi-task, multi-domain scenarios in biomedical and health applications. Availability and implementation Datasets and other resources are available at https://github.com/BIDS-Xu-Lab/BioNER-LLaMA.",
                "authors": "V. Keloth, Yan Hu, Qianqian Xie, Xueqing Peng, Yan Wang, Andrew Zheng, Melih Selek, Kalpana Raja, Chih-Hsuan Wei, Qiao Jin, Zhiyong Lu, Qingyu Chen, Hua Xu",
                "citations": 22
            },
            {
                "title": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models",
                "abstract": "Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM",
                "authors": "Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang",
                "citations": 20
            },
            {
                "title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning",
                "abstract": "In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\\% success rate in the development stage, while attaining 36\\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.",
                "authors": "Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang",
                "citations": 15
            },
            {
                "title": "World Models for Autonomous Driving: An Initial Survey",
                "abstract": "In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and comprehension of this burgeoning field, and inspiring continued innovation and exploration.",
                "authors": "Yanchen Guan, Haicheng Liao, Zhenning Li, Guohui Zhang, Chengzhong Xu",
                "citations": 15
            },
            {
                "title": "SplitLoRA: A Split Parameter-Efficient Fine-Tuning Framework for Large Language Models",
                "abstract": "The scalability of large language models (LLMs) in handling high-complexity models and large-scale datasets has led to tremendous successes in pivotal domains. While there is an urgent need to acquire more training data for LLMs, a concerning reality is the depletion of high-quality public datasets within a few years. In view of this, the federated learning (FL) LLM fine-tuning paradigm recently has been proposed to facilitate collaborative LLM fine-tuning on distributed private data, where multiple data owners collaboratively fine-tune a shared LLM without sharing raw data. However, the staggering model size of LLMs imposes heavy computing and communication burdens on clients, posing significant barriers to the democratization of the FL LLM fine-tuning paradigm. To address this issue, split learning (SL) has emerged as a promising solution by offloading the primary training workload to a server via model partitioning while exchanging activation/activation's gradients with smaller data sizes rather than the entire LLM. Unfortunately, research on the SL LLM fine-tuning paradigm is still in its nascent stage. To fill this gap, in this paper, we propose the first SL LLM fine-tuning framework, named SplitLoRA. SplitLoRA is built on the split federated learning (SFL) framework, amalgamating the advantages of parallel training from FL and model splitting from SL and thus greatly enhancing the training efficiency. It is worth noting that SplitLoRA is the inaugural open-source benchmark for SL LLM fine-tuning, providing a foundation for research efforts dedicated to advancing SL LLM fine-tuning. Extensive simulations validate that SplitLoRA achieves target accuracy in significantly less time than state-of-the-art LLM fine-tuning frameworks, demonstrating the superior training performance of SplitLoRA. The project page is available at https://fduinc.github.io/splitlora/.",
                "authors": "Zheng Lin, Xuanjie Hu, Yu-xin Zhang, Zhe Chen, Zihan Fang, Xianhao Chen, Ang Li, Praneeth Vepakomma, Yue Gao",
                "citations": 15
            },
            {
                "title": "Matryoshka Multimodal Models",
                "abstract": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose M3: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around ~9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.",
                "authors": "Mu Cai, Jianwei Yang, Jianfeng Gao, Yong Jae Lee",
                "citations": 15
            },
            {
                "title": "The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse",
                "abstract": "Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating changes in an edited model's perplexity are strongly correlated with its downstream task performances. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse after only few edits. To facilitate further research, we have utilized GPT-3.5 to develop a new dataset, HardEdit, based on those hard cases. This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse. We hope this work can draw the community's attention to the potential risks inherent in model editing practices.",
                "authors": "Wanli Yang, Fei Sun, Xinyu Ma, Xun Liu, Dawei Yin, Xueqi Cheng",
                "citations": 16
            },
            {
                "title": "When Large Language Models Meet Vector Databases: A Survey",
                "abstract": "This survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration's impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.",
                "authors": "Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Haiyun Xu, Chunjiang Liu, Kehai Chen, Min Zhang",
                "citations": 19
            },
            {
                "title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                "abstract": "Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag.",
                "authors": "Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang",
                "citations": 18
            },
            {
                "title": "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals",
                "abstract": "As the health care industry increasingly embraces large language models (LLMs), understanding the consequence of this integration becomes crucial for maximizing benefits while mitigating potential pitfalls. This paper explores the evolving relationship among clinician trust in LLMs, the transition of data sources from predominantly human-generated to artificial intelligence (AI)–generated content, and the subsequent impact on the performance of LLMs and clinician competence. One of the primary concerns identified in this paper is the LLMs’ self-referential learning loops, where AI-generated content feeds into the learning algorithms, threatening the diversity of the data pool, potentially entrenching biases, and reducing the efficacy of LLMs. While theoretical at this stage, this feedback loop poses a significant challenge as the integration of LLMs in health care deepens, emphasizing the need for proactive dialogue and strategic measures to ensure the safe and effective use of LLM technology. Another key takeaway from our investigation is the role of user expertise and the necessity for a discerning approach to trusting and validating LLM outputs. The paper highlights how expert users, particularly clinicians, can leverage LLMs to enhance productivity by off-loading routine tasks while maintaining a critical oversight to identify and correct potential inaccuracies in AI-generated content. This balance of trust and skepticism is vital for ensuring that LLMs augment rather than undermine the quality of patient care. We also discuss the risks associated with the deskilling of health care professionals. Frequent reliance on LLMs for critical tasks could result in a decline in health care providers’ diagnostic and thinking skills, particularly affecting the training and development of future professionals. The legal and ethical considerations surrounding the deployment of LLMs in health care are also examined. We discuss the medicolegal challenges, including liability in cases of erroneous diagnoses or treatment advice generated by LLMs. The paper references recent legislative efforts, such as The Algorithmic Accountability Act of 2023, as crucial steps toward establishing a framework for the ethical and responsible use of AI-based technologies in health care. In conclusion, this paper advocates for a strategic approach to integrating LLMs into health care. By emphasizing the importance of maintaining clinician expertise, fostering critical engagement with LLM outputs, and navigating the legal and ethical landscape, we can ensure that LLMs serve as valuable tools in enhancing patient care and supporting health care professionals. This approach addresses the immediate challenges posed by integrating LLMs and sets a foundation for their maintainable and responsible use in the future.",
                "authors": "Avishek Choudhury, Zaria Chaudhry",
                "citations": 18
            },
            {
                "title": "Cross-industry frameworks for business process reengineering: Conceptual models and practical executions",
                "abstract": "Cross-industry frameworks for business process reengineering (BPR) have emerged as pivotal tools in enhancing organizational efficiency, agility, and competitiveness across diverse sectors. These frameworks amalgamate conceptual models with practical executions to streamline operations, optimize resource allocation, and foster innovation. This review delves into the essence of cross-industry frameworks for BPR, elucidating their conceptual underpinnings and real-world applications. At the core of cross-industry frameworks lies the recognition that business processes transcend sector boundaries, encompassing common elements such as customer interactions, supply chain management, and operational workflows. Conceptual models form the foundational framework by articulating key principles, methodologies, and best practices for BPR initiatives. One prominent conceptual model is the Business Process Reengineering (BPR) methodology, which advocates for radical redesign rather than incremental improvement of processes. This approach emphasizes fundamental questioning of existing practices, aiming to achieve dramatic enhancements in efficiency, quality, and customer satisfaction. Another influential model is the Capability Maturity Model Integration (CMMI), which provides a roadmap for organizations to systematically improve their processes by advancing through defined maturity levels. Practical executions of cross-industry frameworks entail the translation of conceptual models into actionable strategies tailored to specific organizational contexts. This involves a multifaceted approach encompassing process analysis, stakeholder engagement, technology integration, and change management. Process analysis entails mapping current workflows, identifying bottlenecks, and pinpointing areas for optimization. Stakeholder engagement is crucial for garnering buy-in and fostering a culture of continuous improvement. Technology integration plays a pivotal role in modern BPR initiatives, leveraging digital tools such as business process management (BPM) software, data analytics, and robotic process automation (RPA) to streamline operations and enhance decision-making. Moreover, emerging technologies like artificial intelligence (AI) and blockchain offer new avenues for process innovation and optimization. Change management constitutes a critical component of practical executions, as BPR initiatives often entail significant organizational transformation. Effective change management involves communication, training, and leadership support to mitigate resistance and facilitate smooth transition. Real-world applications exemplify the effectiveness of cross-industry frameworks in driving tangible business outcomes. Case studies spanning diverse sectors, including manufacturing, healthcare, finance, and retail, demonstrate how organizations have leveraged BPR to achieve breakthrough improvements in efficiency, cost reduction, and customer satisfaction. In the manufacturing sector, for instance, companies have adopted lean manufacturing principles to streamline production processes, minimize waste, and enhance flexibility. In healthcare, BPR initiatives have led to streamlined patient care pathways, reduced administrative burdens, and improved clinical outcomes. Financial institutions have leveraged BPR to optimize loan approval processes, mitigate risk, and enhance regulatory compliance. Similarly, retailers have utilized BPR to optimize supply chain management, enhance omnichannel capabilities, and personalize customer experiences. Cross-industry frameworks for business process reengineering represent a potent paradigm for organizational transformation in today's dynamic business landscape. By integrating conceptual models with practical executions, these frameworks empower organizations to enhance operational efficiency, drive innovation, and maintain competitive advantage across diverse sectors.",
                "authors": "Oladapo Adeboye Popoola, Henry Ejiga Adama, Chukwuekem David Okeke, A. Akinoso",
                "citations": 16
            },
            {
                "title": "Demographic bias in misdiagnosis by computational pathology models.",
                "abstract": null,
                "authors": "Anurag Vaidya, Richard J. Chen, Drew F. K. Williamson, Andrew H. Song, Guillaume Jaume, Yuzhe Yang, Thomas Hartvigsen, Emma C Dyer, Ming Y. Lu, Jana Lipková, Muhammad Shaban, Tiffany Y. Chen, Faisal Mahmood",
                "citations": 17
            },
            {
                "title": "HydrogelFinder: A Foundation Model for Efficient Self‐Assembling Peptide Discovery Guided by Non‐Peptidal Small Molecules",
                "abstract": "Abstract Self‐assembling peptides have numerous applications in medicine, food chemistry, and nanotechnology. However, their discovery has traditionally been serendipitous rather than driven by rational design. Here, HydrogelFinder, a foundation model is developed for the rational design of self‐assembling peptides from scratch. This model explores the self‐assembly properties by molecular structure, leveraging 1,377 self‐assembling non‐peptidal small molecules to navigate chemical space and improve structural diversity. Utilizing HydrogelFinder, 111 peptide candidates are generated and synthesized 17 peptides, subsequently experimentally validating the self‐assembly and biophysical characteristics of nine peptides ranging from 1–10 amino acids—all achieved within a 19‐day workflow. Notably, the two de novo‐designed self‐assembling peptides demonstrated low cytotoxicity and biocompatibility, as confirmed by live/dead assays. This work highlights the capacity of HydrogelFinder to diversify the design of self‐assembling peptides through non‐peptidal small molecules, offering a powerful toolkit and paradigm for future peptide discovery endeavors.",
                "authors": "Xuanbai Ren, Jiaying Wei, Xiaoli Luo, Yuansheng Liu, Kenli Li, Qiang Zhang, Xin Gao, Sizhe Yan, Xia Wu, Xingyue Jiang, Mingquan Liu, Dongsheng Cao, Leyi Wei, Xiangxiang Zeng, Junfeng Shi",
                "citations": 11
            },
            {
                "title": "BiomedParse: a biomedical foundation model for image parsing of everything everywhere all at once",
                "abstract": null,
                "authors": "Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Tristan Naumann, Jianfeng Gao, Angela Crabtree, B. Piening, Carlo Bifulco, Mu-Hsin Wei, Hoifung Poon, Sheng Wang",
                "citations": 10
            },
            {
                "title": "uniGradICON: A Foundation Model for Medical Image Registration",
                "abstract": "Conventional medical image registration approaches directly optimize over the parameters of a transformation model. These approaches have been highly successful and are used generically for registrations of different anatomical regions. Recent deep registration networks are incredibly fast and accurate but are only trained for specific tasks. Hence, they are no longer generic registration approaches. We therefore propose uniGradICON, a first step toward a foundation model for registration providing 1) great performance \\emph{across} multiple datasets which is not feasible for current learning-based registration methods, 2) zero-shot capabilities for new registration tasks suitable for different acquisitions, anatomical regions, and modalities compared to the training dataset, and 3) a strong initialization for finetuning on out-of-distribution registration tasks. UniGradICON unifies the speed and accuracy benefits of learning-based registration algorithms with the generic applicability of conventional non-deep-learning approaches. We extensively trained and evaluated uniGradICON on twelve different public datasets. Our code and the uniGradICON model are available at https://github.com/uncbiag/uniGradICON.",
                "authors": "Lin Tian, Hastings Greer, R. Kwitt, François-Xavier Vialard, R. Estépar, Sylvain Bouix, R. Rushmore, Marc Niethammer",
                "citations": 11
            },
            {
                "title": "ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation",
                "abstract": "In the absence of parallax cues, a learning based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pretrained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pretrained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pretrained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embeddings. Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYU Depth v2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GED). For zero shot transfer with a model trained on NYU Depth v2, we report mean relative improvement of (20%, 23%,81%, 25%) over NeWCRF on (Sun-RGBD, iBimsl, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoEDepth. The code is available in our project page.",
                "authors": "Suraj Patni, Aradhye Agarwal, Chetan Arora",
                "citations": 14
            },
            {
                "title": "LSKNet: A Foundation Lightweight Backbone for Remote Sensing",
                "abstract": "Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightweight LSKNet sets new state-of-the-art scores on standard remote sensing classification, object detection and semantic segmentation benchmarks. Our comprehensive analysis further validated the significance of the identified priors and the effectiveness of LSKNet. The code is available at https://github.com/zcablii/LSKNet.",
                "authors": "Yuxuan Li, Xiang Li, Yimain Dai, Qibin Hou, Li Liu, Yongxiang Liu, Ming-Ming Cheng, Jian Yang",
                "citations": 10
            },
            {
                "title": "OmniGlue: Generalizable Feature Matching with Foundation Model Guidance",
                "abstract": "The image matching field has been witnessing a contin-uous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that de-spite these gains, their potential for real-world applications is restricted by their limited generalization capabili-ties to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is de-signed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting general-ization to domains not seen at training time. Addition-ally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appear-ance information, leading to enhanced matching descrip-tors. We perform comprehensive experiments on a suite of 7 datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlue's novel components lead to relative gains on unseen domains of 20.9% with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by 9.5% relatively. Code and model can be found at https://hwjiang151o.github.io/OmniGlue.",
                "authors": "Hanwen Jiang, Hanwen Jiang, Arjun Karpur, Bingyi Cao, Qixing Huang, Qi-Xing Huang",
                "citations": 9
            },
            {
                "title": "EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model",
                "abstract": "Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance on various downstream tasks but also provide interpretable outcomes of the useful patterns within the data. To validate the effectiveness of our model, we extensively evaluate it on various downstream tasks and assess the performance under different transfer settings. Furthermore, we demonstrate how the learned model exhibits transferable anomaly detection performance and provides valuable interpretability of the acquired patterns via self-supervised learning.",
                "authors": "Yuqi Chen, Kan Ren, Kaitao Song, Yansen Wang, Yifan Wang, Dongsheng Li, Lili Qiu",
                "citations": 9
            },
            {
                "title": "Developing comprehensive cybersecurity frameworks for protecting green infrastructure: Conceptual models and practical applications",
                "abstract": "This study investigates the critical intersection of cybersecurity and green infrastructure (GI), aiming to elucidate the challenges, opportunities, and strategic approaches necessary for safeguarding these essential systems against cyber threats. Employing a systematic literature review and content analysis, the research scrutinizes peer-reviewed articles, industry reports, and regulatory publications from 2014 to 2024. The methodology focuses on identifying prevalent cybersecurity vulnerabilities within GI, the evolution of protective practices, the impact of regulatory frameworks, and the strategic implications for diverse stakeholders. Key findings reveal a complex landscape where the integration of digital technologies in GI introduces both innovative solutions and new vulnerabilities. The study highlights the pivotal role of international standards and regulatory bodies in shaping cybersecurity strategies, underscoring the necessity for a holistic approach that encompasses technological, regulatory, and human factors. Strategic recommendations advocate for interdisciplinary collaboration, enhanced regulatory frameworks, and stakeholder engagement to fortify the cybersecurity of GI. The research underscores the imperative of embedding cybersecurity into the fabric of GI planning and management. It calls for future research to explore predictive models and proactive measures, ensuring the resilience and sustainability of green infrastructure in an increasingly digitalized urban environment. This study contributes to the burgeoning discourse on securing sustainable urban systems against cyber threats, offering a foundation for further exploration and development in the field.",
                "authors": "Adebimpe Bolatito Ige, Eseoghene Kupa, Oluwatosin Ilori",
                "citations": 14
            },
            {
                "title": "Brain Tumor Detection and Classification Using Transfer Learning Models",
                "abstract": ": Diagnosing brain tumors is a time-consuming process requiring radiologist expertise. With the growing patient population and increased data volume, conventional procedures have become expensive and ineffective. Scholars have explored algorithms for detecting and classifying brain tumors, focusing on precision and efficiency. Deep learning methodologies are being used to create automated systems that can diagnose or segment brain tumors with precision and efficiency, particularly in brain cancer classification. This approach facilitates transfer learning models in medical imaging. The present study undertakes an evaluation of three foundational models in the domain of computer vision, namely AlexNet, VGG16, and ResNet-50. The VGG16 and ResNet-50 models demonstrated praiseworthy performance, thereby instigating the amalgamation of these models into a groundbreaking hybrid VGG16–ResNet-50 model. The amalgamated model was subsequently implemented on the dataset, yielding a remarkable accuracy of 99.98%, sensitivity of 99.98%, and specificity of 99.98% with an F1 score of 99.98%. Based on a comparative analysis with alternative models, it can be deduced that the suggested framework exhibits a commendable level of dependability in facilitating the timely identification of diverse cerebral neoplasms.",
                "authors": "Vinod Kumar Dhakshnamurthy, Murali Govindan, Kannan Sreerangan, Manikanda Devarajan Nagarajan, Abhijith Thomas",
                "citations": 13
            },
            {
                "title": "Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models",
                "abstract": "In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our findings show that model editing serves as a cost-effective tool for topical red-teaming by methodically applying targeted edits and evaluating the resultant model behavior.",
                "authors": "Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria",
                "citations": 14
            },
            {
                "title": "A Survey on Vision-Language-Action Models for Embodied AI",
                "abstract": "Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks. Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and speech recognition. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs). Their multi-modality capability has become a foundational element in robot learning. Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability. Some models focus on refining specific components. Others aim to develop control policies adept at predicting low-level actions. Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks. Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI. Therefore, it is imperative to capture the evolving landscape through a comprehensive survey.",
                "authors": "Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King",
                "citations": 13
            },
            {
                "title": "Large language models: a primer and gastroenterology applications",
                "abstract": "Over the past year, the emergence of state-of-the-art large language models (LLMs) in tools like ChatGPT has ushered in a rapid acceleration in artificial intelligence (AI) innovation. These powerful AI models can generate tailored and high-quality text responses to instructions and questions without the need for labor-intensive task-specific training data or complex software engineering. As the technology continues to mature, LLMs hold immense potential for transforming clinical workflows, enhancing patient outcomes, improving medical education, and optimizing medical research. In this review, we provide a practical discussion of LLMs, tailored to gastroenterologists. We highlight the technical foundations of LLMs, emphasizing their key strengths and limitations as well as how to interact with them safely and effectively. We discuss some potential LLM use cases for clinical gastroenterology practice, education, and research. Finally, we review critical barriers to implementation and ongoing work to address these issues. This review aims to equip gastroenterologists with a foundational understanding of LLMs to facilitate a more active clinician role in the development and implementation of this rapidly emerging technology.",
                "authors": "Omer Shahab, Bara El Kurdi, Aasma Shaukat, Girish Nadkarni, Ali Soroush",
                "citations": 13
            },
            {
                "title": "Inferring neural activity before plasticity as a foundation for learning beyond backpropagation",
                "abstract": null,
                "authors": "Yuhang Song, Beren Millidge, Tommaso Salvatori, Thomas Lukasiewicz, Zhenghua Xu, R. Bogacz",
                "citations": 8
            },
            {
                "title": "Lens: A Foundation Model for Network Traffic",
                "abstract": "Network traffic refers to the amount of data being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic is challenging due to the diverse nature of data packets, which often feature heterogeneous headers and encrypted payloads lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from massive traffic data. However, these methods typically excel in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundation model for network traffic that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the global information while preserving the generative ability, our model can better learn the representations from raw data. To further enhance pre-training effectiveness, we design a novel loss that combines three distinct tasks: Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous Traffic Prediction (HTP). Evaluation results across various benchmark datasets demonstrate that the proposed Lens outperforms the baselines in most downstream tasks related to both traffic understanding and generation. Notably, it also requires much less labeled data for fine-tuning compared to current methods.",
                "authors": "Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, Gang Zhou, Huajie Shao",
                "citations": 7
            },
            {
                "title": "EgoVideo: Exploring Egocentric Foundation Model and Downstream Adaptation",
                "abstract": "In this report, we present our solutions to the EgoVis Challenges in CVPR 2024, including five tracks in the Ego4D challenge and three tracks in the EPIC-Kitchens challenge. Building upon the video-language two-tower model and leveraging our meticulously organized egocentric video data, we introduce a novel foundation model called EgoVideo. This model is specifically designed to cater to the unique characteristics of egocentric videos and provides strong support for our competition submissions. In the Ego4D challenges, we tackle various tasks including Natural Language Queries, Step Grounding, Moment Queries, Short-term Object Interaction Anticipation, and Long-term Action Anticipation. In addition, we also participate in the EPIC-Kitchens challenge, where we engage in the Action Recognition, Multiple Instance Retrieval, and Domain Adaptation for Action Recognition tracks. By adapting EgoVideo to these diverse tasks, we showcase its versatility and effectiveness in different egocentric video analysis scenarios, demonstrating the powerful representation ability of EgoVideo as an egocentric foundation model. Our codebase and pretrained models are publicly available at https://github.com/OpenGVLab/EgoVideo.",
                "authors": "Baoqi Pei, Guo Chen, Jilan Xu, Yuping He, Yicheng Liu, Kanghua Pan, Yifei Huang, Yali Wang, Tong Lu, Limin Wang, Yu Qiao",
                "citations": 6
            },
            {
                "title": "Evaluation of Quality and Equality in Education Using the European Foundation for Quality Management Excellence Model—A Literature Review",
                "abstract": "Purpose: The purpose of this study was to determine the effects of the European Foundation for Quality Management (EFQM) Excellence Model on education and its contribution to ensuring high-quality education equality. Design/Methodology/Approach: A systematic literature review was conducted based on data from three academic publishers (Taylor & Francis, Emerald, and Elsevier Science Direct). Of the 69 journal articles, 61 were published between 2003 and 2023 and documented the results of the EFQM quality tool, and 8 articles were removed. Findings: The study highlights the results of applying the EFQM Excellence Model in the educational sector. Based on the bibliographic review, the feasibility of using the model in primary, secondary, and tertiary education is identified. We examine the findings related to using the model to ensure equality in high-quality education and the challenges faced by educational systems, with the ultimate goal of meeting student expectations. Research limitations/Implications: An important limitation is that the data were only drawn from three major publishers and the authors did not have access to all the relevant databases, since the search for articles was carried out in English only. The search for articles was limited by the keywords, as the EFQM Excellence Model was originally designed for the private sector. Practical implications: The results and limitations recorded in the study and the presentation of the 88 articles motivate academic researchers to conduct further study and fill the gap left by the limited number of publications on the application of the EFQM Excellence Model in the educational sector. Originality/Value: The EFQM Excellence Model has not been widely implemented in the high-quality educational sector and the existing literature reviews are limited. More research in the field of education is needed to determine the contribution of the excellence model to the evaluation of high-quality education.",
                "authors": "Effrosyni Taraza, Sofia Anastasiadou, Christos Papademetriou, Andreas Masouras",
                "citations": 6
            },
            {
                "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
                "abstract": "While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely\"superficial\". We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.",
                "authors": "Runzhe Zhan, Xinyi Yang, Derek F. Wong, Lidia S. Chao, Yue Zhang",
                "citations": 6
            },
            {
                "title": "SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation",
                "abstract": "Acquiring a multi-task imitation policy in 3D manipulation poses challenges in terms of scene understanding and action prediction. Current methods employ both 3D representation and multi-view 2D representation to predict the poses of the robot's end-effector. However, they still require a considerable amount of high-quality robot trajectories, and suffer from limited generalization in unseen tasks and inefficient execution in long-horizon reasoning. In this paper, we propose SAM-E, a novel architecture for robot manipulation by leveraging a vision-foundation model for generalizable scene understanding and sequence imitation for long-term action reasoning. Specifically, we adopt Segment Anything (SAM) pre-trained on a huge number of images and promptable masks as the foundation model for extracting task-relevant features, and employ parameter-efficient fine-tuning on robot data for a better understanding of embodied scenarios. To address long-horizon reasoning, we develop a novel multi-channel heatmap that enables the prediction of the action sequence in a single pass, notably enhancing execution efficiency. Experimental results from various instruction-following tasks demonstrate that SAM-E achieves superior performance with higher execution efficiency compared to the baselines, and also significantly improves generalization in few-shot adaptation to new tasks.",
                "authors": "Junjie Zhang, Chenjia Bai, Haoran He, Wenke Xia, Zhigang Wang, Bin Zhao, Xiu Li, Xuelong Li",
                "citations": 6
            },
            {
                "title": "RSBuilding: Toward General Remote Sensing Image Building Extraction and Change Detection With Foundation Model",
                "abstract": "Buildings not only constitute a significant proportion of man-made structures but also serve as a crucial component of geographic information databases, closely linked to human activities. The intelligent interpretation of buildings plays a significant role in urban planning and management, macroeconomic analysis, population dynamics, etc. Remote sensing image building interpretation primarily encompasses building extraction and change detection (CD). However, current methodologies often treat these two tasks as separate entities, thereby failing to leverage shared knowledge. Moreover, the complexity and diversity of remote sensing image scenes pose additional challenges, as most algorithms are designed to model individual small datasets, thus lacking cross-scene generalization. In this article, we propose a comprehensive remote sensing image building understanding model, termed RSBuilding, developed from the perspective of the foundation model. RSBuilding is designed to enhance cross-scene generalization and task universality. Specifically, we extract image features based on the prior knowledge of the foundation model and devise a multilevel feature sampler to augment scale information. To unify task representation and integrate image spatiotemporal clues, we introduce a cross-attention decoder with task prompts. Addressing the current shortage of datasets that incorporate annotations for both tasks, we have developed a federated training strategy to facilitate smooth model convergence even when supervision for some tasks is missing, thereby bolstering the complementarity of different tasks. Our model was trained on a dataset comprising up to 245 000 images and validated on multiple building extraction and CD datasets. The experimental results substantiate that RSBuilding can concurrently handle two structurally distinct tasks and exhibits robust zero-shot generalization capabilities. The code will be made available for open-source access at https://github.com/Meize0729/RSBuilding.",
                "authors": "Mingze Wang, Keyan Chen, Lili Su, Cilin Yan, Sheng Xu, Haotian Zhang, Pengcheng Yuan, Xiaolong Jiang, Baochang Zhang",
                "citations": 6
            },
            {
                "title": "BIM in construction waste management: A conceptual model based on the industry foundation classes standard",
                "abstract": null,
                "authors": "A. N. Schamne, Andr e Nagalli, Alfredo Augusto Vieira Soeiro, João Poças Martins",
                "citations": 9
            },
            {
                "title": "SAM-IE: SAM-based image enhancement for facilitating medical image diagnosis with segmentation foundation model",
                "abstract": null,
                "authors": "Changyan Wang, Haobo Chen, Xin Zhou, Meng Wang, Qi Zhang",
                "citations": 8
            },
            {
                "title": "Toward a foundation model of causal cell and tissue biology with a Perturbation Cell and Tissue Atlas",
                "abstract": null,
                "authors": "Jennifer E. Rood, A. Hupalowska, Aviv Regev",
                "citations": 8
            },
            {
                "title": "Performance and environmental impacts of deep foundation excavation in soft soils: A field and modeling-based case study in Nanjing, China",
                "abstract": null,
                "authors": "Chenhe Ge, Meng Yang, Pengfei Li, Mingju Zhang, Zhonghao Zhang",
                "citations": 8
            },
            {
                "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
                "abstract": "In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multimodal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM's vocabulary. To achieve this, we present the Vision-to-Language Tokenizer; abbreviated as V2T Tokenizer, which transforms an image into a “foreign language” with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rig-orous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at https://github.com/zh460045050/V2l-Tokenizer.",
                "authors": "Lei Zhu, Fangyun Wei, Yanye Lu",
                "citations": 13
            },
            {
                "title": "A Foundation Model for Error Correction Codes",
                "abstract": null,
                "authors": "Yoni Choukroun, Lior Wolf",
                "citations": 7
            },
            {
                "title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes",
                "abstract": null,
                "authors": "Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, R. Davuluri, Han Liu",
                "citations": 7
            },
            {
                "title": "Bacterial Emission Factors: A Foundation for the Terrestrial-Atmospheric Modeling of Bacteria Aerosolized by Wildland Fires",
                "abstract": "Wildland fire is a major global driver in the exchange of aerosols between terrestrial environments and the atmosphere. This exchange is commonly quantified using emission factors or the mass of a pollutant emitted per mass of fuel burned. However, emission factors for microbes aerosolized by fire have yet to be determined. Using bacterial cell concentrations collected on unmanned aircraft systems over forest fires in Utah, USA, we determine bacterial emission factors (BEFs) for the first time. We estimate that 1.39 × 1010 and 7.68 × 1011 microbes are emitted for each Mg of biomass consumed in fires burning thinning residues and intact forests, respectively. These emissions exceed estimates of background bacterial emissions in other studies by 3–4 orders of magnitude. For the ∼2631 ha of similar forests in the Fishlake National Forest that burn each year on average, an estimated 1.35 × 1017 cells or 8.1 kg of bacterial biomass were emitted. BEFs were then used to parametrize a computationally scalable particle transport model that predicted over 99% of the emitted cells were transported beyond the 17.25 x 17.25 km model domain. BEFs can be used to expand understanding of global wildfire microbial emissions and their potential consequences to ecosystems, the atmosphere, and humans.",
                "authors": "L. Kobziar, Phinehas Lampman, Ali Tohidi, Adam K. Kochanski, Antonio Cervantes, Andrew T. Hudak, Ryan McCarley, B. Gullett, Johanna Aurell, Rachel Moore, David C. Vuono, B. C. Christner, A. Watts, James Cronan, R. Ottmar",
                "citations": 7
            },
            {
                "title": "Buckling analysis of functionally graded porous variable thickness plates resting on Pasternak foundation using ES-MITC3",
                "abstract": "The main goal of this study is to further expand the ES-MITC3 for analyzing the buckling characteristics of functionally graded porous (FGP) variable thickness (VT) plates with sinusoidal porous distribution. The ES-MITC3 was developed to improve the accuracy of classical triangular elements (Q3) and overcome the locking phenomenon while still ensuring flexibility in discretizing the structural domain of the Q3. The first-order shear deformation theory (FSDT) in combination with ES-MITC3 is used due to its simplicity and effectiveness. The Pasternak foundation (PF) is a two-parameter model with springer stiffness ( 𝑘𝑘 1 ) and shear stiffness ( 𝑘𝑘 2 ) that describes the foundation reaction as a function of the deflection and its Laplacian. The accuracy and performance of the proposed formulation are verified through comparative examples. Moreover, a comprehensive analysis has been undertaken to scrutinize the effects of geometric parameters and material properties on the buckling of FGP VT plates.",
                "authors": "Truong-thanh Nguyen, Truong Son Le, T. Tran, Q. Pham",
                "citations": 5
            },
            {
                "title": "SARATR-X: Toward Building A Foundation Model for SAR Target Recognition",
                "abstract": "Despite the remarkable progress in synthetic aperture radar automatic target recognition (SAR ATR), recent efforts have concentrated on detecting and classifying a specific category, e.g., vehicles, ships, airplanes, or buildings. One of the fundamental limitations of the top-performing SAR ATR methods is that the learning paradigm is supervised, task-specific, limited-category, closed-world learning, which depends on massive amounts of accurately annotated samples that are expensively labeled by expert SAR analysts and have limited generalization capability and scalability. In this work, we make the first attempt towards building a foundation model for SAR ATR, termed SARATR-X. SARATR-X learns generalizable representations via self-supervised learning (SSL) and provides a cornerstone for label-efficient model adaptation to generic SAR target detection and classification tasks. Specifically, SARATR-X is trained on 0.18 M unlabelled SAR target samples, which are curated by combining contemporary benchmarks and constitute the largest publicly available dataset till now. Considering the characteristics of SAR images, a backbone tailored for SAR ATR is carefully designed, and a two-step SSL method endowed with multi-scale gradient features was applied to ensure the feature diversity and model scalability of SARATR-X. The capabilities of SARATR-X are evaluated on classification under few-shot and robustness settings and detection across various categories and scenes, and impressive performance is achieved, often competitive with or even superior to prior fully supervised, semi-supervised, or self-supervised algorithms. Our SARATR-X and the curated dataset are released at https://github.com/waterdisappear/SARATR-X to foster research into foundation models for SAR image interpretation.",
                "authors": "Wei-Jang Li, Wei Yang, Yuenan Hou, Li Liu, Yongxiang Liu, Xiang Li",
                "citations": 5
            },
            {
                "title": "The Horizontal Bearing Characteristics and Microscopic Soil Deformation Mechanism of Pile-Bucket Composite Foundation in Sand",
                "abstract": "The pile-bucket composite foundation represents an innovative foundation form that surpasses the horizontal bearing performance of both single bucket-shaped foundations and pile foundations. The intricate interplay between piles and buckets introduces the complexity of the factors influencing the bearing performance of composite foundations under horizontal loads. In this paper, the indoor model tests were conducted to investigate the effects of relative density and pile-to-barrel diameter ratio on the horizontal bearing capacity and surrounding soil pressure of the pile-bucket composite foundation. A sensitivity analysis on the bearing characteristics of the pile-bucket foundation was performed using ABAQUS/CAE 2020 software. The results reveal a consistent variation in load–displacement curves across diverse diameter ratios of piles to buckets. The pile-bucket diameter ratio significantly impacts the horizontal bearing characteristics of the composite foundation. Reducing the pile-bucket diameter ratio improves the horizontal bearing capacity of the composite foundation. When the diameter ratio of piles to buckets diminishes to ≤0.317, the influence of this ratio on bearing performance becomes markedly pronounced. The displacement range of the surface soil decreases with an increase in relative density, while the influence depth of the surrounding soil of the composite foundation significantly decreases as the pile-to-barrel diameter ratio decreases.",
                "authors": "Xin Zhang, Dongmin Yu, Kaifei Zhu, Aolai Zhao, M. Ren",
                "citations": 5
            },
            {
                "title": "Large Wireless Model (LWM): A Foundation Model for Wireless Channels",
                "abstract": "This paper presents the Large Wireless Model (LWM) -- the world's first foundation model for wireless channels. Designed as a task-agnostic model, LWM generates universal, rich, contextualized channel embeddings (features) that potentially enhance performance across a wide range of downstream tasks in wireless communication and sensing systems. Towards this objective, LWM, which has a transformer-based architecture, was pre-trained in a self-supervised manner on large-scale wireless channel datasets. Our results show consistent improvements in classification and regression tasks when using the LWM embeddings compared to raw channel representations, especially in scenarios with high-complexity machine learning tasks and limited training datasets. This LWM's ability to learn from large-scale wireless data opens a promising direction for intelligent systems that can efficiently adapt to diverse tasks with limited data, paving the way for addressing key challenges in wireless communication and sensing systems.",
                "authors": "Sadjad Alikhani, Gouranga Charan, Ahmed Alkhateeb",
                "citations": 4
            },
            {
                "title": "A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model",
                "abstract": "Remarkable strides in computational pathology have been made in the task-agnostic foundation model that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H\\&E diagnostic whole slide images and their associated pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPath, we propose a novel whole-slide pretraining paradigm which injects multimodal knowledge at the whole-slide context into the pathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed paradigm revolutionizes the workflow of pretraining for CPath, which enables the pathology FM to acquire the whole-slide context. To our knowledge, this is the first attempt to incorporate multimodal knowledge at the slide level for enhancing pathology FMs, expanding the modelling context from unimodal to multimodal knowledge and from patch-level to slide-level. To systematically evaluate the capabilities of mSTAR, extensive experiments including slide-level unimodal and multimodal applications, are conducted across 7 diverse types of tasks on 43 subtasks, resulting in the largest spectrum of downstream tasks. The average performance in various slide-level applications consistently demonstrates significant performance enhancements for mSTAR compared to SOTA FMs.",
                "authors": "Yingxue Xu, Yihui Wang, Fengtao Zhou, Jiabo Ma, Shu Yang, Huangjing Lin, Xin Wang, Jiguang Wang, Li Liang, Anjia Han, R. Chan, Hao Chen",
                "citations": 4
            },
            {
                "title": "Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks",
                "abstract": "Brain age is the estimate of biological age derived from neuroimaging datasets using machine learning algorithms. Increasing brain age with respect to chronological age can reflect increased vulnerability to neurodegeneration and cognitive decline. In this paper, we study NeuroVNN, based on coVariance neural networks, as a paradigm for foundation model for the brain age prediction application. NeuroVNN is pre-trained as a regression model on healthy population to predict chronological age using cortical thickness features and fine-tuned to estimate brain age in different neurological contexts. Importantly, NeuroVNN adds anatomical interpretability to brain age and has a `scale-free' characteristic that allows its transference to datasets curated according to any arbitrary brain atlas. Our results demonstrate that NeuroVNN can extract biologically plausible brain age estimates in different populations, as well as transfer successfully to datasets of dimensionalities distinct from that for the dataset used to train NeuroVNN.",
                "authors": "Saurabh Sihag, Gonzalo Mateos, Alejandro Ribeiro",
                "citations": 4
            },
            {
                "title": "The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources",
                "abstract": "Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. We hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.",
                "authors": "Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, Sayash Kapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, Maribeth Rauh, Aviya Skowron, Bertie Vidgen, Laura Weidinger, Arvind Narayanan, Victor Sanh, D. Adelani, Percy Liang, Rishi Bommasani, Peter Henderson, Sasha Luccioni, Yacine Jernite, Luca Soldaini",
                "citations": 4
            },
            {
                "title": "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications",
                "abstract": "This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",
                "authors": "Hao-Han Guo, Kun Liu, Fei-Yu Shen, Yi-Chen Wu, Fenglong Xie, Kun Xie, Kai-Tuo Xu",
                "citations": 4
            },
            {
                "title": "Deformation Effects of Deep Foundation Pit Excavation on Retaining Structures and Adjacent Subway Stations",
                "abstract": "In complex underground conditions, the excavation of deep foundation pits has a significant impact on the deformation of retaining structures and nearby subway stations. To investigate the influence of deep excavation on the deformation of adjacent structures, a three-dimensional numerical model of the foundation pit, existing subway station, and tunnel structure was established using FLAC 3D software, based on the Shenzhen Bay Super Headquarters C Tower foundation pit project. The study analyzed the deformation characteristics of retaining structures, adjacent subway stations, and tunnels during different stages of deep excavation, and the accuracy of the numerical simulation results was validated through field monitoring data. The results indicate that during the excavation process of the foundation pit, the lateral horizontal displacement of the retaining structure is generally small, with a typical “concave inward” lateral deformation curve; the horizontal displacement value of the contiguous wall section is less than that of the interlocking pile section. The bending moments of the retaining structure show a distribution pattern with larger values in the middle and smaller values at the top and bottom of the pit, with a relatively uniform distribution of internal support forces. The maximum displacement of the nearby subway station is 8.75 mm, and the maximum displacement of the subway tunnel is 2.29 mm. The research findings can provide references for evaluating the impact of newly built foundation pits near subway stations and contribute to the rational design and safe construction of new projects.",
                "authors": "Zhijian Jiang, Shu Zhu, X. Que, Xinliang Ge",
                "citations": 4
            },
            {
                "title": "Semi‐analytical solution for double‐layered elliptical cylindrical foundation model improved by prefabricated vertical drains",
                "abstract": "This work proposes a semi‐analytical solution for a double‐layered elliptical cylindrical soft foundation improved by prefabricated vertical drains. The governing equations, continuity conditions are boundary conditions in the elliptical cylindrical system are introduced first. The Laplace transform is employed to convert the time variable t in partial differential equations into the Laplace complex argument s. Based on the boundary condition and continuity condition, the solution in the frequency domain is derived. By using Abate's fixed Euler Algorithm, the solutions in the time domain are obtained. A finite element analysis is performed to demonstrate the accuracy of the proposed solution. Then, parametric studies reveal that soil permeability and soil compressibility have great influences on the second layer of soil in the elliptical cylindrical system, but relatively small effects on the first layer. Neglecting differences in the hydraulic conductivity and the coefficient of volume compressibility of multi‐layered soil in the elliptical cylindrical system could not predict the excess pore pressure in the second layer correctly.",
                "authors": "Xu-dong Zhao, Nanning Guo, Wenzhao Cao, Wenhui Gong, Yang Liu",
                "citations": 4
            },
            {
                "title": "Learning to Prompt with Text Only Supervision for Vision-Language Models",
                "abstract": "Foundational vision-language models such as CLIP are becoming a new paradigm in vision, due to their excellent generalization abilities. However, adapting these models for downstream tasks while maintaining their generalization remains a challenge. In literature, one branch of methods adapts CLIP by learning prompts using visual information. While effective, most of these works require labeled data which is not practical, and often struggle to generalize towards new datasets due to over-fitting on the source data. An alternative approach resorts to training-free methods by generating class descriptions from large language models (LLMs) and perform prompt ensembling. However, these methods often generate class specific prompts that cannot be transferred to other classes, which incur higher costs by generating LLM descriptions for each class separately. In this work, we propose to combine the strengths of these both streams of methods by learning prompts using only text data derived from LLMs. As supervised training of prompts is not trivial due to absence of images, we develop a training approach that allows prompts to extract rich contextual knowledge from LLM data. Moreover, with LLM contextual data mapped within the learned prompts, it enables zero-shot transfer of prompts to new classes and datasets potentially cutting the LLM prompt engineering cost. To the best of our knowledge, this is the first work that learns generalized prompts using text only data. We perform extensive evaluations on 4 benchmarks where our method improves over prior ensembling works while being competitive to those utilizing labeled images. Our code and pre-trained models are available at https://github.com/muzairkhattak/ProText.",
                "authors": "Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, L. V. Gool, F. Tombari",
                "citations": 12
            },
            {
                "title": "K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning",
                "abstract": "Strategic reasoning is a complex yet essential capability for intelligent agents. It requires Large Language Model (LLM) agents to adapt their strategies dynamically in multi-agent environments. Unlike static reasoning tasks, success in these contexts depends on anticipating other agents' beliefs and actions while continuously adjusting strategies to achieve individual goals. LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables them to dynamically infer others' perspectives and adapt to changing environments. Inspired by the Level-K framework from game theory and behavioral economics, which extends reasoning from simple reactions to structured strategic depth, we propose a novel framework:\"K-Level Reasoning with Large Language Models (K-R).\"This framework employs recursive mechanisms to enable LLMs to achieve varying levels of strategic depth, allowing agents to form higher order beliefs - beliefs about others' beliefs. We validate this framework through rigorous testing on four testbeds: two classical game theory problems and two social intelligence tasks. The results demonstrate the advantages of K-R in strategic reasoning. Our work presents the first recursive implementation of strategic depth in large language models (LLMs). It establishes a foundation for future research into theory of mind and strategic reasoning in LLMs.",
                "authors": "Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, Furu Wei",
                "citations": 12
            },
            {
                "title": "VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis",
                "abstract": "Generalist foundation model has ushered in newfound capabilities in medical domain. However, the contradiction between the growing demand for high-quality annotated data with patient privacy continues to intensify. The utilization of medical artificial intelligence generated content (Med-AIGC) as an inexhaustible resource repository arises as a potential solution to address the aforementioned challenge. Here we harness 1 million open-source synthetic fundus images paired with natural language descriptions, to curate an ethical language-image foundation model for retina image analysis named VisionCLIP. VisionCLIP achieves competitive performance on three external datasets compared with the existing method pre-trained on real-world data in a zero-shot fashion. The employment of artificially synthetic images alongside corresponding textual data for training enables the medical foundation model to successfully assimilate knowledge of disease symptomatology, thereby circumventing potential breaches of patient confidentiality.",
                "authors": "Hao Wei, Bowen Liu, Minqing Zhang, Peilun Shi, Wu Yuan",
                "citations": 3
            },
            {
                "title": "Self-Normalizing Foundation Model for Enhanced Multi-Omics Data Analysis in Oncology",
                "abstract": "Multi-omics research has enhanced our understanding of cancer heterogeneity and progression. Investigating molecular data through multi-omics approaches is crucial for unraveling the complex biological mechanisms underlying cancer, thereby enabling more effective diagnosis, treatment, and prevention strategies. However, predicting patient outcomes through the integration of all available multi-omics data is still an under-study research direction. Here, we present SeNMo, a foundation model that has been trained on multi-omics data across 33 cancer types. SeNMo is particularly efficient in handling multi-omics data characterized by high-width and low-length attributes. We trained SeNMo for the task of overall survival of patients using pan-cancer multi-omics data involving 33 cancer sites from the GDC. The training multi-omics data includes gene expression, DNA methylation, miRNA expression, DNA mutations, protein expression modalities, and clinical data. SeNMo was validated on two independent cohorts: Moffitt Cancer Center and CPTAC lung squamous cell carcinoma. We evaluated the model's performance in predicting patient's overall survival using the C-Index. SeNMo performed consistently well in the training regime, reflected by the validation C-Index of 0.76 on GDC's public data. In the testing regime, SeNMo performed with a C-Index of 0.758 on a held-out test set. The model showed an average accuracy of 99.8% on the task of classifying the primary cancer type on the pan-cancer test cohort. SeNMo demonstrated robust performance on the classification task of predicting the primary cancer type of patients. SeNMo further demonstrated significant performance in predicting tertiary lymph structures from multi-omics data, showing generalizability across cancer types, molecular data types, and clinical endpoints.",
                "authors": "Asim Waqas, Aakash Tripathi, Sabeen Ahmed, Ashwin Mukund, Hamza Farooq, M. Schabath, Paul Stewart, Mia Naeini, Ghulam Rasool",
                "citations": 3
            },
            {
                "title": "zkLLM: Zero Knowledge Proofs for Large Language Models",
                "abstract": "The recent surge in artificial intelligence (AI), characterized by the prominence of large language models (LLMs), has ushered in fundamental transformations across the globe. However, alongside these advancements, concerns surrounding the legitimacy of LLMs have grown, posing legal challenges to their extensive applications. Compounding these concerns, the parameters of LLMs are often treated as intellectual property, restricting direct investigations. In this study, we address a fundamental challenge within the realm of AI legislation: the need to establish the authenticity of outputs generated by LLMs. To tackle this issue, we present zkLLM, which stands as the inaugural specialized zero-knowledge proof tailored for LLMs to the best of our knowledge. Addressing the persistent challenge of non-arithmetic operations in deep learning, we introduce tlookup, a parallelized lookup argument designed for non-arithmetic tensor operations in deep learning, offering a solution with no asymptotic overhead. Furthermore, leveraging the foundation of tlookup, we introduce zkAttn, a specialized zero-knowledge proof crafted for the attention mechanism, carefully balancing considerations of running time, memory usage, and accuracy. Empowered by our fully parallelized CUDA implementation, zkLLM emerges as a significant stride towards achieving efficient zero-knowledge verifiable computations over LLMs. Remarkably, for LLMs boasting 13 billion parameters, our approach enables the generation of a correctness proof for the entire inference process in under 15 minutes. The resulting proof, compactly sized at less than 200 kB, is designed to uphold the privacy of the model parameters, ensuring no inadvertent information leakage.",
                "authors": "Haochen Sun, Jason Li, Hongyang Zhang",
                "citations": 9
            },
            {
                "title": "SiamQuality: a ConvNet-based foundation model for photoplethysmography signals",
                "abstract": "Abstract Objective. Physiological data are often low quality and thereby compromises the effectiveness of related health monitoring. The primary goal of this study is to develop a robust foundation model that can effectively handle low-quality issue in physiological data. Approach. We introduce SiamQuality, a self-supervised learning approach using convolutional neural networks (CNNs) as the backbone. SiamQuality learns to generate similar representations for both high and low quality photoplethysmography (PPG) signals that originate from similar physiological states. We leveraged a substantial dataset of PPG signals from hospitalized intensive care patients, comprised of over 36 million 30 s PPG pairs. Main results. After pre-training the SiamQuality model, it was fine-tuned and tested on six PPG downstream tasks focusing on cardiovascular monitoring. Notably, in tasks such as respiratory rate estimation and atrial fibrillation detection, the model’s performance exceeded the state-of-the-art by 75% and 5%, respectively. The results highlight the effectiveness of our model across all evaluated tasks, demonstrating significant improvements, especially in applications for heart monitoring on wearable devices. Significance. This study underscores the potential of CNNs as a robust backbone for foundation models tailored to physiological data, emphasizing their capability to maintain performance despite variations in data quality. The success of the SiamQuality model in handling real-world, variable-quality data opens new avenues for the development of more reliable and efficient healthcare monitoring technologies.",
                "authors": "C. Ding, Zhicheng Guo, Zhaoliang Chen, Randall J Lee, C. Rudin, Xiao Hu",
                "citations": 2
            },
            {
                "title": "Foundation Settlement Prediction of High-Plateau\nAirport Based on Modified LSTM Model\nand BP Neural Network Model",
                "abstract": "In order to ensure flight safety, the requirement of foundation settlement of high-plateau airport is stricter than that of airport in plain area. In order to monitor the abnormal state of runway foundation in the process of use of a high-plateau airport and prevent and resolve the major risk of foundation settlement to flight safety. It takes a high-plateau airport in the southwest mountain area as an example, selecting two representative A and B sections for analysis. It takes the first 60 days’ monitoring data as training samples, which shows nonlinear characteristics. The Long and Short Term Memory neural network (LSTM) prediction model and BP neural network model are constructed to predict the trend of foundation settlement after construction. In the process of building the LSTM model, the minimum root-mean-square error of test samples was selected as the fitness function, and the parameters of the LSTM model were modified by Genetic Algorithm (GA). And then the modified LSTM prediction model based on the early settlement of the foundation was constructed. The results shows that the modified LSTM model and BP model constructed in this paper are generally consistent with the field measured values in the prediction of airport foundation settlement of high-plateau, but the modified LSTM model is more sensitive to the abrupt change of data and has a more stable trend than the BP model. The predicted values of the modified LSTM model are all greater than those of the BP model, and the predicted values of the modified LSTM model are closer to the monitored values in the field than the predicted values of the BP model, and the relative error between the predicted values and the monitored values is less than 3%. The research can provide a reliable theoretical reference for the design, construction, operation management and later maintenance of high-plateau airport.",
                "authors": "Jun Feng, Xiaomei Lu, Yanjun Liu, Jian Wu, Jizhe He, Zikang Chen, Zhuoya Zhao",
                "citations": 2
            },
            {
                "title": "A Proposed Vision For Developing Institutional Performance In Arab Universities Appropriating The Standards Of The European Foundation For Quality Management (EFQM)",
                "abstract": "The current research study proposes a vision for developing institutional performance in Arab universities using the standards of the European Foundation for Quality Management’s excellence model (EFQM). The descriptive approach is utilized to achieve the research objectives. The reality of institutional performance in some Arab universities in light of the standards of the European model for managing excellence and the differences in perspectives depending on the variables of the country, academic rank, and number of years of experience shall also be identified. The research sample consists of (1150) individuals from some Arab universities in Jordan, Egypt, and Saudi Arabia. A 53-item questionnaire used as a research instrument includes two parts and seven domains. The results indicate the availability of all domains that monitor the reality of institutional performance in some Arab universities in light of the standards of the European Model for Excellence Management from a medium to high degree. The findings also show differences between the means of the research sample’s responses depending on the variables of country, academic rank, and number of years of experience. Given the results, the research paper proposes a vision for developing institutional performance in Arab universities in light of the standards of the European model of excellence management that includes a set of foundations, principles, goals, procedures, and requirements for success.",
                "authors": "Ekhlas Mohammad Abdel-Ghani Alraba’y, Amjad Mahmoud Daradkah, Turki K. Alotaibi, Mohamed G. Hussein, Omar Mohammed Alkharabsheh, khaled M Hamadin, Bayan Thaher Almadi, Areen A. Al-nemrat, Maria Salih Tawalbeh, Burhan Mahmoud Hamadneh, Mohammad Sulieman Jaradat, Muneera M. ALShurman, Mohammad Ahmad Momany, Salma saud aldakheel, Ashraf Mahmoud Ahmed Mahmoud",
                "citations": 2
            },
            {
                "title": "AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis",
                "abstract": "Traffic accident analysis is pivotal for enhancing public safety and developing road regulations. Traditional approaches, although widely used, are often constrained by manual analysis processes, subjective decisions, uni-modal outputs, as well as privacy issues related to sensitive data. This paper introduces the idea of AccidentGPT, a foundation model of traffic accident analysis, which incorporates multi-modal input data to automatically reconstruct the accident process video with dynamics details, and furthermore provide multi-task analysis with multi-modal outputs. The design of the AccidentGPT is empowered with a multi-modality prompt with feedback for task-oriented adaptability, a hybrid training schema to leverage labelled and unlabelled data, and a edge-cloud split configuration for data privacy. To fully realize the functionalities of this model, we proposes several research opportunities. This paper serves as the stepping stone to fill the gaps in traditional approaches of traffic accident analysis and attract the research community attention for automatic, objective, and privacy-preserving traffic accident analysis.",
                "authors": "Kebin Wu, Wenbin Li, Xiaofei Xiao",
                "citations": 2
            },
            {
                "title": "Load-bearing response of deep content mixing soft soil composite foundation based on the geotechnical centrifugal model test",
                "abstract": "Deep cement mixing (DCM) is an effective method of treating soft underwater soil foundations. Currently, the real-time failure development of a composite foundation cannot be obtained by most model tests of DCM pile foundation reinforcement. In this study, a circuit device for identifying pile failure and that can reflect the real-time working state of piles arranged at different positions in a soft soil composite foundation was developed. By applying this device to a geotechnical centrifugal model test, the load-bearing response process of a bank slope reinforced with a soft-soil composite foundation and DCM piles was studied. The failure development law of the DCM piles, ultimate surcharge of the bank slope, failure mode of the foundation, and the pile-soil stress ratio were determined from the test results. The law obtained in this study provides a reference for the application of DCM piles in underwater soft soil foundation engineering.",
                "authors": "Y. Lin, G. Tang, X. Ma, C. Y. Ma, Z. Wang",
                "citations": 2
            },
            {
                "title": "RET-CLIP: A Retinal Image Foundation Model Pre-trained with Clinical Diagnostic Reports",
                "abstract": "The Vision-Language Foundation model is increasingly investigated in the fields of computer vision and natural language processing, yet its exploration in ophthalmology and broader medical applications remains limited. The challenge is the lack of labeled data for the training of foundation model. To handle this issue, a CLIP-style retinal image foundation model is developed in this paper. Our foundation model, RET-CLIP, is specifically trained on a dataset of 193,865 patients to extract general features of color fundus photographs (CFPs), employing a tripartite optimization strategy to focus on left eye, right eye, and patient level to reflect real-world clinical scenarios. Extensive experiments demonstrate that RET-CLIP outperforms existing benchmarks across eight diverse datasets spanning four critical diagnostic categories: diabetic retinopathy, glaucoma, multiple disease diagnosis, and multi-label classification of multiple diseases, which demonstrate the performance and generality of our foundation model. The sourse code and pre-trained model are available at https://github.com/sStonemason/RET-CLIP.",
                "authors": "Jiawei Du, Jia Guo, Weihang Zhang, Shengzhu Yang, Hanruo Liu, Huiqi Li, Ningli Wang",
                "citations": 2
            },
            {
                "title": "Evaluating the representational power of pre-trained DNA language models for regulatory genomics",
                "abstract": "The emergence of genomic language models (gLMs) offers an unsupervised approach to learning a wide diversity of cis-regulatory patterns in the non-coding genome without requiring labels of functional activity generated by wet-lab experiments. Previous evaluations have shown that pre-trained gLMs can be leveraged to improve predictive performance across a broad range of regulatory genomics tasks, albeit using relatively simple benchmark datasets and baseline models. Since the gLMs in these studies were tested upon fine-tuning their weights for each downstream task, determining whether gLM representations embody a foundational understanding of cis-regulatory biology remains an open question. Here we evaluate the representational power of pre-trained gLMs to predict and interpret cell-type-specific functional genomics data that span DNA and RNA regulation. Our findings suggest that probing the representations of pre-trained gLMs do not offer substantial advantages over conventional machine learning approaches that use one-hot encoded sequences. This work highlights a major gap with current gLMs, raising potential issues in conventional pre-training strategies for the non-coding genome.",
                "authors": "Ziqi Tang, Nirali Somia, Yiyang Yu, Peter K. Koo",
                "citations": 11
            },
            {
                "title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models",
                "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations-a significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MediHallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MediHallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work will be released soon.",
                "authors": "Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, Lihua Zhang",
                "citations": 10
            },
            {
                "title": "GENA-LM: a family of open-source foundational DNA language models for long sequences",
                "abstract": "Recent advancements in genomics, propelled by artificial intelligence, have unlocked unprecedented capabilities in interpreting genomic sequences, mitigating the need for exhaustive experimental analysis of complex, intertwined molecular processes inherent in DNA function. A significant challenge, however, resides in accurately decoding genomic sequences, which inherently involves comprehending rich contextual information dispersed across thousands of nucleotides. To address this need, we introduce GENA-LM, a suite of transformer-based foundational DNA language models capable of handling input lengths up to 36,000 base pairs. Notably, integrating the newly-developed Recurrent Memory mechanism allows these models to process even larger DNA segments. We provide pre-trained versions of GENA-LM, including multispecies and taxon-specific models, demonstrating their capability for fine-tuning and addressing a spectrum of complex biological tasks with modest computational demands. While language models have already achieved significant breakthroughs in protein biology, GENA-LM showcases a similarly promising potential for reshaping the landscape of genomics and multi-omics data analysis. All models are publicly available on GitHub https://github.com/AIRI-Institute/GENA_LM and HuggingFace https://huggingface.co/AIRI-Institute. In addition, we provide a web-service https://dnalm.airi.net/ allowing user-friendly DNA annotation with GENA-LM models.",
                "authors": "V. Fishman, Yuri Kuratov, Maxim Petrov, Aleksei Shmelev, Denis Shepelin, N. Chekanov, O. Kardymon, M. Burtsev",
                "citations": 10
            },
            {
                "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
                "abstract": "Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present *ToolSword*, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing **malicious queries** and **jailbreak attacks** in the input stage, **noisy misdirection** and **risky cues** in the execution stage, and **harmful feedback** and **error conflicts** in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data is released in https://github.com/Junjie-Ye/ToolSword.",
                "authors": "Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, Xuanjing Huang",
                "citations": 10
            },
            {
                "title": "MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models",
                "abstract": "Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the diffusion model and attention mechanisms to improve gesture synthesis. However, due to the high computational complexity of these techniques, generating long and diverse sequences with low latency remains a challenge. We explore the potential of state space models (SSMs) to address the challenge, implementing a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures. Leveraging the foundational Mamba block, we introduce MambaTalk, enhancing gesture diversity and rhythm through multimodal integration. Extensive experiments demonstrate that our method matches or exceeds the performance of state-of-the-art models.",
                "authors": "Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, Xiu Li",
                "citations": 11
            },
            {
                "title": "BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models",
                "abstract": "Safety backdoor attacks in large language models (LLMs) enable harmful behaviors to be stealthily triggered while evading detection during normal interactions. The high dimensionality of the trigger search space and the diverse range of potential malicious behaviors in LLMs make this a critical open problem. This paper presents BEEAR, a novel mitigation method based on a key insight: backdoor triggers induce a uniform drift in the model’s embedding space, irrespective of the trigger’s form or targeted behavior. Leveraging this observation, we introduce a bi-level optimization approach. The inner level identifies universal perturbations to the decoder’s embeddings that steer the model towards defender-defined unwanted behaviors; the outer level fine-tunes the model to reinforce safe behaviors against these perturbations. Our experiments demonstrate the effectiveness of this approach, reducing the success rate of safety backdoor attacks from over 95% to <1% for general harmful behaviors and from 47% to 0% for Sleeper Agents, without compromising the model’s helpfulness. Notably, our method relies only on defender-defined sets of safe and unwanted behaviors without any assumptions about the trigger location or attack mechanism. This work represents the first practical framework to counter safety backdoors in LLMs and provides a foundation for future advancements in AI safety and security.",
                "authors": "Yi Zeng, Weiyu Sun, Tran Ngoc Huynh, Dawn Song, Bo Li, Ruoxi Jia",
                "citations": 10
            },
            {
                "title": "WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence",
                "abstract": "The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.",
                "authors": "Jiawei Shao, Jingwen Tong, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang",
                "citations": 11
            },
            {
                "title": "Cystic fibrosis foundation position paper: Redefining the CF care model.",
                "abstract": null,
                "authors": "D. Goetz, R.F. Brown, S. Filigno, S. Bichl, A.L. Nelson, C.A. Merlo, R. Juel, P. Lomas, S. E. Hempstead, Q. Tran, A. W. Brown, P. Flume",
                "citations": 4
            },
            {
                "title": "Seismic response of combined piled raft foundation using advanced liquefaction model",
                "abstract": null,
                "authors": "B. K. Maheshwari, Mohd Firoj",
                "citations": 4
            },
            {
                "title": "Helpless infants are learning a foundation model",
                "abstract": null,
                "authors": "Rhodri Cusack, Marc'Aurelio Ranzato, Christine J. Charvet",
                "citations": 4
            },
            {
                "title": "A bioactivity foundation model using pairwise meta-learning",
                "abstract": null,
                "authors": "Bin Feng, Zequn Liu, Nanlan Huang, Zhiping Xiao, Haomiao Zhang, Srbuhi Mirzoyan, Hanwen Xu, Jiaran Hao, Yinghui Xu, Ming Zhang, Sheng Wang",
                "citations": 4
            },
            {
                "title": "Prostate Cancer Foundation Screening Guidelines for Black Men in the United States.",
                "abstract": "BACKGROUND\nIn the United States, Black men are at highest risk for being diagnosed with and dying from prostate cancer. Given this disparity, we examined relevant data to establish clinical prostate-specific antigen (PSA) screening guidelines for Black men in the United States.\n\n\nMETHODS\nA comprehensive literature search identified 1848 unique publications for screening. Of those screened, 287 studies were selected for full-text review, and 264 were considered relevant and form the basis for these guidelines. The numbers were reported according to PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines.\n\n\nRESULTS\nThree randomized controlled trials provided Level 1 evidence that regular PSA screening of men 50 to 74 years of age of average risk reduced metastasis and prostate cancer death at 16 to 22 years of follow-up. The best available evidence specifically for Black men comes from observational and modeling studies that consider age to obtain a baseline PSA, frequency of testing, and age when screening should end. Cohort studies suggest that discussions about baseline PSA testing between Black men and their clinicians should begin in the early 40s, and data from modeling studies indicate prostate cancer develops 3 to 9 years earlier in Black men compared with non-Black men. Lowering the age for baseline PSA testing to 40 to 45 years of age from 50 to 55 years of age, followed by regular screening until 70 years of age (informed by PSA values and health factors), could reduce prostate cancer mortality in Black men (approximately 30% relative risk reduction) without substantially increasing overdiagnosis.\n\n\nCONCLUSIONS\nThese guidelines recommend that Black men should obtain information about PSA screening for prostate cancer. Among Black men who elect screening, baseline PSA testing should occur between ages 40 and 45. Depending on PSA value and health status, annual screening should be strongly considered. (Supported by the Prostate Cancer Foundation.).",
                "authors": "I. Garraway, Sigrid V. Carlsson, Y. Nyame, J. Vassy, Marina Chilov, Mark Fleming, Stanley K Frencher, Daniel J. George, A. Kibel, Sherita A. King, Rick A Kittles, Brandon A Mahal, Curtis A Pettaway, Timothy R Rebbeck, B. Rose, Randy Vince, Robert A. Winn, K. Yamoah, William K. Oh",
                "citations": 4
            },
            {
                "title": "Iris-SAM: Iris Segmentation Using a Foundation Model",
                "abstract": "Iris segmentation is a critical component of an iris biometric system and it involves extracting the annular iris region from an ocular image. In this work, we develop a pixel-level iris segmentation model from a foundational model, viz., Segment Anything Model (SAM), that has been successfully used for segmenting arbitrary objects. The primary contribution of this work lies in the integration of different loss functions during the fine-tuning of SAM on ocular images. In particular, the importance of Focal Loss is borne out in the fine-tuning process since it strategically addresses the class imbalance problem (i.e., iris versus non-iris pixels). Experiments on ND-IRIS-0405, CASIA-Iris-Interval-v3, and IIT-Delhi-Iris datasets convey the efficacy of the trained model for the task of iris segmentation. For instance, on the ND-IRIS-0405 dataset, an average segmentation accuracy of 99.58% was achieved, compared to the best baseline performance of 89.75%.",
                "authors": "Parisa Farmanifard, Arun Ross",
                "citations": 3
            },
            {
                "title": "Toward the unification of generative and discriminative visual foundation model: a survey",
                "abstract": null,
                "authors": "Xu Liu, Tong Zhou, Chong Wang, Yuping Wang, Yuanxin Wang, Qinjingwen Cao, Weizhi Du, Yonghuan Yang, Junjun He, Yu Qiao, Yiqing Shen",
                "citations": 3
            },
            {
                "title": "Foundation of Floer homotopy theory I: Flow categories",
                "abstract": "We construct a stable infinity category with objects flow categories and morphisms flow bimodules; our construction has many flavors, related to a choice of bordism theory, and we discuss in particular framed bordism and the bordism theory of complex oriented derived orbifolds. In this setup, the construction of homotopy types associated to Floer-theoretic data is immediate: the moduli spaces of solutions to Floer's equation assemble into a flow category with respect to the appropriate bordism theory, and the associated Floer homotopy types arise as suitable mapping spectra in this category. The definition of these mapping spectra is sufficiently explicit to allow a direct interpretation of the Floer homotopy groups as Floer bordism groups. In the setting of framed bordism, we show that the category we construct is a model for the category of spectra. We implement the construction of Floer homotopy types in this new formalism for the case of Hamiltonian Floer theory.",
                "authors": "M. Abouzaid, A. Blumberg",
                "citations": 3
            },
            {
                "title": "Graph foundation model",
                "abstract": null,
                "authors": "Chuan Shi, Junze Chen, Jiawei Liu, Cheng Yang",
                "citations": 3
            },
            {
                "title": "DAM: Towards a Foundation Model for Forecasting",
                "abstract": null,
                "authors": "Luke Darlow, Qiwen Deng, Ahmed Hassan, Martin Asenov, Rajkarn Singh, Artjom Joosen, Adam Barker, A. Storkey",
                "citations": 3
            },
            {
                "title": "multiGradICON: A Foundation Model for Multimodal Medical Image Registration",
                "abstract": "Modern medical image registration approaches predict deformations using deep networks. These approaches achieve state-of-the-art (SOTA) registration accuracy and are generally fast. However, deep learning (DL) approaches are, in contrast to conventional non-deep-learning-based approaches, anatomy-specific. Recently, a universal deep registration approach, uniGradICON, has been proposed. However, uniGradICON focuses on monomodal image registration. In this work, we therefore develop multiGradICON as a first step towards universal *multimodal* medical image registration. Specifically, we show that 1) we can train a DL registration model that is suitable for monomodal *and* multimodal registration; 2) loss function randomization can increase multimodal registration accuracy; and 3) training a model with multimodal data helps multimodal generalization. Our code and the multiGradICON model are available at https://github.com/uncbiag/uniGradICON.",
                "authors": "Basar Demir, Lin Tian, Hastings Greer, R. Kwitt, François-Xavier Vialard, R. Estépar, Sylvain Bouix, R. J. Rushmore, Ebrahim Ebrahim, Marc Niethammer",
                "citations": 2
            },
            {
                "title": "Reducing the Barriers to Entry for Foundation Model Training",
                "abstract": "The world has recently witnessed an unprecedented acceleration in demands for Machine Learning and Artificial Intelligence applications. This spike in demand has imposed tremendous strain on the underlying technology stack in supply chain, GPU-accelerated hardware, software, datacenter power density, and energy consumption. If left on the current technological trajectory, future demands show insurmountable spending trends, further limiting market players, stifling innovation, and widening the technology gap. To address these challenges, we propose a fundamental change in the AI training infrastructure throughout the technology ecosystem. The changes require advancements in supercomputing and novel AI training approaches, from high-end software to low-level hardware, microprocessor, and chip design, while advancing the energy efficiency required by a sustainable infrastructure. This paper presents the analytical framework that quantitatively highlights the challenges and points to the opportunities to reduce the barriers to entry for training large language models.",
                "authors": "Paolo Faraboschi, Ellis Giles, Justin Hotard, Konstanty Owczarek, Andrew Wheeler",
                "citations": 2
            },
            {
                "title": "SeisLM: a Foundation Model for Seismic Waveforms",
                "abstract": "We introduce the Seismic Language Model (SeisLM), a foundational model designed to analyze seismic waveforms -- signals generated by Earth's vibrations such as the ones originating from earthquakes. SeisLM is pretrained on a large collection of open-source seismic datasets using a self-supervised contrastive loss, akin to BERT in language modeling. This approach allows the model to learn general seismic waveform patterns from unlabeled data without being tied to specific downstream tasks. When fine-tuned, SeisLM excels in seismological tasks like event detection, phase-picking, onset time regression, and foreshock-aftershock classification. The code has been made publicly available on https://github.com/liutianlin0121/seisLM.",
                "authors": "Tianlin Liu, Jannes Munchmeyer, Laura Laurenti, C. Marone, Maarten V. de Hoop, Ivan Dokmani'c",
                "citations": 2
            },
            {
                "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
                "abstract": "The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.",
                "authors": "Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, Aman Chadha",
                "citations": 9
            },
            {
                "title": "Prediction of specific cutting energy consumption in eco-benign lubricating environment for biomedical industry applications: Exploring efficacy of GEP, ANN, and RSM models",
                "abstract": "This study emphasizes the criticality of measuring specific cutting energy in machining Hastelloy C276 for biomedical industry applications, offering valuable insights into machinability and facilitating the optimization of tool selection, cutting parameters, and process efficiency. The research employs artificial intelligence-assisted meta-models for cost-effective and accurate predictions of specific cutting energy consumption. Comparative analyses conducted on Hastelloy C276, utilizing a TiAlN-coated solid carbide insert across various media (dry, MQL, LN2, and MQL+LN2), reveal the superiority of hybrid LN2+MQL in reducing specific cutting energy consumption. Subsequently, the analysis of variance underscores the cutting speed as the most influential parameter as compared to other inputs. Finally, a statistical evaluation compares the Gene Expression Programming (GEP) model against the Artificial Neural Network (ANN), and Response Surface Methodology model, demonstrating the superior predictive performance of the GEP meta-model. The GEP model demonstrates validation results with an error range of 0.25%–1.52%, outperforming the ANN and RSM models, which exhibit an error range of 0.49%–8.33% and 2.68%–10.18%, respectively. This study suggests the potential integration of contemporary intelligent methodologies for sustainable superalloy machining in biomedical industry applications, providing a foundation for enhanced productivity and reduced environmental impact of surgical instrument and biomedical device machining.",
                "authors": "Binayak Sen, Abhijit Bhowmik, Chander Prakash, Muhammad Imam Ammarullah",
                "citations": 9
            },
            {
                "title": "Retrieval Augmented End-to-End Spoken Dialog Models",
                "abstract": "We recently developed a joint speech and language model (SLM [1]) which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to dialog applications where the dialog states are inferred directly from the audio signal.Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) models, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a retriever to retrieve text entities given audio inputs. The retrieved entities are then added as text inputs to the underlying LLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 Challenge), and found that the retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to speech tasks requiring custom contextual information or domain-specific entities.",
                "authors": "Mingqiu Wang, Izhak Shafran, H. Soltau, Wei Han, Yuan Cao, Dian Yu, Laurent El Shafey",
                "citations": 9
            },
            {
                "title": "Language Models for Text Classification: Is In-Context Learning Enough?",
                "abstract": "Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.",
                "authors": "A. Edwards, José Camacho-Collados",
                "citations": 8
            },
            {
                "title": "Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment",
                "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.",
                "authors": "Abhinav Agarwalla, Abhay Gupta, Alexandre Marques, Shubhra Pandit, M. Goin, Eldar Kurtic, Kevin Leong, Tuan Nguyen, Mahmoud Salem, Dan Alistarh, Sean Lie, Mark Kurtz",
                "citations": 8
            },
            {
                "title": "A Practical Guide to Sample-based Statistical Distances for Evaluating Generative Models in Science",
                "abstract": "Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes. How do we evaluate the samples these models generate? This work aims to provide an accessible entry point to understanding popular sample-based statistical distances, requiring only foundational knowledge in mathematics and statistics. We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr\\'echet Inception Distance; FID). We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls. To demonstrate how these distances are used in practice, we evaluate generative models from different scientific domains, namely a model of decision-making and a model generating medical images. We showcase that distinct distances can give different results on similar data. Through this guide, we aim to help researchers to use, interpret, and evaluate statistical distances for generative models in science.",
                "authors": "Sebastian Bischoff, Alana Darcher, Michael Deistler, Richard Gao, Franziska Gerken, Manuel Gloeckler, L. Haxel, J. Kapoor, Janne K. Lappalainen, J. H. Macke, Guy Moss, Matthijs Pals, Felix Pei, Rachel Rapp, A. E. Saugtekin, Cornelius Schroder, Auguste Schulz, Zinovia Stefanidi, Shoji Toyota, Linda Ulmer, Julius Vetter",
                "citations": 8
            },
            {
                "title": "AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models",
                "abstract": "Ensuring the security of large language models (LLMs) against attacks has become increasingly urgent, with jailbreak attacks representing one of the most sophisticated threats. To deal with such risks, we introduce an innovative framework that can help evaluate the effectiveness of jailbreak attacks on LLMs. Unlike traditional binary evaluations focusing solely on the robustness of LLMs, our method assesses the effectiveness of the attacking prompts themselves. We present two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework uses a scoring range from 0 to 1, offering unique perspectives and allowing for the assessment of attack effectiveness in different scenarios. Additionally, we develop a comprehensive ground truth dataset specifically tailored for jailbreak prompts. This dataset serves as a crucial benchmark for our current study and provides a foundational resource for future research. By comparing with traditional evaluation methods, our study shows that the current results align with baseline metrics while offering a more nuanced and fine-grained assessment. It also helps identify potentially harmful attack prompts that might appear harmless in traditional evaluations. Overall, our work establishes a solid foundation for assessing a broader range of attack prompts in the area of prompt injection.",
                "authors": "Dong Shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang",
                "citations": 8
            },
            {
                "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
                "abstract": "Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.",
                "authors": "Youyang Qu, Ming Ding, Nan Sun, Kanchana Thilakarathna, Tianqing Zhu, D. Niyato",
                "citations": 7
            },
            {
                "title": "Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy",
                "abstract": "The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our benchmark to a suite of vision-language models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of vision-language modeling.",
                "authors": "Simon Ging, M. A. Bravo, Thomas Brox",
                "citations": 7
            },
            {
                "title": "Exploring Chemical Reaction Space with Machine Learning Models: Representation and Feature Perspective",
                "abstract": "Chemical reactions serve as foundational building blocks for organic chemistry and drug design. In the era of large AI models, data-driven approaches have emerged to innovate the design of novel reactions, optimize existing ones for higher yields, and discover new pathways for synthesizing chemical structures comprehensively. To effectively address these challenges with machine learning models, it is imperative to derive robust and informative representations or engage in feature engineering using extensive data sets of reactions. This work aims to provide a comprehensive review of established reaction featurization approaches, offering insights into the selection of representations and the design of features for a wide array of tasks. The advantages and limitations of employing SMILES, molecular fingerprints, molecular graphs, and physics-based properties are meticulously elaborated. Solutions to bridge the gap between different representations will also be critically evaluated. Additionally, we introduce a new frontier in chemical reaction pretraining, holding promise as an innovative yet unexplored avenue.",
                "authors": "Yuheng Ding, Bo Qiang, Qixuan Chen, Yiqiao Liu, Liangren Zhang, Zhenming Liu",
                "citations": 7
            },
            {
                "title": "Fairness Definitions in Language Models Explained",
                "abstract": "Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as gender and race, limiting their adoption in real-world applications. Therefore, fairness has been extensively explored in LMs, leading to the proposal of various fairness notions. However, the lack of clear agreement on which fairness definition to apply in specific contexts (\\textit{e.g.,} medium-sized LMs versus large-sized LMs) and the complexity of understanding the distinctions between these definitions can create confusion and impede further progress. To this end, this paper proposes a systematic survey that clarifies the definitions of fairness as they apply to LMs. Specifically, we begin with a brief introduction to LMs and fairness in LMs, followed by a comprehensive, up-to-date overview of existing fairness notions in LMs and the introduction of a novel taxonomy that categorizes these concepts based on their foundational principles and operational distinctions. We further illustrate each definition through experiments, showcasing their practical implications and outcomes. Finally, we discuss current research challenges and open questions, aiming to foster innovative ideas and advance the field. The implementation and additional resources are publicly available at https://github.com/LavinWong/Fairness-in-Large-Language-Models/tree/main/definitions.",
                "authors": "Thang Viet Doan, Zhibo Chu, Zichong Wang, Wenbin Zhang",
                "citations": 6
            },
            {
                "title": "Learn \"No\" to Say \"Yes\" Better: Improving Vision-Language Models via Negations",
                "abstract": "Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This paper highlights the limitations of popular VLMs such as CLIP, at understanding the implications of negations, i.e., the effect of the word\"not\"in a given prompt. To enable evaluation of VLMs on fluent prompts with negations, we present CC-Neg, a dataset containing 228,246 images, true captions and their corresponding negated captions. Using CC-Neg along with modifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework, has an improved understanding of negations. This training paradigm improves CoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average gain in top-1 accuracy for zero-shot image classification across 8 datasets. Further, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks such as SugarCREPE by 4.4%, showcasing emergent compositional understanding of objects, relations, and attributes in text. Overall, our work addresses a crucial limitation of VLMs by introducing a dataset and framework that strengthens semantic associations between images and text, demonstrating improved large-scale foundation models with significantly reduced computational cost, promoting efficiency and accessibility.",
                "authors": "Jaisidh Singh, Ishaan Shrivastava, M. Vatsa, Richa Singh, Aparna Bharati",
                "citations": 6
            },
            {
                "title": "Unveiling Linguistic Regions in Large Language Models",
                "abstract": "Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependence, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence.",
                "authors": "Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, Xuanjing Huang",
                "citations": 6
            },
            {
                "title": "A Review of Advancements and Applications of Pre-Trained Language Models in Cybersecurity",
                "abstract": "In this paper, we delve into the transformative role of pre-trained language models (PLMs) in cybersecurity, offering a comprehensive examination of their deployment across a wide array of cybersecurity tasks. Beginning with an exploration of general PLMs, including advancements and the emergence of domain-specific models tailored for cybersecurity, we provide an insightful overview of the foundational technologies driving these developments. The core of our review focuses on the multifaceted applications of PLMs in cybersecurity, ranging from malware and vulnerability detection to more nuanced areas like log analysis, network traffic analysis, and threat intelligence, among others. We also highlight recent strides in the application of large language models (LLMs), showcasing their growing influence in enhancing cybersecurity measures. By charting the landscape of PLM applications and pointing toward future directions, this work serves as a valuable resource for both the research community and industry practitioners, underlining the critical need for continued innovation and exploration in harnessing PLMs to fortify cybersecurity defenses.",
                "authors": "Zefang Liu",
                "citations": 6
            },
            {
                "title": "Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models",
                "abstract": "Large Language Models (LLMs) are a class of generative AI models built using the Transformer network, capable of leveraging vast datasets to identify, summarize, translate, predict, and generate language. LLMs promise to revolutionize society, yet training these foundational models poses immense challenges. Semantic vector search within large language models is a potent technique that can significantly enhance search result accuracy and relevance. Unlike traditional keyword-based search methods, semantic search utilizes the meaning and context of words to grasp the intent behind queries and deliver more precise outcomes. Elasticsearch emerges as one of the most popular tools for implementing semantic search an exceptionally scalable and robust search engine designed for indexing and searching extensive datasets. In this article, we delve into the fundamentals of semantic search and explore how to harness Elasticsearch and Transformer models to bolster large language model processing paradigms. We gain a comprehensive understanding of semantic search principles and acquire practical skills for implementing semantic search in real-world model application scenarios.",
                "authors": "Chunhe Ni, Jiang Wu, Hongbo Wang, Wenran Lu, Chenwei Zhang",
                "citations": 6
            },
            {
                "title": "Dimba: Transformer-Mamba Diffusion Models",
                "abstract": "This paper unveils Dimba, a new text-to-image diffusion model that employs a distinctive hybrid architecture combining Transformer and Mamba elements. Specifically, Dimba sequentially stacked blocks alternate between Transformer and Mamba layers, and integrate conditional information through the cross-attention layer, thus capitalizing on the advantages of both architectural paradigms. We investigate several optimization strategies, including quality tuning, resolution adaption, and identify critical configurations necessary for large-scale image generation. The model's flexible design supports scenarios that cater to specific resource constraints and objectives. When scaled appropriately, Dimba offers substantial throughput and a reduced memory footprint relative to conventional pure Transformers-based benchmarks. Extensive experiments indicate that Dimba achieves comparable performance compared with benchmarks in terms of image quality, artistic rendering, and semantic control. We also report several intriguing properties of architecture discovered during evaluation and release checkpoints in experiments. Our findings emphasize the promise of large-scale hybrid Transformer-Mamba architectures in the foundational stage of diffusion models, suggesting a bright future for text-to-image generation.",
                "authors": "Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, Junshi Huang",
                "citations": 9
            },
            {
                "title": "Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond",
                "abstract": "Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided the flexibility and effectiveness necessary to achieve the objective. In light of this, we develop a generic and extensible personalization generative framework, that can handle a wide range of personalized needs including item recommendation, product search, preference prediction, explanation generation, and further user-guided image generation. Our methodology enhances the capabilities of foundational language models for personalized tasks by seamlessly ingesting interleaved cross-modal user history information, ensuring a more precise and customized experience for users. To train and evaluate the proposed multi-modal personalized tasks, we also introduce a novel and comprehensive benchmark covering a variety of user requirements. Our experiments on the real-world benchmark showcase the model's potential, outperforming competitive methods specialized for each task.",
                "authors": "Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, Xianfeng Tang",
                "citations": 6
            },
            {
                "title": "Foundational Inference Models for Dynamical Systems",
                "abstract": "Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework for zero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trained offline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate that one and the same (pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations -- without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets. Our pretrained model will be available online soon.",
                "authors": "Patrick Seifner, K. Cvejoski, Antonia Korner, Ramsés J. Sánchez",
                "citations": 2
            },
            {
                "title": "Toward Robust Multimodal Learning using Multimodal Foundational Models",
                "abstract": "Existing multimodal sentiment analysis tasks are highly rely on the assumption that the training and test sets are complete multimodal data, while this assumption can be difficult to hold: the multimodal data are often incomplete in real-world scenarios. Therefore, a robust multimodal model in scenarios with randomly missing modalities is highly preferred. Recently, CLIP-based multimodal foundational models have demonstrated impressive performance on numerous multimodal tasks by learning the aligned cross-modal semantics of image and text pairs, but the multimodal foundational models are also unable to directly address scenarios involving modality absence. To alleviate this issue, we propose a simple and effective framework, namely TRML, Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML employs generated virtual modalities to replace missing modalities, and aligns the semantic spaces between the generated and missing modalities. Concretely, we design a missing modality inference module to generate virtual modaliites and replace missing modalities. We also design a semantic matching learning module to align semantic spaces generated and missing modalities. Under the prompt of complete modality, our model captures the semantics of missing modalities by leveraging the aligned cross-modal semantic space. Experiments demonstrate the superiority of our approach on three multimodal sentiment analysis benchmark datasets, CMU-MOSI, CMU-MOSEI, and MELD.",
                "authors": "Xianbing Zhao, Soujanya Poria, Xuejiao Li, Yixin Chen, Buzhou Tang",
                "citations": 2
            },
            {
                "title": "Advanced Modelling of Soil Organic Carbon Content in Coal Mining Areas Using Integrated Spectral Analysis: A Dengcao Coal Mine Case Study",
                "abstract": "Effective modelling and integrated spectral analysis approaches can advance modelling precision. To develop an integrated spectral forecast modelling of soil organic carbon (SOC), this research investigated a mining coal in Dengcao Coal Mine Area, Zhengzhou. The study utilizes the Lasso and Ranger algorithms were utilized in spectral band analysis. Four primary models employed during this process include Artificial Neural Network (ANN), Support Vector Machine, Random Forest (RF), and Partial Least Squares Regression (PLSR). The ideal model was chosen. The results showed that, in contrast to when band collection was based on Lasso algorithm modelling, model precision was higher when it was based on the Ranger algorithm. ANN model had an ideal goodness acceptance, and the modelling developed by RF showed the steadiest modelling consequences. Based on the results, a distinct method is proposed in this study for band assortment at the earlier stage of integrated spectral modelling of SOC. The Ranger method can be used to check the spectral particles, and RF or ANN can be chosen to develop the prediction modelling based on different statistics sets, which is appropriate to create the prediction modelling of SOC content in Dengcao Coal Mine Area. This research avails a position for the integrated spectral of Analysis for Advanced Modelling of Soil Organic Carbon Content in Coal Sources alongside a theoretical foundation for innovating portable device for the integrated spectral assessment of SOC content in coal mining habitats. This study might be significant for the changing modelling and monitoring of SOC in mining and environmental areas.",
                "authors": "Gill Ammara, Xiaojun Nie, Chang -hua LIU",
                "citations": 928
            },
            {
                "title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
                "abstract": "This work presents Depth Anything11While the grammatical soundness of this name may be questionable, we treat it as a whole and pay homage to Segment Anything [26]., a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability (Figure 1). Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released here.",
                "authors": "Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao",
                "citations": 403
            },
            {
                "title": "SAM 2: Segment Anything in Images and Videos",
                "abstract": "We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.",
                "authors": "Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya K. Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloé Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross B. Girshick, Piotr Doll'ar, Christoph Feichtenhofer",
                "citations": 238
            },
            {
                "title": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
                "abstract": "In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings. To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility. The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding.",
                "authors": "Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu",
                "citations": 172
            },
            {
                "title": "Qwen2 Technical Report",
                "abstract": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.",
                "authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-Yang Chen, Kexin Yang, Mei Li, Min Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yunyang Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, Zhi-Wei Fan",
                "citations": 452
            },
            {
                "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
                "abstract": "Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.",
                "authors": "Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang",
                "citations": 445
            },
            {
                "title": "NEW PCA THICKNESS DESIGN PROCEDURE FOR CONCRETE HIGHWAY AND STREET PAVEMENTS",
                "abstract": "This scientific article outlines the new PCA Thickness Design Procedure for Concrete Highway and Street Pavements. Authored by Robert G. Packard and Shiraz D. Tayabji from the Portland Cement Association, the paper presents a revised design method that integrates new practices and understandings gained since the original 1966 publication. Key updates include addressing undoweled joints, lean concrete subbases, and erosion criteria due to foundation issues affecting pavement performance. The revision incorporates sophisticated theoretical methods for modeling stresses, deflections, and pressures in pavements, ensuring designs reflect a broader range of variable combinations and practical performance data. The article details the application of the design procedure across various pavement types, including plain, plain doweled, reinforced, and continuously reinforced concrete pavements. Each type is evaluated based on factors like flexural strength, subgrade support, axle load types, and expected service periods, using both detailed and simplified approaches depending on available data. In-depth theoretical and practical analyses, including finite element modeling and performance-based criteria, establish a robust framework for pavement design. The procedure accounts for traditional fatigue and new erosion damage considerations, offering enhanced reliability and predictive capabilities for pavement performance. The design implications of various construction details such as joint types, shoulder integration, and subbase qualities are also explored to optimize pavement thickness and durability under varying traffic and environmental conditions. (Abstract generated by AI tool ChatGT 4)",
                "authors": "R. Packard, S. Tayabji",
                "citations": 48
            },
            {
                "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
                "abstract": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
                "authors": "Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui",
                "citations": 117
            },
            {
                "title": "Genie: Generative Interactive Environments",
                "abstract": "We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",
                "authors": "Jake Bruce, Michael D. Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Y. Aytar, Sarah Bechtle, Feryal M. P. Behbahani, Stephanie Chan, N. Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rocktaschel",
                "citations": 79
            },
            {
                "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
                "abstract": "Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as a valuable framework for future research collaborations that aim to bridge gaps in resources.",
                "authors": "Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzemi'nski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Minh Chien Vu, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, A. Ustun, Marzieh Fadaee, Sara Hooker",
                "citations": 87
            },
            {
                "title": "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation",
                "abstract": "We introduce LlamaGen, a new family of image generation models that apply original ``next-token prediction'' paradigm of large language models to visual generation domain. It is an affirmative answer to whether vanilla autoregressive models, e.g., Llama, without inductive biases on visual signals can achieve state-of-the-art image generation performance if scaling properly. We reexamine design spaces of image tokenizers, scalability properties of image generation models, and their training data quality. The outcome of this exploration consists of: (1) An image tokenizer with downsample ratio of 16, reconstruction quality of 0.94 rFID and codebook usage of 97% on ImageNet benchmark. (2) A series of class-conditional image generation models ranging from 111M to 3.1B parameters, achieving 2.18 FID on ImageNet 256x256 benchmarks, outperforming the popular diffusion models such as LDM, DiT. (3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment. (4) We verify the effectiveness of LLM serving frameworks in optimizing the inference speed of image generation models and achieve 326% - 414% speedup. We release all models and codes to facilitate open-source community of visual generation and multimodal foundation models.",
                "authors": "Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, Zehuan Yuan",
                "citations": 82
            },
            {
                "title": "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein",
                "abstract": "Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to an advanced 3D structural prediction model that surpasses existing language model-based tools. 2) xTrimoPGLM not only can generate de novo protein sequences following the principles of natural ones, but also can perform programmable generation after supervised fine-tuning (SFT) on curated sequences. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences, contributing to the evolving landscape of foundation models in protein science. Trained weight for the xTrimoPGLM model, and downstream datasets are available at https://huggingface.co/proteinglm.",
                "authors": "Bo Chen, Xingyi Cheng, Yangli-ao Geng, Shengyin Li, Xin Zeng, Bo Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Leo T. Song",
                "citations": 74
            },
            {
                "title": "RSMamba: Remote Sensing Image Classification With State Space Model",
                "abstract": "Remote sensing image classification forms the foundation of various understanding tasks, serving a crucial function in remote sensing image interpretation. The recent advancements of convolutional neural networks (CNNs) and transformers have markedly enhanced classification accuracy. Nonetheless, remote sensing scene classification remains a significant challenge, especially given the complexity and diversity of remote sensing scenarios and the variability of spatiotemporal resolutions. The capacity for whole-image understanding can provide more precise semantic cues for scene discrimination. In this letter, we introduce RSMamba, a novel architecture for remote sensing image classification. RSMamba is based on the state space model (SSM) and incorporates an efficient, hardware-aware design known as the Mamba. It integrates the advantages of both a global receptive field and linear modeling complexity. To overcome the limitation of the vanilla Mamba, which can only model causal sequences and is not adaptable to 2-D image data, we propose a dynamic multipath activation mechanism to augment Mamba’s capacity to model noncausal data. Notably, RSMamba maintains the inherent modeling mechanism of the vanilla Mamba, yet exhibits superior performance across multiple remote sensing image classification datasets, e.g., F1 scores of 95.25, 92.63, and 95.18 on the UC Merced, AID, and RESISC45 classification datasets, respectively, exceeding those of concurrent Vim and VMamba. This indicates that RSMamba holds significant potential to function as the backbone of future visual foundation models. The code is available at https://github.com/KyanChen/RSMamba.",
                "authors": "Keyan Chen, Bo-Ying Chen, Chenyang Liu, Wenyuan Li, Zhengxia Zou, Z. Shi",
                "citations": 60
            },
            {
                "title": "Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks and Operators in Scientific Computing: Fluid and Solid Mechanics",
                "abstract": "\n Advancements in computing power have recently made it possible to utilize machine learning and deep learning to push scientific computing forward in a range of disciplines, such as fluid mechanics, solid mechanics, materials science, etc. The incorporation of neural networks is particularly crucial in this hybridization process. Due to their intrinsic architecture, conventional neural networks cannot be successfully trained and scoped when data is sparse, which is the case in many scientific and engineering domains. Nonetheless, neural networks provide a solid foundation to respect physics-driven or knowledge-based constraints during training. Generally speaking, there are three distinct neural network frameworks to enforce the underlying physics: (i) physics-guided neural networks (PgNNs), (ii) physics-informed neural networks (PiNNs), and (iii) physics-encoded neural networks (PeNNs). These methods provide distinct advantages for accelerating the numerical modeling of complex multiscale multi-physics phenomena. In addition, the recent developments in neural operators (NOs) add another dimension to these new simulation paradigms, especially when the real-time prediction of complex multi-physics systems is required. All these models also come with their own unique drawbacks and limitations that call for further fundamental research. This study aims to present a review of the four neural network frameworks (i.e., PgNNs, PiNNs, PeNNs, and NOs) used in scientific computing research. The state-of-the-art architectures and their applications are reviewed, limitations are discussed, and future research opportunities are presented in terms of improving algorithms, considering causalities, expanding applications, and coupling scientific and deep learning solvers.",
                "authors": "Salah A. Faroughi, Nikhil M. Pawar, Célio Fernandes, M. Raissi, Subasish Das, N. Kalantari, S. K. Mahjour",
                "citations": 47
            },
            {
                "title": "SaulLM-7B: A pioneering Large Language Model for Law",
                "abstract": "In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the MIT License.",
                "authors": "Pierre Colombo, T. Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, André Martins, Fabrizio Esposito, Vera L'ucia Raposo, Sofia Morgado, Michael Desa",
                "citations": 43
            },
            {
                "title": "Enhancing AIKU Adoption: Insights from the Role of Habit in Behavior Intention",
                "abstract": "This research focuses on exploring and understanding the role of Habit in sustaining the adoption and continuous use of AIKU technology as its primary objective. The UTAUT2 model was chosen, emphasizing the Habit variable (HT) along with 6 other key variables: Performance Expectancy (PEX), Effort Expectancy (EEX), Price Value (PV), Social Influence (SIN) on Behavioral Intention (BIN), and User Behavior (UB), with the addition of external variables Perceived Risk (PR) and moderating Gender and Experience. The research methodology involves data analysis using SmartPLS 4.0 software and the UTAUT2 model as the framework and theoretical foundation. Survey data were collected from 414 AIKU users in Indonesia. Findings based on data analysis indicate that factors such as PE, EE, and PV significantly positively influence BI to continue using AIKU. In addition to filling gaps in knowledge regarding the role of Habit in air quality technology literature, another finding is that H significantly positively influences BIN and UB, while BIN significantly positively influences UB. This research’s novelty and primary contribution lie in the implications concerning the crucial emphasis on the Habit factor in sustaining AIKU usage. The development implications underscore the importance of psychological factors in technology acceptance and retention, providing valuable insights for future strategy and policy development. Limitations of this research lie in the focus on the AIKU system affecting result generalization. Future research, expanding coverage, considering air quality system variations, employing diverse data collection methods such as in-depth interviews are recommended.",
                "authors": "Irwan Sembiring, U. Rahardja, Danny Manongga, Q. Aini, Abdul Wahab",
                "citations": 41
            },
            {
                "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems",
                "abstract": "Large Language Model (LLM) systems are inherently compositional, with individual LLM serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on. Along with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems. However, existing studies on LLM security often focus on individual LLM, but without examining the ecosystem through the lens of LLM systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs. To do so, we build on top of the information flow and formulate the security of LLM systems as constraints on the alignment of the information flow within LLM and between LLM and other objects. Based on this construction and the unique probabilistic nature of LLM, the attack surface of the LLM system can be decomposed into three key components: (1) multi-layer security analysis, (2) analysis of the existence of constraints, and (3) analysis of the robustness of these constraints. To ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art LLM system, OpenAI GPT4. Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components. We found that although the OpenAI GPT4 has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers. To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user's chat history, all without the need to manipulate the user's input or gain direct access to OpenAI GPT4. Our demo is in the link: https://fzwark.github.io/LLM-System-Attack-Demo/",
                "authors": "Fangzhou Wu, Ning Zhang, Somesh Jha, P. McDaniel, Chaowei Xiao",
                "citations": 41
            },
            {
                "title": "Recent Technological Advancements in BIM and LCA Integration for Sustainable Construction: A Review",
                "abstract": "In the high-energy, high-carbon landscape of the construction industry, a detailed and precise life cycle assessment (LCA) is essential. This review examines the role of building information modeling (BIM) software in streamlining the LCA process to enhance efficiency and accuracy. Despite its potential, challenges such as software interoperability and compatibility persist, with no unified standard for choosing BIM-integrated LCA software. Besides, the review explores the capabilities and limitations of various BIM software, LCA tools, and energy consumption tools, and presents characteristics of BIM-LCA integration cases. It critically discusses BIM-LCA integration methods and data exchange techniques, including bill of quantities import, Industry Foundation Classes (IFC) import, BIM viewer usage, direct LCA calculations with BIM plugins, and LCA plugin calculations. Finally, concluding with future perspectives, the study aims to guide the development of advanced LCA tools for better integration with BIM software, addressing a vital need in sustainable construction practices.",
                "authors": "Zhonghao Chen, Lin Chen, Xingyang Zhou, Lepeng Huang, M. Sandanayake, P. Yap",
                "citations": 40
            },
            {
                "title": "A multimodal generative AI copilot for human pathology",
                "abstract": null,
                "authors": "Ming Y. Lu, Bowen Chen, Drew F. K. Williamson, Richard J. Chen, Melissa Zhao, Aaron K Chow, Kenji Ikemura, Ahrong Kim, Dimitra Pouli, Ankush Patel, Amr Soliman, Chengkuan Chen, Tong Ding, Judy J. Wang, Georg K. Gerber, Ivy Liang, L. Le, Anil V. Parwani, Luca L Weishaupt, Faisal Mahmood",
                "citations": 55
            },
            {
                "title": "Mechanistic Interpretability for AI Safety - A Review",
                "abstract": "Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.",
                "authors": "Leonard Bereska, E. Gavves",
                "citations": 52
            },
            {
                "title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",
                "abstract": "Intelligent agents stand out as a potential path toward artificial general intelligence (AGI). Thus, researchers have dedicated significant effort to diverse implementations for them. Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications -- from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities. This paper surveys current research to provide an in-depth overview of LLM-based intelligent agents within single-agent and multi-agent systems. It covers their definitions, research frameworks, and foundational components such as their composition, cognitive and planning methods, tool utilization, and responses to environmental feedback. We also delve into the mechanisms of deploying LLM-based agents in multi-agent systems, including multi-role collaboration, message passing, and strategies to alleviate communication issues between agents. The discussions also shed light on popular datasets and application scenarios. We conclude by envisioning prospects for LLM-based agents, considering the evolving landscape of AI and natural language processing.",
                "authors": "Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, Xiuqiang He",
                "citations": 46
            },
            {
                "title": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness",
                "abstract": "Traditional feedback learning for hallucination reduction relies on labor-intensive manual labeling or expensive proprietary models. This leaves the community without foundational knowledge about how to build high-quality feedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally explores open-source MLLMs from two perspectives, including high-quality feedback data generation for preference learning and self-feedback guidance for inference-time scaling. Extensive experiments on six benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models at both preference learning and inference time. RLAIF-V 7B reduces object hallucination by 80.7\\% and overall hallucination by 33.7\\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of open-source MLLMs, where the model can learn from feedback of itself to achieve super GPT-4V trustworthiness.",
                "authors": "Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Dawn Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun",
                "citations": 40
            },
            {
                "title": "Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap",
                "abstract": "Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM's further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this paper provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: LLM-enhanced EA and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.",
                "authors": "Xingyu Wu, Sheng-hao Wu, Jibin Wu, Liang Feng, Kay Chen Tan",
                "citations": 37
            },
            {
                "title": "Technopreneurship in Pro-Environmental Behavior for Sustainable Carbon Emission Reduction in Central Kalimantan",
                "abstract": "This study examines the integration of technopreneurship with pro-environmental behavior to achieve sustainable carbon emission reduction in Central Kalimantan. Focusing on the Dayak-Banjar communities, the research aims to develop a model that leverages local wisdom and technopreneurial activities to promote environmental conservation. The methodology includes qualitative research through interviews and questionnaires distributed in Baung, Muara Dua, and Kuala Pembuang villages. Findings reveal that local communities, deeply affected by environmental degradation, are willing to adopt pro-environmental behaviors. Key factors influencing this change include attitudes, subjective norms, perceived behavioral control, and knowledge. The study highlights the potential of technopreneurship in promoting sustainable practices and provides a foundation for designing policies that support pro-environmental behavior through technopreneurship. By fostering pro-environmental behavior within local communities, this research demonstrates that it is possible to balance ecological conservation with economic growth, aligning with SDG 13, SDG 15, and SDG 8.",
                "authors": "Sylviana Andhella, H. Djajadikerta, Martinus Yuwana Marjuka",
                "citations": 30
            },
            {
                "title": "A Temporal Kolmogorov-Arnold Transformer for Time Series Forecasting",
                "abstract": "Capturing complex temporal patterns and relationships within multivariate data streams is a difficult task. We propose the Temporal Kolmogorov-Arnold Transformer (TKAT), a novel attention-based architecture designed to address this task using Temporal Kolmogorov-Arnold Networks (TKANs). Inspired by the Temporal Fusion Transformer (TFT), TKAT emerges as a powerful encoder-decoder model tailored to handle tasks in which the observed part of the features is more important than the a priori known part. This new architecture combined the theoretical foundation of the Kolmogorov-Arnold representation with the power of transformers. TKAT aims to simplify the complex dependencies inherent in time series, making them more\"interpretable\". The use of transformer architecture in this framework allows us to capture long-range dependencies through self-attention mechanisms.",
                "authors": "R'emi Genet, Hugo Inzirillo",
                "citations": 30
            },
            {
                "title": "AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks",
                "abstract": "The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance communication-computing latency and training convergence. Extensive simulations across various datasets validate that our proposed AdaptSFL framework takes considerably less time to achieve a target accuracy than benchmarks, demonstrating the effectiveness of the proposed strategies.",
                "authors": "Zhengyi Lin, Guanqiao Qu, Wei Wei, Xianhao Chen, Kin K. Leung",
                "citations": 27
            },
            {
                "title": "The Challenges of Machine Learning: A Critical Review",
                "abstract": "The concept of learning has multiple interpretations, ranging from acquiring knowledge or skills to constructing meaning and social development. Machine Learning (ML) is considered a branch of Artificial Intelligence (AI) and develops algorithms that can learn from data and generalize their judgment to new observations by exploiting primarily statistical methods. The new millennium has seen the proliferation of Artificial Neural Networks (ANNs), a formalism able to reach extraordinary achievements in complex problems such as computer vision and natural language recognition. In particular, designers claim that this formalism has a strong resemblance to the way the biological neurons operate. This work argues that although ML has a mathematical/statistical foundation, it cannot be strictly regarded as a science, at least from a methodological perspective. The main reason is that ML algorithms have notable prediction power although they cannot necessarily provide a causal explanation about the achieved predictions. For example, an ANN could be trained on a large dataset of consumer financial information to predict creditworthiness. The model takes into account various factors like income, credit history, debt, spending patterns, and more. It then outputs a credit score or a decision on credit approval. However, the complex and multi-layered nature of the neural network makes it almost impossible to understand which specific factors or combinations of factors the model is using to arrive at its decision. This lack of transparency can be problematic, especially if the model denies credit and the applicant wants to know the specific reasons for the denial. The model’s “black box” nature means it cannot provide a clear explanation or breakdown of how it weighed the various factors in its decision-making process. Secondly, this work rejects the belief that a machine can simply learn from data, either in supervised or unsupervised mode, just by applying statistical methods. The process of learning is much more complex, as it requires the full comprehension of a learned ability or skill. In this sense, further ML advancements, such as reinforcement learning and imitation learning denote encouraging similarities to similar cognitive skills used in human learning.",
                "authors": "Enrico Barbierato, Alice Gatti",
                "citations": 27
            },
            {
                "title": "Theoretical approaches to AI in supply chain optimization: Pathways to efficiency and resilience",
                "abstract": "The integration of Artificial Intelligence (AI) into supply chain management has emerged as a pivotal avenue for enhancing efficiency and resilience in contemporary business operations. This paper explores various theoretical approaches to AI within the context of supply chain optimization, delineating pathways to achieve heightened performance and adaptability. Commencing with a historical overview, the paper delves into the evolution of AI techniques in supply chain management, elucidating how these methodologies have transformed the landscape of logistics and operations. Fundamental to this exploration is the discussion on mathematical modeling and algorithmic frameworks that underpin supply chain optimization, providing the theoretical foundation for subsequent AI applications. A key focus of the paper lies in the application of machine learning techniques for demand forecasting and inventory management, which leverage data-driven insights to optimize resource allocation and mitigate risks associated with supply-demand fluctuations. Additionally, network theory and graph algorithms play a crucial role in optimizing the structure and dynamics of supply chain networks, enabling efficient transportation, distribution, and inventory routing. Strategic decision-making in supply chains is addressed through the lens of game theory, which offers theoretical frameworks to model interactions among multiple stakeholders and optimize outcomes in competitive environments. Moreover, swarm intelligence and multi-agent systems provide innovative solutions for coordination and collaboration within complex supply chain ecosystems. Evolutionary algorithms and artificial neural networks are discussed as powerful tools for supply chain design, predictive analytics, and risk management, offering capabilities for optimizing decision-making processes across various operational domains. Furthermore, reinforcement learning techniques empower dynamic decision-making in real-time operational settings, fostering adaptive and resilient supply chain management practices. By integrating multiple AI techniques, hybrid approaches offer synergistic solutions that capitalize on the strengths of diverse methodologies to address multifaceted challenges in supply chain optimization. Through a synthesis of theoretical insights and practical case studies, this paper provides valuable insights into the current state and future directions of AI-driven supply chain optimization.",
                "authors": "Gerald Adeyemi Abaku, Emmanuel Adeyemi Abaku, Tolulope Esther Edunjobi, Agnes Clare Odimarha",
                "citations": 25
            },
            {
                "title": "Genetic Algorithms for Optimized Selection of Biodegradable Polymers in Sustainable Manufacturing Processes",
                "abstract": "Sustainable Manufacturing Practices (SMP), particularly in the selection of materials, have become essential due to environmental issues caused by the expansion of industry. Compared to conventional polymers, biodegradable Polymer Materials (BPM) are growing more commonly as an approach to reducing trash pollution. Suitable materials can be challenging due to numerous considerations, like ecological impact, expenditure, and material properties. When addressing sophisticated trade-offs, standard approaches drop. To compete with such challenges, employing Genetic Algorithms (GA) may be more successful, as they have their foundation in the basic concepts of biological development and the natural selection process. With a focus on BPM, this study provides a GA model for optimal packaging substance selection. Out of the four algorithms for computation used for practical testing—PSO, ACO, and SA—the GA model is the most effective. The findings demonstrate that GA can be used to enhance SMP and performs well in enormous search spaces that contain numerous different combinations of materials.",
                "authors": "Shaymaa Hussein Nowfal, Vijaya Bhaskar Sadu, Sudhakar Sengab, R. G, Anjaneyulu Naik R, Sreekanth K",
                "citations": 29
            },
            {
                "title": "Ecological footprints, carbon emissions, and energy transitions: the impact of artificial intelligence (AI)",
                "abstract": null,
                "authors": "Qiang Wang, Yuanfan Li, Rongrong Li",
                "citations": 35
            },
            {
                "title": "GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations",
                "abstract": "As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and (2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Most open-source LLMs, e.g., CodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than commercial LLMs, e.g., GPT-4, in complex games, yet the recently released Llama-3-70b-Instruct makes up for this shortcoming. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. We further characterize the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games. Detailed error profiles are provided for a better understanding of LLMs' behavior. We hope our research provides standardized protocols and serves as a foundation to spur further explorations in the strategic reasoning of LLMs.",
                "authors": "Jinhao Duan, Renming Zhang, James Diffenderfer, B. Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu",
                "citations": 36
            },
            {
                "title": "Essay: Quantum Sensing with Atomic, Molecular, and Optical Platforms for Fundamental Physics.",
                "abstract": "Atomic, molecular, and optical (AMO) physics has been at the forefront of the development of quantum science while laying the foundation for modern technology. With the growing capabilities of quantum control of many atoms for engineered many-body states and quantum entanglement, a key question emerges: what critical impact will the second quantum revolution with ubiquitous applications of entanglement bring to bear on fundamental physics? In this Essay, we argue that a compelling long-term vision for fundamental physics and novel applications is to harness the rapid development of quantum information science to define and advance the frontiers of measurement physics, with strong potential for fundamental discoveries. As quantum technologies, such as fault-tolerant quantum computing and entangled quantum sensor networks, become much more advanced than today's realization, we wonder what doors of basic science can these tools unlock. We anticipate that some of the most intriguing and challenging problems, such as quantum aspects of gravity, fundamental symmetries, or new physics beyond the minimal standard model, will be tackled at the emerging quantum measurement frontier. Part of a series of Essays which concisely present author visions for the future of their field.",
                "authors": "Jun Ye, Peter Zoller",
                "citations": 22
            },
            {
                "title": "UV-SAM: Adapting Segment Anything Model for Urban Village Identification",
                "abstract": "Urban villages, defined as informal residential areas in or around urban centers, are characterized by inadequate infrastructures and poor living conditions, closely related to the Sustainable Development Goals (SDGs) on poverty, adequate housing, and sustainable cities. Traditionally, governments heavily depend on field survey methods to monitor the urban villages, which however are time-consuming, labor-intensive, and possibly delayed. Thanks to widely available and timely updated satellite images, recent studies develop computer vision techniques to detect urban villages efficiently. However, existing studies either focus on simple urban village image classification or fail to provide accurate boundary information. To accurately identify urban village boundaries from satellite images, we harness the power of the vision foundation model and adapt the Segment Anything Model (SAM) to urban village segmentation, named UV-SAM. Specifically, UV-SAM first leverages a small-sized semantic segmentation model to produce mixed prompts for urban villages, including mask, bounding box, and image representations, which are then fed into SAM for fine-grained boundary identification. Extensive experimental results on two datasets in China demonstrate that UV-SAM outperforms existing baselines, and identification results over multiple years show that both the number and area of urban villages are decreasing over time, providing deeper insights into the development trends of urban villages and sheds light on the vision foundation models for sustainable cities. The dataset and codes of this study are available at https://github.com/tsinghua-fib-lab/UV-SAM.",
                "authors": "Xin Zhang, Yu Liu, Yuming Lin, Qingmin Liao, Yong Li",
                "citations": 22
            },
            {
                "title": "Interfacial flow contact resistance effect for thermal consolidation of layered viscoelastic saturated soils with semi‐permeable boundaries",
                "abstract": "Laminar flow phenomena may occur when pore water flows at low velocities across the interfaces between soils of different properties, thus causing flow contact resistance. To explore the impacts of interfacial flow contact resistance and rheological characteristics on the thermal consolidation process of layered viscoelastic saturated soil foundation featuring semi‐permeable boundaries. This paper established a new thermal consolidation model by introducing a fractional order derivative model, Hagen–Poiseuille law and time‐dependent loadings. The semi‐analytical solutions for the proposed thermal consolidation model are derived through the Laplace transform and its inverse transform. The reliability and correctness of the solutions are verified with the experimental data in literatures. The influence of constitutive parameters, flow contact resistance model parameters on thermal consolidation process and the interfacial flow contact resistance on foundation settlement, is further explored. The results indicate that the impact of the constitutive parameters and permeability coefficient on the thermal consolidation of viscoelastic saturated soil is related to the flow contact resistance. The enhanced flow contact resistance effect leads to a significant increase in pore water pressure and displacement during the consolidation process.",
                "authors": "Jiahao Xie, Minjie Wen, Pan Ding, Yuan Tu, Dazhi Wu, Kaifu Liu, Kejie Tang, Menghuan Chen",
                "citations": 20
            },
            {
                "title": "Estimation of the installation torque-capacity correlation of helical pile considering spatially variable clays",
                "abstract": "As the offshore industry moves into deeper water, helical piles are emerging as a potential foundation solution option. A large number of studies have been published to explore the installation torque-capacity correlation. However, in most previous studies, the inherent spatial variability of soil strength was neglected. The present research explores the installation and extraction behavior of helical piles through a large deformation random finite element method. A strain-softening soil constitutive model proposed in past literature is employed to model the soil strength remoulding. The validity of the numerical model used to simulate the installation process and the subsequent uplift process is verified by the installation torque and the uplift capacity, respectively. The spatial variation of soil strength is modeled using random field, and then a series of Monte Carlo simulations are performed to investigate the torque-capacity correlation of the helical piles under different random realizations. The analysis results show that for the helical piles with different penetration depths, the spatially random soil strength markedly affects the torque-capacity correlation. Moreover, a probabilistic analysis of the torque-capacity correlation is conducted, which may be of great interest to engineering practitioners in the design method of the helical pile.",
                "authors": "Po Cheng, Fei Liu, Xue-jian Chen, Yuhe Zhang, Kai Yao",
                "citations": 21
            },
            {
                "title": "Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image",
                "abstract": "In this paper, we propose Flash3D, a method for scene reconstruction and novel view synthesis from a single image which is both very generalisable and efficient. For generalisability, we start from a\"foundation\"model for monocular depth estimation and extend it to a full 3D shape and appearance reconstructor. For efficiency, we base this extension on feed-forward Gaussian Splatting. Specifically, we predict a first layer of 3D Gaussians at the predicted depth, and then add additional layers of Gaussians that are offset in space, allowing the model to complete the reconstruction behind occlusions and truncations. Flash3D is very efficient, trainable on a single GPU in a day, and thus accessible to most researchers. It achieves state-of-the-art results when trained and tested on RealEstate10k. When transferred to unseen datasets like NYU it outperforms competitors by a large margin. More impressively, when transferred to KITTI, Flash3D achieves better PSNR than methods trained specifically on that dataset. In some instances, it even outperforms recent methods that use multiple views as input. Code, models, demo, and more results are available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.",
                "authors": "Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, João F. Henriques, Christian Rupprecht, Andrea Vedaldi",
                "citations": 20
            },
            {
                "title": "Modeling embryo-endometrial interface recapitulating human embryo implantation",
                "abstract": "The initiation of human pregnancy is marked by the implantation of an embryo into the uterine environment; however, the underlying mechanisms remain largely elusive. To address this knowledge gap, we developed hormone-responsive endometrial organoids (EMO), termed apical-out (AO)–EMO, which emulate the in vivo architecture of endometrial tissue. The AO-EMO comprise an exposed apical epithelium surface, dense stromal cells, and a self-formed endothelial network. When cocultured with human embryonic stem cell–derived blastoids, the three-dimensional feto-maternal assembloid system recapitulates critical implantation stages, including apposition, adhesion, and invasion. Endometrial epithelial cells were subsequently disrupted by syncytial cells, which invade and fuse with endometrial stromal cells. We validated this fusion of syncytiotrophoblasts and stromal cells using human blastocysts. Our model provides a foundation for investigating embryo implantation and feto-maternal interactions, offering valuable insights for advancing reproductive medicine.",
                "authors": "Shun Shibata, Shun Endo, L. A. E. Nagai, Eri H Kobayashi, Akira Oike, Norio Kobayashi, Akane Kitamura, Takeshi Hori, Yuji Nashimoto, Ryuichiro Nakato, Hirotaka Hamada, H. Kaji, Chie Kikutake, M. Suyama, Masatoshi Saito, Nobuo Yaegashi, Hiroaki Okae, Takahiro Arima",
                "citations": 18
            },
            {
                "title": "VmambaIR: Visual State Space Model for Image Restoration",
                "abstract": "Image restoration is a critical task in low-level computer vision, aiming to restore high-quality images from degraded inputs. Various models, such as convolutional neural networks (CNNs), generative adversarial networks (GANs), transformers, and diffusion models (DMs), have been employed to address this problem with significant impact. However, CNNs have limitations in capturing long-range dependencies. DMs require large prior models and computationally intensive denoising steps. Transformers have powerful modeling capabilities but face challenges due to quadratic complexity with input image size. To address these challenges, we propose VmambaIR, which introduces State Space Models (SSMs) with linear complexity into comprehensive image restoration tasks. We utilize a Unet architecture to stack our proposed Omni Selective Scan (OSS) blocks, consisting of an OSS module and an Efficient Feed-Forward Network (EFFN). Our proposed omni selective scan mechanism overcomes the unidirectional modeling limitation of SSMs by efficiently modeling image information flows in all six directions. Furthermore, we conducted a comprehensive evaluation of our VmambaIR across multiple image restoration tasks, including image deraining, single image super-resolution, and real-world image super-resolution. Extensive experimental results demonstrate that our proposed VmambaIR achieves state-of-the-art (SOTA) performance with much fewer computational resources and parameters. Our research highlights the potential of state space models as promising alternatives to the transformer and CNN architectures in serving as foundational frameworks for next-generation low-level visual tasks.",
                "authors": "Yuan Shi, Bin Xia, Xiaoyu Jin, Xing Wang, Tianyu Zhao, Xin Xia, Xuefeng Xiao, Wenming Yang",
                "citations": 33
            },
            {
                "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
                "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro",
                "authors": "Alexey Bochkovskiy, Amaël Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, V. Koltun",
                "citations": 16
            },
            {
                "title": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System",
                "abstract": "The booming success of LLMs initiates rapid development in LLM agents. Though the foundation of an LLM agent is the generative model, it is critical to devise the optimal reasoning strategies and agent architectures. Accordingly, LLM agent research advances from the simple chain-of-thought prompting to more complex ReAct and Reflection reasoning strategy; agent architecture also evolves from single agent generation to multi-agent conversation, as well as multi-LLM multi-agent group chat. However, with the existing intricate frameworks and libraries, creating and evaluating new reasoning strategies and agent architectures has become a complex challenge, which hinders research investigation into LLM agents. Thus, we open-source a new AI agent library, AgentLite, which simplifies this process by offering a lightweight, user-friendly platform for innovating LLM agent reasoning, architectures, and applications with ease. AgentLite is a task-oriented framework designed to enhance the ability of agents to break down tasks and facilitate the development of multi-agent systems. Furthermore, we introduce multiple practical applications developed with AgentLite to demonstrate its convenience and flexibility. Get started now at: \\url{https://github.com/SalesforceAIResearch/AgentLite}.",
                "authors": "Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla Kumar Choubey, Tian Lan, Jason Wu, Huan Wang, Shelby Heinecke, Caiming Xiong, Silvio Savarese",
                "citations": 15
            },
            {
                "title": "Visual Mamba: A Survey and New Outlooks",
                "abstract": "Mamba, a recent selective structured state space model, excels in long sequence modeling, which is vital in the large model era. Long sequence modeling poses significant challenges, including capturing long-range dependencies within the data and handling the computational demands caused by their extensive length. Mamba addresses these challenges by overcoming the local perception limitations of convolutional neural networks and the quadratic computational complexity of Transformers. Given its advantages over these mainstream foundation architectures, Mamba exhibits great potential to be a visual foundation architecture. Since January 2024, Mamba has been actively applied to diverse computer vision tasks, yielding numerous contributions. To help keep pace with the rapid advancements, this paper reviews visual Mamba approaches, analyzing over 200 papers. This paper begins by delineating the formulation of the original Mamba model. Subsequently, it delves into representative backbone networks, and applications categorized using different modalities, including image, video, point cloud, and multi-modal data. Particularly, we identify scanning techniques as critical for adapting Mamba to vision tasks, and decouple these scanning techniques to clarify their functionality and enhance their flexibility across various applications. Finally, we discuss the challenges and future directions, providing insights into new outlooks in this fast evolving area. A comprehensive list of visual Mamba models reviewed in this work is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models.",
                "authors": "Rui Xu, Shu Yang, Yihui Wang, Yu Cai, Bo Du, Hao Chen",
                "citations": 17
            },
            {
                "title": "New insights into MHD squeezing flows of reacting-radiating Maxwell nanofluids via Wakif's–Buongiorno point of view",
                "abstract": null,
                "authors": "A. El Harfouf, A. Wakif, S. Hayani Mounir",
                "citations": 16
            },
            {
                "title": "DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training",
                "abstract": "Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data. Code is available at \\url{https://github.com/thu-ml/DPOT}.",
                "authors": "Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, A. Anandkumar, Jian Song, Jun Zhu",
                "citations": 15
            },
            {
                "title": "Magnetically driven capsules with multimodal response and multifunctionality for biomedical applications",
                "abstract": null,
                "authors": "Yuxuan Sun, Wang Zhang, Junnan Gu, Liangyu Xia, Yinghao Cao, Xinhui Zhu, Hao Wen, Shaowei Ouyang, Ruiqi Liu, jialong li, Zhenxing Jiang, Denglong Cheng, Y. Lv, Xiaotao Han, Wu Qiu, Kailin Cai, Enmin Song, Q. Cao, Liang Li",
                "citations": 16
            },
            {
                "title": "The Effect of Digital Green Strategic Orientation On Digital Green Innovation Performance: From the Perspective of Digital Green Business Model Innovation",
                "abstract": "Digital technology is gradually penetrating into all kinds of business of green innovation and green development. However, China’s industrial Internet technology foundation is weak. Scholars’ researches on the impact of digital green strategic orientation (DGSO) on digital green innovation performance (DGIP) and the intermediary role of digital green business model innovation (DGBMI) is relatively backward. Therefore, it is particularly important to divide the dimensions of DGSO from the perspective of DGBMI to describe the mechanism of DGIP. In this study, multiple regression approach was used to test the effect of DGSO on DGIP. The mediating role of DGBMI was explored through 562 questionnaires. In addition, the study divides the DGSO into three dimensions: green market orientation (GMO), digital green technology orientation (DGTO), and government orientation. DGBMI is divided into two dimensions: efficiency type and integration type. The results are as follows. (1) GMO, DGTO and government orientation all have positive effects on the improvement of DGIP. (2) GMO has a greater impact on integration DGBMI (IDGBMI). The influence of DGTO on efficient DGBMI (EDGBMI) is more significant. (3) Government orientation has a positive impact on EDGBMI. The influence of DGSO on DGIP can be realized through the intermediary role of EDGBMI and IDGBMI. The essence of this research is to help enterprises understand the characteristics of internal and external environment and make digital-green strategy-oriented choices to improve their DGIP. This study has a certain theoretical contribution to elucidating the influence of DGSO on DGIP and the mediating role of DGBMI. This study also provides a practical basis for enterprises’ digital green innovation practice based on their own environmental optimization and matching of DGSO to improve competitiveness.",
                "authors": "S. Yin, Yuanyuan Yu, Nan Zhang",
                "citations": 16
            },
            {
                "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
                "abstract": "The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \\url{aka.ms/wavllm}.",
                "authors": "Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, S. Sivasankaran, Linquan Liu, Furu Wei",
                "citations": 30
            },
            {
                "title": "A Survey on Visual Mamba",
                "abstract": "State space models (SSM) with selection mechanisms and hardware-aware architectures, namely Mamba, have recently shown significant potential in long-sequence modeling. Since the complexity of transformers’ self-attention mechanism is quadratic with image size, as well as increasing computational demands, researchers are currently exploring how to adapt Mamba for computer vision tasks. This paper is the first comprehensive survey that aims to provide an in-depth analysis of Mamba models within the domain of computer vision. It begins by exploring the foundational concepts contributing to Mamba’s success, including the SSM framework, selection mechanisms, and hardware-aware design. Then, we review these vision Mamba models by categorizing them into foundational models and those enhanced with techniques including convolution, recurrence, and attention to improve their sophistication. Furthermore, we investigate the widespread applications of Mamba in vision tasks, which include their use as a backbone in various levels of vision processing. This encompasses general visual tasks, medical visual tasks (e.g., 2D/3D segmentation, classification, image registration, etc.), and remote sensing visual tasks. In particular, we introduce general visual tasks from two levels: high/mid-level vision (e.g., object detection, segmentation, video classification, etc.) and low-level vision (e.g., image super-resolution, image restoration, visual generation, etc.). We hope this endeavor will spark additional interest within the community to address current challenges and further apply Mamba models in computer vision.",
                "authors": "Hanwei Zhang, Ying Zhu, Dan Wang, Lijun Zhang, Tianxiang Chen, Zi Ye",
                "citations": 29
            },
            {
                "title": "Determinants of LLM-assisted Decision-Making",
                "abstract": "Decision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decision-specific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs.",
                "authors": "Eva Eigner, Thorsten Händler",
                "citations": 28
            },
            {
                "title": "Harnessing adversarial machine learning for advanced threat detection: AI-driven strategies in cybersecurity risk assessment and fraud prevention",
                "abstract": "The abstract is \"The rapid evolution of cyber threats necessitates innovative defenses, particularly in the domains of risk assessment and fraud detection. This paper explores the integration of Artificial Intelligence (AI) and Adversarial Machine Learning (ML) techniques as a formidable strategy against increasingly sophisticated cyber-attacks. We present a comprehensive framework that leverages AI to dynamically assess cybersecurity risks and detect fraudulent activities with unprecedented accuracy and speed. Firstly, we delve into the foundational principles of adversarial machine learning, outlining how these techniques can be employed to simulate potential cyber threats, thereby enabling the development of more resilient AI-driven cybersecurity systems. We highlight the dual role of adversarial ML in both enhancing security defenses and potentially serving as a vector for sophisticated attacks, underscoring the importance of developing robust, adversarial-resistant models. Subsequently, we introduce a novel adaptive risk assessment methodology that incorporates real-time data analysis, machine learning algorithms, and predictive modeling to accurately identify and prioritize threats. This method adapts to the evolving digital landscape, ensuring that cybersecurity measures are always one step ahead of potential attackers. In the context of fraud detection, we explore -how AI algorithms can sift through vast datasets to detect anomalies and patterns indicative of fraudulent behavior. Through case studies and empirical analysis, we demonstrate the effectiveness of AI in identifying fraud across various sectors, from financial transactions to online identity verification processes. Our research contributes to the cybersecurity field by providing a detailed examination of how AI and adversarial ML can be harnessed to fortify digital defenses, improve risk assessment techniques, and enhance fraud detection capabilities. The insights garnered from this study not only advance theoretical understanding but also offer practical guidance for organizations seeking to implement AI-driven security solutions. As cyber threats continue to evolve, the integration of AI and adversarial ML in cybersecurity strategies will be paramount in safeguarding digital assets and maintaining the integrity of online systems.\"",
                "authors": "Idoko Peter, Onuh Matthew Ijiga, Idoko Peter Idoko, Godslove Isenyo, Ebiega, F. I. Olajide, Timilehin Isaiah Olatunde, Chukwunonso Ukaegbu",
                "citations": 25
            },
            {
                "title": "SPMamba: State-space model is all you need in speech separation",
                "abstract": "In speech separation, both CNN-and Transformer-based models have demonstrated robust separation capabilities, garnering significant attention within the research community. However, CNN-based methods have limited modelling capability for long-sequence audio, leading to suboptimal separation performance. Conversely, Transformer-based methods are limited in practical applications due to their high computational complexity. Notably, within computer vision, Mamba-based methods have been celebrated for their formidable performance and reduced computational requirements. In this paper, we propose a network architecture for speech separation using a state-space model, namely SPMamba. We adopt the TF-GridNet model as the foundational framework and substitute its Transformer component with a bidirectional Mamba module, aiming to capture a broader range of contextual information. Our experimental results reveal an important role in the performance aspects of Mamba-based models. SPMamba demonstrates superior performance with a significant advantage over existing separation models in a dataset built on Librispeech. Notably, SP-Mamba achieves a substantial improvement in separation quality, with a 2.42 dB enhancement in SI-SNRi compared to the TF-GridNet. The source code for SPMamba is publicly accessible at https://github.com/JusperLee/SPMamba .",
                "authors": "Kai Li, Guo Chen",
                "citations": 28
            },
            {
                "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
                "abstract": "Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs. This paper introduces\"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as\"key\"tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.",
                "authors": "Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath",
                "citations": 28
            },
            {
                "title": "EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters",
                "abstract": "Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models. We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18-billion parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin. Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B) employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the potential of EVA-style weak-to-strong visual model scaling. With our model weights made publicly available, we hope to facilitate future research in vision and multimodal foundation models.",
                "authors": "Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Xinlong Wang",
                "citations": 27
            },
            {
                "title": "Accelerating ionizable lipid discovery for mRNA delivery using machine learning and combinatorial chemistry.",
                "abstract": null,
                "authors": "Bowen Li, Idris O Raji, Akiva G R Gordon, Lizhuang Sun, Theresa M Raimondo, Favour A Oladimeji, A. Jiang, Andrew Varley, Robert S Langer, Daniel G Anderson",
                "citations": 24
            },
            {
                "title": "Strategizing IoT Network Layer Security Through Advanced Intrusion Detection Systems and AI-Driven Threat Analysis",
                "abstract": "This research introduces an algorithmic framework for enhancing the security of Internet of Things (IoT) networks. The Enhanced Anomaly Detection (EAD) algorithm initiates the process by detecting anomalies in real-time IoT data, serving as the foundational layer. The Behavior Analysis for Profiling (BAP) algorithm builds upon EAD, adding behavior analysis for profiling and adaptive identification of abnormal behavior. Signature-Based Detection (SBD) involves pre-identified attack signatures, which supports detection of known attacks and provides proactive defense measures against documented threats. The MLID, or the Machine Learning-Based Intrusion Detection, algorithm uses trained machine learning models in order to detect anomalies and the adaptability to changing security risks. The Real-Time Threat Intelligence Integration (RTI) algorithm integrates updated threat intelligence feeds, which improves the framework's responsiveness to emerging threats. The visual representations illustrate once again the idea of the new framework being very accurate at intergration, applicability, and overal security effectiveness. The research makes a standard solution which proves to be a smart and responsive way guarding the IoT networks reducing and even fighting known and potential threats in a real-time mode.",
                "authors": "Piyush Piyush, Dr. Akhilesh A. Waoo, Murlidhar Prasad Singh, P. Pareek, Shoaib Kamal, S. Pandit",
                "citations": 23
            },
            {
                "title": "Fine-tuning and Utilization Methods of Domain-specific LLMs",
                "abstract": "Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs, foundational models, and methods for domain-specific pre-training. Focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for LLM fine-tuning in finance. Addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance. In the practical application of LLM fine-tuning, the study outlines the procedure and implementation for generating domain-specific LLMs in finance. Various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extraction, and customer service enhancement, are exemplified. The study explores the potential of LLMs in the financial domain, identifies limitations, and proposes directions for improvement, contributing valuable insights for future research. Ultimately, it advances natural language processing technology in business, suggesting proactive LLM utilization in financial services across industries.",
                "authors": "CheonSu Jeong",
                "citations": 23
            },
            {
                "title": "The Impact Analysis of Operational Overvoltage on Traction Transformers for High-Speed Trains Based on the Improved Capacitor Network Methodology",
                "abstract": "When a high-speed train approaches an insulated split-phase section embedded between neighboring power supply arms carrying powers with different phases, the vehicle-mounted vacuum circuit breaker (VCB) needs to be switched off to avoid short-circuit fault during the phase-changing procedure. At the moment of switching the VCB contacts, the arc is triggered easily, accompanying with overvoltage impulse. As the core of the whole system, the traction transformer frequently suffers the invasion of the overvoltage impulse, which possibly causes partial discharge or breakdown accidents. To evaluate the impact of overvoltage on the transformer, it is essential to observe the overvoltage distribution at different positions in the transformer. As an equivalent circuit (EC) model of the traction transformer involving the spatially distributed capacitance is built based on the improved capacitor network methodology (ICNM), the partial discharge risk in the transformer has been assessed. Ultimately, the multiphysics coupling models based on the finite-element method (FEM) have been launched to observe the specific distribution of the multiphysics field for verifying the validity of an EC model based on ICNM. These works lay the theoretical foundation for guiding the design of insulation and the thermal dissipation settings of the traction transformer, considering the threat brought from impulse.",
                "authors": "Song Xiao, Zijing Wang, Guangning Wu, Yujun Guo, Guoqiang Gao, Xueqin Zhang, Ye Cao, Yuhui Zhang, Jie Yu, Puyang Liu, Pupu Li",
                "citations": 23
            },
            {
                "title": "SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention",
                "abstract": "Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses current state-of-the-art methods and is on par with the biggest foundation model MOIRAI while having significantly fewer parameters. The code is available at https://github.com/romilbert/samformer.",
                "authors": "Romain Ilbert, Ambroise Odonnat, Vasilii Feofanov, Aladin Virmaux, Giuseppe Paolo, Themis Palpanas, I. Redko",
                "citations": 14
            },
            {
                "title": "Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V",
                "abstract": "Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. We present COME-robot, the first closed-loop framework utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios. We meticulously construct a library of action primitives for robot exploration, navigation, and manipulation, serving as callable execution modules for GPT-4V in task planning. On top of these modules, GPT-4V serves as the brain that can accomplish multimodal reasoning, generate action policy with code, verify the task progress, and provide feedback for replanning. Such design enables COME-robot to (i) actively perceive the environments, (ii) perform situated reasoning, and (iii) recover from failures. Through comprehensive experiments involving 8 challenging real-world tabletop and manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~25%) compared to state-of-the-art baseline methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning.",
                "authors": "Peiyuan Zhi, Zhiyuan Zhang, Muzhi Han, Zeyu Zhang, Zhitian Li, Ziyuan Jiao, Baoxiong Jia, Siyuan Huang",
                "citations": 14
            },
            {
                "title": "Optimizing Automated Picking Systems in Warehouse Robots Using Machine Learning",
                "abstract": "With the rapid growth of global e-commerce, the demand for automation in the logistics industry is increasing. This study focuses on automated picking systems in warehouses, utilizing deep learning and reinforcement learning technologies to enhance picking efficiency and accuracy while reducing system failure rates. Through empirical analysis, we demonstrate the effectiveness of these technologies in improving robot picking performance and adaptability to complex environments. The results show that the integrated machine learning model significantly outperforms traditional methods, effectively addressing the challenges of peak order processing, reducing operational errors, and improving overall logistics efficiency. Additionally, by analyzing environmental factors, this study further optimizes system design to ensure efficient and stable operation under variable conditions. This research not only provides innovative solutions for logistics automation but also offers a theoretical and empirical foundation for future technological development and application.",
                "authors": "Keqin Li, Jin Wang, Xubo Wu, Xirui Peng, Runmian Chang, Xiaoyu Deng, Yiwen Kang, Yue Yang, Fanghao Ni, Bo Hong",
                "citations": 14
            },
            {
                "title": "Towards a transferable fermionic neural wavefunction for molecules",
                "abstract": null,
                "authors": "Michael Scherbela, Leon Gerard, Philipp Grohs",
                "citations": 13
            },
            {
                "title": "YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision",
                "abstract": "This paper presents a comprehensive review of the evolution of the YOLO (You Only Look Once) object detection algorithm, focusing on YOLOv5, YOLOv8, and YOLOv10. We analyze the architectural advancements, performance improvements, and suitability for edge deployment across these versions. YOLOv5 introduced significant innovations such as the CSPDarknet backbone and Mosaic Augmentation, balancing speed and accuracy. YOLOv8 built upon this foundation with enhanced feature extraction and anchor-free detection, improving versatility and performance. YOLOv10 represents a leap forward with NMS-free training, spatial-channel decoupled downsampling, and large-kernel convolutions, achieving state-of-the-art performance with reduced computational overhead. Our findings highlight the progressive enhancements in accuracy, efficiency, and real-time performance, particularly emphasizing their applicability in resource-constrained environments. This review provides insights into the trade-offs between model complexity and detection accuracy, offering guidance for selecting the most appropriate YOLO version for specific edge computing applications.",
                "authors": "Muhammad Hussain",
                "citations": 13
            },
            {
                "title": "Integrated Thermal and Energy Management of Connected Hybrid Electric Vehicles Using Deep Reinforcement Learning",
                "abstract": "The climate-adaptive mymargin energy management system (EMS) holds promising potential for harnessing the concealed energy-saving capabilities of connected plug-in hybrid electric vehicles (PHEVs). This research focuses on exploring the synergistic effects of artificial intelligence control and traffic preview to enhance the performance of the EMS. A high-fidelity model of a multimode connected PHEV is calibrated using the experimental data as a foundation. Subsequently, a model-free multistate deep reinforcement learning (DRL) algorithm is proposed to develop the integrated thermal and energy management (ITEM) system, incorporating the features of engine smart warm-up and engine-assisted heating for cold climate conditions. The optimality and adaptability of the proposed system are evaluated through both offline tests and online hardware-in-the-loop (HIL) tests, encompassing a homologation driving cycle and a real-world driving cycle in China with real-time traffic data. The results demonstrate that ITEM achieves a close to dynamic programming (DP) fuel economy performance with a margin of 93.7%, while reducing fuel consumption ranging from 2.2% to 9.6% as ambient temperature decreases from 15 °C to −15 °C in comparison to the state-of-the-art DRL-based EMS solutions.",
                "authors": "H. Zhang, Boli Chen, Nuo Lei, Bingbing Li, Rulong Li, Zhi Wang",
                "citations": 12
            },
            {
                "title": "Toward Ensuring Safety for Autonomous Driving Perception: Standardization Progress, Research Advances, and Perspectives",
                "abstract": "Perception systems play a crucial role in autonomous driving by reading the sensory data and providing meaningful interpretation of the operating environment for decision-making and planning. Guaranteeing a safe perception performance is the foundation for high-level autonomy, so that we can hand over the driving and monitoring tasks to the machine with ease. With the motivation of improving the perception systems’ safety, this survey analyzes and reviews the current achievements of safety-related standards and definitions, sensory modeling, and metrics for perception tasks in autonomous driving applications. Furthermore, it covers the generic categorization of potential failures and causal analysis in perception tasks, correlates the effect with the scenario modelling choices, and highlights major triumphs and noted limitations encountered by current research efforts. The new safety challenges laid out by the information exchange stage of the connected autonomous vehicle application have also been summarized. The open research questions and future directions are outlined to welcome researchers and practitioners to this exciting domain.",
                "authors": "Chen Sun, Ruihe Zhang, Yukun Lu, Yaodong Cui, Zejian Deng, Dongpu Cao, A. Khajepour",
                "citations": 12
            },
            {
                "title": "ChatGPT: a game changer for knowledge management in organizations",
                "abstract": "PurposeGenerative AI and more specifically ChatGPT has brought a revolution in the lives  of people by providing them with required knowledge that it has learnt from an exponentially large knowledge base. In this viewpoint, we are initiating the debate and offer the first step towards Generative AI based knowledge management systems in organizations.Design/methodology/approachThis study is a viewpoint and develops a conceptual foundation using existing literature on how ChatGPT can enhance the KM capability based on Nonaka’s SECI model. It further supports the concept by collecting data from a public sector univesity in Hong Kong to strenghten our argument of ChatGPT mediated knowledge management system.FindingsWe posit that all four processes, that is Socialization, Externalization, Combination and Internalization can significantly improve when integrated with ChatGPT. ChatGPT users are, in general, satisfied with the use of ChatGPT being capable of facilitating knowledge generation and flow in organizations.Research limitations/implicationsThe study provides a conceptual foundation to further the knowledge on how ChatGPT can be integrated within organizations to enhance the knowledge management capability of organizations. Further, it develops an understanding on how managers and executives can use ChatGPT for effective knowledge management through improving the four processes of Nonaka’s SECI model.Originality/valueThis is one of the earliest studies on the linkage of knowledge management with ChatGPT and lays a foundation for ChatGPT mediated knowledge management system in organizations.",
                "authors": "M. S. Sumbal, Quratulain Amber",
                "citations": 13
            },
            {
                "title": "A multi-factor combination prediction model of carbon emissions based on improved CEEMDAN.",
                "abstract": null,
                "authors": "Guohui Li, Hao Wu, Hong Yang",
                "citations": 13
            },
            {
                "title": "Large-Scale and Knowledge-Based Dynamic Multiobjective Optimization for MSWI Process Using Adaptive Competitive Swarm Optimization",
                "abstract": "Municipal solid waste incineration (MSWI) process is a complex industrial process with strong nonlinearity. It is a challenge to build a model for the MSWI process and carry out the corresponding optimization works. To solve this problem, the multiobjective optimization studies are conducted for both modeling and concerned indexes of the MSWI process, including the nitrogen oxides (NOx) emissions and the combustion efficiency (CE). First, a data-driven-based multiple-input multiple-output model is established for the NOx emissions and the CE of the MSWI process based on Takagi–Sugeno–Kang fuzzy neural network. Second, an adaptive large-scale multiobjective competitive swarm optimization (ALMOCSO) algorithm is designed for solving the multiobjective optimization problems (MOPs) of the MSWI process. A comprehensive evaluation system is proposed to complete the optimization foundation, and an adaptive scheme and multistrategy learning are proposed to improve the optimization effect of the ALMOCSO algorithm in solving complex MOPs. Then, a Pareto optimal set obtained from massive historical data is utilized as optimization reference to realize the dynamic multiobjective optimization for the NOx emissions and the CE of the MSWI process. Finally, the feasibility and effectiveness of the proposed methodology for optimizing the MSWI process are confirmed by the experiments using the data collected from a real MSWI plant. The results indicate that the modeling accuracy is satisfactory, and the CE is improved over 10% and the reduction of the NOx emissions is achieved 15.58%.",
                "authors": "Weimin Huang, Haixu Ding, Junfei Qiao",
                "citations": 12
            },
            {
                "title": "The meta-commerce paradox: exploring consumer non-adoption intentions",
                "abstract": "PurposeThe primary objective of this study is to explore consumers' non-adoption intentions towards meta-commerce (or metaverse retailing). Utilizing the Innovation Resistance Theory (IRT) as the theoretical foundation, this study investigates the impact of diverse barriers on non-adoption intentions within the meta-commerce context.Design/methodology/approachA total of 356 responses were gathered to test the proposed hypotheses. Structural Equation Modelling (SEM) with SmartPLS 4 software was used to examine these hypotheses.FindingsThe findings of this study show that perceived cyber risk, perceived regulatory uncertainty, perceived switching cost and perceived technical uncertainty are significantly linked to non-adoption intention towards meta-commerce. Furthermore, the study suggests that the moderating influence of technostress on these connections is more pronounced for consumers with high technostress compared to those with low technostress.Originality/valueThis study makes a significant contribution to the current body of literature by providing valuable insights into the fundamental barriers that consumers encounter when contemplating the adoption of meta-commerce. This contribution is particularly noteworthy as it fills a gap in the existing literature, as no prior study has comprehensively examined the primary obstacles that shape consumer intentions towards meta-commerce adoption. This novel perspective offers scholars, businesses and policymakers a foundation for developing strategies to address these barriers effectively.",
                "authors": "Ahmed Samed Al-Adwan",
                "citations": 12
            },
            {
                "title": "Space Group Constrained Crystal Generation",
                "abstract": "Crystals are the foundation of numerous scientific and industrial applications. While various learning-based approaches have been proposed for crystal generation, existing methods seldom consider the space group constraint which is crucial in describing the geometry of crystals and closely relevant to many desirable properties. However, considering space group constraint is challenging owing to its diverse and nontrivial forms. In this paper, we reduce the space group constraint into an equivalent formulation that is more tractable to be handcrafted into the generation process. In particular, we translate the space group constraint into two parts: the basis constraint of the invariant logarithmic space of the lattice matrix and the Wyckoff position constraint of the fractional coordinates. Upon the derived constraints, we then propose DiffCSP++, a novel diffusion model that has enhanced a previous work DiffCSP by further taking space group constraint into account. Experiments on several popular datasets verify the benefit of the involvement of the space group constraint, and show that our DiffCSP++ achieves promising performance on crystal structure prediction, ab initio crystal generation and controllable generation with customized space groups.",
                "authors": "Rui Jiao, Wenbing Huang, Yu Liu, Deli Zhao, Yang Liu",
                "citations": 13
            },
            {
                "title": "Soluble and multivalent Jag1 DNA origami nanopatterns activate Notch without pulling force",
                "abstract": null,
                "authors": "Ioanna Smyrlaki, Ferenc Fördős, Iris Rocamonde-Lago, Yang Wang, B. Shen, Antonio Lentini, Vincent C. Luca, B. Reinius, Ana I Teixeira, Björn Högberg",
                "citations": 10
            },
            {
                "title": "Deep Learning for Tomato Disease Detection with YOLOv8",
                "abstract": "Tomato production plays a crucial role in Saudi Arabia, with significant yield variations due to factors such as diseases. While automation offers promising solutions, accurate disease detection remains a challenge. This study proposes a deep learning approach based on the YOLOv8 algorithm for automated tomato disease detection. Augmenting an existing Roboflow dataset, the model achieved an overall accuracy of 66.67%. However, class-specific performance varies, highlighting challenges in differentiating certain diseases. Further research is suggested, focusing on data balancing, exploring alternative architectures, and adopting disease-specific metrics. This work lays the foundation for a robust disease detection system to improve crop yields, quality, and sustainable agriculture in Saudi Arabia.",
                "authors": "Hafedh M. Zayani, Ikhlass Ammar, R. Ghodhbani, Albia Maqbool, T. Saidani, Jihane Ben Slimane, Amani Kachoukh, Marouan Kouki, Mohamed Kallel, Amjad A. Alsuwaylimi, Sami M. Alenezi",
                "citations": 11
            },
            {
                "title": "Cyber-Physical Attack Launched From EVSE Botnet",
                "abstract": "With the development of electric vehicles, millions of electric vehicle service equipment (EVSE) are integrated into the power grid. These EVSEs are weakly protected from cyber-attacks. By exploiting vulnerabilities, attackers could establish a botnet integrating hundred-MW-level EVSE loads from the Internet. Dynamic load-altering attack (DLAA) is powerful to amplify the load disturbance to destabilize the power grid. However, different from large-scale loads controlled by existing DLAA, the EVSE botnet is formulated by tens of thousands of bots widely distributed on various power grid nodes, rendering it challenging to be coordinated. In this article, we analyze the approach to launching a DLAA from the distributed EVSE botnet. Firstly, we adopt a time-division model to evaluate the impact of communication limitations on the attack vectors of the botnet. Then we develop an analytical model to calculate the coordinated effect of attack vectors from different nodes and with varying phase angles; thus, we provide a foundation for the optimal coordination of distributed bots. Subsequently, considering the power grid has adopted defense measures to eliminate or suppress the attack, we formulate an optimization algorithm to make the attack robust against these potential defense measures. Numerical simulations validate the effectiveness of the proposed approach.",
                "authors": "Fanrong Wei, Xiangning Lin",
                "citations": 10
            },
            {
                "title": "A Vehicle Classification Method Based on Machine Learning",
                "abstract": "In this article, we propose a machine learning based fine-grained vehicle classification method VehiClassNet, which effectively solves the challenges of precise vehicle recognition and classification in intelligent transportation systems by combining multi-scale feature extraction, cross modal fusion, and attention mechanisms. Our model demonstrated excellent performance on the Stanford Cars-196 dataset. The innovation of this study lies in its advanced multi-scale feature extraction, cross modal fusion, and attention mechanism. It not only improves classification accuracy, but also lays the foundation for further research on intelligent transportation systems and autonomous vehicle technology.",
                "authors": "Xinjin Li, Jinghao Chang, Tiexin Li, Wenhan Fan, Yu Ma, Haowei Ni",
                "citations": 11
            },
            {
                "title": "Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications",
                "abstract": "We introduce Deformable Convolution v4 (DCNv4), a highly efficient and effective operator designed for a broad spectrum of vision applications. DCNv4 addresses the limitations of its predecessor, DCNv3, with two key enhancements: 1. removing softmax normalization in spatial aggregation to enhance its dynamic property and expressive power and 2. optimizing memory access to minimize redundant operations for speedup. These improvements result in a significantly faster convergence compared to DCNv3 and a substantial increase in processing speed, with DCNv4 achieving more than three times the forward speed. DCNv4 demonstrates exceptional performance across various tasks, including image classification, instance and semantic segmentation, and notably, image generation. When integrated into generative models like U-Net in the latent diffusion model, DCNv4 outperforms its baseline, underscoring its possibility to enhance generative models. In practical applications, replacing DCNv3 with DCNv4 in the InternImage model to create FlashInternImage results in up to 80% speed increase and further performance improvement without further modifications. The advancements in speed and efficiency of DCNv4, combined with its robust performance across diverse vision tasks, show its potential as a foundational building block for future vision models.",
                "authors": "Yuwen Xiong, Zhiqi Li, Yuntao Chen, Feng Wang, Xizhou Zhu, Jiapeng Luo, Wenhai Wang, Tong Lu, Hongsheng Li, Yu Qiao, Lewei Lu, Jie Zhou, Jifeng Dai",
                "citations": 21
            },
            {
                "title": "UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation",
                "abstract": "Traditionally for improving the segmentation performance of models, most approaches prefer to use adding more complex modules. And this is not suitable for the medical field, especially for mobile medical devices, where computationally loaded models are not suitable for real clinical environments due to computational resource constraints. Recently, state-space models (SSMs), represented by Mamba, have become a strong competitor to traditional CNNs and Transformers. In this paper, we deeply explore the key elements of parameter influence in Mamba and propose an UltraLight Vision Mamba UNet (UltraLight VM-UNet) based on this. Specifically, we propose a method for processing features in parallel Vision Mamba, named PVM Layer, which achieves excellent performance with the lowest computational load while keeping the overall number of processing channels constant. We conducted comparisons and ablation experiments with several state-of-the-art lightweight models on three skin lesion public datasets and demonstrated that the UltraLight VM-UNet exhibits the same strong performance competitiveness with parameters of only 0.049M and GFLOPs of 0.060. In addition, this study deeply explores the key elements of parameter influence in Mamba, which will lay a theoretical foundation for Mamba to possibly become a new mainstream module for lightweighting in the future. The code is available from https://github.com/wurenkai/UltraLight-VM-UNet .",
                "authors": "Renkai Wu, Yinghao Liu, Pengchen Liang, Qing Chang",
                "citations": 22
            },
            {
                "title": "Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B",
                "abstract": "This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.",
                "authors": "Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang",
                "citations": 22
            },
            {
                "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset",
                "abstract": "Using vision-language models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML. Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored. We posit that this is mainly due to the absence of a suitable, high-quality dataset. This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots. We fine-tune a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code. To accelerate the research in this area, we open-source WebSight.",
                "authors": "Hugo Laurençon, Léo Tronchon, Victor Sanh",
                "citations": 20
            },
            {
                "title": "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations",
                "abstract": "Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (\"Generative Recommenders\"), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations.",
                "authors": "Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yin-Hua Lu, Yu Shi",
                "citations": 20
            },
            {
                "title": "Comparative Evaluation of LLMs in Clinical Oncology.",
                "abstract": "BACKGROUND\nAs artificial intelligence (AI) tools become widely accessible, more patients and medical professionals will turn to them for medical information. Large language models (LLMs), a subset of AI, excel in natural language processing tasks and hold considerable promise for clinical use. Fields such as oncology, in which clinical decisions are highly dependent on a continuous influx of new clinical trial data and evolving guidelines, stand to gain immensely from such advancements. It is therefore of critical importance to benchmark these models and describe their performance characteristics to guide their safe application to clinical oncology. Accordingly, the primary objectives of this work were to conduct comprehensive evaluations of LLMs in the field of oncology and to identify and characterize strategies that medical professionals can use to bolster their confidence in a model's response.\n\n\nMETHODS\nThis study tested five publicly available LLMs (LLaMA 1, PaLM 2, Claude-v1, generative pretrained transformer 3.5 [GPT-3.5], and GPT-4) on a comprehensive battery of 2044 oncology questions, including topics from medical oncology, surgical oncology, radiation oncology, medical statistics, medical physics, and cancer biology. Model prompts were presented independently of each other, and each prompt was repeated three times to assess output consistency. For each response, models were instructed to provide a self-appraised confidence score (from 1 to 4). Model performance was also evaluated against a novel validation set comprising 50 oncology questions curated to eliminate any risk of overlap with the data used to train the LLMs.\n\n\nRESULTS\nThere was significant heterogeneity in performance between models (analysis of variance, P<0.001). Relative to a human benchmark (2013 and 2014 examination results), GPT-4 was the only model to perform above the 50th percentile. Overall, model performance varied as a function of subject area across all models, with worse performance observed in clinical oncology subcategories compared with foundational topics (medical statistics, medical physics, and cancer biology). Within the clinical oncology subdomain, worse performance was observed in female-predominant malignancies. A combination of model selection, prompt repetition, and confidence self-appraisal allowed for the identification of high-performing subgroups of questions with observed accuracies of 81.7 and 81.1% in the Claude-v1 and GPT-4 models, respectively. Evaluation of the novel validation question set produced similar trends in model performance while also highlighting improved performance in newer, centrally hosted models (GPT-4 Turbo and Gemini 1.0 Ultra) and local models (Mixtral 8×7B and LLaMA 2).\n\n\nCONCLUSIONS\nOf the models tested on a standardized set of oncology questions, GPT-4 was observed to have the highest performance. Although this performance is impressive, all LLMs continue to have clinically significant error rates, including examples of overconfidence and consistent inaccuracies. Given the enthusiasm to integrate these new implementations of AI into clinical practice, continued standardized evaluations of the strengths and limitations of these products will be critical to guide both patients and medical professionals. (Funded by the National Institutes of Health Clinical Center for Research and the Intramural Research Program of the National Institutes of Health; Z99 CA999999.).",
                "authors": "N. Rydzewski, Deepak Dinakaran, Shuang G. Zhao, E. Ruppin, B. Turkbey, D. E. Citrin, Krishnan R. Patel",
                "citations": 21
            },
            {
                "title": "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining",
                "abstract": "Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.",
                "authors": "Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, Jia Li",
                "citations": 21
            },
            {
                "title": "Towards Analyzing and Understanding the Limitations of DPO: A Theoretical Perspective",
                "abstract": "Direct Preference Optimization (DPO), which derives reward signals directly from pairwise preference data, has shown its effectiveness on aligning Large Language Models (LLMs) with human preferences. Despite its widespread use across various tasks, DPO has been criticized for its sensitivity to the SFT's effectiveness and its hindrance to the learning capacity towards human-preferred responses, leading to less satisfactory performance. To overcome those limitations, the theoretical understanding of DPO are indispensable but still lacking. To this end, we take a step towards theoretically analyzing and understanding the limitations of DPO. Specifically, we provide an analytical framework using the field theory to analyze the optimization process of DPO. By analyzing the gradient vector field of the DPO loss function, we find that the DPO loss function decreases the probability of producing human dispreferred data at a faster rate than it increases the probability of producing preferred data. This provides theoretical insights for understanding the limitations of DPO discovered in the related research experiments, thereby setting the foundation for its improvement.",
                "authors": "Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, Wenqiang Lei",
                "citations": 21
            },
            {
                "title": "Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking",
                "abstract": "Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions. Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition. Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages. Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction. Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models. Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain.",
                "authors": "Y. Fung, Ruining Zhao, Jae Doo, Chenkai Sun, Heng Ji",
                "citations": 20
            },
            {
                "title": "Artificial Intelligence for Literature Reviews: Opportunities and Challenges",
                "abstract": "This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.",
                "authors": "Francisco Bolanos, Angelo Salatino, Francesco Osborne, Enrico Motta",
                "citations": 20
            },
            {
                "title": "Building Vibration Measurement and Prediction during Train Operations",
                "abstract": "Urban societies face the challenge of working and living in environments filled with vibration caused by transportation systems. This paper conducted field measurements to obtain the characteristics of vibration transmission from soil to building foundations and within building floors. Subsequently, a prediction method was developed to anticipate building vibrations by considering the soil and structure interaction. The rigid foundation model was simplified into a foundation–soil system connected via spring damping, and the building model is based on axial wave transmission within the columns and attached floors. Building vibrations were in response to measured input vibration levels at the ground and were validated through field measurements. The influence of different building heights on soil and structure vibration propagation was studied. The results showed that the predicted vibrations match well with the measured vibrations. The proposed prediction model can reasonably predict the building vibration caused by train operations. The closed-form method is an efficient tool for predicting floor vibrations prior to construction.",
                "authors": "Lingshan He, Ziyu Tao",
                "citations": 9
            },
            {
                "title": "Advancing Financial Risk Prediction through Optimized LSTM Model Performance and Comparative Analysis",
                "abstract": "This paper focuses on the application and optimization of LSTM model in financial risk prediction. The study starts with an overview of the architecture and algorithm foundation of LSTM, and then details the model training process and hyperparameter tuning strategy, and adjusts network parameters through experiments to improve performance. Comparative experiments show that the optimized LSTM model shows significant advantages in AUC index compared with random forest, BP neural network and XGBoost, which verifies its efficiency and practicability in the field of financial risk prediction, especially its ability to deal with complex time series data, which lays a solid foundation for the application of the model in the actual production environment.",
                "authors": "Ke Xu, Yu Cheng, Shiqing Long, Junjie Guo, Jue Xiao, Mengfang Sun",
                "citations": 9
            },
            {
                "title": "Factors Affecting the Adoption and Use of ChatGPT in Higher Education",
                "abstract": "The current study aims at assessing the factors which could affect students' use of ChatGPT. The study proposed a theoretical model that included five factors. Data were collected from 136 students using a questionnaire. The data were analyzed using two steps: CFA for measuring the model and SEM for analyzing the relationships and testing hypothesis. The findings revealed that both performance expectancy and facilitating conditions significantly influenced students' intentions to use ChatGPT. Contrary to expectations, both social influence and effort expectancy had insignificant effects. By elucidating the core factors influencing the utilization of ChatGPT, this study can provide valuable insights for policymakers. Furthermore, this study contributes to the existing literature and lays the foundation for future research seeking a deeper understanding of the factors influencing the use of other AI technologies in teaching and learning.",
                "authors": "Sultan Hammad Alshammari, Mohammed Habib Alshammari",
                "citations": 9
            },
            {
                "title": "Importance of higher modes for dynamic soil structure interaction of monopile‐supported offshore wind turbines",
                "abstract": "Offshore wind turbines (OWTs) have emerged as one of the most sustainable and renewable sources of energy. The size of OWTs has been increasing, which creates more challenges in the design of foundations due to the potential higher‐mode effects involved in the dynamic soil‐structure interaction (DSSI) response. Several foundation modeling techniques are available for calculating the OWT fundamental frequency; however, their capability to predict the higher modes by considering real geometric configurations is unclear. The main aim of this study is to perform a rigorous modal analysis of the NREL 5MW reference OWT to investigate the higher mode effects using the 3D finite element method. A detailed parametric analysis is also performed to study the effects of soil inhomogeneity, initial soil modulus, and the monopile dimensions (diameter, thickness, and embedded pile depth) on higher modes' natural frequencies and effective mass participation ratios. The study shows that dynamic soil‐structure interaction has a significant role in modal response and the simplified foundation models are not accurate enough. Given the significant contribution from higher modes, they should not be simply ignored in the OWT design, particularly in earthquake‐prone zones.",
                "authors": "Upendra Kumar Sah, Jun Yang",
                "citations": 8
            },
            {
                "title": "Application of Natural Language Processing in Financial Risk Detection",
                "abstract": "This paper explores the application of Natural Language Processing (NLP) in financial risk detection. By constructing an NLP-based financial risk detection model, this study aims to identify and predict potential risks in financial documents and communications. First, the fundamental concepts of NLP and its theoretical foundation, including text mining methods, NLP model design principles, and machine learning algorithms, are introduced. Second, the process of text data preprocessing and feature extraction is described. Finally, the effectiveness and predictive performance of the model are validated through empirical research. The results show that the NLP-based financial risk detection model performs excellently in risk identification and prediction, providing effective risk management tools for financial institutions. This study offers valuable references for the field of financial risk management, utilizing advanced NLP techniques to improve the accuracy and efficiency of financial risk detection.",
                "authors": "Liyang Wang, Yu Cheng, Ao Xiang, Jingyu Zhang, Haowei Yang",
                "citations": 8
            },
            {
                "title": "DNABERT-S: Pioneering Species Differentiation with Species-Aware DNA Embeddings",
                "abstract": "We introduce DNABERT-S, a tailored genome model that develops species-aware embeddings to naturally cluster and segregate DNA sequences of different species in the embedding space. Differentiating species from genomic sequences (i.e., DNA and RNA) is vital yet challenging, since many real-world species remain uncharacterized, lacking known genomes for reference. Embedding-based methods are therefore used to differentiate species in an unsupervised manner. DNABERT-S builds upon a pre-trained genome foundation model named DNABERT-2. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enhance it with the proposed Curriculum Contrastive Learning (C2LR) strategy. Empirical results on 23 diverse datasets show DNABERT-S’s effectiveness, especially in realistic label-scarce scenarios. For example, it identifies twice more species from a mixture of unlabeled genomic sequences, doubles the Adjusted Rand Index (ARI) in species clustering, and outperforms the top baseline’s performance in 10-shot species classification with just a 2-shot training. Model, codes, and data is publicly available at https://github.com/MAGlCS-LAB/DNABERT_S.",
                "authors": "Zhihan Zhou, Weimin Wu, Harrison Ho, Jiayi Wang, Lizhen Shi, R. Davuluri, Zhong Wang, Han Liu",
                "citations": 8
            },
            {
                "title": "Autonomous last-mile delivery robots: a literature review",
                "abstract": null,
                "authors": "Elin Alverhed, Simon Hellgren, Hanna Isaksson, Lisa Olsson, Hanna Palmqvist, Jonas Flodén",
                "citations": 19
            },
            {
                "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining",
                "abstract": "We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences. Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation. Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general multimodal capabilities. Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT), transforming Lumina-mGPT into a foundation model that seamlessly achieves omnipotent task unification. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like flexible text-to-image generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multiturn visual question answering. Additionally, we analyze the differences and similarities between diffusion-based and autoregressive methods in a direct comparison.",
                "authors": "Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, Peng Gao",
                "citations": 19
            },
            {
                "title": "Analyzing the Impact of Quantum Computing on Current Encryption Techniques",
                "abstract": "As the field of quantum computing progresses, the disruption to traditional encryption methods, which secure vast amounts of sensitive data, becomes an imminent threat, and conventional encryption techniques, primarily based on mathematical complexity, may no longer suffice in the era of quantum supremacy. This research systematically analyzes the vulnerabilities of current encryption standards in the face of advanced quantum computing capabilities, focusing specifically on widely-used cryptographic protocols such as RSA and AES, which are foundational to modern cybersecurity. Employing the SmartPLS method, the study models the interaction between quantum computing power and the robustness of existing encryption techniques, involving simulating quantum attacks on sample cryptographic algorithms to evaluate their quantum resistance. The findings reveal that quantum computing possesses the capacity to significantly compromise traditional encryption methods within the next few decades, with RSA encryption showing substantial vulnerabilities while AES requires considerably larger key sizes to maintain security. This study underscores the urgency for the development of quantum-resistant encryption techniques, critical to safeguarding future digital communication and data integrity, and advocates for a paradigm shift in cryptographic research and practice, emphasizing the need for 'quantum-proof' algorithms. It also contributes to the strategic planning for cybersecurity in the quantum age and provides a methodological framework using SmartPLS for further exploration into the impact of emerging technologies on existing security protocols.",
                "authors": "Rama Azhari, Agita Nisa Salsabila",
                "citations": 18
            },
            {
                "title": "Predicting and improving complex beer flavor through machine learning",
                "abstract": null,
                "authors": "Michiel Schreurs, Supinya Piampongsant, Miguel Roncoroni, Lloyd Cool, Beatriz Herrera-Malaver, Christophe Vanderaa, Florian A Theßeling, Łukasz Kreft, Alexander Botzki, Philippe Malcorps, Luk Daenen, T. Wenseleers, K. Verstrepen",
                "citations": 16
            },
            {
                "title": "Transforming Multidimensional Time Series into Interpretable Event Sequences for Advanced Data Mining",
                "abstract": "This paper introduces a novel spatiotemporal feature representation model designed to address the limitations of traditional methods in multidimensional time series (MTS) analysis. The proposed approach converts MTS into one-dimensional sequences of spatially evolving events, preserving the complex coupling relationships between dimensions. By employing a variable-length tuple mining method, key spatiotemporal features are extracted, enhancing the interpretability and accuracy of time series analysis. Unlike conventional models, this unsupervised method does not rely on large training datasets, making it adaptable across different domains. Experimental results from motion sequence classification validate the model's superior performance in capturing intricate patterns within the data. The proposed framework has significant potential for applications across various fields, including backend services for monitoring and optimizing IT infrastructure, medical diagnosis through continuous patient monitoring and health trend analysis, and internet businesses for tracking user behavior and forecasting sales. This work offers a new theoretical foundation and technical support for advancing time series data mining and its practical applications in human behavior recognition and other domains.",
                "authors": "Xu Yan, Yaoting Jiang, Wenyi Liu, Didi Yi, Jianjun Wei",
                "citations": 17
            },
            {
                "title": "H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation",
                "abstract": "In the field of medical image segmentation, variant models based on Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs) as the base modules have been very widely developed and applied. However, CNNs are often limited in their ability to deal with long sequences of information, while the low sensitivity of ViTs to local feature information and the problem of secondary computational complexity limit their development. Recently, the emergence of state-space models (SSMs), especially 2D-selective-scan (SS2D), has had an impact on the longtime dominance of traditional CNNs and ViTs as the foundational modules of visual neural networks. In this paper, we extend the adaptability of SS2D by proposing a High-order Vision Mamba UNet (H-vmunet) for medical image segmentation. Among them, the proposed High-order 2D-selective-scan (H-SS2D) progressively reduces the introduction of redundant information during SS2D operations through higher-order interactions. In addition, the proposed Local-SS2D module improves the learning ability of local features of SS2D at each order of interaction. We conducted comparison and ablation experiments on three publicly available medical image datasets (ISIC2017, Spleen, and CVC-ClinicDB), and the results all demonstrate the strong competitiveness of H-vmunet in medical image segmentation tasks. The code is available from https://github.com/wurenkai/H-vmunet .",
                "authors": "Renkai Wu, Yinghao Liu, Pengchen Liang, Qing Chang",
                "citations": 17
            },
            {
                "title": "An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project",
                "abstract": "Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student’s perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration.CCS CONCEPTS• Software and its engineering → Software development techniques; • Applied computing → Education.",
                "authors": "Sanka Rasnayaka, Guanlin Wang, Ridwan Shariffdeen, Ganesh Neelakanta Iyer",
                "citations": 17
            },
            {
                "title": "Multi-purpose RNA language modelling with motif-aware pretraining and type-guided fine-tuning",
                "abstract": null,
                "authors": "Ning Wang, Jiang Bian, Yuchen Li, Xuhong Li, Shahid Mumtaz, Linghe Kong, Haoyi Xiong",
                "citations": 16
            },
            {
                "title": "The application of Augmented Reality (AR) in Remote Work and Education",
                "abstract": "With the rapid advancement of technology, Augmented Reality (AR) technology, known for its ability to deeply integrate virtual information with the real world, is gradually transforming traditional work modes and teaching methods. Particularly in the realms of remote work and online education, AR technology demonstrates a broad spectrum of application prospects. This paper delves into the application potential and actual effects of AR technology in remote work and education. Through a systematic literature review, this study outlines the key features, advantages, and challenges of AR technology. Based on theoretical analysis, it discusses the scientific basis and technical support that AR technology provides for enhancing remote work efficiency and promoting innovation in educational teaching models. Additionally, by designing an empirical research plan and analyzing experimental data, this article reveals the specific performance and influencing factors of AR technology in practical applications. Finally, based on the results of the experiments, this research summarizes the application value of AR technology in remote work and education, looks forward to its future development trends, and proposes forward-looking research directions and strategic suggestions, offering empirical foundation and theoretical guidance for further promoting the in-depth application of AR technology in related fields.",
                "authors": "Keqin Li, Peng Xirui, Jintong Song, Bo Hong, Jin Wang",
                "citations": 15
            },
            {
                "title": "M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought",
                "abstract": "Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) Domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M$^3$CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M$^3$CoT and there remains a large gap between existing VLLMs and human performance in M$^3$CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M$^3$CoT can serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.",
                "authors": "Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, Wanxiang Che",
                "citations": 17
            },
            {
                "title": "Multi-purpose RNA language modelling with motif-aware pretraining and type-guided fine-tuning",
                "abstract": null,
                "authors": "Ning Wang, Jiang Bian, Yuchen Li, Xuhong Li, Shahid Mumtaz, Linghe Kong, Haoyi Xiong",
                "citations": 16
            },
            {
                "title": "SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity, ECG and Respiratory Signals",
                "abstract": "Sleep is a complex physiological process evaluated through various modalities recording electrical brain, cardiac, and respiratory activities. We curate a large polysomnography dataset from over 14,000 participants comprising over 100,000 hours of multi-modal sleep recordings. Leveraging this extensive dataset, we developed SleepFM, the first multi-modal foundation model for sleep analysis. We show that a novel leave-one-out approach for contrastive learning significantly improves downstream task performance compared to representations from standard pairwise contrastive learning. A logistic regression model trained on SleepFM's learned embeddings outperforms an end-to-end trained convolutional neural network (CNN) on sleep stage classification (macro AUROC 0.88 vs 0.72 and macro AUPRC 0.72 vs 0.48) and sleep disordered breathing detection (AUROC 0.85 vs 0.69 and AUPRC 0.77 vs 0.61). Notably, the learned embeddings achieve 48% top-1 average accuracy in retrieving the corresponding recording clips of other modalities from 90,000 candidates. This work demonstrates the value of holistic multi-modal sleep modeling to fully capture the richness of sleep recordings. SleepFM is open source and available at https://github.com/rthapa84/sleepfm-codebase.",
                "authors": "Rahul Thapa, Bryan He, Magnus Ruud Kjær, IV HyattE.Moore, Gauri Ganjoo, Emmanuel Mignot, James Zou",
                "citations": 7
            },
            {
                "title": "The Science of Virtue",
                "abstract": "Integrating psychological and philosophical research on virtue and moral development, this book presents a real-world program for virtue science. Offering empirically testable hypotheses, the chapters deliver theoretical and methodological guidance that shows how existing research can become a cohesive and truly interdisciplinary science of virtue. The authors' unique 'STRIVE-4 Model' defines a unifying conceptual framework, making the book an indispensable resource for a new generation of scholars and students. This empirically tested model provides the much-needed foundation that can put to rest traditional worries about moral science. While mapping out the relevant areas of psychology and value-focused inquiry, the book lays out an interdisciplinary approach to many questions, including the problem of knowledge about character. Written for those researching virtue drawing on personality, developmental, moral, and positive psychology, as well as moral philosophy and character education, the book demonstrates the importance and applications of studying virtues empirically.",
                "authors": "B. Fowers, Bradford Cokelet, Nathan D. Leonhardt",
                "citations": 7
            },
            {
                "title": "Circadian disruption, clock genes, and metabolic health",
                "abstract": "A growing body of research has identified circadian-rhythm disruption as a risk factor for metabolic health. However, the underlying biological basis remains complex, and complete molecular mechanisms are unknown. There is emerging evidence from animal and human research to suggest that the expression of core circadian genes, such as circadian locomotor output cycles kaput gene (CLOCK), brain and muscle ARNT-Like 1 gene (BMAL1), period (PER), and cyptochrome (CRY), and the consequent expression of hundreds of circadian output genes are integral to the regulation of cellular metabolism. These circadian mechanisms represent potential pathophysiological pathways linking circadian disruption to adverse metabolic health outcomes, including obesity, metabolic syndrome, and type 2 diabetes. Here, we aim to summarize select evidence from in vivo animal models and compare these results with epidemiologic research findings to advance understanding of existing foundational evidence and potential mechanistic links between circadian disruption and altered clock gene expression contributions to metabolic health–related pathologies. Findings have important implications for the treatment, prevention, and control of metabolic pathologies underlying leading causes of death and disability, including diabetes, cardiovascular disease, and cancer.",
                "authors": "Lauren A. Schrader, Sean M. Ronnekleiv-Kelly, J. Hogenesch, Christopher A Bradfield, K. Malecki",
                "citations": 14
            },
            {
                "title": "MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds",
                "abstract": "We introduce 4D Motion Scaffolds (MoSca), a modern 4D reconstruction system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models and lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions/deformations. The scene geometry and appearance are then disentangled from the deformation field and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera focal length and poses can be solved using bundle adjustment without the need of any other pose estimation tools. Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks and its effectiveness on real videos.",
                "authors": "Jiahui Lei, Yijia Weng, Adam Harley, Leonidas J. Guibas, Kostas Daniilidis",
                "citations": 14
            },
            {
                "title": "Spectroscopy and modeling of $^{171}$Yb Rydberg states for high-fidelity two-qubit gates",
                "abstract": "Highly excited Rydberg states and their interactions play an important role in quantum computing and simulation. These properties can be predicted accurately for alkali atoms with simple Rydberg level structures. However, an extension of these methods to more complex atoms such as alkaline-earth atoms has not been demonstrated or experimentally validated. Here, we present multichannel quantum defect (MQDT) models for highly excited $^{174}$Yb and $^{171}$Yb Rydberg states with $L \\leq 2$. The models are developed using a combination of existing literature data and new, high-precision laser and microwave spectroscopy in an atomic beam, and validated by detailed comparison with experimentally measured Stark shifts and magnetic moments. We then use these models to compute interaction potentials between two Yb atoms, and find excellent agreement with direct measurements in an optical tweezer array. From the computed interaction potential, we identify an anomalous F\\\"orster resonance that likely degraded the fidelity of previous entangling gates in $^{171}$Yb using $F=3/2$ Rydberg states. We then identify a more suitable $F=1/2$ state, and achieve a state-of-the-art controlled-Z gate fidelity of $F=0.994(1)$, with the remaining error fully explained by known sources. This work establishes a solid foundation for the continued development of quantum computing, simulation and entanglement-enhanced metrology with Yb neutral atom arrays.",
                "authors": "Michael Peper, Yiyi Li, Daniel Y. Knapp, Mila Bileska, Shuo Ma, Genyue Liu, Pai Peng, Bichen Zhang, Sebastian P. Horvath, Alex P. Burgers, Jeff D. Thompson",
                "citations": 13
            },
            {
                "title": "Deep learning-based retinal abnormality detection from OCT images with limited data",
                "abstract": "In the realm of medical diagnosis, the challenge posed by retinal diseases is considerable, given their potential to complicate vision and overall ocular health. A promising avenue for achieving highly accurate classifiers in detecting retinal diseases involves the application of deep learning models. However, overfitting issues often undermine the performance of these models due to the scarcity of image samples in retinal disease datasets. To address this challenge, a novel deep triplet network is proposed as a metric learning approach for detecting retinal diseases using Optical Coherence Tomography (OCT) images. Incorporating a conditional loss function tailored to the constraints of limited data samples, this deep triplet network enhances the model’s accuracy. Drawing inspiration from pre-trained models such as VGG16, the foundational architecture of our model is established. Experiments use open-access datasets comprising retinal OCT images to validate our proposed approach. The performance of the suggested model is demonstrated to surpass that of state-of-the-art models in terms of accuracy. This substantiates the effectiveness of the deep triplet network in addressing overfitting issues associated with limited data samples in retinal disease datasets.",
                "authors": "Mohammad Talebzadeh, Abolfazl Sodagartojgi, Zahra Moslemi, Sara Sedighi, Behzad Kazemi, Faezeh Akbari",
                "citations": 12
            },
            {
                "title": "ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching",
                "abstract": "The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-design solution to address the challenges imposed by KV caching. On the algorithm level, ALISA prioritizes tokens that are most important in generating a new token via a Sparse Window Attention (SWA) algorithm. SWA introduces high sparsity in attention layers and reduces the memory footprint of KV caching at negligible accuracy loss. On the system level, ALISA employs three-phase token-level dynamical scheduling and optimizes the trade-off between caching and recomputation, thus maximizing the overall performance in resource-constrained systems. In a single GPU-CPU system, we demonstrate that under varying workloads, ALISA improves the throughput of baseline systems such as FlexGen and vLLM by up to $3 \\times$ and $1.9 \\times$, respectively.",
                "authors": "Youpeng Zhao, Di Wu, Jun Wang",
                "citations": 12
            },
            {
                "title": "Matching Anything by Segmenting Anything",
                "abstract": "The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially multiple object tracking (MOT). Current methods predominantly rely on labeled domain-specific video datasets, which limits the cross-domain generalization of learned similarity embeddings. We propose MASA, a novel method for robust instance association learning, capable of matching any objects within videos across diverse domains without tracking labels. Leveraging the rich object segmentation from the Segment Anything Model (SAM), MASA learns instance-level correspondence through exhaustive data transformations. We treat the SAM outputs as dense object region proposals and learn to match those regions from a vast image collection. We further design a universal MASA adapter which can work in tandem with foundational segmentation or detection models and enable them to track any detected objects. Those combinations present strong zero-shot tracking ability in complex domains. Extensive tests on multiple challenging MOT and MOTS benchmarks indicate that the proposed method, using only unlabeled static images, achieves even better performance than state-of-the-art methods trained with fully annotated in-domain video sequences, in zero-shot association. Our code is available at github.com/siyuanliii/masa.",
                "authors": "Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, L. V. Gool, Fisher Yu",
                "citations": 12
            },
            {
                "title": "Ensemble Methodology: Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble",
                "abstract": "In the realm of consumer lending, accurate credit default prediction stands as a critical element in risk mitigation and lending decision optimization. Extensive research has sought continuous improvement in existing models to enhance customer experiences and ensure the sound economic functioning of lending institutions. This study responds to the evolving landscape of credit default prediction, challenging conventional models and introducing innovative approaches. By building upon foundational research and recent innovations, our work aims to redefine the standards of accuracy in credit default prediction, setting a new benchmark for the industry. To overcome these challenges, we present an Ensemble Methods framework comprising LightGBM, XGBoost, and LocalEnsemble modules, each making unique contributions to amplify diversity and improve generalization. By utilizing distinct feature sets, our methodology directly tackles limitations identified in previous studies, with the overarching goal of establishing a novel standard for credit default prediction accuracy. Our experimental findings validate the effectiveness of the ensemble model on the dataset, signifying substantial contributions to the field. This innovative approach not only addresses existing obstacles but also sets a precedent for advancing the accuracy and robustness of credit default prediction models.",
                "authors": "Mengran Zhu, Ye Zhang, Yulu Gong, Kaijuan Xing, Xu Yan, Jintong Song",
                "citations": 12
            },
            {
                "title": "ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation",
                "abstract": "Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple scales of CLIP to reason anomaly positions. Then, we design a novel Multi-level Mask Refinement (MMR) module, which utilizes the positional information as multi-level prompts for SAM to acquire hierarchical levels of masks and merges them. Extensive experiments validate the effectiveness of our approach, achieving the optimal segmentation performance on the MVTec-AD and VisA datasets.",
                "authors": "Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu, Tao Chen",
                "citations": 13
            },
            {
                "title": "Towards Distributed and Intelligent Integrated Sensing and Communications for 6G Networks",
                "abstract": "This paper introduces the distributed and intelligent integrated sensing and communications (DISAC) concept, a transformative approach for 6G wireless networks that extends the emerging concept of integrated sensing and communications (ISAC). DISAC addresses the limitations of the existing ISAC models and, to overcome them, it introduces two novel foundational functionalities for both sensing and communications: a distributed architecture (enabling large-scale and energy-efficient tracking of connected users and objects, leveraging the fusion of heterogeneous sensors) and a semantic and goal-oriented framework (enabling the transition from classical data fusion to the composition of semantically selected information).",
                "authors": "E. Strinati, G. C. Alexandropoulos, N. Amani, Maurizio Crozzoli, Giyyarpuram Madhusudan, Sami Mekki, Francois Rivet, Vincenzo Sciancalepore, Philippe Sehier, Maximilian Stark, H. Wymeersch",
                "citations": 13
            },
            {
                "title": "IDGenRec: LLM-RecSys Alignment with Textual ID Learning",
                "abstract": "Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potential for a foundational generative recommendation model. Experiments show that our framework consistently surpasses existing models in sequential recommendation under standard experimental setting. Then, we explore the possibility of training a foundation recommendation model with the proposed method on data collected from 19 different datasets and tested its recommendation performance on 6 unseen datasets across different platforms under a completely zero-shot setting. The results show that the zero-shot performance of the pre-trained foundation model is comparable to or even better than some traditional recommendation models based on supervised training, showing the potential of the IDGen paradigm serving as the foundation model for generative recommendation. Code and data are open-sourced at https://github.com/agiresearch/IDGenRec.",
                "authors": "Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, Yongfeng Zhang",
                "citations": 12
            },
            {
                "title": "Shapley value: from cooperative game to explainable artificial intelligence",
                "abstract": null,
                "authors": "Meng Li, Hengyang Sun, Yanjun Huang, Hong Chen",
                "citations": 13
            },
            {
                "title": "Enhancing Crop Mapping through Automated Sample Generation Based on Segment Anything Model with Medium-Resolution Satellite Imagery",
                "abstract": "Crop mapping using satellite imagery is crucial for agriculture applications. However, a fundamental challenge that hinders crop mapping progress is the scarcity of samples. The latest foundation model, Segment Anything Model (SAM), provides an opportunity to address this issue, yet few studies have been conducted in this area. This study investigated the parcel segmentation performance of SAM on commonly used medium-resolution satellite imagery (i.e., Sentinel-2 and Landsat-8) and proposed a novel automated sample generation framework based on SAM. The framework comprises three steps. First, an image optimization automatically selects high-quality images as the inputs for SAM. Then, potential samples are generated based on the masks produced by SAM. Finally, the potential samples are subsequently subjected to a sample cleaning procedure to acquire the most reliable samples. Experiments were conducted in Henan Province, China, and southern Ontario, Canada, using six proven effective classifiers. The effectiveness of our method is demonstrated through the combination of field-survey-collected samples and differently proportioned generated samples. Our results indicated that directly using SAM for parcel segmentation remains challenging, unless the parcels are large, regular in shape, and have distinct color differences from surroundings. Additionally, the proposed approach significantly improved the performance of classifiers and alleviated the sample scarcity problem. Compared to classifiers trained only by field-survey-collected samples, our method resulted in an average improvement of 16% and 78.5% in Henan and Ontario, respectively. The random forest achieved relatively good performance, with weighted-average F1 of 0.97 and 0.996 obtained using Sentinel-2 imagery in the two study areas, respectively. Our study contributes insights into solutions for sample scarcity in crop mapping and highlights the promising application of foundation models like SAM.",
                "authors": "Jialin Sun, Shuai Yan, T. Alexandridis, X. Yao, Han Zhou, Bingbo Gao, Jianxi Huang, Jianyu Yang, Ying Li",
                "citations": 6
            },
            {
                "title": "A comprehensive review of numerical simulation methods for hydraulic fracturing",
                "abstract": "Hydraulic fracturing unlocks previously inaccessible hydrocarbons in unconventional reservoirs by creating artificial pathways in the unconventional reservoir. Numerical simulation expands the scope of hydraulic fracturing design for various reservoir conditions. This review paper explores the synergy between numerical simulation and hydraulic fracturing modeling, focusing on critical elements like geomechanical behavior, geological conditions, and fluid dynamics. Analytical models in hydraulic fracturing design are discussed to underscore their foundational importance. The assumption of constant fracture height limits the application of Perkins–Kern–Nordgren model (PKN) and Kristianovich‐Geertsma‐de Klerk model (KGD). Radial models assume 3D flow but lack reliability in nonradial settings, and the Pseudo 3D model (P3D) shares PKN's assumptions with variable fracture height, sacrificing some details for efficiency. Planar 3D model (PL3D) enhances accuracy by discarding PKN's elastic response assumption but requires extended computation. Unconventional fracture model is effective for complex scenarios but relies on DFN modeling parameters for accuracy. The choice of numerical simulation method in hydraulic fracturing depends on the specific aspect studied, each with its strengths and limitations. For instance, boundary element methods are efficient for exterior problems, finite element modeling suits 3D nonplanar fractures, and the extended finite element method excels in hydraulic and natural fracture interactions. Peridynamics shows potential but needs further development for cost‐effectiveness. PFC and UDEC/3DEC‐based simulations can explore microscopic mechanisms. Combining these methods with other approaches provides a comprehensive study of realistic reservoir conditions. This review guides the selection of a suitable numerical simulation methodology based on the study's scope.",
                "authors": "Atif Ismail, Saman Azadbakht",
                "citations": 11
            },
            {
                "title": "(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection",
                "abstract": "In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes.",
                "authors": "Francesco Periti, Haim Dubossarsky, Nina Tahmasebi",
                "citations": 10
            },
            {
                "title": "Unbiased construction of constitutive relations for soft materials from experiments via rheology-informed neural networks",
                "abstract": "Significance Development of phenomenological constitutive relations that describe the stress response of soft materials to an imposed deformation is commonly associated with generalizations and idealized assumptions (biases). Thus, science-based data-driven methods capable of describing the physical dynamical behavior of soft materials from limited experiments can create a new paradigm in how constitutive models are constructed in general, and in new fundamental physics discovery. In this work, through a concerted theoretical, experimental, and data-driven approach, rheology-informed neural networks are developed for unbiased construction of rheologically relevant constitutive models, without compromising rigor or foundational sciences. The platform developed here is general enough that it can be extended to areas well beyond complex fluids or soft matter physics and across other disciplines as well.",
                "authors": "Mohammadamin Mahmoudabadbozchelou, Krutarth M. Kamani, Simon A. Rogers, S. Jamali",
                "citations": 11
            },
            {
                "title": "GOA-optimized deep learning for soybean yield estimation using multi-source remote sensing data",
                "abstract": null,
                "authors": "Jian Lu, Hongkun Fu, Xuhui Tang, Zhao Liu, Jujian Huang, Wenlong Zou, Hui Chen, Yue Sun, Xiangyu Ning, Jian Li",
                "citations": 11
            },
            {
                "title": "PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation",
                "abstract": "Recently, the scale of transformers has grown rapidly, which introduces considerable challenges in terms of training overhead and inference efficiency in the scope of task adaptation. Existing works, namely Parameter-Efficient Fine-Tuning (PEFT) and model compression, have separately investigated the challenges. However, PEFT cannot guarantee the inference efficiency of the original backbone, especially for large-scale models. Model compression requires significant training costs for structure searching and re-training. Consequently, a simple combination of them cannot guarantee accomplishing both training efficiency and inference efficiency with minimal costs. In this paper, we propose a novel Parallel Yielding Re-Activation (PYRA) method for such a challenge of training-inference efficient task adaptation. PYRA first utilizes parallel yielding adaptive weights to comprehensively perceive the data distribution in downstream tasks. A re-activation strategy for token modulation is then applied for tokens to be merged, leading to calibrated token features. Extensive experiments demonstrate that PYRA outperforms all competing methods under both low compression rate and high compression rate, demonstrating its effectiveness and superiority in maintaining both training efficiency and inference efficiency for large-scale foundation models. Our code is available at https://github.com/THU-MIG/PYRA.",
                "authors": "Yizhe Xiong, Hui Chen, Tianxiang Hao, Zijia Lin, Jungong Han, Yuesong Zhang, Guoxin Wang, Yongjun Bao, Guiguang Ding",
                "citations": 10
            },
            {
                "title": "A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning",
                "abstract": "The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to a particular domain or task. Model MoErging methods aim to recycle expert models to create an aggregate system with improved performance or generalization. A key component of MoErging methods is the creation of a router that decides which expert model(s) to use for a particular input or application. The promise, effectiveness, and large design space of MoErging has spurred the development of many new methods over the past few years. This rapid pace of development has made it challenging to compare different MoErging methods, which are rarely compared to one another and are often validated in different experimental setups. To remedy such gaps, we present a comprehensive survey of MoErging methods that includes a novel taxonomy for cataloging key design choices and clarifying suitable applications for each method. Apart from surveying MoErging research, we inventory software tools and applications that make use of MoErging. We additionally discuss related fields of study such as model merging, multitask learning, and mixture-of-experts models. Taken as a whole, our survey provides a unified overview of existing MoErging methods and creates a solid foundation for future work in this burgeoning field.",
                "authors": "Prateek Yadav, Colin Raffel, Mohammed Muqeeth, Lucas Caccia, Haokun Liu, Tian-Xiang Chen, Mohit Bansal, Leshem Choshen, Alessandro Sordoni",
                "citations": 10
            },
            {
                "title": "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception",
                "abstract": "The pervasive spread of misinformation and disinformation in social media underscores the critical importance of detecting media bias. While robust Large Language Models (LLMs) have emerged as foundational tools for bias prediction, concerns about inherent biases within these models persist. In this work, we investigate the presence and nature of bias within LLMs and its consequential impact on media bias detection. Departing from conventional approaches that focus solely on bias detection in media content, we delve into biases within the LLM systems themselves. Through meticulous examination, we probe whether LLMs exhibit biases, particularly in political bias prediction and text continuation tasks. Additionally, we explore bias across diverse topics, aiming to uncover nuanced variations in bias expression within the LLM framework. Importantly, we propose debiasing strategies, including prompt engineering and model fine-tuning. Extensive analysis of bias tendencies across different LLMs sheds light on the broader landscape of bias propagation in language models. This study advances our understanding of LLM bias, offering critical insights into its implications for bias detection tasks and paving the way for more robust and equitable AI systems",
                "authors": "Luyang Lin, Lingzhi Wang, Jinsong Guo, Kam-Fai Wong",
                "citations": 11
            },
            {
                "title": "AlignSAM: Aligning Segment Anything Model to Open Context via Reinforcement Learning",
                "abstract": "Powered by massive curated training data, Segment Any-thing Model (SAM) has demonstrated its impressive generalization capabilities in open-world scenarios with the guidance of prompts. However, the vanilla SAM is class-agnostic and heavily relies on user-provided prompts to segment objects of interest. Adapting this method to diverse tasks is crucial for accurate target identification and to avoid suboptimal segmentation results. In this paper, we propose a novel framework, termed AlignSAM, designed for automatic prompting for aligning SAM to an open context through reinforcement learning. Anchored by an agent, AlignSAM enables the generality of the SAM model across diverse downstream tasks while keeping its parameters frozen. Specifically, AlignSAM initiates a prompting agent to iteratively refine segmentation predictions by interacting with the foundational model. It integrates a reinforcement learning policy network to provide informative prompts to the foundational models. Additionally, a semantic recal-ibration module is introduced to provide fine-grained labels of prompts, enhancing the model's proficiency in handling tasks encompassing explicit and implicit semantics. Experiments conducted on various challenging segmentation tasks among existing foundation models demonstrate the superiority of the proposed AlignSAM over state-of-the-art approaches. Project page: https://github.com/Duojun-Huang/AIignSAM-CVPR2024.",
                "authors": "Duojun Huang, Xinyu Xiong, Jie Ma, Jichang Li, Zequn Jie, Lin Ma, Guanbin Li",
                "citations": 10
            },
            {
                "title": "A Deep Convolutional Neural Network for Pneumonia Detection in X-ray Images with Attention Ensemble",
                "abstract": "In the domain of AI-driven healthcare, deep learning models have markedly advanced pneumonia diagnosis through X-ray image analysis, thus indicating a significant stride in the efficacy of medical decision systems. This paper presents a novel approach utilizing a deep convolutional neural network that effectively amalgamates the strengths of EfficientNetB0 and DenseNet121, and it is enhanced by a suite of attention mechanisms for refined pneumonia image classification. Leveraging pre-trained models, our network employs multi-head, self-attention modules for meticulous feature extraction from X-ray images. The model’s integration and processing efficiency are further augmented by a channel-attention-based feature fusion strategy, one that is complemented by a residual block and an attention-augmented feature enhancement and dynamic pooling strategy. Our used dataset, which comprises a comprehensive collection of chest X-ray images, represents both healthy individuals and those affected by pneumonia, and it serves as the foundation for this research. This study delves deep into the algorithms, architectural details, and operational intricacies of the proposed model. The empirical outcomes of our model are noteworthy, with an exceptional performance marked by an accuracy of 95.19%, a precision of 98.38%, a recall of 93.84%, an F1 score of 96.06%, a specificity of 97.43%, and an AUC of 0.9564 on the test dataset. These results not only affirm the model’s high diagnostic accuracy, but also highlight its promising potential for real-world clinical deployment.",
                "authors": "Qiuyu An, Wei Chen, Wei Shao",
                "citations": 11
            },
            {
                "title": "Tokenization Is More Than Compression",
                "abstract": "Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document’s text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.",
                "authors": "Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, Chris Tanner",
                "citations": 11
            },
            {
                "title": "SamLP: A Customized Segment Anything Model for License Plate Detection",
                "abstract": "With the emergence of foundation model, this novel paradigm of deep learning has encouraged many powerful achievements in natural language processing and computer vision. There are many advantages of foundation model, such as excellent feature extraction power, mighty generalization ability, great few-shot and zero-shot learning capacity, etc. which are beneficial to vision tasks. As the unique identity of vehicle, different countries and regions have diverse license plate (LP) styles and appearances, and even different types of vehicles have different LPs. However, recent deep learning based license plate detectors are mainly trained on specific datasets, and these limited datasets constrain the effectiveness and robustness of LP detectors. To alleviate the negative impact of limited data, an attempt to exploit the advantages of foundation model is implement in this paper. We customize a vision foundation model, i.e. Segment Anything Model (SAM), for LP detection task and propose the first LP detector based on vision foundation model, named SamLP. Specifically, we design a Low-Rank Adaptation (LoRA) fine-tuning strategy to inject extra parameters into SAM and transfer SAM into LP detection task. And then, we further propose a promptable fine-tuning step to provide SamLP with prompatable segmentation capacity. The experiments show that our proposed SamLP achieves promising detection performance compared to other LP detectors. Meanwhile, the proposed SamLP has great few-shot and zero-shot learning ability, which shows the potential of transferring vision foundation model. The code is available at https://github.com/Dinghaoxuan/SamLP",
                "authors": "Haoxuan Ding, Junyuan Gao, Yuan Yuan, Qi Wang",
                "citations": 5
            },
            {
                "title": "Literary and Historical Notes of Pennsylvania Colony: A Model of Multicultural Society",
                "abstract": "American society's multicultural nature stems from its diverse ethnicities, cultures, religions, and denominations underpinned by democratic principles as the foundation of its sociopolitical system. This diversity is not a spontaneous occurrence but the result of a long historical process dating back to the colonial era. The Pennsylvania colony is considered one of the significant contributors to early American multicultural society. This study examines the strategies employed by William Penn, the colony's proprietor, to establish Pennsylvania's multicultural foundation. Utilizing Nash Smith's interdisciplinary approach, which combines historical and literary perspectives, this research identifies three key strategies implemented by Penn to foster equal relationships through friendship: incorporating a tolerance clause in the First Frame of Government, which served as the colony's constitution, and recognizing the life and liberty of each ethnic group. The first strategy aimed to persuade various ethnic groups, particularly Native Americans, to coexist peacefully with European settlers, as illustrated in the poem \"Treaty of Penn.\" The second strategy provides a legal framework to ensure mutual trust among all inhabitants. The third strategy represented the practical application of the second strategy and demonstrated William Penn's commitment, as evidenced in a letter he wrote in London before founding the Pennsylvania colony. These strategies were rooted in Penn's Quaker beliefs, which emphasized the equality of all human beings, regardless of race, gender, nationality, religion, language, or other differences.",
                "authors": "Nuriadi Nuriadi, Eka Fitriana, R. Ilham, Jumadil Saputra",
                "citations": 5
            },
            {
                "title": "Learning the language of antibody hypervariability",
                "abstract": "Protein language models (PLMs) based on machine learning have demon-strated impressive success in predicting protein structure and function. However, general-purpose (“foundational”) PLMs have limited performance in predicting antibodies due to the latter’s hypervariable regions, which do not conform to the evolutionary conservation principles that such models rely on. In this study, we propose a new transfer learning framework called AbMAP, which fine-tunes foundational models for antibody-sequence inputs by supervising on antibody structure and binding specificity examples. Our feature representations accurately predict an antibody’s 3D structure, mutational effects on antigen binding, and paratope identification. AbMAP’s scalability paves the way for large-scale analyses of human antibody repertoires. AbMAP representations of immune repertoires reveal a remarkable overlap across individuals, overcoming the limitations of sequence analyses. Our findings provide compelling evidence for the hypothesis that antibody repertoires of individuals tend to converge towards comparable structural and functional coverage. We validate AbMAP for antibody optimization, applying it to optimize a set of antibodies that bind to a SARS-CoV-2 peptide and obtaining 82% hit-rate and upto 22-fold increase in binding affinity. We anticipate AbMAP will accelerate the efficient design and modeling of antibodies and expedite the discovery of antibody-based therapeutics. Availability:https://github.com/rs239/ablm",
                "authors": "Rohit Singh, Chiho Im, Yu Qiu, Brian C Mackness, Abhinav Gupta, Taylor Sorenson, Samuel Sledzieski, Lena Erlach, Maria Wendt, Y. F. Nanfack, Bryan D. Bryson, Bonnie Berger",
                "citations": 9
            },
            {
                "title": "RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision",
                "abstract": null,
                "authors": "Fernando P'erez-Garc'ia, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, M. Lungren, M. Wetscherek, Noel Codella, Stephanie L. Hyland, Javier Alvarez-Valle, O. Oktay",
                "citations": 9
            },
            {
                "title": "Optimizing Sulfur Emission Control Areas for Shipping",
                "abstract": "The design of emission control areas (ECAs), including ECA width and sulfur limits, plays a central role in reducing sulfur emissions from shipping. To promote sustainable shipping, we investigate an ECA design problem that considers the response of liner shipping companies to ECA designs. We propose a mathematical programming model from the regulator’s perspective to optimize the ECA width and sulfur limit, with the aim of minimizing the total sulfur emissions. Embedded within this regulator’s model, we develop an internal model from the shipping liner’s perspective to determine the detoured voyage, sailing speed, and cargo transport volume with the aim of maximizing the liner’s profit. Then, we develop a tailored hybrid algorithm to solve the proposed models based on the variable neighborhood search meta-heuristic and a proposition. We validate the effectiveness of the proposed methodology through extensive numerical experiments and conduct sensitivity analyses to investigate the effect of important ECA design parameters on the final performance. The proposed methodology is then extended to incorporate heterogeneous settings for sulfur limits, which can help regulators to improve ECA design in the future. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72025103, 71831008, 72201163, 72071173, 72371221, 72394360, 72394362, 72361137001 and HKSAR RGC TRS T32-707/22-N]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2023.0278 .",
                "authors": "Lu Zhen, Dan Zhuge, Shuanglu Zhang, Shuaian Wang, H. Psaraftis",
                "citations": 9
            },
            {
                "title": "Advances and prospects of deep learning for medium-range extreme weather forecasting",
                "abstract": "Abstract. In recent years, deep learning models have rapidly emerged as a stand-alone alternative to physics-based numerical models for medium-range weather forecasting. Several independent research groups claim to have developed deep learning weather forecasts that outperform those from state-of-the-art physics-based models, and operational implementation of data-driven forecasts appears to be drawing near. However, questions remain about the capabilities of deep learning models with respect to providing robust forecasts of extreme weather. This paper provides an overview of recent developments in the field of deep learning weather forecasts and scrutinises the challenges that extreme weather events pose to leading deep learning models. Lastly, it argues for the need to tailor data-driven models to forecast extreme events and proposes a foundational workflow to develop such models.\n",
                "authors": "Leonardo Olivetti, Gabriele Messori",
                "citations": 9
            },
            {
                "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
                "abstract": "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designedtoken, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.",
                "authors": "Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou",
                "citations": 8
            },
            {
                "title": "The Mathematical Modeling of the Host–Virus Interaction in Dengue Virus Infection: A Quantitative Study",
                "abstract": "Infectious diseases, such as Dengue fever, pose a significant public health threat. Developing a reliable mathematical model plays a crucial role in quantitatively elucidating the kinetic characteristics of antibody–virus interactions. By integrating previous models and incorporating the antibody dynamic theory, we have constructed a novel and robust model that can accurately simulate the dynamics of antibodies and viruses based on a comprehensive understanding of immunology principles. It explicitly formulates the viral clearance effect of antibodies, along with the positive feedback stimulation of virus–antibody complexes on antibody regeneration. In addition to providing quantitative insights into the dynamics of antibodies and viruses, the model exhibits a high degree of accuracy in capturing the kinetics of viruses and antibodies in Dengue fever patients. This model offers a valuable solution to modeling the differences between primary and secondary Dengue infections concerning IgM/IgG antibodies. Furthermore, it demonstrates that a faster removal rate of antibody–virus complexes might lead to a higher peak viral loading and worse clinical symptom. Moreover, it provides a reasonable explanation for the antibody-dependent enhancement of heterogeneous Dengue infections. Ultimately, this model serves as a foundation for constructing an optimal mathematical model to combat various infectious diseases in the future.",
                "authors": "Zhaobin Xu, Hongmei Zhang, Dongying Yang, Dongqing Wei, Jacques Demongeot, Qiangcheng Zeng",
                "citations": 8
            },
            {
                "title": "Video Diffusion Alignment via Reward Gradients",
                "abstract": "We have made significant progress towards building foundational video diffusion models. As these models are trained using large-scale unsupervised data, it has become crucial to adapt these models to specific downstream tasks. Adapting these models via supervised fine-tuning requires collecting target datasets of videos, which is challenging and tedious. In this work, we utilize pre-trained reward models that are learned via preferences on top of powerful vision discriminative models to adapt video diffusion models. These models contain dense gradient information with respect to generated RGB pixels, which is critical to efficient learning in complex search spaces, such as videos. We show that backpropagating gradients from these reward models to a video diffusion model can allow for compute and sample efficient alignment of the video diffusion model. We show results across a variety of reward models and video diffusion models, demonstrating that our approach can learn much more efficiently in terms of reward queries and computation than prior gradient-free approaches. Our code, model weights,and more visualization are available at https://vader-vid.github.io.",
                "authors": "Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, Deepak Pathak",
                "citations": 8
            },
            {
                "title": "LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning",
                "abstract": "Understanding human instructions to identify the target objects is vital for perception systems. In recent years, the advancements of Large Language Models (LLMs) have introduced new possibilities for image segmentation. In this work, we delve into reasoning segmentation, a novel task that enables segmentation system to reason and interpret implicit user intention via large language model reasoning and then segment the corresponding target. Our work on reasoning segmentation contributes on both the methodological design and dataset labeling. For the model, we propose a new framework named LLM-Seg. LLM-Seg effectively connects the current foundational Segmentation Anything Model and the LLM by mask proposals selection. For the dataset, we propose an automatic data generation pipeline and construct a new reasoning segmentation dataset named LLM-Seg40K. Experiments demonstrate that our LLM-Seg exhibits competitive performance compared with existing methods. Furthermore, our proposed pipeline can efficiently produce high-quality reasoning segmentation datasets. The LLM-Seg40K dataset, developed through this pipeline, serves as a new benchmark for training and evaluating various reasoning segmentation approaches. Our code, models and dataset are at https://github.com/wangjunchi/LLMSeg.",
                "authors": "Junchi Wang, Lei Ke",
                "citations": 8
            },
            {
                "title": "PointRWKV: Efficient RWKV-Like Model for Hierarchical Point Cloud Learning",
                "abstract": "Transformers have revolutionized the point cloud learning task, but the quadratic complexity hinders its extension to long sequence and makes a burden on limited computational resources. The recent advent of RWKV, a fresh breed of deep sequence models, has shown immense potential for sequence modeling in NLP tasks. In this paper, we present PointRWKV, a model of linear complexity derived from the RWKV model in the NLP field with necessary modifications for point cloud learning tasks. Specifically, taking the embedded point patches as input, we first propose to explore the global processing capabilities within PointRWKV blocks using modified multi-headed matrix-valued states and a dynamic attention recurrence mechanism. To extract local geometric features simultaneously, we design a parallel branch to encode the point cloud efficiently in a fixed radius near-neighbors graph with a graph stabilizer. Furthermore, we design PointRWKV as a multi-scale framework for hierarchical feature learning of 3D point clouds, facilitating various downstream tasks. Extensive experiments on different point cloud learning tasks show our proposed PointRWKV outperforms the transformer- and mamba-based counterparts, while significantly saving about 42\\% FLOPs, demonstrating the potential option for constructing foundational 3D models.",
                "authors": "Qingdong He, Jiangning Zhang, Jinlong Peng, Haoyang He, Yabiao Wang, Chengjie Wang",
                "citations": 8
            },
            {
                "title": "Spectrum-Energy-Efficient Mode Selection and Resource Allocation for Heterogeneous V2X Networks: A Federated Multi-Agent Deep Reinforcement Learning Approach",
                "abstract": "Heterogeneous communication environments and broadcast feature of safety-critical messages bring great challenges to mode selection and resource allocation problem. In this paper, we propose a federated multi-agent deep reinforcement learning (DRL) scheme with action awareness to solve mode selection and resource allocation problem for ensuring quality of service (QoS) in heterogeneous V2X environments. The proposed scheme includes an action-observation-based DRL and a model parameter aggregation algorithm considering local model historical parameters. By observing the actions of adjacent agents and dynamically balancing the historical samples of rewards, the action-observation-based DRL can ensure fast convergence of each agent’ individual model. By randomly sampling historical model parameters and adding them to the foundation model aggregation process, the model parameter aggregation algorithm improves foundation model generalization. The generalized model is only sent to each new agent, so each old agent can retain the personality of its individual model. Simulation results show that the proposed scheme outperforms the comparison algorithms in the key performance indicators.",
                "authors": "Jinsong Gui, Liyan Lin, Xiaoheng Deng, Lin Cai",
                "citations": 4
            },
            {
                "title": "Patronus charm: a comparison of benefactor plants and climate mediation effects on diversity",
                "abstract": "Deserts are subject to significant anthropogenic pressure. The capacity to buffer against changes in the local environment and biodiversity are critical for ecosystem functioning. Foundation species can be a solution to rapidly assess ecological function and provide a simple nature‐based solution to protect against continuing biodiversity losses. A foundation species is defined as a species that exerts and promotes a positive set of processes for the biotic network. Two different shrub species in the central drylands of California were used to assay a potential buffer for plant species richness and to examine the species‐specificity of foundation facilitation. A five‐year dataset in two distinct regions differing in aridity was used to test the hypothesis that the direct effects of foundation plants facilitate other plant species and buffer diversity losses to a changing climate. The predicted positive effects of both shrub species on species richness increased with increasing local temperatures sampled. Finally, projected temperature increases for the region in trained Bayesian models demonstrated that both shrub species can profoundly increase in their capacity to facilitate plant species richness. Colloquially, this positive ecological effect can be described as the patronus charm hypothesis because regardless of the form of the protector, shrub species provided a talisman against local loss of richness driven by temperature increases.",
                "authors": "C. Lortie, Amanda R. Liczner, Ally Ruttan, J. Braun, Diego A. Sotomayor, M. Westphal, Rachel King, Alessandro Filazzola",
                "citations": 4
            },
            {
                "title": "Recognition Model for Tea Grading and Counting Based on the Improved YOLOv8n",
                "abstract": "Grading tea leaves efficiently in a natural environment is a crucial technological foundation for the automation of tea-picking robots. In this study, to solve the problems of dense distribution, limited feature-extraction ability, and false detection in the field of tea grading recognition, an improved YOLOv8n model for tea grading and counting recognition was proposed. Firstly, the SPD-Conv module was embedded into the backbone of the network model to enhance the deep feature-extraction ability of the target. Secondly, the Super-Token Vision Transformer was integrated to reduce the model’s attention to redundant information, thus improving its perception ability for tea. Subsequently, the loss function was improved to MPDIoU, which accelerated the convergence speed and optimized the performance. Finally, a classification-positioning counting function was added to achieve the purpose of classification counting. The experimental results showed that, compared to the original model, the precision, recall and average precision improved by 17.6%, 19.3%, and 18.7%, respectively. The average precision of single bud, one bud with one leaf, and one bud with two leaves were 88.5%, 89.5% and 89.1%. In this study, the improved model demonstrated strong robustness and proved suitable for tea grading and edge-picking equipment, laying a solid foundation for the mechanization of the tea industry.",
                "authors": "Yuxin Xia, Zejun Wang, Zhiyong Cao, Yaping Chen, Limei Li, Lijiao Chen, Shihao Zhang, Chun Wang, Hongxu Li, Baijuan Wang",
                "citations": 4
            },
            {
                "title": "P2P: Transforming from Point Supervision to Explicit Visual Prompt for Object Detection and Segmentation",
                "abstract": "Point-supervised vision tasks, including detection and segmentation, aiming to learn a network that transforms from points to pseudo labels, have attracted much attention in recent years. However, the lack of precise object size and boundary annotations in the point-supervised condition results in a large performance gap between point- and fully-supervised methods. In this paper, we propose a novel iterative learning framework, Point to Prompt (P2P), for point-supervised object detection and segmentation, with the key insight of transforming from point supervision to explicit visual prompt of the foundation model. The P2P is formulated as an iterative refinement process of two stages: Semantic Explicit Prompt Generation (SEPG) and Prompt Guided Spatial Refinement (PGSR). Specifically, SEPG serves as a prompt generator for generating semantic-explicit prompts from point input via a group-based learning strategy. In the PGSR stage, prompts guide the visual foundation model to further refine the object regions, by leveraging the outstanding generalization ability of the foundation model. The two stages are iterated multiple times to improve the quality of predictions progressively. Experimental results on multiple datasets demonstrate that P2P achieves SOTA performance in both detection and segmentation tasks, further narrowing the performance gap with fully-supervised methods. The source code and supplementary material can be found at https://github.com/guangqian-guo/P2P.",
                "authors": "Guangqian Guo, Dian Shao, Chenguang Zhu, Sha Meng, Xuan Wang, Shan Gao",
                "citations": 3
            },
            {
                "title": "Modeling Techniques, Seismic Performance, and the Application of Rocking Shallow Foundations: A Review",
                "abstract": "The intriguing rocking behavior of foundations has attracted the attention of both researchers and professionals, owing to its beneficial characteristics such as energy absorption and self-adjusting capability. This paper offers a thorough examination of various modeling techniques, seismic performance evaluation methods, and the practical application of innovative rocking shallow foundations. While conventional fixed-base designs can absorb seismic energy, they often suffer from lasting damage due to residual deformation. In contrast, rocking foundation structures facilitate controlled rocking movements by loosening the connection between the structure and the foundation, thereby enhancing overall stability. Historical studies dating back to the 19th century demonstrate the effectiveness of rocking foundations in reducing seismic impact and ductility demands, leading to cost savings. Furthermore, this paper extends its focus to contemporary considerations, exploring modern modeling techniques, seismic performance assessments, and practical applications for rocking shallow foundations. By highlighting their role in improving structural resilience, this study investigates seismic hazard analysis, geological factors, and site-specific conditions influencing foundation behavior. It covers essential aspects such as dynamic responses and modeling methodologies, drawing insights from real-world case studies. Through a comprehensive review of both numerical and experimental investigations, the article provides a synthesis of current knowledge and identifies avenues for future research.",
                "authors": "M. Al-Janabi, Duaa Al-Jeznawi, L. Bernardo",
                "citations": 3
            },
            {
                "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
                "abstract": "This research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed the latest fine-tuning methodologies together with open-sourced LLMs, and demonstrated a practical and efficient approach to automating the final execution stages of an SLR process that involves knowledge synthesis. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labor-intensive processes of conducting literature reviews. Given the potential of this approach and its applicability across all research domains, this foundational study also advocated for updating PRISMA reporting guidelines to incorporate AI-driven processes, ensuring methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, setting a new standard for conducting comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies.",
                "authors": "Teo Susnjak, Peter Hwang, N. Reyes, A. Barczak, Timothy R. McIntosh, Surangika Ranathunga",
                "citations": 7
            },
            {
                "title": "On the Scalability of GNNs for Molecular Graphs",
                "abstract": "Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation. Practitioners have observed a strong relationship between model size, dataset size, and performance. However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to the lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures. We address this drawback of GNNs by studying their scaling behavior. Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs. For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules, number of labels, and the diversity in the pretraining datasets. We further demonstrate strong finetuning scaling behavior on 38 highly competitive downstream tasks, outclassing previous large models. This gives rise to MolGPS, a new graph foundation model that allows to navigate the chemical space, outperforming the previous state-of-the-arts on 26 out the 38 downstream tasks. We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery.",
                "authors": "Maciej Sypetkowski, Frederik Wenkel, Farimah Poursafaei, Nia Dickson, Karush Suri, Philip Fradkin, Dominique Beaini",
                "citations": 7
            },
            {
                "title": "Solving Motion Planning Tasks with a Scalable Generative Model",
                "abstract": "As autonomous driving systems being deployed to millions of vehicles, there is a pressing need of improving the system's scalability, safety and reducing the engineering cost. A realistic, scalable, and practical simulator of the driving world is highly desired. In this paper, we present an efficient solution based on generative models which learns the dynamics of the driving scenes. With this model, we can not only simulate the diverse futures of a given driving scenario but also generate a variety of driving scenarios conditioned on various prompts. Our innovative design allows the model to operate in both full-Autoregressive and partial-Autoregressive modes, significantly improving inference and training speed without sacrificing generative capability. This efficiency makes it ideal for being used as an online reactive environment for reinforcement learning, an evaluator for planning policies, and a high-fidelity simulator for testing. We evaluated our model against two real-world datasets: the Waymo motion dataset and the nuPlan dataset. On the simulation realism and scene generation benchmark, our model achieves the state-of-the-art performance. And in the planning benchmarks, our planner outperforms the prior arts. We conclude that the proposed generative model may serve as a foundation for a variety of motion planning tasks, including data generation, simulation, planning, and online training. Source code is public at https://github.com/HorizonRobotics/GUMP/",
                "authors": "Yi Hu, Siqi Chai, Zhening Yang, Jingyu Qian, Kun Li, Wenxin Shao, Haichao Zhang, Wei Xu, Qiang Liu",
                "citations": 7
            },
            {
                "title": "Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain Specific Knowledge",
                "abstract": "In the rapidly evolving semiconductor industry, where research, design, verification, and manufacturing are intricately linked, the potential of Large Language Models to revolutionize hardware design and security verification is immense. The primary challenge, however, lies in the complexity of hardware-specific issues that are not adequately addressed by the natural language or software code knowledge typically acquired during the pretraining stage. Additionally, the scarcity of datasets specific to the hardware domain poses a significant hurdle in developing a foundational model. Addressing these challenges, this paper introduces Hardware Phi-1.5B, an innovative large language model specifically tailored for the hardware domain of the semiconductor industry. We have developed a specialized, tiered dataset—comprising small, medium, and large subsets—and focused our efforts on pretraining using the medium dataset. This approach harnesses the compact yet efficient architecture of the Phi-1.5B model. The creation of this first pre-trained, hardware domain-specific large language model marks a significant advancement, offering improved performance in hardware design and verification tasks and illustrating a promising path forward for AI applications in the semiconductor sector.",
                "authors": "Weimin Fu, Shijie Li, Yifang Zhao, Haocheng Ma, R. Dutta, Xuan Zhang, Kaichen Yang, Yier Jin, Xiaolong Guo",
                "citations": 7
            },
            {
                "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
                "authors": "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang",
                "citations": 7
            },
            {
                "title": "Hibou: A Family of Foundational Vision Transformers for Pathology",
                "abstract": "Pathology, the microscopic examination of diseased tissue, is critical for diagnosing various medical conditions, particularly cancers. Traditional methods are labor-intensive and prone to human error. Digital pathology, which converts glass slides into high-resolution digital images for analysis by computer algorithms, revolutionizes the field by enhancing diagnostic accuracy, consistency, and efficiency through automated image analysis and large-scale data processing. Foundational transformer pretraining is crucial for developing robust, generalizable models as it enables learning from vast amounts of unannotated data. This paper introduces the Hibou family of foundational vision transformers for pathology, leveraging the DINOv2 framework to pretrain two model variants, Hibou-B and Hibou-L, on a proprietary dataset of over 1 million whole slide images (WSIs) representing diverse tissue types and staining techniques. Our pretrained models demonstrate superior performance on both patch-level and slide-level benchmarks, surpassing existing state-of-the-art methods. Notably, Hibou-L achieves the highest average accuracy across multiple benchmark datasets. To support further research and application in the field, we have open-sourced the Hibou models, which can be accessed at https://github.com/HistAI/hibou.",
                "authors": "Dmitry Nechaev, Alexey Pchelnikov, Ekaterina Ivanova",
                "citations": 7
            },
            {
                "title": "MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation",
                "abstract": "In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements.",
                "authors": "Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang",
                "citations": 7
            },
            {
                "title": "Needle In A Multimodal Haystack",
                "abstract": "With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.",
                "authors": "Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, Xizhou Zhu, Ping Luo, Yu Qiao, Jifeng Dai, Wenqi Shao, Wenhai Wang",
                "citations": 7
            },
            {
                "title": "What is (quantitative) system dynamics modeling? Defining characteristics and the opportunities they create",
                "abstract": "A clear definition of system dynamics modeling can provide shared understanding and clarify the impact of the field. We introduce a set of characteristics that define quantitative system dynamics, selected to capture core philosophy, describe theoretical and practical principles, and apply to historical work but be flexible enough to remain relevant as the field progresses. The defining characteristics are: (1) models are based on causal feedback structure, (2) accumulations and delays are foundational, (3) models are equation‐based, (4) concept of time is continuous, and (5) analysis focuses on feedback dynamics. We discuss the implications of these principles and use them to identify research opportunities in which the system dynamics field can advance. These research opportunities include causality, disaggregation, data science and AI, and contributing to scientific advancement. Progress in these areas has the potential to improve both the science and practice of system dynamics. © 2024 The Authors. System Dynamics Review published by John Wiley & Sons Ltd on behalf of System Dynamics Society.",
                "authors": "A. Naugle, Saeed P. Langarudi, Timothy Clancy",
                "citations": 7
            },
            {
                "title": "OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving",
                "abstract": "The rise of multi-modal large language models(MLLMs) has spurred their applications in autonomous driving. Recent MLLM-based methods perform action by learning a direct mapping from perception to action, neglecting the dynamics of the world and the relations between action and world dynamics. In contrast, human beings possess world model that enables them to simulate the future states based on 3D internal visual representation and plan actions accordingly. To this end, we propose OccLLaMA, an occupancy-language-action generative world model, which uses semantic occupancy as a general visual representation and unifies vision-language-action(VLA) modalities through an autoregressive model. Specifically, we introduce a novel VQVAE-like scene tokenizer to efficiently discretize and reconstruct semantic occupancy scenes, considering its sparsity and classes imbalance. Then, we build a unified multi-modal vocabulary for vision, language and action. Furthermore, we enhance LLM, specifically LLaMA, to perform the next token/scene prediction on the unified vocabulary to complete multiple tasks in autonomous driving. Extensive experiments demonstrate that OccLLaMA achieves competitive performance across multiple tasks, including 4D occupancy forecasting, motion planning, and visual question answering, showcasing its potential as a foundation model in autonomous driving.",
                "authors": "Julong Wei, Shanshuai Yuan, Pengfei Li, Qingda Hu, Zhongxue Gan, Wenchao Ding",
                "citations": 7
            },
            {
                "title": "UnstrPrompt: Large Language Model Prompt for Driving in Unstructured Scenarios",
                "abstract": "The integration of language descriptions or prompts with Large Language Models (LLMs) into visual tasks is currently a focal point in the advancement of autonomous driving. This study has showcased notable advancements across various standard datasets. Nevertheless, the progress in integrating language prompts faces challenges in unstructured scenarios, primarily due to the limited availability of paired data. To address this challenge, we introduce a groundbreaking language prompt set called “UnstrPrompt.” This prompt set is derived from three prominent unstructured autonomous driving datasets: IDD, ORFD, and AutoMine, collectively comprising a total of 6K language descriptions. In response to the distinctive features of unstructured scenarios, we have developed a structured approach for prompt generation, encompassing three key components: scene, road, and instance. Additionally, we provide a detailed overview of the language generation process and the validation procedures. We conduct tests on segmentation tasks, and our experiments have demonstrated that text-image fusion can improve accuracy by more than 3% on unstructured data. Additionally, our description architecture outperforms the generic urban architecture by more than 0.1%. This work holds the potential to advance various aspects such as interaction and foundational models in this scenario.",
                "authors": "Yuchen Li, Luxi Li, Zizhang Wu, Zhenshan Bing, Zhe Xuanyuan, Alois Knoll, Long Chen",
                "citations": 6
            },
            {
                "title": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes",
                "abstract": "Large Language Models (LLMs) have gained widespread adoption across various domains, including chatbots and auto-task completion agents. However, these models are susceptible to safety vulnerabilities such as jailbreaking, prompt injection, and privacy leakage attacks. These vulnerabilities can lead to the generation of malicious content, unauthorized actions, or the disclosure of confidential information. While foundational LLMs undergo alignment training and incorporate safety measures, they are often subject to fine-tuning, or doing quantization resource-constrained environments. This study investigates the impact of these modifications on LLM safety, a critical consideration for building reliable and secure AI systems. We evaluate foundational models including Mistral, Llama series, Qwen, and MosaicML, along with their fine-tuned variants. Our comprehensive analysis reveals that fine-tuning generally increases the success rates of jailbreak attacks, while quantization has variable effects on attack success rates. Importantly, we find that properly implemented guardrails significantly enhance resistance to jailbreak attempts. These findings contribute to our understanding of LLM vulnerabilities and provide insights for developing more robust safety strategies in the deployment of language models.",
                "authors": "Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, P. Harshangi",
                "citations": 5
            },
            {
                "title": "A Language Model for Particle Tracking",
                "abstract": "Particle tracking is crucial for almost all physics analysis programs at the Large Hadron Collider. Deep learning models are pervasively used in particle tracking related tasks. However, the current practice is to design and train one deep learning model for one task with supervised learning techniques. The trained models work well for tasks they are trained on but show no or little generalization capabilities. We propose to unify these models with a language model. In this paper, we present a tokenized detector representation that allows us to train a BERT model for particle tracking. The trained BERT model, namely TrackingBERT, offers latent detector module embedding that can be used for other tasks. This work represents the first step towards developing a foundational model for particle detector understanding.",
                "authors": "Andris Huang, Yash Melkani, P. Calafiura, Alina Lazar, D. Murnane, Minh-Tuan Pham, Xiangyang Ju",
                "citations": 5
            },
            {
                "title": "Scalable Visual State Space Model with Fractal Scanning",
                "abstract": "Foundational models have significantly advanced in natural language processing (NLP) and computer vision (CV), with the Transformer architecture becoming a standard backbone. However, the Transformer's quadratic complexity poses challenges for handling longer sequences and higher resolution images. To address this challenge, State Space Models (SSMs) like Mamba have emerged as efficient alternatives, initially matching Transformer performance in NLP tasks and later surpassing Vision Transformers (ViTs) in various CV tasks. To improve the performance of SSMs, one crucial aspect is effective serialization of image patches. Existing methods, relying on linear scanning curves, often fail to capture complex spatial relationships and produce repetitive patterns, leading to biases. To address these limitations, we propose using fractal scanning curves for patch serialization. Fractal curves maintain high spatial proximity and adapt to different image resolutions, avoiding redundancy and enhancing SSMs' ability to model complex patterns accurately. We validate our method in image classification, detection, and segmentation tasks, and the superior performance validates its effectiveness.",
                "authors": "Lv Tang, Haoke Xiao, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Bo Li",
                "citations": 5
            },
            {
                "title": "Predicting vs. Acting: A Trade-off Between World Modeling & Agent Modeling",
                "abstract": "RLHF-aligned LMs have shown unprecedented ability on both benchmarks and long-form text generation, yet they struggle with one foundational task: next-token prediction. As RLHF models become agent models aimed at interacting with humans, they seem to lose their world modeling -- the ability to predict what comes next in arbitrary documents, which is the foundational training objective of the Base LMs that RLHF adapts. Besides empirically demonstrating this trade-off, we propose a potential explanation: to perform coherent long-form generation, RLHF models restrict randomness via implicit blueprints. In particular, RLHF models concentrate probability on sets of anchor spans that co-occur across multiple generations for the same prompt, serving as textual scaffolding but also limiting a model's ability to generate documents that do not include these spans. We study this trade-off on the most effective current agent models, those aligned with RLHF, while exploring why this may remain a fundamental trade-off between models that act and those that predict, even as alignment techniques improve.",
                "authors": "Margaret Li, Weijia Shi, Artidoro Pagnoni, Peter West, Ari Holtzman",
                "citations": 4
            },
            {
                "title": "Evaluation of Taekwondo Poomsae movements using skeleton points",
                "abstract": "Taekwondo is a widely practised martial art and an Olympic sport. In Taekwondo, Poomsae movements are essential, as they form the foundation of the sport and are fundamental for success in competitions. The evaluation of Poomsae movements in Taekwondo has been a subjective process, relying heavily on human judgments. This study addresses the above issue by developing a systematic approach to evaluate Poomsae movements using computer vision. A long short-term memory-based (LSTM-based) machine learning (ML) model was developed and evaluated for its effectiveness in Poomsae movement evaluation. The study also aimed to develop this model as an assistant for self-evaluation, that enables Taekwondo players to enhance their skills at their own pace. For this study, a dataset was created specially by recording Poomsae movements of Taekwondo players from the University of Colombo. The technical infrastructure used to capture skeleton point data was cost-effective and easily replicable in other settings. Small video clips containing Taekwondo movements were recorded using a mobile phone camera and the skeleton point data was extracted using the MediaPipe Python library. The model was able to achieve 61% of accuracy when compared with the domain experts’ results. Overall, the study successfully achieved its objectives of defining a self-paced approach to evaluate Poomsae while overcoming human subjectivity otherwise unavoidable in manual evaluation processes. The feedback of domain experts was also considered to finetune the model for better performance.",
                "authors": "W.M.U. Fernando, K.D. Sandaruwan, A.M.K.B. Athapaththu",
                "citations": 1
            },
            {
                "title": "Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",
                "abstract": "Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we fine-tune different sizes of the GPT-2 family. The GPT-2-small model achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90% success. We also show that GPT-4 can help in the labor-intensive task of evaluating the quality of the distilled models, using it as a zero-shot classifier. Using triple-human review as a guide, the classifier achieves a Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.",
                "authors": "Andrew Brown, Jiading Zhu, Mohamed Abdelwahab, Alec Dong, Cindy Wang, Jonathan Rose",
                "citations": 3
            },
            {
                "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
                "abstract": "We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions: We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an instruction tuning dataset accordingly. The fine-tuning with this dataset substantially improves the model's user experience in practical applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. This design choice ensures the model's ability to capture critical semantic and detailed information across various visual tasks. We posit that a proficient Vision-Language Model should, foremost, possess strong language abilities. To ensure the preservation of LLM capabilities during pretraining, we investigate an effective VL pretraining strategy by integrating LLM training from the beginning and carefully managing the competitive dynamics observed between vision and language modalities. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a vision-language chatbot in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language benchmarks at the same model size while maintaining robust performance on language-centric benchmarks. We have made both 1.3B and 7B models publicly accessible to foster innovations based on this foundation model.",
                "authors": "Haoyu Lu, Wen Liu, Bo Zhang, Bing-Li Wang, Kai Dong, Bo Liu (Benjamin Liu), Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, C. Deng, Hanwei Xu, Zhenda Xie, C. Ruan",
                "citations": 153
            },
            {
                "title": "Open-Sora: Democratizing Efficient Video Production for All",
                "abstract": "Vision and language are the two foundational senses for humans, and they build up our cognitive ability and intelligence. While significant breakthroughs have been made in AI language ability, artificial visual intelligence, especially the ability to generate and simulate the world we see, is far lagging behind. To facilitate the development and accessibility of artificial visual intelligence, we created Open-Sora, an open-source video generation model designed to produce high-fidelity video content. Open-Sora supports a wide spectrum of visual generation tasks, including text-to-image generation, text-to-video generation, and image-to-video generation. The model leverages advanced deep learning architectures and training/inference techniques to enable flexible video synthesis, which could generate video content of up to 15 seconds, up to 720p resolution, and arbitrary aspect ratios. Specifically, we introduce Spatial-Temporal Diffusion Transformer (STDiT), an efficient diffusion framework for videos that decouples spatial and temporal attention. We also introduce a highly compressive 3D autoencoder to make representations compact and further accelerate training with an ad hoc training strategy. Through this initiative, we aim to foster innovation, creativity, and inclusivity within the community of AI content creation. By embracing the open-source principle, Open-Sora democratizes full access to all the training/inference/data preparation codes as well as model weights. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.",
                "authors": "Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, Yang You",
                "citations": 70
            },
            {
                "title": "Drivers of Digital Transformation in SMEs",
                "abstract": "This study aims to identify and analyze factors determining the adoption of digital technologies in SMEs. Drawing on the technology–organization–environment framework, the study highlights enabling factors from three different contexts and hypothesizes their relationship with digital technology adoption. The data used were collected from 15 346 European Union and non-European Union SMEs to test an ordered logit regression model that highlights the factors associated with an increased level of digital technologies adoption in SMEs. The empirical results show that the technology context (IT infrastructure and digital tools) along with the existing level of innovation are the main drivers that act as stepping stones in digital technology adoption. Corporate regulation, available skills, and financial resources (as organizational variables) also play a significant role in the adoption decision. Unexpectedly, the influence of the environmental context is marginal. The implications of this study are emphasized for theory and practice, laying a foundation for further empirical studies in this field. Managerial relevance statement: This article reviews the empirical research on digital technologies adoption and examines the drivers of such adoption in SMEs. The factors identified provide guidance for practitioners adopting digital technologies in SMEs, by suggesting they assess the readiness of their firms before investing in digital technology. This research helps advance the conversation on digitalization drivers especially by bringing the discussion into the organization boundaries, as our findings highlight the predominance of organizational drivers over the technological and environmental ones. SMEs have to overcome the challenges associated with constructing an IT infrastructure capable of implementing new technologies. Indeed, while striving to adopt new digital technologies (e.g., AI, big data, IoT), many SMEs are still unprepared. Therefore, rather than adopting mimetic behaviors based on external pressure, SMEs that aim for digitalization should first assess their existing technologies, and further develop a meticulous technological roadmap that includes skills upgrades and investments in upskilling employees’ capabilities. Therefore, developing a fully integrated strategic approach is crucial before the adoption of digital technologies",
                "authors": "Nessrine Omrani, Nada Rejeb, A. Maalaoui, Marina Dabić, S. Kraus",
                "citations": 77
            },
            {
                "title": "PointMamba: A Simple State Space Model for Point Cloud Analysis",
                "abstract": "Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity, making the design of a linear complexity method with global modeling appealing. In this paper, we propose PointMamba, transferring the success of Mamba, a recent representative state space model (SSM), from NLP to point cloud analysis tasks. Unlike traditional Transformers, PointMamba employs a linear complexity algorithm, presenting global modeling capacity while significantly reducing computational costs. Specifically, our method leverages space-filling curves for effective point tokenization and adopts an extremely simple, non-hierarchical Mamba encoder as the backbone. Comprehensive evaluations demonstrate that PointMamba achieves superior performance across multiple datasets while significantly reducing GPU memory usage and FLOPs. This work underscores the potential of SSMs in 3D vision-related tasks and presents a simple yet effective Mamba-based baseline for future research. The code will be made available at \\url{https://github.com/LMD0311/PointMamba}.",
                "authors": "Dingkang Liang, Xin Zhou, Wei Xu, Xingkui Zhu, Zhikang Zou, Xiaoqing Ye, Xiao Tan, Xiang Bai",
                "citations": 58
            },
            {
                "title": "PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation",
                "abstract": "In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution. PixArt-\\Sigma represents a significant advancement over its predecessor, PixArt-\\alpha, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-\\Sigma is its training efficiency. Leveraging the foundational pre-training of PixArt-\\alpha, it evolves from the `weaker' baseline to a `stronger' model via incorporating higher quality data, a process we term\"weak-to-strong training\". The advancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data: PixArt-\\Sigma incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-\\Sigma achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-\\Sigma's capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.",
                "authors": "Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, Zhenguo Li",
                "citations": 36
            },
            {
                "title": "Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model",
                "abstract": "The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic segmentation tasks.",
                "authors": "Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, Chun Yuan",
                "citations": 30
            },
            {
                "title": "The Impact of Social Media Analytics on SME Strategic Decision Making",
                "abstract": "In the digital age, the emergence of social media has transformed the operational landscape of small and medium enterprises (SMEs). Recognized as a tool that generates comprehensive insights, social media analytics supports strategic decision-making. However, the concrete impact of social media analytics on strategic decisions in SMEs still requires further exploration. This study aims to assess the impact of social media analytics on strategic decision-making in SMEs, considering mediating variables such as organizational innovation and adaptability. Employing Structural Equation Modeling (SEM) with SmartPLS 4.1 software, the study analyzed data collected from 200 SMEs actively using social media for operations and marketing. The findings reveal that social media analytics significantly enhances organizational innovation and adaptability, which in turn positively affects strategic decision-making. This analysis underscores the importance of social media as a strategic resource in a dynamic business environment. The study provides valuable insights for SME owners on the critical role of social media analytics in enhancing strategic decisions. Theoretically, it extends the literature on social media analytics and strategic management within the SME context. Practically, the results serve as a foundation for SMEs to integrate social media analytics technology into their decision-making processes, thereby boosting adaptability and innovation in their operations.",
                "authors": "Dimas Nugroho, Putri Angela",
                "citations": 30
            },
            {
                "title": "Spin-mediated promotion of Co catalysts for ammonia synthesis",
                "abstract": "Over the past two decades, there has been growing interest in developing catalysts to enable Haber-Bosch ammonia synthesis under milder conditions than currently pertain. Rational catalyst design requires theoretical guidance and clear mechanistic understanding. Recently, a spin-mediated promotion mechanism was proposed to activate traditionally unreactive magnetic materials such as cobalt (Co) for ammonia synthesis by introducing hetero metal atoms bound to the active site of the catalyst surface. We combined theory and experiment to validate this promotion mechanism on a lanthanum (La)/Co system. By conducting model catalyst studies on Co single crystals and mass-selected Co nanoparticles at ambient pressure, we identified the active site for ammonia synthesis as the B5 site of Co steps with La adsorption. The turnover frequency of 0.47 ± 0.03 per second achieved on the La/Co system at 350°C and 1 bar surpasses those of other model catalysts tested under identical conditions. Editor’s summary Haber-Bosch ammonia synthesis was a foundational achievement of the modern chemical industry and still produces vast quantities of fertilizer a century later. However, the process requires high temperature and pressure, and chemists continue to seek options for less demanding conditions. Zhang et al. report that suppressing magnetism in the catalyst may be a key to lower temperature operation (see the Perspective by Rupprecther). Combining theory and experiments, the authors showed that using lanthanum to quench the magnetic moment of adjacent cobalt centers enhances the cobalt’s catalytic activity for nitrogen cleavage well below the temperature where Haber iron catalysts operate. —Jake S. Yeston Lanthanum quenching of cobalt magnetism improves cobalt’s lower temperature catalytic activity for ammonia synthesis.",
                "authors": "Ke Zhang, Ang Cao, L. H. Wandall, J. Vernieres, J. Kibsgaard, Jens K. Nørskov, I. Chorkendorff",
                "citations": 31
            },
            {
                "title": "Efficiency and Driving Factors of Agricultural Carbon Emissions: A Study in Chinese State Farms",
                "abstract": "Promoting low-carbon agriculture is vital for climate action and food security. State farms serve as crucial agricultural production bases in China and are essential in reducing China’s carbon emissions and boosting emission efficiency. This study calculates the carbon emissions of state farms across 29 Chinese provinces using the IPCC method from 2010 to 2022. It also evaluates emission efficiency with the Super-Slack-Based Measure (Super-SBM model) and analyzes influencing factors using the Logarithmic Mean Divisia Index (LMDI) method. The findings suggest that the three largest carbon sources are rice planting, chemical fertilizers, and land tillage. Secondly, agricultural carbon emissions in state farms initially surge, stabilize with fluctuations, and ultimately decline, with higher emissions observed in northern and eastern China. Thirdly, the rise of agricultural carbon emission efficiency is driven primarily by technological progress. Lastly, economic development and industry structure promote agricultural carbon emissions, while production efficiency and labor scale reduce them. To reduce carbon emissions from state farms in China and improve agricultural carbon emission efficiency, the following measures can be taken: (1) Improve agricultural production efficiency and reduce carbon emissions in all links; (2) Optimize the agricultural industrial structure and promote the coordinated development of agriculture; (3) Reduce the agricultural labor scale and promote the specialization, professionalization, and high-quality development of agricultural labor; (4) Accelerate agricultural green technology innovation and guide the green transformation of state farms. This study enriches the theoretical foundation of low-carbon agriculture and develops a framework for assessing carbon emissions in Chinese state farms, offering guidance for future research and policy development in sustainable agriculture.",
                "authors": "Guanghe Han, Jiahui Xu, Xin Zhang, Xin Pan",
                "citations": 31
            },
            {
                "title": "CogniFiber: Harnessing Biocompatible and Biodegradable 1D Collagen Nanofibers for Sustainable Nonvolatile Memory and Synaptic Learning Applications",
                "abstract": "Here, resistive switching (RS) devices are fabricated using naturally abundant, nontoxic, biocompatible, and biodegradable biomaterials. For this purpose, 1D chitosan nanofibers (NFs), collagen NFs, and chitosan–collagen NFs are synthesized by using an electrospinning technique. Among different NFs, the collagen‐NFs‐based device shows promising RS characteristics. In particular, the optimized Ag/collagen NFs/fluorine‐doped tin oxide RS device shows a voltage‐tunable analog memory behavior and good nonvolatile memory properties. Moreover, it can also mimic various biological synaptic learning properties and can be used for pattern classification applications with the help of the spiking neural network. The time series analysis technique is employed to model and predict the switching variations of the RS device. Moreover, the collagen NFs have shown good cytotoxicity and anticancer properties, suggesting excellent biocompatibility as a switching layer. The biocompatibility of collagen NFs is explored with the help of NRK‐52E (Normal Rat Kidney cell line) and MCF‐7 (Michigan Cancer Foundation‐7 cancer cell line). Additionally, the biodegradability of the device is evaluated through a physical transient test. This work provides a vital step toward developing a biocompatible and biodegradable switching material for sustainable nonvolatile memory and neuromorphic computing applications.",
                "authors": "K. A. Rokade, Dhananjay D. Kumbhar, S. L. Patil, S. Sutar, K. V. More, P. Dandge, Rajanish K. Kamat, T. Dongale",
                "citations": 26
            },
            {
                "title": "AI APPLICATIONS IN RESERVOIR MANAGEMENT: OPTIMIZING PRODUCTION AND RECOVERY IN OIL AND GAS FIELDS",
                "abstract": "This paper explores various AI applications in reservoir management, highlighting how machine learning, data analytics, and advanced algorithms contribute to optimizing production and recovery in oil and gas fields. From predictive modeling to real-time monitoring and control, AI offers promising solutions to address the industry's evolving demands and enhance operational performance Reservoir management in oil and gas fields has historically presented challenges demanding continuous monitoring, analysis, and optimization for maximizing production and recovery. The integration of artificial intelligence (AI) technologies has ushered in a new era, providing reservoir engineers and operators with potent tools to augment decision-making processes and enhance overall efficiency. This paper delves into diverse AI applications within reservoir management, emphasizing the pivotal roles played by machine learning, data analytics, and advanced algorithms in optimizing production and recovery processes in oil and gas fields. The exploration encompasses predictive modeling, real-time monitoring, and control applications, showcasing how AI offers promising solutions to meet the dynamic demands of the industry while significantly improving operational performance. As we navigate through this technological evolution, the synergistic relationship between reservoir management and AI promises not only to address current challenges but also to lay the foundation for a more efficient and sustainable future in the oil and gas sector. The integration of artificial intelligence (AI) technologies has ushered in a new era, providing reservoir engineers and operators with potent tools to augment decision-making processes and enhance overall efficiency. The exploration encompasses predictive modeling, real-time monitoring, and control applications, showcasing how AI offers promising solutions to meet the dynamic demands of the industry while significantly improving operational performance. As we navigate through this technological evolution, the synergistic relationship between reservoir management and AI promises not only to address current challenges but also to lay the foundation for a more efficient and sustainable future in the oil and gas sector. \nKeywords: Reservoir Management, Oil And Gas Fields, Optimization, Recovery, Machine Learning, Data Analytics, Predictive Modeling.",
                "authors": "Gideon Oluseyi Daramola, Boma Sonimitiem Jacks, Olakunle Abayomi Ajala, A. Akinoso",
                "citations": 21
            },
            {
                "title": "Iterated Denoising Energy Matching for Sampling from Boltzmann Densities",
                "abstract": "Efficiently generating statistically independent samples from an unnormalized probability distribution, such as equilibrium samples of many-body systems, is a foundational problem in science. In this paper, we propose Iterated Denoising Energy Matching (iDEM), an iterative algorithm that uses a novel stochastic score matching objective leveraging solely the energy function and its gradient -- and no data samples -- to train a diffusion-based sampler. Specifically, iDEM alternates between (I) sampling regions of high model density from a diffusion-based sampler and (II) using these samples in our stochastic matching objective to further improve the sampler. iDEM is scalable to high dimensions as the inner matching objective, is simulation-free, and requires no MCMC samples. Moreover, by leveraging the fast mode mixing behavior of diffusion, iDEM smooths out the energy landscape enabling efficient exploration and learning of an amortized sampler. We evaluate iDEM on a suite of tasks ranging from standard synthetic energy functions to invariant $n$-body particle systems. We show that the proposed approach achieves state-of-the-art performance on all metrics and trains $2-5\\times$ faster, which allows it to be the first method to train using energy on the challenging $55$-particle Lennard-Jones system.",
                "authors": "Tara Akhound-Sadegh, Jarrid Rector-Brooks, A. Bose, Sarthak Mittal, Pablo Lemos, Cheng-Hao Liu, Marcin Sendera, Siamak Ravanbakhsh, Gauthier Gidel, Y. Bengio, Nikolay Malkin, Alexander Tong",
                "citations": 21
            },
            {
                "title": "Deep-Learning-Based Lithium Battery Defect Detection via Cross-Domain Generalization",
                "abstract": "This research addresses the critical challenge of classifying surface defects in lithium electronic components, crucial for ensuring the reliability and safety of lithium batteries. With a scarcity of specific defect data, we introduce an innovative Cross-Domain Generalization (CDG) approach, incorporating Cross-domain Augmentation, Multi-task Learning, and Iteration Learning. Leveraging a steel surface defect dataset as foundational knowledge, our approach compensates for the limited lithium-specific data and enhances model generalization. We also introduce the Lithium Electronic Surface Defect Classification (IESDC) dataset, demonstrating significant accuracy improvements over baseline methods. Our comprehensive evaluation covers model interpretability, robustness, and adaptability. Beyond battery technology, this methodology offers a framework for data scarcity challenges in various industries, emphasizing the importance of adaptable learning methods.",
                "authors": "Xuhesheng Chen, Mingyue Liu, Yongjie Niu, Xukang Wang, Ying Cheng Wu",
                "citations": 21
            },
            {
                "title": "Product Demand Prediction with Spatial Graph Neural Networks",
                "abstract": "In the rapidly evolving online marketplace, accurately predicting the demand for pre-owned items presents a significant challenge for sellers, impacting pricing strategies, product presentation, and marketing investments. Traditional demand prediction methods, while foundational, often fall short in addressing the dynamic and heterogeneous nature of e-commerce data, which encompasses textual descriptions, visual elements, geographic contexts, and temporal dynamics. This paper introduces a novel approach utilizing the Graph Neural Network (GNN) to enhance demand prediction accuracy by leveraging the spatial relationships inherent in online sales data, named SGNN. Drawing from the rich dataset provided in the fourth Kaggle competition, we construct a spatially aware graph representation of the marketplace, integrating advanced attention mechanisms to refine predictive accuracy. Our methodology defines the product demand prediction problem as a regression task on an attributed graph, capturing both local and global spatial dependencies that are fundamental to accurate predicting. Through attention-aware message propagation and node-level demand prediction, our model effectively addresses the multifaceted challenges of e-commerce demand prediction, demonstrating superior performance over traditional statistical methods, machine learning techniques, and even deep learning models. The experimental findings validate the effectiveness of our GNN-based approach, offering actionable insights for sellers navigating the complexities of the online marketplace. This research not only contributes to the academic discourse on e-commerce demand prediction but also provides a scalable and adaptable framework for future applications, paving the way for more informed and effective online sales strategies.",
                "authors": "Jiale Li, Li Fan, Xuran Wang, Tiejiang Sun, Mengjie Zhou",
                "citations": 20
            },
            {
                "title": "Key influences on university students’ physical activity: a systematic review using the Theoretical Domains Framework and the COM-B model of human behaviour",
                "abstract": null,
                "authors": "Catherine E. B. Brown, Karyn E. Richardson, Bengianni Halil-Pizzirani, Lou Atkins, Murat Yücel, Rebecca A. Segrave",
                "citations": 20
            },
            {
                "title": "Robust Drone Delivery with Weather Information",
                "abstract": "Problem definition: Drone delivery has recently garnered significant attention due to its potential for faster delivery at a lower cost than other delivery options. When scheduling drones from a depot for delivery to various destinations, the dispatcher must take into account the uncertain wind conditions, which affect the delivery times of drones to their destinations, leading to late deliveries. Methodology/results: To mitigate the risk of delivery delays caused by wind uncertainty, we propose a two-period drone scheduling model to robustly optimize the delivery schedule. In this framework, the scheduling decisions are made in the morning, with the provision for different delivery schedules in the afternoon that adapt to updated weather information available by midday. Our approach minimizes the essential riskiness index, which can simultaneously account for the probability of tardy delivery and the magnitude of lateness. Using wind observation data, we characterize the uncertain flight times via a cluster-wise ambiguity set, which has the benefit of tractability while avoiding overfitting the empirical distribution. A branch-and-cut (B&C) algorithm is developed for this adaptive distributionally framework to improve its scalability. Our adaptive distributionally robust model can effectively reduce lateness in out-of-sample tests compared with other classical models. The proposed B&C algorithm can solve instances to optimality within a shorter time frame than a general modeling toolbox. Managerial implications: Decision makers can use the adaptive robust model together with the cluster-wise ambiguity set to effectively reduce service lateness at customers for drone delivery systems. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72101049 and 72232001], the Natural Science Foundation of Liaoning Province [Grant 2023-BS-091], the Fundamental Research Funds for the Central Universities [Grant DUT23RC(3)045], and the Major Project of the National Social Science Foundation [Grant 22&ZD151]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/msom.2022.0339 .",
                "authors": "Chun Cheng, Y. Adulyasak, Louis-Martin Rousseau",
                "citations": 21
            },
            {
                "title": "Multi-Channel Optimization Generative Model for Stable Ultra-Sparse-View CT Reconstruction",
                "abstract": "Score-based generative model (SGM) has risen to prominence in sparse-view CT reconstruction due to its impressive generation capability. The consistency of data is crucial in guiding the reconstruction process in SGM-based reconstruction methods. However, the existing data consistency policy exhibits certain limitations. Firstly, it employs partial data from the reconstructed image of the iteration process for image updates, which leads to secondary artifacts with compromising image quality. Moreover, the updates to the SGM and data consistency are considered as distinct stages, disregarding their interdependent relationship. Additionally, the reference image used to compute gradients in the reconstruction process is derived from the intermediate result rather than ground truth. Motivated by the fact that a typical SGM yields distinct outcomes with different random noise inputs, we propose a Multi-channel Optimization Generative Model (MOGM) for stable ultra-sparse-view CT reconstruction by integrating a novel data consistency term into the stochastic differential equation model. Notably, the unique aspect of this data consistency component is its exclusive reliance on original data for effectively confining generation outcomes. Furthermore, we pioneer an inference strategy that traces back from the current iteration result to ground truth, enhancing reconstruction stability through foundational theoretical support. We also establish a multi-channel optimization reconstruction framework, where conventional iterative techniques are employed to seek the reconstruction solution. Quantitative and qualitative assessments on 23 views datasets from numerical simulation, clinical cardiac and sheep’s lung underscore the superiority of MOGM over alternative methods. Reconstructing from just 10 and 7 views, our method consistently demonstrates exceptional performance.",
                "authors": "Weiwen Wu, Jiayi Pan, Yanyang Wang, Shaoyu Wang, Jianjia Zhang",
                "citations": 19
            },
            {
                "title": "YOLOv8-C2f-Faster-EMA: An Improved Underwater Trash Detection Model Based on YOLOv8",
                "abstract": "Anthropogenic waste deposition in aquatic environments precipitates a decline in water quality, engendering pollution that adversely impacts human health, ecological integrity, and economic endeavors. The evolution of underwater robotic technologies heralds a new era in the timely identification and extraction of submerged litter, offering a proactive measure against the scourge of water pollution. This study introduces a refined YOLOv8-based algorithm tailored for the enhanced detection of small-scale underwater debris, aiming to mitigate the prevalent challenges of high miss and false detection rates in aquatic settings. The research presents the YOLOv8-C2f-Faster-EMA algorithm, which optimizes the backbone, neck layer, and C2f module for underwater characteristics and incorporates an effective attention mechanism. This algorithm improves the accuracy of underwater litter detection while simplifying the computational model. Empirical evidence underscores the superiority of this method over the conventional YOLOv8n framework, manifesting in a significant uplift in detection performance. Notably, the proposed method realized a 6.7% increase in precision (P), a 4.1% surge in recall (R), and a 5% enhancement in mean average precision (mAP). Transcending its foundational utility in marine conservation, this methodology harbors potential for subsequent integration into remote sensing ventures. Such an adaptation could substantially enhance the precision of detection models, particularly in the realm of localized surveillance, thereby broadening the scope of its applicability and impact.",
                "authors": "Jin Zhu, Tao Hu, Linhan Zheng, Nan Zhou, Huilin Ge, Zhichao Hong",
                "citations": 19
            },
            {
                "title": "The factors influencing teacher education students’ willingness to adopt artificial intelligence technology for information-based teaching",
                "abstract": "ABSTRACT This study, rooted in the Technology Acceptance Model (TAM), investigates the multifaceted factors that influence teacher education students in Information-Based Teaching to embrace artificial intelligence technologies. To enrich the TAM framework, we have incorporated elements such as Artificial Intelligence Literacy (AIL), Subjective Norms (SN), and Output Quality (OQ), with the aim of examining their respective effects on the willingness of teacher education students to adopt AI technologies. To substantiate this theoretical framework, we conducted empirical research involving teacher education students from various Chinese universities. Our findings affirm the robustness of the TAM in explaining the inclination of teacher education students, engaged in the actual teaching process within a digitized educational environment, to adopt AI technologies. Through this model, our study underscores the pivotal role of Artificial Intelligence Literacy (AIL) in influencing educators’ acceptance of AI technologies, establishing a foundational cornerstone for subsequent explorations within the theoretical landscape of the TAM. In this study, we identify Perceived Usefulness (PU) and Artificial Intelligence Literacy (AIL) as the primary factors affecting Behavioral Intention (BI) to use AI technologies. Consequently, to foster broader adoption of AI technologies by educators, it is essential to emphasize their tangible benefits and superiority in teaching, with the goal of promoting the extended utilization of AI in digitalized instruction.",
                "authors": "Shuaiyao Ma, Lei Lei",
                "citations": 18
            },
            {
                "title": "VideoPrism: A Foundational Visual Encoder for Video Understanding",
                "abstract": "We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 31 out of 33 video understanding benchmarks.",
                "authors": "Long Zhao, N. B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, Boqing Gong",
                "citations": 18
            },
            {
                "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token",
                "abstract": "This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation. xRAG reinterprets document embeddings in dense retrieval--traditionally used solely for retrieval--as features from the retrieval modality. By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate. In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53. Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems",
                "authors": "Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, Dongyan Zhao",
                "citations": 15
            },
            {
                "title": "A foundational large language model for edible plant genomes",
                "abstract": "Significant progress has been made in the field of plant genomics, as demonstrated by the increased use of high-throughput methodologies that enable the characterization of multiple genome-wide molecular phenotypes. These findings have provided valuable insights into plant traits and their underlying genetic mechanisms, particularly in model plant species. Nonetheless, effectively leveraging them to make accurate predictions represents a critical step in crop genomic improvement. We present AgroNT, a foundational large language model trained on genomes from 48 plant species with a predominant focus on crop species. We show that AgroNT can obtain state-of-the-art predictions for regulatory annotations, promoter/terminator strength, tissue-specific gene expression, and prioritize functional variants. We conduct a large-scale in silico saturation mutagenesis analysis on cassava to evaluate the regulatory impact of over 10 million mutations and provide their predicted effects as a resource for variant characterization. Finally, we propose the use of the diverse datasets compiled here as the Plants Genomic Benchmark (PGB), providing a comprehensive bench-mark for deep learning-based methods in plant genomic research. The pre-trained AgroNT model is publicly available on HuggingFace at https://huggingface.co/InstaDeepAI/agro-nucleotide-transformer-1b for future research purposes.",
                "authors": "Javier Mendoza-Revilla, Evan Trop, Liam Gonzalez, Maša Roller, Hugo Dalla-torre, B. P. de Almeida, Guillaume Richard, Jonathan Caton, Nicolás López Carranza, Marcin Skwark, Alexandre Laterre, Karim Beguir, Thomas Pierrot, Marie Lopez",
                "citations": 17
            },
            {
                "title": "Predict Students' Dropout and Academic Success with XGBoost",
                "abstract": "The attrition rate of students in higher education is a worldwide issue that profoundly affects both individuals and institutions. Students who fail to complete their studies often encounter economic and social difficulties, while educational institutions suffer a deterioration in reputation and operational efficacy. This paper proposes the creation of a prediction model utilizing the XGBoost algorithm to assess students' academic progress and dropout risk. The model incorporates several elements, such as academic, demographic, and socio-economic, to yield comprehensive insights into students' educational trends. This research utilizes the Predict Students' Dropout and Academic Success dataset, comprising 4,424 data points and 36 attributes. The data underwent normalization via StandardScaler and was divided into five scenarios for training and testing, ranging from a 50:50 to a 90:10 split. The evaluation of the model was conducted utilizing accuracy, precision, recall, and F1-Score criteria. The findings indicate that the model attains peak performance in the 80:20 scenario, exhibiting 88% precision and an 81% F1-Score, signifying an ideal equilibrium between predictive accuracy and risk identification capability. This study demonstrates that XGBoost can serve as a dependable predictive instrument to aid decision-making in the education sector. These findings establish a foundation for formulating targeted interventions aimed at enhancing student retention. Subsequent study may investigate the use of real-time data and sophisticated models to enhance predictive accuracy.",
                "authors": "Achmad Ridwan, Arif Mudi Priyatno",
                "citations": 15
            },
            {
                "title": "Improving fine-grained understanding in image-text pre-training",
                "abstract": "We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thoroughly evaluate our proposed method and show improved performance over competing approaches both on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level tasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation. Moreover, SPARC improves model faithfulness and captioning in foundational vision-language models.",
                "authors": "Ioana Bica, Anastasija Ili'c, Matthias Bauer, Goker Erdogan, Matko Bovsnjak, Christos Kaplanis, A. Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, Jovana Mitrovi'c",
                "citations": 15
            },
            {
                "title": "Damage identification of steel bridge based on data augmentation and adaptive optimization neural network",
                "abstract": "With the advancement of deep learning, data-driven structural damage identification (SDI) has shown considerable development. However, collecting vibration signals related to structural damage poses certain challenges, which can undermine the accuracy of the identification results produced by data-driven SDI methods in scenarios where data is scarce. This paper introduces an innovative approach to bridge SDI in a few-shot context by integrating an adaptive simulated annealing particle swarm optimization-convolutional neural network (ASAPSO-CNN) as the foundational framework, augmented by data enhancement techniques. Firstly, three specific types of noise are introduced to augment the source signals used for training. Subsequently, the source signals and augmented signals are recombined to construct a four-dimensional matrix as the input to the CNN, while defining the damage feature vector as the output. Secondly, a CNN is constructed to establish the mapping relationship between the input and output. Then, an adaptive fitness function is proposed that simultaneously considers the accuracy of SDI, model complexity, and training efficiency. The ASAPSO is employed to adaptively optimize the hyperparameters of the CNN. The proposed method is validated on an experimental model of a three-span continuous beam. It is compared with four other data-driven methods, demonstrating good effectiveness and robustness of SDI under cases of scarce data. Finally, the effectiveness of this SDI method is validated in a real-world case of a steel truss bridge.",
                "authors": "Minshui Huang, Jianwei Zhang, Jun Li, Zhihang Deng, Jin Luo",
                "citations": 15
            },
            {
                "title": "Transolver: A Fast Transformer Solver for PDEs on General Geometries",
                "abstract": "Transformers have empowered many milestones across various fields and have recently been applied to solve partial differential equations (PDEs). However, since PDEs are typically discretized into large-scale meshes with complex geometries, it is challenging for Transformers to capture intricate physical correlations directly from massive individual points. Going beyond superficial and unwieldy meshes, we present Transolver based on a more foundational idea, which is learning intrinsic physical states hidden behind discretized geometries. Specifically, we propose a new Physics-Attention to adaptively split the discretized domain into a series of learnable slices of flexible shapes, where mesh points under similar physical states will be ascribed to the same slice. By calculating attention to physics-aware tokens encoded from slices, Transovler can effectively capture intricate physical correlations under complex geometrics, which also empowers the solver with endogenetic geometry-general modeling capacity and can be efficiently computed in linear complexity. Transolver achieves consistent state-of-the-art with 22% relative gain across six standard benchmarks and also excels in large-scale industrial simulations, including car and airfoil designs. Code is available at https://github.com/thuml/Transolver.",
                "authors": "Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, Mingsheng Long",
                "citations": 17
            },
            {
                "title": "Cellular communities reveal trajectories of brain ageing and Alzheimer's disease.",
                "abstract": null,
                "authors": "G. Green, Masashi Fujita, Hyun-Sik Yang, M. Taga, Anael Cain, C. McCabe, Natacha Comandante-Lou, Charles C. White, A. Schmidtner, L. Zeng, Alina Sigalov, Yangling Wang, Aviv Regev, Hans Klein, Vilas Menon, David A. Bennett, Naomi Habib, P. D. de Jager",
                "citations": 14
            },
            {
                "title": "Sustainable Digital Marketing Under Big Data: An AI Random Forest Model Approach",
                "abstract": "Digital marketing refers to the process of promoting, selling, and delivering products or services through online platforms and channels using the internet and electronic devices in a digital environment. Its aim is to attract and engage target audiences through various strategies and methods, driving brand promotion and sales growth. The primary objective of this scholarly study is to seamlessly integrate advanced big data analytics and artificial intelligence (AI) technology into the realm of digital marketing, thereby fostering the progression and optimization of sustainable digital marketing practices. First, the characteristics and applications of big data involving vast, diverse, and complex datasets are analyzed. Understanding their attributes and scope of application is essential. Subsequently, a comprehensive investigation into AI-driven learning mechanisms is conducted, culminating in the development of an AI random forest model (RFM) tailored for sustainable digital marketing. Subsequent to this, leveraging a real-world case study involving enterprise X, fundamental customer data is collected and subjected to meticulous analysis. The RFM model, ingeniously crafted in this study, is then deployed to prognosticate the anticipated count of prospective customers for said enterprise. The empirical findings spotlight a pronounced prevalence of university-affiliated individuals across diverse age cohorts. In terms of occupational distribution within the customer base, the categories of workers and educators emerge as dominant, constituting 41% and 31% of the demographic, respectively. Furthermore, the price distribution of patrons exhibits a skewed pattern, whereby the price bracket of 0–150 encompasses 17% of the population, whereas the range of 150–300 captures a notable 52%. These delineated price bands collectively constitute a substantial proportion, whereas the range exceeding 450 embodies a minority, accounting for less than 20%. Notably, the RFM model devised in this scholarly endeavor demonstrates a remarkable proficiency in accurately projecting forthcoming passenger volumes over a seven-day horizon, significantly surpassing the predictive capability of logistic regression. Evidently, the AI-driven RFM model proffered herein excels in the precise anticipation of target customer counts, thereby furnishing a pragmatic foundation for the intelligent evolution of sustainable digital marketing strategies.",
                "authors": "Keyan Jin, Zoe Ziqi Zhong, Elena Yifei Zhao",
                "citations": 14
            },
            {
                "title": "PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar Nonlinear Conservation Laws",
                "abstract": "Can we build a single large model for a wide range of PDE-related scientific learning tasks? Can this model generalize to new PDEs, even of new forms, without any fine-tuning? In-context operator learning and the corresponding model In-Context Operator Networks (ICON) represent an initial exploration of these questions. The capability of ICON regarding the first question has been demonstrated previously. In this paper, we present a detailed methodology for solving PDE problems with ICON, and show how a single ICON model can make forward and reverse predictions for different equations with different strides, provided with appropriately designed data prompts. We show the positive evidence to the second question, i.e., ICON can generalize well to some PDEs with new forms without any fine-tuning. This is exemplified through a study on 1D scalar nonlinear conservation laws, a family of PDEs with temporal evolution. We also show how to broaden the range of problems that an ICON model can address, by transforming functions and equations to ICON's capability scope. We believe that the progress in this paper is a significant step towards the goal of training a foundation model for PDE-related tasks under the in-context operator learning framework.",
                "authors": "Liu Yang, Stanley Osher",
                "citations": 14
            },
            {
                "title": "DF-DM: A foundational process model for multimodal data fusion in the artificial intelligence era",
                "abstract": "In the big data era, integrating diverse data modalities poses significant challenges, particularly in complex fields like healthcare. This paper introduces a new process model for multimodal Data Fusion for Data Mining, integrating embeddings and the Cross-Industry Standard Process for Data Mining with the existing Data Fusion Information Group model. Our model aims to decrease computational costs, complexity, and bias while improving efficiency and reliability. We also propose ”disentangled dense fusion,” a novel embedding fusion method designed to optimize mutual information and facilitate dense inter-modality feature interaction, thereby minimizing redundant information. We demonstrate the model’s efficacy through three use cases: predicting diabetic retinopathy using retinal images and patient metadata, domestic violence prediction employing satellite imagery, internet, and census data, and identifying clinical and demographic features from radiography images and clinical notes. The model achieved a Macro F1 score of 0.92 in diabetic retinopathy prediction, an R-squared of 0.854 and sMAPE of 24.868 in domestic violence prediction, and a macro AUC of 0.92 and 0.99 for disease prediction and sex classification, respectively, in radiological analysis. These results underscore the Data Fusion for Data Mining model’s potential to significantly impact multimodal data processing, promoting its adoption in diverse, resource-constrained settings.",
                "authors": "David Restrepo, Chenwei Wu, Constanza Vásquez-Venegas, L. Nakayama, L. A. Celi, Diego M. Lopez",
                "citations": 13
            },
            {
                "title": "Enhancing DDoS Attack Detection and Mitigation in SDN Using an Ensemble Online Machine Learning Model",
                "abstract": "Software Defined Networks (SDN) offer dynamic reconfigurability and scalability, revolutionizing traditional networking. However, countering Distributed Denial of Service (DDoS) attacks remains a formidable challenge for both traditional and SDN-based networks. The integration of Machine Learning (ML) into SDN holds promise for addressing these threats. While recent research demonstrates ML’s accuracy in distinguishing legitimate from malicious traffic, it faces difficulties in handling emerging, low-rate, and zero-day DDoS attacks due to limited feature scope for training. The ever-evolving DDoS landscape, driven by new protocols, necessitates continuous ML model retraining. In response to these challenges, we propose an ensemble online machine-learning model designed to enhance DDoS detection and mitigation. This approach utilizes online learning to adapt the model with expected attack patterns. The model is trained and evaluated using SDN simulation (Mininet and Ryu). Its dynamic feature selection capability overcomes conventional limitations, resulting in improved accuracy across diverse DDoS attack types. Experimental results demonstrate a remarkable 99.2% detection rate, outperforming comparable models on our custom dataset as well as various benchmark datasets, including CICDDoS2019, InSDN, and slow-read-DDoS. Moreover, the proposed model undergoes comparison with industry-standard commercial solutions. This work establishes a strong foundation for proactive DDoS threat identification and mitigation in SDN environments, reinforcing network security against evolving cyber risks.",
                "authors": "A. Alashhab, Mohd Soperi Mohd Zahid, Babangida Isyaku, Asma Abbas Hassan Elnour, W. Nagmeldin, Abdelzahir Abdelmaboud, Talal A. A. Abdullah, U. Maiwada",
                "citations": 12
            },
            {
                "title": "Exploring Bacillus subtilis: Ecology, biotechnological applications, and future prospects",
                "abstract": "From its early identification by Christian Gottfried Ehrenberg to its current prominence in scientific research, Bacillus subtilis (B. subtilis) has emerged as a foundational model organism in microbiology. This comprehensive review delves deep into its genetic, physiological, and biochemical intricacies, revealing a sophisticated cellular blueprint. With the incorporation of advanced techniques such as clustered regularly interspaced short palindromic repeats/CRISPR‐associated protein 9 and integrative computational methodologies, the potential applications of B. subtilis span diverse sectors. These encompass its significant contributions to biotechnology, agriculture, and medical fields and its potential for aiding environmental cleanup efforts. Yet, as we move forward, we must grapple with concerns related to safety, ethics, and the practical implementation of our lab findings in everyday scenarios. As our understanding of B. subtilis deepens, it is evident that its contributions will be central to pioneering sustainable solutions for global challenges in the years to come.",
                "authors": "A. Akinsemolu, H. Onyeaka, Samuel Odion, Idris Adebanjo",
                "citations": 11
            },
            {
                "title": "RefinedRust: A Type System for High-Assurance Verification of Rust Programs",
                "abstract": "Rust is a modern systems programming language whose ownership-based type system statically guarantees memory safety, making it particularly well-suited to the domain of safety-critical systems. In recent years, a wellspring of automated deductive verification tools have emerged for establishing functional correctness of Rust code. However, none of the previous tools produce foundational proofs (machine-checkable in a general-purpose proof assistant), and all of them are restricted to the safe fragment of Rust. This is a problem because the vast majority of Rust programs make use of unsafe code at critical points, such as in the implementation of widely-used APIs. We propose RefinedRust, a refinement type system—proven sound in the Coq proof assistant—with the goal of establishing foundational semi-automated functional correctness verification of both safe and unsafe Rust code. We have developed a prototype verification tool implementing RefinedRust. Our tool translates Rust code (with user annotations) into a model of Rust embedded in Coq, and then checks its adherence to the RefinedRust type system using separation logic automation in Coq. All proofs generated by RefinedRust are checked by the Coq proof assistant, so the automation and type system do not have to be trusted. We evaluate the effectiveness of RefinedRust by verifying a variant of Rust’s Vec implementation that involves intricate reasoning about unsafe pointer-manipulating code.",
                "authors": "Lennard Gäher, Michael Sammler, Ralf Jung, Robbert Krebbers, Derek Dreyer",
                "citations": 10
            },
            {
                "title": "Conv3D-Based Video Violence Detection Network Using Optical Flow and RGB Data",
                "abstract": "Detecting violent behavior in videos to ensure public safety and security poses a significant challenge. Precisely identifying and categorizing instances of violence in real-life closed-circuit television, which vary across specifications and locations, requires comprehensive understanding and processing of the sequential information embedded in these videos. This study aims to introduce a model that adeptly grasps the spatiotemporal context of videos within diverse settings and specifications of violent scenarios. We propose a method to accurately capture spatiotemporal features linked to violent behaviors using optical flow and RGB data. The approach leverages a Conv3D-based ResNet-3D model as the foundational network, capable of handling high-dimensional video data. The efficiency and accuracy of violence detection are enhanced by integrating an attention mechanism, which assigns greater weight to the most crucial frames within the RGB and optical-flow sequences during instances of violence. Our model was evaluated on the UBI-Fight, Hockey, Crowd, and Movie-Fights datasets; the proposed method outperformed existing state-of-the-art techniques, achieving area under the curve scores of 95.4, 98.1, 94.5, and 100.0 on the respective datasets. Moreover, this research not only has the potential to be applied in real-time surveillance systems but also promises to contribute to a broader spectrum of research in video analysis and understanding.",
                "authors": "Jae-Hyuk Park, Mohamed Mahmoud, H. Kang",
                "citations": 11
            },
            {
                "title": "Predictomes: A classifier-curated database of AlphaFold-modeled protein-protein interactions",
                "abstract": "Protein-protein interactions (PPIs) are ubiquitous in biology, yet a comprehensive structural characterization of the PPIs underlying biochemical processes is lacking. Although AlphaFold-Multimer (AF-M) has the potential to fill this knowledge gap, standard AF-M confidence metrics do not reliably separate relevant PPIs from an abundance of false positive predictions. To address this limitation, we used machine learning on well curated datasets to train a Structure Prediction and Omics informed Classifier called SPOC that shows excellent performance in separating true and false PPIs, including in proteome-wide screens. We applied SPOC to an all-by-all matrix of nearly 300 human genome maintenance proteins, generating ∼40,000 predictions that can be viewed at predictomes.org, where users can also score their own predictions with SPOC. High confidence PPIs discovered using our approach suggest novel hypotheses in genome maintenance. Our results provide a framework for interpreting large scale AF-M screens and help lay the foundation for a proteome-wide structural interactome.",
                "authors": "Ernst W. Schmid, Johannes C. Walter",
                "citations": 10
            },
            {
                "title": "Unsupervised Learning Approach for Anomaly Detection in Industrial Control Systems",
                "abstract": "Industrial control systems (ICSs) play a crucial role in managing and monitoring critical processes across various industries, such as manufacturing, energy, and water treatment. The connection of equipment from various manufacturers, complex communication methods, and the need for the continuity of operations in a limited environment make it difficult to detect system anomalies. Traditional approaches that rely on supervised machine learning require time and expertise due to the need for labeled datasets. This study suggests an alternative approach to identifying anomalous behavior within ICSs by means of unsupervised machine learning. The approach employs unsupervised machine learning to identify anomalous behavior within ICSs. This study shows that unsupervised learning algorithms can effectively detect and classify anomalous behavior without the need for pre-labeled data using a composite autoencoder model. Based on a dataset that utilizes HIL-augmented ICSs (HAIs), this study shows that the model is capable of accurately identifying important data characteristics and detecting anomalous patterns related to both value and time. Intentional error data injection experiments could potentially be used to validate the model’s robustness in real-time monitoring and industrial process performance optimization. As a result, this approach can improve system reliability and operational efficiency, which can establish a foundation for safe and sustainable ICS operations.",
                "authors": "Woo-Hyun Choi, Jongwon Kim",
                "citations": 11
            },
            {
                "title": "Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology",
                "abstract": "Featurizing microscopy images for use in biological research remains a significant challenge, especially for large-scale experiments spanning millions of images. This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training with increasingly larger model backbones and microscopy datasets. Our results show that ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5% relative improvement when recalling known biological relationships curated from public databases. Additionally, we develop a new channel-agnostic MAE architecture (CA-MAE) that allows for inputting images of different numbers and orders of channels at inference time. We demonstrate that CA-MAEs effectively generalize by inferring and evaluating on a microscopy image dataset (JUMP-CP) generated under different experimental conditions with a different channel structure than our pretraining data (RPI-93M). Our findings motivate continued research into scaling self-supervised learning on microscopy data in order to create powerful foundation models of cellular biology that have the potential to catalyze advancements in drug discovery and beyond. Relevant code and select models released with this work can be found at: https://github.com/recursionpharma/maes_microscopy.",
                "authors": "Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Vasudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, Dominique Beaini, Maciej Sypetkowski, Chi Vicky Cheng, Kristen Morse, Maureen Makes, Ben Mabey, Berton A. Earnshaw",
                "citations": 11
            },
            {
                "title": "Spatio-Temporal Few-Shot Learning via Diffusive Neural Network Generation",
                "abstract": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
                "authors": "Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li",
                "citations": 9
            },
            {
                "title": "Epithelial zonation along the mouse and human small intestine defines five discrete metabolic domains.",
                "abstract": null,
                "authors": "Rachel K. Zwick, Petr Kasparek, Brisa Palikuqi, Sara Viragova, Laura Weichselbaum, Christopher S. McGinnis, Kara L. McKinley, Asoka Rathnayake, Dedeepya Vaka, Vinh Nguyen, Coralie Trentesaux, Efren Reyes, Alexander R. Gupta, Zev J. Gartner, R. Locksley, James M Gardner, S. Itzkovitz, Dario Boffelli, O. Klein",
                "citations": 9
            },
            {
                "title": "Towards Generalist Robot Learning from Internet Video: A Survey",
                "abstract": "Scaling deep learning to massive, diverse internet data has yielded remarkably general capabilities in visual and natural language understanding and generation. However, data has remained scarce and challenging to collect in robotics, seeing robot learning struggle to obtain similarly general capabilities. Promising Learning from Videos (LfV) methods aim to address the robotics data bottleneck by augmenting traditional robot data with large-scale internet video data. This video data offers broad foundational information regarding physical behaviour and the underlying physics of the world, and thus can be highly informative for a generalist robot. In this survey, we present a thorough overview of the emerging field of LfV. We outline fundamental concepts, including the benefits and challenges of LfV. We provide a comprehensive review of current methods for extracting knowledge from large-scale internet video, addressing key challenges in LfV, and boosting downstream robot and reinforcement learning via the use of video data. The survey concludes with a critical discussion of challenges and opportunities in LfV. Here, we advocate for scalable foundation model approaches that can leverage the full range of available internet video to improve the learning of robot policies and dynamics models. We hope this survey can inform and catalyse further LfV research, driving progress towards the development of general-purpose robots.",
                "authors": "Robert McCarthy, Daniel C.H. Tan, Dominik Schmidt, Fernando Acero, Nathan Herr, Yilun Du, T. G. Thuruthel, Zhibin Li",
                "citations": 8
            },
            {
                "title": "Boosting Determinant Quantum Monte Carlo with Submatrix Updates: Unveiling the Phase Diagram of the 3D Hubbard Model",
                "abstract": "The study of strongly correlated fermionic systems, crucial for understanding condensed matter physics, has been significantly advanced by numerical computational methods. Among these, the Determinant Quantum Monte Carlo (DQMC) method stands out for its ability to provide exact numerical solutions. However, the computational complexity of DQMC, particularly in dealing with large system sizes and the notorious sign problem, limits its applicability. We introduce an innovative approach to enhance DQMC efficiency through the implementation of submatrix updates. Building upon the foundational work of conventional fast updates and delay updates, our method leverages a generalized submatrix update algorithm to address challenges in simulating strongly correlated fermionic systems with both onsite and extended interactions at both finite and zero temperatures. We demonstrate the method's superiority by comparing it with previous update methods in terms of computational complexity and efficiency. Specifically, our submatrix update method significantly reduces the computational overhead, enabling the simulation of system sizes up to 8,000 sites without pushing hard. This advancement allows for a more accurate determination of the finite temperature phase diagram of the 3D Hubbard model at half-filling. Our findings not only shed light on the phase transitions within these complex systems but also pave the way for more effective simulations of strongly correlated electrons, potentially guiding experimental efforts in cold atom simulations of the 3D Hubbard model.",
                "authors": "Fanjie Sun, Xiao Yan Xu",
                "citations": 8
            },
            {
                "title": "Advancing Time Series Classification with Multimodal Language Modeling",
                "abstract": "For the advancements of time series classification, scrutinizing previous studies, most existing methods adopt a common learning-to-classify paradigm - a time series classifier model tries to learn the relation between sequence inputs and target label encoded by one-hot distribution. Although effective, this paradigm conceals two inherent limitations: (1) encoding target categories with one-hot distribution fails to reflect the comparability and similarity between labels, and (2) it is very difficult to learn transferable model across domains, which greatly hinder the development of universal serving paradigm. In this work, we propose InstructTime, a novel attempt to reshape time series classification as a learning-to-generate paradigm. Relying on the powerful generative capacity of the pre-trained language model, the core idea is to formulate the classification of time series as a multimodal understanding task, in which both task-specific instructions and raw time series are treated as multimodal inputs while the label information is represented by texts. To accomplish this goal, three distinct designs are developed in the InstructTime. Firstly, a time series discretization module is designed to convert continuous time series into a sequence of hard tokens to solve the inconsistency issue across modal inputs. To solve the modality representation gap issue, for one thing, we introduce an alignment projected layer before feeding the transformed token of time series into language models. For another, we highlight the necessity of auto-regressive pre-training across domains, which can facilitate the transferability of the language model and boost the generalization performance. Extensive experiments are conducted over benchmark datasets, whose results uncover the superior performance of InstructTime and the potential for a universal foundation model in time series classification.",
                "authors": "Mingyue Cheng, Yiheng Chen, Qi Liu, Zhiding Liu, Yucong Luo",
                "citations": 8
            }
        ]
    }
]